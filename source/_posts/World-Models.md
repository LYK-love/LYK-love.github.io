---
title: World Models
tags:
  - RL
  - SSL
categories:
  - - Research
  - - Machine Learning
mathjax: true
date: 2023-11-18 22:55:25
---


Ref:

1. [*Recurrent World Models Facilitate Policy Evolution*](https://worldmodels.github.io/) (NIPS 2018 Oral).
2. [*Mastering Diverse Domains through World Models*](https://danijar.com/project/dreamerv3/), aka *Dreamer V3*.
3. [*A Generalist Agent*](https://arxiv.org/abs/2205.06175).
4. [*Learning General World Models in a Handful of Reward-Free Deployments*](https://arxiv.org/abs/2210.12719).
5. [*Learning General World Models in a Handful of Reward-Free Deployments*](https://arxiv.org/abs/2210.12719).

# Intro

所谓的world model, 就是一个人为构造的关于真实世界(real world, or actual environment)的模型.

传统的RL方法中, agent需要在真实世界中训练, 其训练效果受到真实世界的限制. World model的作用就是让agent可以在在world model中训练, 不需要接触真实世界.

* [*Recurrent World Models Facilitate Policy Evolution*](https://worldmodels.github.io/) presents a very simple and compact world model architecture(VAE + RNN). It indicates that training the agent entirely in the "dream"(world model) shows equal performance as traing in real world.
* [*Mastering Diverse Domains through World Models*](https://danijar.com/project/dreamerv3/), aka *Dreamer V3* extends the favorable **generility** and **scalibality** features of world model. 
  * generility: continuous and discrete actions, visual and low-dimensional inputs, 2D and 3D worlds, different data budgets, reward frequencies, and reward scales.
  * scalibality: It generates world model using **fixed hyperparameters**, making it easy to transfer between different domains.
* [*A Generalist Agent*](https://arxiv.org/abs/2205.06175) increase the generality through implementing an algorithm that genetes **multi-modal** world models. It converts multi-modal inputs to flat sequence of tokens separately(**tokenization**). Then uses a **transformer** sequence model to deal with them uniformly.
* [*Learning General World Models in a Handful of Reward-Free Deployments*](https://arxiv.org/abs/2210.12719), aka CASCADE presents a **reward-free** problem setting instead of task-specific reward functions to gather diverse, highly informative data. Thus improving generality.
  * The reason is that traditional RL methods collect only a few transitions with a single exploration policy, they likely produce a *homogenous dataset* when deployed at scale, which does not optimally improve the model.
  * inspired by Bayesian Active Learning
* [*DayDreamer: World Models for Physical Robot Learning*](https://arxiv.org/abs/2206.14176): 





使得agent可以:

1. Train in actual environment, test in world model.
2. Train in world model, test in actual environment.

第二点尤其有趣: agent只需要在world model中训练, 就可以在实际任务中取得很好的表现. 这让我们联想到一个神经科学的结论: 我们人脑对世界, 对现实形势的认知, 也是一种world model. 大脑的world model也和论文中的world model一样, 是构建出来的, 并不能等同于真实世界.

“*The image of the world around us, which we carry in our head, **is just a model**. **Nobody in his head imagines all the world**, government or country. He has only **selected concepts**, and relationships between them, and uses those to represent the real system.*”[^1]

人脑构建world model所需要的信息包含了时间, 空间信息, 包含了图像, 声音, 味觉和触觉, 这些都是通过人体感官(眼睛, 耳朵, 舌头, 皮肤)获取的. 并且由于两个原因, 这个model不能等同于真实世界:

1. 人体感官的限制. 例如人的眼睛只能识别出特定光谱频率的颜色; 再例如近视眼所看到的图像不如健康眼睛所看到的清晰.
2. 人并不需要也不能知道**整个世界(whole world)**的信息. 一个人不可能去过地球上的所有国家, 尽管他生活在地球. 他也不可能知道银河系乃至宇宙的全部信息, 尽管他生活在这个宇宙. 

但是, 尽管只有关于外界环境有限的信息, 尽管人脑中的world model永远不等于真实世界, 这个world model已经足够每个人类使用了. 

(事实上, 人类的所有科学知识也都属于world model. 以物理学为例, 物理学定律全都是对由人提出来的用于解释真实物理现象的模型, 并不能等同于物理世界的真实, 只是因为这些定律经过了实验验证, 并且目前也没有观测到反例, 我们才认为物理学这个world model比较solid.)





# World Models

Our *world model* can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment.



By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even **train our agent entirely inside of its own dream environment generated by its world model**, and transfer this policy back into the actual environment.



To handle the vast amount of information that flows through our daily lives, our brain learns an abstract representation of both spatial and temporal aspects of this information. 



One way of understanding the predictive model inside of our brains is that it might not be about just predicting the future in general, but **predicting future sensory data** given our current motor actions [11, 12]. We are able to instinctively act on this predictive model and **perform fast** reflexive behaviours when we face danger [13], <u>without the need to consciously plan out a course of action.</u>

# World Models

我们把这个思想套用到AI上. We build a world model for an agent. Its function is **predicting future sensory data** given our current motor actions.

Specifically, at each timestamp $t$, the action $a_{t+1}$ taken by an agent depends on $v_t$ and $h_t$, where

* $z_t$ is the embedding of the sensory data at time $t$.
* $h_t$ is the *hidden state* of the world model at time $t$. 



![img](https://worldmodels.github.io/assets/world_model_overview.svg)

The preceding figure is the architecture of the world model presented in the paper [Recurrent World Models Facilitate Policy Evolution](https://worldmodels.github.io/).

It has three components:

1. Vision Model $V$. At time stamp $t$, it Encodes the input visual information to embeddings $z_t$.
   * Implementaion: Variational Autoencoder (VAE).
2. Memory RNN $M$. At time stamp $t$, it outputs the probability distribution $P(z_{t+1}|a_t, z_t, h_t)$ of the next latent vector $z_{t+1}$ given the current and past information.
   * Implementaion: RNN.
   * Actually, it'll output $h_{t+1}$ rather than $z_{t+1}$ since the former suffice to represent the latter.
3. A very simple Controller $C$. At time stamp $t$, it maps $z_t$ and $h_t$ directly to action $a_t$.
   * Implementaion: linear layer nn.

**World model = $V + M$.**



[World Models]()这篇paper还做了如下的细节处理:

1. 我们知道现实世界并不是确定性的, 随时会有意外发生, 所以world model并不总是能做出正确的预测. 例如, 我开车的前2h都没有见过路障, 但某个时刻突然迎面而来一只兔子, 这是前所未有的信息. 为了让world model能够模拟出现实世界的随机性, 该paper引入了一个temperature变量来调整world model的预测的随机值. 同时world model的输出也不是确定性的未来, 而是未来的一个概率分布.
2. agent有可能利用world model的规则漏洞(毕竟它太简单了), 导致over fit. 但我没看懂该paper对该问题的处理.





DreamerV3的工作是证明了world model具有很强的generality, 原来的world model只应用来2D的自动导航和射击游戏里, 现在可以用在2D和3D中. 并且DreamerV3还指出了world model具有良好的可扩展性(scaling properties).

不过我不能理解DreamerV3的arch, 那个actor-critic是干嘛的?











# VAE (V) Model

The role of the $V$ model is to learn an abstract, compressed representation of each observed input frame.

![img](https://worldmodels.github.io/assets/vae.svg)

The $V$ model compresses each frame it receives at time step $t$ into a low dimensional *latent vector* $z_t$. This compressed representation can be used to reconstruct the original image.



# MDN-RNN (M) Model

The M model serves as a predictive model of the future $z$ vectors that $V$ is expected to produce. 

Because many complex environments are stochastic in nature, we train our RNN to <u>output a probability density function</u> $p(z)$ <u>instead of a deterministic prediction</u> of $z$.



In our approach, we approximate $p(z)$ as a mixture of Gaussian distribution, and train the **RNN** to output the probability distribution of the next latent vector $z_{t+1}$ given the current and past information made available to it.





![img](https://worldmodels.github.io/assets/mdn_rnn_new.svg)



During sampling, we can adjust a *temperature* parameter $τ$ to control model uncertainty.

# Controller (C) Model

The Controller ($C$) model is responsible for determining the course of actions to take in order to maximize the expected cumulative reward of the agent during a rollout of the environment. In our experiments, we deliberately make $C$ as simple and small as possible, and trained separately from V and M, so that most of our agent’s complexity resides in the **world model** (V and M).

$C$ is a simple single layer linear model that maps $z_t$ and $h_t$ directly to action $a_t$ at each time step:
$$
a_t = W_c[z_t \quad h_t] + b_c
$$
In this linear model, $W_c$ and $b_c$ are the weight matrix and bias vector that maps the concatenated input vector $[z_t h_t]$ to the output action vector $a_t$.

# Putting Everything Together

The following flow diagram illustrates how V, M, and C interacts with the environment:

![img](https://worldmodels.github.io/assets/world_model_schematic.svg)

Below is the pseudocode for how our agent model is used in the OpenAI Gym [28] environment. Running this function on a given `controller` C will return the cumulative reward during a rollout of the environment.

```python
def rollout(controller):
  ''' env, rnn, vae are '''
  ''' global variables  '''
  obs = env.reset()
  h = rnn.initial_state()
  done = False
  cumulative_reward = 0
  while not done:
    z = vae.encode(obs)
    a = controller.action([z, h])
    obs, reward, done = env.step(a)
    cumulative_reward += reward
    h = rnn.forward([a, z, h])
  return cumulative_reward
```

Problem: since the function of world model is to predict $z_t$ according to $z_{t-1}, h_{t-1}, a_{t-1}$, then from what I understand, for an image representation sequence $z_0, z_1, z_2, \cdots, z_t$, only the $z_0$ is encoded by image encoder from the actual environment, the folloowing $z_t, t \ge 1$ are all generated by the world model itself. So the input information of the whole system is only one image frame from the actual environment. Isn't it weird? I know there's an error in my logic. Please help me point it out.

# Training in Real Env

1. Collect 10,000 rollouts from a random policy.
2. Train VAE (V) to encode each frame into a latent vector $z$.
3. Train MDN-RNN (M) to model  $P(z_{t+1}|a_t, z_t, h_t)$ (Actually, M will putput $h_t$).
4. Evolve Controller (C) to maximize the expected cumulative reward of a rollout.

# Training Inside of the Dream

To summarize the Car Racing experiment, below are the steps taken:

1. 

# Interesting

An interesting connection to the neuroscience literature is the work on hippocampal replay that examines how the brain replays recent experiences when an animal rests or sleeps. Replaying recent experiences plays an important role in memory consolidation [68] — where hippocampus-dependent memories become independent of the hippocampus over a period of time [67]. As Foster [68] puts it, replay is “less like dreaming and more like thought”.



![img](https://worldmodels.github.io/assets/memory_consolidation.svg)



# DreamerV3

![image-20231026234358177](/Users/lyk/Library/Application Support/typora-user-images/image-20231026234358177.png)



the world model, the critic, and the actor—that are trained concurrently from replayed experience without sharing gradients,

![image-20231026234344123](/Users/lyk/Library/Application Support/typora-user-images/image-20231026234344123.png)



# Gato

![image-20231117210859998](/Users/lyk/Library/Application Support/typora-user-images/image-20231117210859998.png)

## Tokenization

- Text is encoded via SentencePiece (Kudo & Richardson, 2018) with 32000 subwords into the integer range [0, 32000).
- Images are first transformed into sequences of non-overlapping 16 × 16 patches in raster order, as done in ViT (Dosovitskiy et al., 2020). Each pixel in the image patches is then normalized between and divided by the square-root of the patch size (i.e.
- Discrete values, e.g. Atari button presses, are flattened into sequences of integers in row-major order. The tokenized result is a sequence of integers within the range of [0, 1024).
- Continuous values, e.g. proprioceptive inputs or joint torques, are first flattened into sequences of floating point values in row-major order. The values are mu-law encoded to the range [−1, 1] if not already there (see Figure 14 for details), then discretized to 1024 uniform bins. The discrete integers are then shifted to the range of [32000, 33024).

## Training

Given a sequence of tokens $s_{1: L}$ and parameters $\theta$, we model the data using the chain rule of probability:
$$
\log p_\theta\left(s_1, \ldots, s_L\right)=\sum_{l=1}^L \log p_\theta\left(s_l \mid s_1, \ldots, s_{l-1}\right),
$$

Let $b$ index a training batch of sequences $\mathcal{B}$. We define a masking function $m$ such that $m(b, l)=1$ if the token at index $l$ is either from text or from the logged action of an agent, and 0 otherwise. The training loss for a batch $\mathcal{B}$ can then be written as
$$
\mathcal{L}(\theta, \mathcal{B})=-\sum_{b=1}^{|\mathcal{B}|} \sum_{l=1}^L m(b, l) \log p_\theta\left(s_l^{(b)} \mid s_1^{(b)}, \ldots, s_{l-1}^{(b)}\right)
$$

As described above, Gato's network architecture has two main components: 

1. the parameterized embedding function which transforms tokens to token embeddings, and 
2. the sequence model which outputs a distribution over the next discrete token. While any general sequence model can work for next token prediction, we chose a **transformer** (Vaswani et al., 2017) for simplicity and scalability. 



Because **distinct tasks within a domain can share identical embodiments**, observation formats and action specifications, the model sometimes needs further context **to disambiguate tasks**. Rather than providing e.g. one-hot task identifiers, we instead take inspiration from (Sanh et al., 2022; Wei et al., 2021; Brown et al., 2020) and use **prompt conditioning**. During training, for 25% of the sequences in each batch, <u>a prompt sequence is prepended</u>, coming from an episode generated by the same source agent on the same task. Half of the prompt sequences are from the end of the episode, acting as a form of goal conditioning for many domains; and the other half are uniformly sampled from the episode. During evaluation, the agent can be prompted using a successful demonstration of the desired task, which we do by default in all control results that we present here.

## Running

![image-20231117211254165](/Users/lyk/Library/Application Support/typora-user-images/image-20231117211254165.png)

# CASCADE

## Problem Statement

Reinforcement learning (RL) considers training an agent to solve a Markov Decision Process (MDP), represented as a tuple $\mathcal{M}=\{\mathcal{S}, \mathcal{A}, P, R, \rho, \gamma\}$, where 

* $s \in \mathcal{S}$ and $a \in \mathcal{A}$ are the set of states and actions respectively, 
* $P\left(s^{\prime} \mid s, a\right)$ is a probability distribution over next states given a previous state and action, 
* $R\left(s, a, s^{\prime}\right) \rightarrow r$ is a reward function mapping a transition to a scalar reward, $\rho$ is an initial state distribution and
* $\gamma$ is a discount factor. 

A policy $\pi$ acting in the environment produces a trajectory $\tau=\left\{s_1, a_1, \ldots, s_H, a_H\right\}$ for an episode with horizon $H$. 

Since actions in the trajectory are sampled from a policy, we can then **define the RL problem as finding a policy $\pi$ that maximizes expected returns in the environment, i.e.**
$$
\pi^{\star}=\arg \max _\pi \mathbb{E}_{\tau \sim \pi}[R(\tau)] .
$$


We seek to learn policies that can <u>transfer to any MDP within a family of MDPs</u>. This can be formalized as a Contextual MDP [51], where observations, dynamics and rewards can vary given a context. In this paper we consider settings where only the reward varies, thus, if the test-time context is unknown at training time we must collect data that sufficiently covers the space of possible reward functions. 

Finally, to facilitate scalability, we operate in the **deployment efficient** paradigm [67], whereby policy learning and exploration are completely separate, and during a given deployment, we gather a large quantity of data without further policy retraining (c.f. online approaches like DER [112], which take multiple gradient steps per exploration timestep in the real environment). Taken together, we consider the reward-free deployment efficiency problem. 

This differs from previous work as follows: 1) unlike previous deployment efficiency work, our exploration is task agnostic; 2) unlike previous reward-free RL work, we cannot update our exploration policy $\pi_{\mathrm{EXP}}$ during deployment. Thus, the focus of our work is on how to train $\pi_{\mathrm{EXP}}$ offline such that it gathers heterogeneous and informative data which facilitate zero-shot transfer to unknown tasks.

In this paper we make use of model-based RL (MBRL), where the goal is to learn a model of the environment (or **world model**) and then use it to subsequently train policies to solve downstream tasks. **To do this, the world model needs to approximate both $P$ and $R$.** 

Typically, the model will be a neural network, parameterized by $\psi$, hence we denote the approximate dynamics and reward functions as $P_\psi$ and $R_\psi$, which produces a new "imaginary" $\operatorname{MDP}, \mathcal{M}_\psi=\left(\mathcal{S}, \mathcal{A}, P_\psi, R_\psi, \rho\right)$. 

We focus on Dyna-style MBRL [104], whereby we train a policy $\left(\pi_\theta\right.$ parameterized by $\theta$ ) with model-free RL solely using "imagined" transitions inside $\mathcal{M}_\psi$. Furthermore, we can train the policy on a single GPU with parallelized rollouts since the simulator is a neural network [54]. The general form of all methods in this paper is shown in Algorithm 1, with the key difference being step 5: We aim to update $\pi_{\mathrm{EXP}}$ in the new imaginary MDP $\mathcal{M}_\psi$ such that it continues to collect a large, diverse quantity of reward-free data. Note that $\pi_{\mathrm{EXP}}$ need not be a single policy, but could also refer to a collection of policies that we can deploy (either in parallel or in series), such that $\pi \in \pi_{\mathrm{EXP}}$.

## Experiments

![image-20231117212750128](/Users/lyk/Library/Application Support/typora-user-images/image-20231117212750128.png)



# Problems

Reinforcement learning has enabled computers to solve individual tasks through interaction, such as surpassing humans in the games of Go and Dota.



# Conclusions

* DreamerV3: 

  * World models carry the potential for substantial transfer between tasks. Therefore, we see training larger models to solve multiple tasks across overlapping domains as a promising direction for future investigations.

    

    

  * behave well **across a wide range of domains** with fixed hyperparameters. These domains include continuous and discrete actions, visual and low-dimensional inputs, 2D(Atari games ) and 3D(DMLab and Minecraft) worlds.

  * with fixed hyperparameters, outperforming specialized algorithms.

  * We observe favorable scaling properties of DreamerV3,

[^1]: **Counterintuitive behavior of social systems**  [[link\]](https://en.wikipedia.org/wiki/Mental_model)
