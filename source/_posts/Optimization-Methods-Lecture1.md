---
title: Optimization Methods- Lecture1
tags: Optimization Methods
categories: Mathematics
mathjax: true
date: 2021-09-07 21:47:40
---

ref: [Optimization Methods](http://www.lamda.nju.edu.cn/chengq/course/opt2021/opt2021.html)

Outline:

* **Mathematical Optimization**
* **Least-squares**
* **Linear Programming**
* **Convex Optimization**
* **Nonlinear Optimization**
* **Summary**

<!--more-->

# Mathematical Optimization

* **Optimization Problem**
  $$
  min \quad \quad f_0(x) \\
  s.t. \quad f_i(x) \leq b_i, \quad i=1,2,\dots,n
  $$

  * Optimization Variable: $x=(x_1,\dots,x_n)$
  * Objective Function: $f_0:R^n \rarr R$
  *  Constraint Functions: $f_i:R^n \rarr R$â€‹

* $x^*$â€‹ is called **optimal** or a solution

  * $f_i(x^*)\leq b_i$ , $i=1,\dots,m $
  * For any $z$ with $f_i(z)\leq b_i $, we have $f_0(z)\geq f_0(x^*)$

----

* **Linear Program**  çº¿æ€§å‡½æ•°çš„å®šä¹‰
  $$
  f_i(\alpha x + \beta y) = \alpha f_i(x) + \beta f_i(y)
  $$

  * For all $x,y \in R^n$ and all $\alpha, \beta \in R$â€‹
  * $i$â€‹ å¯ä»¥ä¸º$0$â€‹ï¼ˆç›®æ ‡å‡½æ•°ï¼‰ï¼Œ ä¹Ÿå¯ä»¥ä¸º$1,2,\dots,n$â€‹â€‹â€‹â€‹( çº¦æŸå‡½æ•° )ã€‚ å½“ä¼˜åŒ–é—®é¢˜çš„ç›®æ ‡å‡½æ•°å’Œçº¦æŸå‡½æ•°éƒ½æ˜¯çº¿æ€§å‡½æ•°æ—¶ï¼Œ æ•´ä¸ªé—®é¢˜ç§°ä¸º**çº¿æ€§è§„åˆ’ï¼ˆçº¿æ€§é—®é¢˜ï¼‰**

* **Nonlinear Program**

  * If the Optimization Problem is not linear

* **Convex Optimization Problem** 
  $$
  f_i(\alpha x + \beta y) \leq \alpha f_i(x) + \beta f_i(y)
  $$

  * For all $x,y \in R^n$â€‹ and all $\alpha, \beta \in R$â€‹ with $\alpha + \beta=1, \alpha \geq 0, \beta \geq 0$

  * çº¿æ€§è§„åˆ’æ˜¯å‡¸ä¼˜åŒ–çš„ä¸€ä¸ªç‰¹ä¾‹
    * åªè¦æ±‚å¯¹$\alpha + \beta=1, \alpha \geq 0, \beta \geq 0$çš„$\alpha, \beta$æˆç«‹ï¼Œ å®é™…ä¸ŠæŠŠæ¡ä»¶æ”¾æ¾äº†â€‹

## Applications

$$
min \quad \quad f_0(x) \\
s.t. \quad f_i(x) \leq b_i, \quad i=1,2,\dots,n
$$



* **Abstraction**
  * $x$â€‹ represents the choice made 
  * $f_i(x) \leq b_i$ represent firm requirements that limit the possible choices
  * $f_0(x)$â€‹ represents the cost of choosing
  * A solution corresponds to a choice that has minimum cost, among all choices that meet the requirements

### Portfolio Optimization

* **Variables**
  * $x_i$ represents the investment in the $ğ‘–_th$ asset
  * $x \in R$â€‹ describes the overall portfolio allocation across the set of asset

* **Constraints**
  * A limit on the budget the requirement
  * Investments are nonnegative
  * A minimum acceptable value of expected return for the whole portfolio
* Objective 
  * Minimize the variance of the portfolio return æŠ•èµ„çš„å›æŠ¥æœ€ç¨³å®šï¼ˆ ä¹Ÿå¯ä»¥æŠŠç›®æ ‡æ¢æˆæœŸæœ›çš„å›æŠ¥æœ€é«˜ ï¼‰

### Device Sizing

* **Variables** 
  * $x \in R^n$â€‹  describes the widths and lengths of the devices
* **Constraints**  
  * Limits on the device 
  * Timing 
  * A limit on the total area of the circuit
* Objective 
  * Minimize the total power consumed by the circuit

### Data Fitting

* **Variables** 
  * $x \in R^n$ describes parameters in the model
* **Constraints** 
  *  Prior information 
  * Required limits on the parameters (such as nonnegativity)
* Objective 
  * Minimize the prediction error between the observed data and the values predicted by the model

## Solving Optimization Problems

* **General Optimization Problem** 
  * Very difficult to solve 
  * Constraints can be very complicated, the number of variables can be very large çº¦æŸå¤æ‚ï¼Œ å˜é‡å¤š
  * Methods involve some compromise, e.g., computation time, or suboptimal solution
* **Exceptions** 
  * Least-squares problems 
  *  Linear programming problems 
  *  Convex optimization problems

# **Least-squares**

* **The Problem**
  $$
  min \quad ||Ax-b||_2^2 = \sum_{i=1}^k { (a_i^T x - b_i)^2 }
  $$

  * $A \in R^{k \times n}$â€‹ ,  $a_i^T$â€‹ is the $i_{th}$â€‹ row of $A,b \in R^k$â€‹
  * $A \in R^n$â€‹ is the optimization variableâ€‹

* **Setting the gradient to be 0**
  $$
  2A^T(Ax-b)=0 \\
  \rarr \quad A^TAx = A^Tb \\
  \rarr \quad x = (A^TA)^{-1}A^Tb
  $$

---

* **A Set of Linear Equations**
  $$
  A^TAx = A^Tb
  $$

* **Solving least-squares problems** 
  * Reliable and efficient algorithms and software
  * Computation time proportional to $n^2k( A \in R^{k \times n})$ ; less if structured
  *  A mature technology 
  * Challenging for **extremely large** problems

## Using Least-squares

* **Easy to Recognize** 

* **Weighted least-squares**
  $$
  \sum\limits_{i=1}^k w_i( a_i^Tx-b_i )^2
  $$
  æŠŠ$w_i$ç§»åˆ°æ‹¬å·é‡Œå°±æ˜¯æ ‡å‡†æœ€å°äºŒä¹˜

  * Different importance

* **Different importance**
  $$
  \sum\limits_{i=1}^k ( a_i^Tx-b_i )^2 + \rho \sum\limits_{i=1}^n x_i^2
  $$
  ä¹Ÿå¯åŒ–ä¸ºæ ‡å‡†æœ€å°äºŒä¹˜

  * More stableï¼Œé¿å…è¿‡æ‹Ÿåˆ

# **Linear Programming**

* **The Problem**
  $$
  min \quad c^Tx \\
  s.t. \quad a_i^Tx \leq b_i, \quad i = 1, \dots, m
  $$

  * $c,a_1,\dots,a_m \in R^n, b_1, \dots, b_m \in R$

* **Solving Linear Programs**
  * No analytical formula for solution 
  * Reliable and efficient algorithms and software 
  * Computation time proportional to $ğ‘›^2ğ‘š$ if $ m \geq ğ‘›$; less with structure
  * A mature technology 
  * Challenging for **extremely large** problems

## Using Linear Programming

* **Not as easy to recognize** 
*  **Chebyshev Approximation Problem**

$$
min \quad \max\limits_{i=1,\dots,k} |a_i^Tx-b_i| \\
\iff min \quad t \quad \quad \quad s.t. \quad t = \max\limits_{i=1,\dots,k} |a_i^Tx-b_i| \\
  \quad\quad  \quad \iff min \quad t \quad \quad \quad s.t. \quad t \geq  |a_i^Tx-b_i|, {i=1,\dots,k} ( å› ä¸ºæœ€å°åŒ–ï¼Œæ‰€ä»¥å¯ä»¥ç­‰ä»· )\\
 \quad \quad \quad\iff min \quad t \quad \quad \quad s.t. \quad t \geq  |a_i^Tx-b_i|, {i=1,\dots,k} \\
 \quad \quad \quad \quad \quad \iff min \quad t \quad \quad \quad s.t. \quad -t \leq  a_i^Tx-b_i \leq t, {i=1,\dots,k} \\
$$

# Convex Optimization

* **Local minimizers are also global minimizers**

* The Problem
  $$
  f_i(\alpha x + \beta y) \leq \alpha f_i(x) + \beta f_i(y)
  $$

  * For all $x,y \in R^n$ and all $\alpha, \beta \in R$ with $\alpha + \beta=1, \alpha \geq 0, \beta \geq 0$

  * Least-squares and linear programs as special cases

---

## Solving Convex Optimization Problems

* No analytical solution 
* Reliable and efficient algorithms (e.g., interior-point methods)
* Computation time (roughly) proportional to $\max\{n^3,n^2m,F\}$â€‹
  * $F$ is cost of evaluating $f_i$â€‹'s and their first and second derivatives
* **Almost** a technology

---

* **Often difficult to recognize**
* **Many tricks for transforming problems into convex form**
* **Surprisingly many problems can be solved via convex optimization**

# **Nonlinear Optimization**

* An optimization problem when the objective or constraint functions are not linear, but not known to be convex
* Sadly, there are no effective methods for solving the general nonlinear programming problem 
  * Could be NP-hard
* We need **compromise**

## Local Optimization Methods
* Find a point that minimizes $f_0$ among feasible points near it 
  * The compromise is to give up seeking the optimal $x$
* Fast, can handle large problems 
  * Differentiability
* Require initial guess 
* Provide no information about distance to (global) optimum
* Local optimization methods are more art **than technology**
  * å…¨å‡­è¿æ°”

## Global Optimization

* **Find the global solution** 
  * The compromise is efficiency
* **Worst-case complexity grows exponentially with problem size**
*  **Applications** 
  *  A small number of variables, where computing time is not critical
  *  The value of finding the true global solution is very high

---

* **Worst-case Analysis of a high value or safety-critical system** 
  * Variables represent uncertain parameters 
  * Objective function is a utility function 
  * Constraints represent prior knowledge 
  * If the worst-case value is acceptable, we can certify the system as safe or reliable èƒ½è¯æ˜ç³»ç»Ÿå¯é 
* **Local optimization methods can be tried** 
  * If finding values that yield unacceptable performance, then the system is not reliable å±€éƒ¨ä¼˜åŒ–å·²ç»å‘ç°äº†ä¸å¯æ¥å—çš„å€¼ï¼Œé‚£ç³»ç»Ÿå°±ä¸å¯é 
  * But it cannot certify the system as reliable åªèƒ½è¯æ˜ç³»ç»Ÿä¸å¯é ï¼Œä¸èƒ½è¯æ˜å¯é 

## Role of Convex Optimization in Nonconvex Problems

* **Initialization for local optimization** 
  * An approximate, but convex, formulation
* **Convex heuristics for nonconvex optimization** 
  * Sparse solutions (compressive sensing)
* **Bounds for global optimization** é€šè¿‡å‡¸ä¼˜åŒ–æ¥æ‰¾å…¨å±€ä¼˜åŒ–é—®é¢˜çš„ä¸‹ç•Œ
  * Relaxation æŠŠæ¡ä»¶æ¾å¼›
  * Lagrangian relaxation
