---
title: Shannon's Noisy Coding Theorem
tags:
  - Information Theory
categories: Mathematics
mathjax: true
---

# Shannon's Noisy Coding Theorem

$$
C = C^{(I)}
$$

As a result, we have theorem: For any DMC, if $R<C$, then $R$ is achievable. Conversely, if $R>C$, it is not achievable.

Proof: 

# Direct

We start proving that, if $R<C$, then $R$ is achievable. In order to do so, it is enough to construct a particular sequence of codes with rate $R$ such that $\lambda^{(n)} \rightarrow 0$. 

Our strategy to complete this goal will be to consider codes which are randomly generated and evaluate the arithmetic error probabilities of decoding these codes using jointy typicality decoding, i.e. decode the channel output as the codeword which is jointly typical with the channel output. 

Through the arithmetic error probabilities of these randomly generated codewords, we will obtain proof of the existence of a code with maximal probability of error going to zero as $n \rightarrow \infty$.

Our proof of achievability will proceed in two steps. 

We will first show the existence of a sequence of codes that can transmit $M=2^{n R+1}$ messages (rate $R+\frac{1}{n}$ ) where the average error $p_e^{(n)}=\frac{\left.\sum_{i=1}^M \lambda_i^{(} n\right)}{M}$ goes to zero. 

We will then construct a sequence of codes that transmit $\frac{M}{2}=2^{n R}$ messages (rate $R$ ) where the maximum error $\lambda^n$ goes to zero.

Consider the random code generated by the following algorithm:

1. Fix $p(x)$ such that $I(X, Y)=C$ and generate $M=2^{n R+1}$ codewords independently $^2$ according to $p\left(x^n\right)=\prod_{i=1}^n p\left(x_i\right)$. Call the collection of codewords, $\mathcal{C}$, the codebook.
2. Assign each message $W$ in $\{1, \ldots, M\}$ at random to a codeword $X^n(W)$ in $\mathcal{C}$.
3. Assume the codebook $\mathcal{C}$ and $p(y \mid x)$ are known beforehand to the decoder.
4. $\hat{W}=c$ for $c \in\{1, \ldots, M\}$ if $c$ is the only message such that $\left(X^n(c), Y^n\right)$ are jointly typical. Otherwise, define $\hat{W}=0$.

From step 2 in the algorithm, observe that the conditional probability of error $\lambda_i^{(n)}$ is the same for all messages $i \in\{1, \ldots, M\}$. Hence, $p_e^{(n)}=\lambda_1^{(n)}$. Here both conditional and arithmetic probability of error are expectations over the random draw of the codewords.
$$
\begin{gathered}
\lambda_1^{(n)}=P(\hat{W} \neq 1 \mid W=1)=P\left(\left(X^n(1), Y^n(1)\right) \notin A_\epsilon^{(n)} \cup\left(\exists i \neq 1:\left(X^n(i), Y^n(1)\right) \in A_\epsilon^{(n)}\right)\right) \\
\leq P\left(\left(X^n(1), Y^n(1)\right) \notin A_\epsilon^{(n)}\right)+P\left(\left(\exists i \neq 1:\left(X^n(i), Y^n(1)\right) \in A_\epsilon^{(n)}\right)\right)
\end{gathered}
$$

From Lemma 16.5 , property 1 observe that $P\left(\left(X^n(1), Y^n(1)\right) \notin A_\epsilon^{(n)}\right) \stackrel{n \rightarrow \infty}{\longrightarrow} 0$. Next, by the union bound and symmetry[^3] of the code,
$$
P\left(\exists i \neq 1:\left(X^n(i), Y^n(1)\right) \in A_\epsilon^{(n)}\right) \leq \sum_{i=2}^{2^{n R+1}} P\left(\left(X^n(i), Y^n(1)\right) \in A_\epsilon^{(n)}\right) \leq 2^{n R+1} P\left(\left(X^n(2), Y^n(1)\right) \in A_\epsilon^{(n)}\right)
$$

Also recall that, by step 1 in the algorithm, $X^n(1)$ is independent of $X^n(2)$. Thus, since $Y^n(1)$ is a function of $X^n(1)$ and randomness introduced by the channel, it is independent of $X_n(2)$. Applying Lemma 16.5 , property 4 and also step 1 of the algorithm,
$$
2^{n R+1} P\left(\left(X^n(2), Y^n(1)\right) \in A_\epsilon^{(n)}\right) \leq 2^{n R+1} 2^{-n(I(X, Y)-3 \epsilon)}=2^{-n\left(C-R-3 \epsilon-\frac{1}{n}\right)}
$$

Putting the last 3 paragraphs together, conclude that:
$$
\lambda_1^{(n)} \leq \epsilon+2^{-n\left(C-R-3 \epsilon-\frac{1}{n}\right)}
$$
and if $R<C$, for every $\delta>0$ there exists $n^*$ such that, if $n>n^*, \lambda_1^{(n)}<\delta$. Hence, for $n>n^*(\delta)$ :
$$
\delta>\lambda_1^{(n)}=p_e^{(n)}=\sum_{\mathcal{C}} P(\mathcal{C}) p_e^{(n)}(\mathcal{C}) \geq \min _{\mathcal{C}}\left\{p_e^{(n)}(\mathcal{C})\right\}
$$

//WHY??

Conclude[^4] that there exists a sequence of codes with rates $R+\frac{1}{n}$ such that $p_e^{(n)} \rightarrow 0$.
To control the maximal probability of error, consider a new sequence of codes constructed in the following manner: for each code in the previous sequence, remove the $\frac{M}{2}$ messages with the worst probabilities of error.



This trick is called expurgating a code. This new sequence is such that $\lambda^{(n)}$ is at most 2 times the old $p_e^{(n)}$. To verify this, assume the contrary. Then the worst codeword in the new codebook has error greater than $2 p_e^{(n)}$ calculated in the old codebook. Observe that the words which were removed from the old codebook all have errors worse than any word in the new codebook and, thus, their errors are larger than $2 p_e^{(n)}$. Observe that there are $\frac{M}{2}$ such words. Hence, the arithmetic average of the errors for the $M$ words has to be larger than $p_e^{(n)}$. A contradiction, since the arithmetic average of these errors is precisely $p_e^{(n)}$. Hence, for this new sequence, $\lambda^{(n)} \rightarrow 0$. Moreover, the rate of the new code is $(\log (M / 2)) / n=\left(\log 2^{n R}\right) / n=R$. Thus, the proof of achievability is complete.

到这一步, 证明了$C^{(I)} \le C$, 因为$C^{(I)}$以下的全部都能实现.

# Converse

Next, take $R>C$. We will prove that this rate is not achievable, i.e. the arithmetic (and hence the maximal) probability of error is bounded away from 0 . 

For an arbitrary sequence of codes with rate $R$, observe that $p_e^{(n)}=P(W \neq \hat{W})$ when we take a uniform distribution on $W$. 

Recall that, from Fano's Inequality: //WHY?
$$
\begin{gathered}
H\left(W \mid Y^n\right) \leq \log \left(2^{n R}\right) P(W \neq \hat{W})+1 \\
P(W \neq \hat{W}) \geq \frac{H\left(W \mid Y^n\right)-1}{n R}
\end{gathered}
$$

Applying Fano's Inequality for the <u>uniform</u> distribution on $W$ :
$$
p_e^{(n)} \geq \frac{H(W)-I\left(W ; Y^n\right)-1}{n R}=\frac{n R-I\left(W ; Y^n\right)-1}{n R}
$$

$H(W) = nR$: 看起来像是熵的定义? 不确定.



Next, recall that, by the Data Processing Inequality, $I\left(W, Y^n\right) \leq I\left(X^n ; Y^n\right)$. 

Also observe that $I\left(X^n ; Y^n\right)=$ $H\left(Y^n\right)-H\left(Y^n \mid X^n\right) \leq \sum_{i=1}^n H\left(Y_i\right)-\sum_{i=1}^n H\left(Y_i \mid X^n, Y_{i-1}, \ldots, Y_i\right)$. 

Since the channel is a DMC, observe that $H\left(Y_i \mid X^n, Y_{i-1}, \ldots, Y_i\right)=H\left(Y_i \mid X_i\right)$. 

Hence, using all these inequalities, conclude that
$$
I\left(W, Y^n\right) \leq \sum_{i=1}^n\left[H\left(Y_i\right)-H\left(Y_i \mid X_i\right)\right]=\sum_{i=1}^n I\left(X_i ; Y_i\right) \leq n C .
$$


由于之前证明了$C^{(I)} \le C$, 所以现在第二个不等号成立( $I(X_i | Y_i) \le C$ ).



Hence:
$$
p_e^{(n)} \geq \frac{n R-I\left(W ; Y^n\right)-1}{n R} \geq \frac{n R-n C-1}{n R} \stackrel{n \rightarrow \infty}{\longrightarrow} \frac{R-C}{R}
$$

Hence, if $R>C, p_e^{(n)}$ does not converge to 0 . 

Since $\lambda^{(n)} \geq p_e^{(n)}$, it also doesn't converge to 0 . Since the sequence of codes was arbitrary, the rate $R$ is not achievable, which completes the proof.

# Conclusion

Channel coding theorem promises the existence of block codes that allow us to transmit information at rates below capacity with an arbitrary small probability of error if block length is large enough. 

However, the search for such optimal yet practical (easily implementable) codes is ongoing. Random codes used by Shannon are not practical since they require exponential look-up tables for encoding and decoding.



Examples of practical codes include block codes such as Hamming code, Reed-Muller codes, Reed-Solomon codes, Goppa codes, BCH (Bose-Chaudhary-Hocquenhem) codes, which encode messages into blocks. More recently, convolution codes have been invented that get pretty close to Shannon's capacity limit. The convolution codes don't encode in blocks, instead they read and transmit bits continuously, where transmitted bits are a linear combination of previous source bits. These codes include Turbo codes, LDPC (Low Density Parity Check) codes, Digital Fountain, codes etc.



[^4]: For $n^*\left(\frac{1}{2}^m\right)=n_m$, choose the code induced by the codebooks $\arg \min _{\mathcal{C}}\left\{p_e^{\left(n_m\right)}(\mathcal{C})\right\}$. For $n_m<n<n_{m+1}$ keep the same code as the one selected for $n_m$. This sequence of codes is such that the rates of the codes are $R+\frac{1}{n}$ and $p_e^{(n)} \rightarrow 0$.

