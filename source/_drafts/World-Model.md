---
title: World Models
tags:
---

Ref:

1. [World Models](https://worldmodels.github.io/). (NIPS 2018 Oral)
2. [DreaverV3](https://danijar.com/project/dreamerv3/)



所谓的world model, 就是一个人为构造的关于实际世界(real world, or actual environment)的模型, 使得agent可以:

1. Train in actual environment, test in world model.
2. Train in world model, test in actual environment.

第二点尤其有趣: agent只需要在world model中训练, 就可以在实际任务中取得很好的表现. 这让我们联想到一个神经科学的结论: 我们人脑对世界, 对现实形势的认知, 也是一种world model. 大脑的world model也和论文中的world model一样, 是构建出来的, 并不能等同于真实世界.

“*The image of the world around us, which we carry in our head, **is just a model**. **Nobody in his head imagines all the world**, government or country. He has only **selected concepts**, and relationships between them, and uses those to represent the real system.*”

人脑构建world model所需要的信息包含了时间, 空间信息, 包含了图像, 声音, 味觉和触觉, 这些都是通过人体感官(眼睛, 耳朵, 舌头, 皮肤)获取的. 并且由于两个原因, 这个model不能等同于真实世界:

1. 人体感官的限制. 例如人的眼睛只能识别出特定光谱频率的颜色; 再例如近视眼所看到的图像不如健康眼睛所看到的清晰.
2. 人并不需要也不能知道**整个世界(whole world)**的信息. 一个人不可能去过地球上的所有国家, 尽管他生活在地球. 他也不可能知道银河系乃至宇宙的全部信息, 尽管他生活在这个宇宙. 

但是, 尽管只有关于外界环境有限的信息, 尽管人脑中的world model永远不等于真实世界, 这个world model已经足够每个人类使用了. 

(事实上, 人类的所有科学知识也都属于world model. 以物理学为例, 物理学定律全都是对由人提出来的用于解释真实物理现象的模型, 并不能等同于物理世界的真实, 只是因为这些定律经过了实验验证, 并且目前也没有观测到反例, 我们才认为物理学这个world model比较solid.)

我们把这个思想套用到AI上. 对于agent, 给它构建一个world model. 在每个时刻$t$, agent的每个action $a_t$都取决于:

1. agent在当前时刻感知到的信息$z_t$.
2. world model提供的历史信息$h_t$(这是啥??).


$$
a_t = f(z_t h_t)
$$


[World Models](https://worldmodels.github.io/)这篇paper还做了如下的细节处理:

1. 我们知道现实世界并不是确定性的, 随时会有意外发生, 所以world model并不总是能做出正确的预测. 例如, 我开车的前2h都没有见过路障, 但某个时刻突然迎面而来一只兔子, 这是前所未有的信息. 为了让world model能够模拟出现实世界的随机性, 该paper引入了一个temperature变量来调整world model的预测的随机值. 同时world model的输出也不是确定性的未来, 而是未来的一个概率分布.
2. agent有可能利用world model的规则漏洞(毕竟它太简单了), 导致over fit. 但我没看懂该paper对该问题的处理.
3. 



DreamerV3的工作是证明了world model具有很强的generality, 原来的world model只应用来2D的自动导航和射击游戏里, 现在可以用在2D和3D中. 并且DreamerV3还指出了world model具有良好的可扩展性(scaling properties).

不过我不能理解DreamerV3的arch, 那个actor-critic是干嘛的?



# World Models

Our *world model* can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment.



By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even **train our agent entirely inside of its own dream environment generated by its world model**, and transfer this policy back into the actual environment.





To handle the vast amount of information that flows through our daily lives, our brain learns an abstract representation of both spatial and temporal aspects of this information. 



One way of understanding the predictive model inside of our brains is that it might not be about just predicting the future in general, but **predicting future sensory data** given our current motor actions [11, 12]. We are able to instinctively act on this predictive model and **perform fast** reflexive behaviours when we face danger [13], <u>without the need to consciously plan out a course of action.</u>

# Agent Model

We present a simple model inspired by our own cognitive system. In this model, our agent has a visual sensory component that compresses what it sees into a small representative code. It also has a memory component that makes predictions about future codes based on historical information. Finally, our agent has a decision-making component that decides what actions to take based only on the representations created by its vision and memory components.

![img](https://worldmodels.github.io/assets/world_model_overview.svg)

# MDN-RNN Model

The M model serves as a predictive model of the future $z$ vectors that V is expected to produce. Because many complex environments are stochastic in nature, we train our RNN to <u>output a probability density function</u> $p(z)$ <u>instead of a deterministic prediction</u> of $z$.



In our approach, we approximate $p(z)$ as a mixture of Gaussian distribution, and train the RNN to output the probability distribution of the next latent vector $z_{t+1}$ given the current and past information made available to it.





![img](https://worldmodels.github.io/assets/mdn_rnn_new.svg)



More specifically, the RNN will model $P(z_{t+1}|a_t, z_t, h_t)$, where $a_t$ is the action taken at time $t$ and $h_t$ is the *hidden state* of the RNN at time $t$. During sampling, we can adjust a *temperature* parameter $τ$ to control model uncertainty, as done in [35] — we will find adjusting $τ$ to be useful for training our controller later on.

# Controller (C) Model

The Controller (C) model is responsible for determining the course of actions to take in order to maximize the expected cumulative reward of the agent during a rollout of the environment. In our experiments, we deliberately make C as simple and small as possible, and trained separately from V and M, so that most of our agent’s complexity resides in the world model (V and M).

C is a simple single layer linear model that maps ��*z**t* and ℎ�*h**t* directly to action ��*a**t* at each time step:
$$
a_t = W_c[z_t h_t] + b_c
$$
In this linear model, $W_c$ and $b_c$ are the weight matrix and bias vector that maps the concatenated input vector $[z_t h_t]$ to the output action vector $a_t$.

# Putting Everything Together

The following flow diagram illustrates how V, M, and C interacts with the environment:

![img](https://worldmodels.github.io/assets/world_model_schematic.svg)

Below is the pseudocode for how our agent model is used in the OpenAI Gym [28] environment. Running this function on a given `controller` C will return the cumulative reward during a rollout of the environment.

```python
def rollout(controller):
  ''' env, rnn, vae are '''
  ''' global variables  '''
  obs = env.reset()
  h = rnn.initial_state()
  done = False
  cumulative_reward = 0
  while not done:
    z = vae.encode(obs)
    a = controller.action([z, h])
    obs, reward, done = env.step(a)
    cumulative_reward += reward
    h = rnn.forward([a, z, h])
  return cumulative_reward
```

Problem: since the function of world model is to predict $z_t$ according to $z_{t-1}, h_{t-1}, a_{t-1}$, then from what I understand, for an image representation sequence $z_0, z_1, z_2, \cdots, z_t$, only the $z_0$ is encoded by image encoder from the actual environment, the folloowing $z_t, t \ge 1$ are all generated by the world model itself. So the input information of the whole system is only one image frame from the actual environment. Isn't it weird? I know there's an error in my logic. Please help me point it out.



# Procedure

To summarize the Car Racing experiment, below are the steps taken:

1. Collect 10,000 rollouts from a random policy.
2. Train VAE (V) to encode each frame into a latent vector $z$.
3. Train MDN-RNN (M) to model  $P(z_{t+1}|a_t, z_t, h_t)$
4. Evolve Controller (C) to maximize the expected cumulative reward of a rollout.

# Interesting

An interesting connection to the neuroscience literature is the work on hippocampal replay that examines how the brain replays recent experiences when an animal rests or sleeps. Replaying recent experiences plays an important role in memory consolidation [68] — where hippocampus-dependent memories become independent of the hippocampus over a period of time [67]. As Foster [68] puts it, replay is “less like dreaming and more like thought”.



![img](https://worldmodels.github.io/assets/memory_consolidation.svg)



# DreamerV3

the world model, the critic, and the actor—that are trained concurrently from replayed experience without sharing gradients,

![image-20231026234344123](/Users/lyk/Library/Application Support/typora-user-images/image-20231026234344123.png)



# Problems

Reinforcement learning has enabled computers to solve individual tasks through interaction, such as surpassing humans in the games of Go and Dota.



# Conclusions

* DreamerV3: 

  * World models carry the potential for substantial transfer between tasks. Therefore, we see training larger models to solve multiple tasks across overlapping domains as a promising direction for future investigations.

    ![image-20231026234358177](/Users/lyk/Library/Application Support/typora-user-images/image-20231026234358177.png)

    

  * behave well **across a wide range of domains** with fixed hyperparameters. These domains include continuous and discrete actions, visual and low-dimensional inputs, 2D(Atari games ) and 3D(DMLab and Minecraft) worlds.

  * with fixed hyperparameters, outperforming specialized algorithms.

  * We observe favorable scaling properties of DreamerV3,
