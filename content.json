{"meta":{"title":"LYK-love","subtitle":"","description":"","author":"有多远滚多远","url":"http://lyk-love.cn","root":"/"},"pages":[{"title":"About","date":"2021-05-10T13:09:49.000Z","updated":"2022-09-26T06:39:34.945Z","comments":true,"path":"about/index.html","permalink":"http://lyk-love.cn/about/index.html","excerpt":"","text":"Any suggestion is welcomed: 191820133@smail.nju.edu.cn"},{"title":"c","date":"2021-05-09T14:48:09.000Z","updated":"2022-09-26T06:39:34.945Z","comments":true,"path":"c/index.html","permalink":"http://lyk-love.cn/c/index.html","excerpt":"","text":""},{"title":"Categories","date":"2021-05-10T13:08:19.000Z","updated":"2022-09-26T06:39:34.945Z","comments":true,"path":"categories/index.html","permalink":"http://lyk-love.cn/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2021-05-10T13:09:24.000Z","updated":"2022-09-26T06:39:34.945Z","comments":true,"path":"tags/index.html","permalink":"http://lyk-love.cn/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Computer Memory Error Correction","slug":"Computer-Memory-Error-Correction","date":"2022-10-10T20:41:28.000Z","updated":"2022-10-10T13:21:22.632Z","comments":true,"path":"2022/10/11/Computer-Memory-Error-Correction/","link":"","permalink":"http://lyk-love.cn/2022/10/11/Computer-Memory-Error-Correction/","excerpt":"Outline: Intro Error Correction Odd-Even Check Hamming Code Cyclic Redundancy Check","text":"Outline: Intro Error Correction Odd-Even Check Hamming Code Cyclic Redundancy Check Intro A semiconductor memory system is subject to errors. These can be categorized as hard failures and soft errors. A hard failure is a permanent physical defect. 受影响的存储单元不能可靠地存储数据. can be caused by harsh environmental abuse, manufacturing defects, and wear. A soft error is a random, nondestructive event. 它改变了某个或某些存储单元的内容, 但没有损坏机器 can be caused by power supply problems or alpha particles. These particles result from radioactive decay and are distressingly common because radioactive nuclei are found in small quantities in nearly all materials. Both hard and soft errors are clearly undesirable, and most modern main memory systems include logic for both detecting and correcting errors. Error Correction Error Correction的基本思想就是使用额外的checkbit. Steps: 记输入的数据为 $D$ , 有 $M$ 位. 我们使用函数 $f$ 将其编码为 $K$ 位 checkcode, 记为 $C$. The actual size of the stored word is $ M + K $ bits 接收方收到 $M + K$ bits 数据, 其中的 $M$ 位数据记为 $D'$ , $K$ 位checkcode记为 $C'$ . 对 $D'$ 再次使用 $f$ 得到 $K$ 位的 $C''$. 将 $C'$ 和 $C''$ 做比较( 事实上就是异或 ), 得到故障字(syndrome word) $S$ : $$ S = C' \\oplus C'' $$ No errors are detected. 使用数据 $D'$ An error is detected, and it is possible to correct the error. 使用 $D'$ 来生成正确的数据 $D''$ , 并使用 $D''$ An error is detected, but it is not possible to correct it. This condition is reported. Odd-Even Check 最简单的方法是奇偶校验. 增加1位checkbit $C$ , 表示$D$中1的数量是奇数还是偶数. 假设数据为$𝐷=𝐷_M \\cdots 𝐷_2𝐷_1$: 奇校验: $$ C=D_{M} \\oplus \\cdots \\oplus D_{2} \\oplus D_{1} \\oplus $$ 偶校验: $$ C=D_{M} \\oplus \\cdots \\oplus D_{2} \\oplus D_{1} $$ Error Detection: $$ S = C' \\oplus C'' $$ $𝑆 = 0$ : $C + D$ 正确 or 其中出错的位数为偶数 $𝑆 = 1$ : $C+D$ 中出错, 且出错的位数为奇数 Steps 以偶校验为例: 发送方计算$C$. 将 $C$ 与 $D$ 一同发送 接收方收到$C'$ 和 $D'$, 计算 $C''$ : $$ C''=D'{M} \\oplus \\cdots \\oplus D'{2} \\oplus D'_{1} $$ 计算$S$ $$ S = C' \\oplus C'' $$ Drawbacks 不能检测到偶数个bit出错 发现错误后不能校正, 因为不能定位到出错的bit Hamming Code The simplest of the error-correcting codes is the Hamming code devised by Richard Hamming at Bell Laboratories. Hamming Code is a single-error-correcting (SEC) code 只能检测和纠正1-bit errors Idea 见如下Venn diagram: 假设$ M = 4 $ : 我们画三个圆, 中间相交的四个部分分别填4 data bits ( 如图a ). 剩下的最外面三个部分填 parity bits ( 也就是checkcode ). Each parity bit is chosen so that the total number of 1s in its circle is even( 如图b ) Now, if an error changes one of the data bits ( 如图c ), it is eas- ily found. By checking the parity bits, discrepancies are found in circle A and circle C but not in circle B. Only one of the seven compartments is in A and C but not B ( 如图d ). The error can therefore be corrected by changing that bit. Steps 将数据分成几组, 对每一组都使用奇偶校验进行检错. 我们默认使用偶校验 数据输入:为数据 $D$ 中每组生成checkbit, 合并得到 $K$ 位check code $C$ 数据输出:为数据 $D'$ 中每组生成checkbit, 合并得到 $K$ 位check code $C''$ 检错:将 $C''$ 和取出的 $C’$ 按位进行异或, 生成 $K $位 syndrome word $S$ 全部是0:没有检测到错误 有且仅有1位是1:错误发生在校验码中的某一位，不需要纠正 有多位为1:错误发生在数据中的某一位，将𝐷′中对应数据位 取反即可纠正(得到𝐷&quot;) Check Code Length 从上表可以看到, 使用checkcode会使得实际可用的主存( 用户可见的主存 ) 比 事实上的主存 大小更小. 对于64-bit的内存, 如果使用7bit的hamming code, 它的实际word-size是 64 + 7 = 71bit, 而用户只能看见其中的64bit. 假设最多1位发生错误, 可能的情况有: $D$ 中有1位出现错误: $M$ $C$ 中有1位出现错误: $K$ 没有错误: $1$ 因此, $K$ 位的$S$ 需要能表示 $M + K + 1$ 种情况, 即满足: $$ 2^{K}-1 \\geq M+K $$ E.g. for a word of 8 data bits ( $M = 8$ ), we have $K=3: 2^{3}-1&lt;8+3$ $K=4: 2^{4}-1&gt;8+4$ 因此, $M=8$ 时 $K=4$ Checkbit 假设$M=8, K=4$ checkbit的position number是2的幂: $\\mathrm{C}1$: 0001 $\\mathrm{C}3$: 0010 $\\mathrm{C}4$: 0100 $\\mathrm{C}8$: 1000 ... Each check bit operates on every data bit whose position number contains a 1 in the same bit position as the position number of that check bit. 比如说, $\\mathrm{C}1$ 是 $0001$ , 因此$\\mathrm{C}1$ 就和所有的 position number 的第一位为 $1$ 的 databit 有关, 即data bit positions 3, 5, 7, 9, and 11 (D1, D2, D4, D5, D7) bit positions 3, 6, 7, 10, and 11 all contain a 1 in the second bit position, as does C2; Looked at another way, bit position n is checked by those bits $C_i$ such that $$ n=\\sum_{i=1}^{K} (C_{i}) $$ For example, position 7 is checked by bits in position 4, 2, and 1; and 7 = 4 + 2 + 1. Example Let us verify that this scheme works with an example. Assume that the 8-bit input word is $00111001$, with data bit $\\mathrm{D}1$ in the rightmost position. The calculations are as follows: Sender: Compute $C$ : 假设 $\\mathrm{D}3$ 出错, 从0变成了1. 接收方收到 $D'$ and $C'$, then Compute $C''$ : 最后计算故障字$S$: $$ S = C' \\oplus C'' $$ 得到$0110 = 3$, 发现是$\\mathrm{D}3$ 出错 Why Only One-Bit Errors? In computer operation 2-bit errors are very, very unlikely. 假设: the probability of a 1-bit error = $10^{-9}$ , if a computer makes 10,000,000 moves a second On average you get an error every 100 seconds = less than 2 minutes. 相对应的, 2-bit error的概率大概是 $10^{-18}$, 使用同样的电脑, you get a 2-bit error once every 1011 seconds = once every 3,171 years. There may be other considerations, specifically to do with data communications. It is quite common to establish communications over noisy lines, for example, and then the probability of errors increase dramatically. It often happens that there is a short period when may be multiple-bit errors, and it would be impracticable to use a Hamming code in this situation. Other schemes more appropriate to this problem are used instead. SEC-DED Hamming Code只能检查和纠正1-bit error. More commonly, semiconductor memory is equipped with a single-error-correcting, double-error-detecting (SEC-DED) code. 这通过添加一个额外的checkbit完成 单纠错, 双检错. ( 可以找到两个位产生的错误, 并纠正一个位的错误 ) Idea This figure illustrates how such a code works, again with a 4-bit data word. If two errors occur ( 如图c ) The checking procedure goes astray ( 如图d ) The situation worsens the problem by creating a third error ( 如图e ) To overcome the problem, an eighth bit is added that is set so that the total number of 1s in the diagram is even. The extra parity bit catches the error (f). Steps 添加: $$ C_{5}=D_{1} \\oplus D_{2} \\oplus D_{3} \\oplus D_{5} \\oplus D_{6} \\oplus D_{8} $$ SED-DED的故障字 $S$ 的分析: 都是0: 没有检测到错误 1位为1: 在5个校验位中有一个发生了错误, 不需要修正 2位为1: 有2位数据和校验位出现错误，但找不到错误的位置 3位为1: 8位数据位中有1位发生了错误，该错误可以被纠正 3位以上均为1: 严重情况，检查硬件 Drawbacks Hamming Code需要额外的存储空间 Hamming Code需要先对数据进行分组, 这无法处理流式数据 Cyclic Redundancy Check 循环冗余校验(Cyclic Redundancy Check, CRC) 适用于以流格式存储和传输大量数据 用数学函数生成数据和校验码之间的关系 Steps Get Check Code: 假设数据有 $M$ 位，左移数据 $K$ 位(右侧补0), 并用 $K+1$ 位生成多项式除它(模2运算) 采用 $K$ 位余数作为checkcode 把checkcode放在数据(不含补的0 )后面, 一同存储或传输 校错: 如果M+K位内容可以被生成多项式除尽，则没有检测到错误 • 否则，发生错误 Example 假设数据是 $100011$, 生成多项式为$x^3 + 1$ ( 二进制表示为$1001$ ): 最后生成的checkcode是$111$","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"Computer Architecture","slug":"Computer-Architecture","permalink":"http://lyk-love.cn/tags/Computer-Architecture/"}]},{"title":"Github","slug":"Github","date":"2022-09-28T18:17:23.000Z","updated":"2022-09-29T07:16:19.673Z","comments":true,"path":"2022/09/29/Github/","link":"","permalink":"http://lyk-love.cn/2022/09/29/Github/","excerpt":"Import Repository Transfer","text":"Import Repository Transfer Import Repository Side of profile picture you will find + button click on that then there will be option to import repository. our old repository’s clone URL is required which is gitlab repo url in your case. then select Owner and then type name for this repo and click to begin import button. Transfer 可以把个人仓库转给别人或者组织. 在仓库的Settings主页的最末尾的Transfer ownership. Organization Within in a Organization, each Repository defines its restrictions by Permission associated to Team/s and Collaborator/s. There are 2 Roles within a GitHub Organization: Owner or Member. A GitHub user becomes a Member by &quot;joining&quot; an Organization (by default). Note: An invitation from an Owner is needed before hand. Members within an GitHub Organization can be distributed in Teams. A member can join one or more teams. Collaborators (members or external user from the organization) and teams can be defined for each repository. For each of them should be assigned a Permission Level As a summary: Repository permission levels for an organization. 成员权限 member默认只有read权限, 可以更改配置, 使其拥有push权限: settings --&gt; Member privileges --&gt; Base permissions --&gt; 选择Write","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[]},{"title":"矿工笔记","slug":"矿工笔记","date":"2022-09-26T06:39:34.944Z","updated":"2022-09-26T06:39:34.944Z","comments":true,"path":"2022/09/26/矿工笔记/","link":"","permalink":"http://lyk-love.cn/2022/09/26/%E7%9F%BF%E5%B7%A5%E7%AC%94%E8%AE%B0/","excerpt":"注：我不挖矿","text":"注：我不挖矿 问题 [原文链接]（https://zhuanlan.zhihu.com/p/73819309） 挖矿与囤币 我先抛出我对这个问题的答案：在大多数情况下，挖矿的收益是比较高的。为什么这么说呢？其实大家可以思考一个很简单的问题，那到底是挖矿麻烦呢，还是直接买币麻烦？囤币的话这个答案是很显而易见的，肯定是挖矿要比买币麻烦很多。那为什么这么多人，这么多矿业的从业人员，要放弃简单的买币操作方式，选择一个比较复杂的呢？ 我先讲一下，什么样的条件下，囤币的收益可能会比挖矿的收益高吧。在我六年的从业经历里面，我确实观察到在某一些特定的情况下，囤币的收益会比挖矿高。第一个情况：当币价暴涨的时候，币价涨得特别快，类似于几天就翻几倍，这种情况下，挖矿的收益肯定没有直接囤币的收益高。这种情况一般发生在牛市的尾声，或者牛市非常早期，一些小币种身上，可能两三天三四天就翻倍了。这种情况下挖矿，可能你买的矿机都还在路上呢，矿机可能会有些涨幅，但是肯定没有币价涨幅快。 还有一种情况，是在这个牛市转熊市这样的一种前提条件下。这个时候币价已经到了高位，矿机也很贵，如果这个时候币价开始从牛转熊开始下跌，这时候挖矿，会碰到我们称之为“戴维斯双杀”的一个过程：币价跌，难度还在涨。这时候对于挖矿矿机的收益来讲，它是双重打击：这个时候收益下降的速度非常快，矿机的贬值速度也很快。这个时候可能就是，你的矿机的贬值幅度扣掉你挖出来的币，会比你直接囤币的损失要多。 除了这种比较极端的行情外，我挖矿都可能会比囤币的收益高。**对挖矿来说，最友好的就是币价温和的上涨，甚至币价横盘的阶段。**以我们当时的2015年为例，那个时候可能币价一年都没涨，如果你囤币的话呢，可能这一年都没有任何收益，但那个时候买矿机都已经回本了，这就意味着矿机已经取得了100%的收益。 除了从收益的角度考虑，我觉得挖矿可能还有额外的两个好处。一个是它会增强你对数字货币的理解，让你清楚的知道整个数字货币是怎么运作的，因为挖矿是整个数字货币，尤其是像比特币这种，POW挖矿是运行的非常重要的环节，你如果理解了挖矿，非常有助于你去理解对应数字货币。这样还有一个好处呢，就是它可以做成一个业务。你囤币的话，我不能说他绝对不能做业务，但很难。挖矿的话，你可能会接触到，销售啊，云算力啊，矿池啊，这些未来可能让你在囤币之外，还能额外赚取一些收入。这就是我们所说的“卖水的钱”，其实就是提供业务的价值。 还有两个好处，第一，它对新人来说是比较友好的，因为你可以把它理解为一个生意，就是我投了设备，设备每天会有产出，我根据产出计算什么时间能回本，这跟传统生意是比较像的。但是如果你直接屯币的话，需要你对比特币建立一些很强的信仰，因为这个数字货币是非常新的，要理解这个数字货币的价值来源于哪里，它值不值得长期持有......如果没有这个这些知识，背景，那你就会很难拿得住币。币圈有句名言叫“守币比守寡难”。 所以综合上面的这些原因，我觉得在大多数的情况下，挖矿的收益会比囤币的收益更好。再加上它对新人也比较友好，也有一些额外的好处，所以我觉得如果大家是刚接触数字货币的新人的话，尝试挖矿是一条相对来说更好的投资数字货币的方式吧。 挖矿收益最大化 这个其实是一个择时的问题，那就我的从业经验而言的话选择一个好的进入时机，买一个对的机型，你才会取得比较好的收益。 用一个比较通俗易懂的比喻叫天时地利人和，那什么叫天时呢？天时主要是币价和难度这两个因素，因为这两个因素都不是你能控制的。第二个是地利，地利主要是矿场和矿机。矿场是可以提前准备的，矿机对于矿机生产厂商来说是可以提前准备的，但是对大多数人来说呢，只能等厂家发货。 人和呢，这里面主要是这个两个因素。一个是你的资金情况，还有一个是你的策略。接下来呢，我会对这六个因素进行一一解读，让大家知道这些因素怎么来评估。 首先我们来讲天时里面的两个因素，币价和挖矿难度。币价呢，我们肯定是希望我们买完矿机之后币价是上涨的。我们绝对不希望买完之后这个币价就下跌。但币价的判断是比较困难的。我觉得可以做一个比较简单的定性判断，就是到底是在熊市还是在牛市，或者是横盘阶段进入。在熊市里面和在这个横盘阶段，对挖矿是比较友好的，因为这期间币价都是涨的可能性要比跌的可能性大。 影响难度的两个主要因素呢，一个是要不要会减半，另外一个是你在购买之后会不会有更多的新矿机进入挖矿市场。第一个因素是比较容易判断，因为减半这个事情是固定的。第二个因素呢，你可以从这个矿机有没有足够多的利润，就是你买矿机的价格和它生产矿机成本相比，是不是有很大的溢价。如果说你买的矿机，它和它的生产产品相比有很大的增长利润空间的话，那么矿机生产厂商就有足够的动力去生产更多的矿机，来投往市场里，也就这个时候到后期的话，难度增长会是比较快的，但是如果说利润很低，那就比较慢。 地利主要是矿场和矿机。矿场这个因素是什么呢？就是你在挖矿之前最好能够提前准备，或者是找到一个靠谱的、稳定的、电费相对合理的一个矿场。这个因素可能会被很多人忽略掉，就是觉得我是不是随时进来挖矿都可以找到一个矿场，其实不是这样的，这样的矿场，有的时候非常非常稀缺。机位难求，或者说有的非常不靠谱，可能你矿机到了之后发现跑不起来，甚至根本完全跑不动，或者跑的效率很差，这个事情在挖矿市场上也是经常有听闻的。 矿机呢主要是说，是不是在你想挖矿的时候，你就马上能买到一个价格比较合理的矿机。这个因素，对于大多数人来说其实也是不可控的，有可能厂商，他可能会断货，甚至没有货，或者只有期货。以现在为例，如果你想买比特矿机，像好的比特矿机的话，都只有10月份，11月份的期货。和换成四五月份的时候，那时候就可以当天付完钱之后，第二天第三天就能拿到矿机，是不可同日而语的。 人和这里面，我觉得嗯，两个要素，一个是资金，一个策略，那所谓资金你要把资金准备好，到一个好的机会的时候，其他因素都具备的时候，你要马上能够把这个机会抓住。策略也很重要，你要想好你在什么样的条件下要出手，并且提前把能准备的这些条件都准备好，比如说我先准备好矿场，准备好资金，然后一旦币价合适，也有比较好的矿机的时候，就杀进来，我觉得也是非常非常重要的，就是要形成一个自己的一个相对完整的投资策略。 最后我想说的是这些因素呢有的时候是互相排斥的。就是当一个因素变好的时候，可能另外一个因素就会变差。币价现在开始上涨了，而且涨的还不错，那可能这个时候矿机就开始缺货了，矿场也没有位置，难度后面可能也跟着涨了，不是一成不变的。所以你要综合考虑找到一个各项条件相对来说都比较好的挖矿的时候。 主流矿机呢，我可以介绍一下住目前的主流矿机，从大的分类来说呢，可以分成比特矿机，莱特矿机，显卡矿机，还有一些小币种矿机这四大类吧。主流的，如果我们只说比特矿机的话，那么主流的其实也可以分成两代，一代是S9类的，还有一代是比S9更好的新一代。s9这一代里面有841啊，就是L的841啊，新增的机器，还有神马的M3，还有一些小牌子的S9，T9啊，这个都是比较常见的。比较好的机型的话，那主要就是比特大陆的。S17、T17系列还有神马的，M20、M21系列，芯动的T3系列。 莱特主流矿机主要是L3+就是比特大陆的和芯动的A4和A6。小币种矿机就很多了，我就不一一介绍了。显卡矿机呢，主要是AMD的这个，X170、570、580系列，还有N卡的1060、1080、1070。还有最新的一代是这个1660、2660系列。 以租代售，共享矿机 以租代售这个可能大家也观察到了，我们挖易最近推了一些算力租赁的服务。那么这个模式会成为未来的一个发展方向吗，我觉得这个要从两个维度来讲，一个维度是对这种大资金的进入，就是你真的看好挖矿市场，资金体量非常大，那么我估计还是多半会选择自己去直接买矿机挖矿，这样的收益才最高。但是我觉得这个租赁矿机服务呢，其实对很多新人来说是非常非常友好的，它比云算力，还要友好。因为有一些产品的周期特别短，购买成本会非常便宜，就非常适合新人。 其实挖矿里面有一个很核心的可能大家以前忽略的点就是它其实需要你有一个逐步的上手的投资额。挖矿存在着投资窗口期的问题，可能好的投资窗口期就那么几个月，甚至一年，这样子。如果说啊，你花了很多的机会，很多时间才开始挖矿，然后等你这个弄明白了之后，可能这个投资窗口期已经过了。那么我觉得租赁矿机呢，对很多新人来说呢，是非常友好的，因为它很便宜，比如说像我们最初30天的租赁，那么它的价格可能只有直接买矿机的1/12，因为便宜，就方便你快速体验什么是挖矿，挖矿是怎么回事儿。 有了这些经验之后，你就可以加大这个投资的规模，然后抓住更好的投资机会，取得更好的收益。","categories":[{"name":"Technology","slug":"Technology","permalink":"http://lyk-love.cn/categories/Technology/"}],"tags":[{"name":"Block Chain","slug":"Block-Chain","permalink":"http://lyk-love.cn/tags/Block-Chain/"}]},{"title":"长津湖","slug":"长津湖","date":"2022-09-26T06:39:34.944Z","updated":"2022-09-26T06:39:34.944Z","comments":true,"path":"2022/09/26/长津湖/","link":"","permalink":"http://lyk-love.cn/2022/09/26/%E9%95%BF%E6%B4%A5%E6%B9%96/","excerpt":"易烊千玺主演, 抗美援朝电影，国庆头号烂片。","text":"易烊千玺主演, 抗美援朝电影，国庆头号烂片。 本片是一部爆米花电影，我没有看出来任何新颖的有价值的思想。 表达的还是老一套，也就是爱国主旋律，表达得很差， 没有代入感不说， 叙事能力都不足，手法还偏要往欧美战争电影的宏大路线靠，典型的眼高手低。 整个电影用那么多特效、 高空摄影、 宏大的管弦乐，丝毫没有代入感。 高格调的表现手法是建立在对基本功的极度自信的情况下的， 导演连基本故事都没讲明白，还在往电影里添油加醋，是为了让影片更有“国际感么”？ 作为教科书，诺兰的《敦刻尔克》拍摄手法非常新颖， 全片表达爱国思想的就那么几句台词，这几句台词的分量比整部《长津湖》还重 全片相对而言最好看的，就是火车出发时，一个女孩给易烊千玺送围巾的那一幕，其实除了演员的脸之外也没什么出彩之处。 剩下的部分一点也不煽情，为了表现战场浩大的声势，用了管弦乐，可是一点情感的调动都没有，我看的时候心情毫无波澜，觉得荧幕上那些人和现实中的我没什么关系。这么差的代入感剧情、表演和配乐等等因素共同造成的 最后杨根思的片段还是挺感动的 最大也是最明显的问题：全程用配音。不管是美军还是中国人，都用的配音，且声音也不合适，听着很僵硬，破坏了观影体验 根本没有讲清楚历史，即“抗美援朝为什么中国能赢”。全片讲了美军的精良，但中国军队是怎么战胜如此强大的美军的呢？ 靠几个神枪手？ 炮手？ 和美军拼刺刀总是赢？ 为啥中国军队总是能埋伏美军，美军总是不知道中国军队的动向？ 就靠志愿军战士精神坚定，宁死不暴露电台吗？ 还有作战计划的调度等等，一场战争的胜利，肯定有其必然性在里面，尤其是双方实力如此悬殊的情况下，胜利者必定有大量的优秀的战术和决策，这些我都没有在电影院看到。只有易烊千玺和吴京、胡军他们在唠嗑，彭司令说了两句话，仗就打赢啦？？而且这些戏份都非常不合时宜，安插在本应该继续煽情的地方，使得战场如同儿戏，没看出多少战争对人的锻炼 有一段儿易烊千玺要杀美国军官，被吴京制止了，说“有的枪必须开，有的枪可以不开”，这段是唯一达到该系列影片正常水平的一段内容。 许多媒体对此大书特书， 我不觉得这有什么好夸的， 都2021年了要是还没有这种思想，只能说明导演的世界观还停留在清朝。 即使如此，这部片的意识形态宣传还是不到位的，因为它连爱国都没说清楚 《长津湖》能有这么高票房，有三大因素：政治， 同期的国庆档没有大片， 以及作为长期疫情封锁后，的第一部国产大片，观众和院线已经饥不择食了。 说到底，这就是一部爆米花电影，而且拍的还很烂， 连娱乐观众都做不到。 不过，《长津湖》不是近段时间最烂的电影，因为我看了《1921》， 这部电影，一言以蔽之： 烂透了。 《1921》比《长津湖》还烂， 这是相当有难度的一件事情，我真佩服制作人员 《1921》最离奇的是， 每个共产党早期革命斗士，无论男女，皆为衣着时尚，面容精致（ 并且妆容也很精致 ）， 居住和出入的地方都是奢华高雅的上流场所。灯红酒绿的眩目，使观众完全看不到革命的任何艰辛。 我非常愿意相信， 这些角色打扮得这么漂亮是给影院的粉丝看的，他们也十分清楚，《1921》的主要受众，也就是爱慕虚荣人云亦云的普通大众。 《1921》和《长津湖》能成为这2021年最高规格的主旋律影片，令人丝毫不觉悲哀，相反烂地还很让人想笑， 什么样的市场催生什么样的产品。 国内观众如此，中国电影也就那样","categories":[{"name":"Art","slug":"Art","permalink":"http://lyk-love.cn/categories/Art/"}],"tags":[{"name":"Movie","slug":"Movie","permalink":"http://lyk-love.cn/tags/Movie/"}]},{"title":"AI Intro","slug":"导论-人工智能","date":"2022-09-26T06:39:34.943Z","updated":"2022-09-26T06:39:34.943Z","comments":true,"path":"2022/09/26/导论-人工智能/","link":"","permalink":"http://lyk-love.cn/2022/09/26/%E5%AF%BC%E8%AE%BA-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/","excerpt":"Outline: What is AI What will we learn Agent Ref: Artificial Intelligence A Modern Approach","text":"Outline: What is AI What will we learn Agent Ref: Artificial Intelligence A Modern Approach What is AI 强人工智能：think like humans 弱人工智能： act rationally 目前的研究都是弱人工智能 发展： 达特茅斯会议→ 推理期（60s-70s）→ 知识期(80s初期 - 90s中期) → 学习期（2006） What will we learn Search 搜索与规划 knowledge 知识表达与处理 Uncertainty 不确定建模 Learning 机器学习 Agent（智能体） The agent function maps from percept histories to actions: $$ f: P^* \\rarr A $$ Agent types Simple reflex agents 简单条件反射，作者把规则写进去 Reflex agents with state 有记忆能力 Goal-based agents 没有规则，只有目标 Utility-based agents 最大化得分 All these can be turned into learning agents Learning agents 能够生成当下目标（Problem generator）","categories":[{"name":"Artificial Intelligence","slug":"Artificial-Intelligence","permalink":"http://lyk-love.cn/categories/Artificial-Intelligence/"}],"tags":[{"name":"AI","slug":"AI","permalink":"http://lyk-love.cn/tags/AI/"}]},{"title":"Transformation of RE, NFA, DFA","slug":"Transformation of RE, NFA, DFA","date":"2022-09-26T06:39:34.942Z","updated":"2022-09-26T06:39:34.942Z","comments":true,"path":"2022/09/26/Transformation of RE, NFA, DFA/","link":"","permalink":"http://lyk-love.cn/2022/09/26/Transformation%20of%20RE,%20NFA,%20DFA/","excerpt":"Outline: REGEX -&gt; NFA NFA -&gt; DFA Minimizing DFA DFA -&gt; REGEX Prove DFA &lt;=&gt; REGEX","text":"Outline: REGEX -&gt; NFA NFA -&gt; DFA Minimizing DFA DFA -&gt; REGEX Prove DFA &lt;=&gt; REGEX RE -&gt; NFA ref 使用Thompson 构造法, 基本思想是 按结构归纳 对正则定义的每个规则，分别建立一个图的映射，因此所有正则表达式都可以表现为这些子图的组合 NFA -&gt; DFA 用子集构造法 ref Subset-Construction(NFA) let Dtran be a table DFA_States = &#123;ε-closure(NFA.s0)&#125; # DFA 状态集合的初始状态为 NFA 初始状态的闭包，并且未标记 while (exist T in DFA_States not marked) &#123; # 存在未标记的 DFA 状态 mark T # 标记 T，表示查过 T 状态的所有后续状态了 for (a in Σ) &#123; Tc = ε-closure(move(T, a)) # 找到所有输入字符对应的下一个状态 if (Tc not in DFA_States) &#123; # 将状态加入到 DFA_States push Tc in DFA_States &amp; unmarked Tc &#125;# 如果新状态已经在Dtran中，那么不会添加新状态，只会添加[T, a] Dtran[T, a] = Tc &#125; &#125; return Dtran 简言之，根据连通性不断添加新状态，直到所有状态成为闭包 Minimizing DFA ref Min-DFA(Dtran) let U = &#123;S\\F, F&#125; # 初始的等价类有两个组，分别是非终止状态和终止状态 for (G in U) &#123; # 对等价类中每个组做划分 while (G can be divided) &#123; g1, g2, g3, ... = divide(G) remove G from U add g1, g2, g3, ... into U &#125; &#125; 首先将上面的 DFA 分为非终止状态和终止状态两组，然后对每一组进行检查是否能够再做划分，若可则对划分后的组再划分，直到不可再划分 最后根据划分后的组重建 Dtran 表并重绘 DFA 图 不可划分： 若对于组内所有状态，对于所有输入都有相同的输出状态，则称该组不可再被划分。 Prove DFA &lt;=&gt; RE 验证DFA和其转化出的REGEX的等价性, 使用Kleene闭包 ref 符号归约： 假设有向图中节点编号为 $0$(初始状态)到 $n − 1$ $R_{ij}^k$ : 从节点 i 到节点 j、且中间节点编号不大于k的所有路径 $R_{ij}^{-1}$: 从节点 i 到节点 j、且不经过中间节点的所有路径 $\\empty$: 路径不可达， 我们有 $\\empty$ r = r $\\empty$= $\\empty$ 和 $\\empty$|r = r $\\epsilon$: 空路径，即状态不发生变化的边（回到同一状态） Kleene闭包： $R_{ij}^{k} = R_{ij}^{k-1} (R_{ij}^{k})^* R_{ij}^{k-1} | R_{ij}^{k-1} $ ( 就是Floyd算法 ) 步骤： 动态规划思想，从$k=-1$开始不断迭代，直到有$R_{ij}^{n-1}$，其中 $i$ 和 $j$ 是起点和终点 验证结果是否与原来的正则等价","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"Compilers","slug":"Compilers","permalink":"http://lyk-love.cn/tags/Compilers/"}]},{"title":"The Story of Art: Chapter 26,27","slug":"The-Story-of-Art-19-20世纪","date":"2022-09-26T06:39:34.941Z","updated":"2022-09-26T06:39:34.941Z","comments":true,"path":"2022/09/26/The-Story-of-Art-19-20世纪/","link":"","permalink":"http://lyk-love.cn/2022/09/26/The-Story-of-Art-19-20%E4%B8%96%E7%BA%AA/","excerpt":"Outline: 19世纪末的艺术，：寻求新标准 20世纪上半叶的艺术：实验艺术 艺术史的总结 我终于理解了“从来没有什么艺术，只有一个一个的艺术家”这句话，对它的解释写在了结尾，我真是太聪明了！","text":"Outline: 19世纪末的艺术，：寻求新标准 20世纪上半叶的艺术：实验艺术 艺术史的总结 我终于理解了“从来没有什么艺术，只有一个一个的艺术家”这句话，对它的解释写在了结尾，我真是太聪明了！ 寻求新标准与实验性艺术 19世纪末 寻求新标准 19世纪晚期，艺术越来越成为一种程式化的工作，在建筑领域，建筑师首先创作的是建筑物，而不是艺术品。 艺术品的创造只需要在一本论“历史风格”的艺术范本中找到一种艺术形式。 在绘画领域，所谓的“印象派”其实并非是对传统的颠覆，在艺术目标方面，印象主义和文艺复兴时期发现自然依赖建立的艺术传统别无二致。 印象派和保守派的争论主要在艺术手法方面，而不是艺术目标方面。事实上，直到印象主义，绘画才完全征服了自然，画家真正可以画其所见。 但是，纯粹地描摹视觉印象并不是艺术的全部，也不能使所有艺术家感到满意。 塞尚 塞尚既认同印象主义的思想，即必须客观真实地描摹自然，又不愿意放弃古典主义中那些规则的程式，以及它们所蕴含的宏伟和秩序感觉（尽管这是以违反自然为代价的）。 这相互矛盾的两个目标使得塞尚做出了不懈的探索。 他既用印象主义的手法描摹自然，又将“自然”中不符合他目标的部分加以歪曲，这种无视现实的手法事实上是开拓性的。 归根到底，塞尚是想表现坚实感和深度感，为此可以牺牲传统的自然轮廓的“正确性”，这实质上是在探索形状（古典派）和色彩（印象派）的关系。 修拉 塞尚的目的是将印象主义和画面有秩序这两个要求加以调和，修拉采取了完全不同的，更科学的方法。 他研究色彩视觉的科学理论，采用点彩的技术，使得色彩在人眼中自由地混合，但又不失去强度和明度（另一方面，色彩的强度和明度意味着轮廓和立体感）。但这种方法在实践上非常复杂，点彩使得形象难以辨识，修拉不得不对形象进行简化，他的点彩艺术越来越趋向于描摹图案而不是自然。 凡高 凡高从印象派和修拉的点彩艺术得到启发，他也采用纯色点画的技术，但是使用强烈的笔触而不是点彩。凡高欣赏日本版画带来的直接而强烈的效果，渴望创造一种纯真的艺术。凡高的画意在表现他的内心感受，而不是正确地描摹自然，在这一点上他和塞尚同途异路 高更 高更和凡高一样，中年开始学画，他的艺术理念是对文明的怀疑，这在深层次是对理性的怀疑。他晚年跑去塔希提岛与土著生活在一起，以探索原始艺术。 事实上，自从艺术家自觉意识到艺术的“风格”之后，就开始了对传统的程式的怀疑。 艺术不应该是所谓的“风格”，某种程式，某些形式上的特征，而是更激情更主观的东西。高更因此厌恶现代文明，因为这意味着刻板的程式化的艺术，而他想要探求的是更纯真无邪的，没有形式约束的艺术，这使得他的作品的一大特征是“粗野”。为了表现土著人一般的高度纯真，他可以放弃西方的技术传统。 19世纪末艺术的总结 19世纪末艺术的基本问题，在技术上是要求运用色调层次暗示深度感和希望保留美丽、自然色彩的期望相矛盾的问题。 这实际上是古典艺术和印象派，或者是文艺复兴以来回到自然的艺术传统的冲突，前者要求和谐、宏伟，后者要求对自然光影的真实描摹。 日本艺术给当时人启发： 如果为了大胆的简化而牺牲立体造型和细节描绘，一幅画就会给人以强烈的印象。 塞尚和修拉沿这条路有所发展，他们都加强了色彩而牺牲了深度感。 这种风格当然使插画艺术和广告艺术受益，大大增强了艺术的“装饰性” ， 因为人们在看清画的内容之前就看见了了悦目的图案。 在艺术的意义上，塞尚看到印象派过度注重色彩和自然，而忽视了自然的坚实和持久的特性（说这是特性也好，是人们后天赋予的趣味也罢，对和谐和永恒的追求是西方艺术的悠久传统）；凡高看到印象派只描摹其所见，只描摹客观自然，而忽视了内心的探索和表达，如果绘画沦为了了对光学表现的机械复现，那么艺术就将失去感动和激情；高更干脆反对整个西方艺术，他认为程式化的艺术传统、技法有违艺术创作的初心，他的艺术是为了追求更原始纯真的东西。这三者分别影响了后来的立体主义、表现主义、原始主义，也对应着艺术的结构、表现力和单纯性。 20世纪上半叶 实验性艺术 现代艺术看似是对传统的决裂，但是它与过去的艺术一样，其出现是对一些明确的问题的反应。例如，为了探索一种新风格，在建筑领域诞生了“现代建筑”。 包豪斯 包豪斯是一所建筑学习，其艺术风格可被概括为“实用主义”，其宗旨是对设计的东西的功能性的强调。 当然，这个理念需要机智地设计，因为产品必须符合目的，还能“用起来合适”，因此也具有一定的美感。 由功能主义可以看出，现代艺术已经成为了一种实验，艺术家可以自由地进行探索。 由此产生的问题是： 艺术家为何实验艺术，而不是只单单地描绘艺术。 我们看到，艺术中单纯地“描绘”是不可能的。首先，直到印象主义之前，艺术家都无法“画其所见”，在古典时期，艺术品都被用于各种场合和目的（讲述故事、宣传宗教），艺术家没有画其所见的余地； 文艺复兴之后，有了透视法、渐隐法、各种色彩的研究，艺术家才开始“画其所见”，但还是受到传统的程式的困扰，艺术家只能用所学的形式，而不是所见的形式来创作；19世纪末，随着印象主义的出现和塞尚等人对传统程式的清除，艺术家总算能够“画其所见”了。 其次，单纯地“画其所见”其实并不存在，因为科学早就证明，“所见”与“所知”是不可分割的，程式违背了“所见”，但它“所知”的结果。 我们对客观世界的认识，是感官和大脑认知共同作用的结果，我们接收到一个感官印象，最终还是要从某种”程式“，比如点、线入手，将其表达出来。因此程式，或者说获取程式的冲动依然是必要的（另一方面，既有的程式，比如西方的艺术传统，是可以抛弃的，艺术家只需要“获取程式”这一本能）。 表现主义 表现主义就是歪曲自然形象以表达内心的情感。人对事物的情感的确会影响对事物的印象，也会印象事物在内心的形状，这是显而易见的。表现主义就构建在这一事实上。 在凡高之前，已经有了表现主义艺术家，比如爱德华蒙克，其代表作《呐喊》。表现主义者，在当时的世道，都有愤世嫉俗、关心社会现实的一面，它们的作品因此看起来痛苦和丑陋 ---- 或者说，以前所谓“美丽”的艺术品和艺术程式，比如古典艺术，其实是对现实的歪曲，现实中劳苦大众的命运，其实确实是像表现主义那样丑陋、悲惨的，表现主义只是更真实地描绘了这个社会。 表现主义在技术上，是通过色彩和线条的选择去表现感情，而不是一味模仿自然。 这个理论进一步发展，就有理有追问： 是否能够完全抛弃客观自然，而追求单纯的色彩和形状的表达，这样是否能够创造更纯粹的艺术? 抽象艺术 康定斯基和他的抽象艺术学派，是这一理念发展的结果，也涉及下文中20世纪艺术对“形式”的探寻。表现主义者大多也是神秘主义者，其目标如上所述，而其终极目的，是使得绘画成为建筑一样的构成性的艺术。 即在表面的色彩和形状背后，真的有一个永恒不变的实在。抽象艺术通过对客观现实的抛弃，来获得形式的纯粹，以达到这一神秘的目的。 抽象艺术也反映了整个现代艺术的核心思想： 艺术家想创造事物，而不仅仅是描绘事物。 艺术家想要创造一个前所未有的东西，并相信（起码他们自己相信）这些东西对人们有强烈的意义，这也是神秘主义的，因为他们想要创造比现实本身更为真实的东西。 立体主义 19世纪末期对装饰性的图案的注重，使艺术家丧失了用光影塑造形体的方法。这当然意味着丧失了对客观现实的描摹。 凡高和高更的作品抛弃了图案的复杂性和华而不实，而更注重对色彩的应用。这推动了野兽派的诞生，它为了色彩的愉悦而抛弃了明暗法。 但是，毕加索对这个问题给出了自己的答案，即在保持平面性的基础上维持立体感和深度感。 立体主义的艺术宗旨是： 描摹某物的构成而不是某物本身。 因为某物在人们心灵中的样子，确实不同于它实际的样子，因此从三角、圆锥、立方体来描摹某物也是合理的，因为该物品在人们的心目中，可能就是由这些突出的“特征”构成的。这实际上和埃及艺术的原则很想像，即描摹某物的“特征”而不是全部。 当然，立体主义不是唯一的表现世界的方式，只是一种艺术上的“发现”。毕加索说 “人人都想理解艺术，为什么不设法去理解鸟的歌声呢？”， 20世纪艺术的一大特征就是“发现”新方法。 对于古典时代的艺术家而言，艺术的题材居于首位，而在委托工作少见的后代，艺术家需要自己选择题材，他们一般选择可以表现自己手艺的某个题材，或者说，他们选择题材是为了更好地展示他们的手法， 此时的艺术家认为题材不过是一个机会，用来使色彩和图案达到平衡。 塞尚的静物画就是一例，它表达的只是对色彩和图案的探索，而不是物体本身。这种倾向继续下去，就发展出了“形式”第一，“题材”第二的艺术观念，艺术家的使命就是不断地“发现”新方法来解决“形式”问题，而不需要探索什么题材。 原始主义 原始主义从高更开始追求艺术的纯真，20世纪的艺术家，如卢梭等人，身体力行，证明了这种理论的可行性。甚至卢梭没有经过专业画家的训练，就可以创造出全新的，人们不而能不否定其为“艺术”的东西。当然，原始主义过份地追求天真和单纯，这是有矛盾的，因为人不可能随心所欲地变成原始人。 在通往孩童般的纯真的艺术道路中，有些艺术家“故作愚昧”，开辟了一些新的艺术方向，比如幻觉艺术（也称为超现实主义）. 超现实主义 超现实主义是原始主义的延伸，或者说二者都是现代艺术的分支。 其核心思想，依然是要创造比现实本身更真实的东西。 他们吸收弗洛伊德的理论：“当我们的头脑麻木之后，潜藏在身上的童心和野性就会活跃起来”， 这使得超现实主义者提倡感性，反对理性。 该理论并不新颖，因为唯心主义的艺术观自古有之，艺术家从艺术中获得“神性的迷狂”。 20世纪上半叶艺术、兼整个艺术史的总结 牡蛎要创造一颗珍珠，需要一些物料，需要一颗沙粒或者一块小东西，以便围绕着它形成珍珠。 没有那样一颗坚硬的核心，就可能长出一团不成形状的东西。 艺术家也需要一个坚硬的核心 --- 一项明确的任务， 来发挥其才智。 所谓的任务，要么由社会显示指定，要么由社会传统来指定， 比如艺术应该复制自然这个任务，完全是社会传统要求的结果，并不是艺术的内在使命。当然，这些任务并非与艺术的本质毫无关连，因为这些任务对艺术提出了各种各样的问题，而艺术，或者说艺术家的使命就是对这些问题的解决。 因此“艺术是什么”这个问题很好回答： 艺术，是对各种任务的解决，而对任务的解决过程，也就是艺术家发挥聪明才智的过程，构成了艺术品。 任务如同沙粒，而艺术家在任务上发挥的聪明才智有如包裹沙粒的珍珠质。 艺术必须要以任务为核心，但最终感动我们的并非是沙粒，而是闪耀的珍珠。正如我们可能不会对某个宗教题材感兴趣，但我们被描绘该题材的艺术作品感动；我们讨厌某人吹牛，但要是这个人吹牛吹的特别棒，我们也不得不承认这是一种艺术。 动机与过程的分离，揭示了艺术的构成。“没有什么艺术，有的只有艺术家”，这话也很好理解： 艺术本身不是一个目的，也不是一个抽象的实体，我们看到的艺术，其实都是艺术家的创作，对某个任务的解决，也就是牡蛎中的珍珠。 我们欣赏的是珍珠，是艺术家的努力，是艺术作品。但是抽象的艺术，所谓的“为艺术而艺术”， 所谓的“艺术家应该创作艺术”是不存在的，因为艺术只是对任务的解决，其本身不是一个实体，没有任务、没有沙粒，哪来的珍珠呢？ 评论： 首先，作者认为艺术是对问题的解决，本书确实有力地论述了这一观点，我们确实能看到，所有的艺术流派，艺术作品和理念，都是对某个任务，以及对该任务提出的问题进行解决的尝试。 作者认为没有独立的纯粹的艺术，这是彻底的唯物主义。 唯物主义很残酷，它的答案有时不足够令人振奋，但它的解释往往最有效。 我认同这样的观点。","categories":[{"name":"Art","slug":"Art","permalink":"http://lyk-love.cn/categories/Art/"}],"tags":[{"name":"Art Theory","slug":"Art-Theory","permalink":"http://lyk-love.cn/tags/Art-Theory/"}]},{"title":"Static Link && Dynamic Link","slug":"Static-Link-Dynamic-Link","date":"2022-09-26T06:39:34.940Z","updated":"2022-09-26T06:39:34.940Z","comments":true,"path":"2022/09/26/Static-Link-Dynamic-Link/","link":"","permalink":"http://lyk-love.cn/2022/09/26/Static-Link-Dynamic-Link/","excerpt":"Outline: static link static load dynamic link &amp;&amp; load","text":"Outline: static link static load dynamic link &amp;&amp; load Link &amp; Load 静态库： .a 动态库( aka 共享对象 ): .so static link 需求： 允许引用其他文件（C标准称为编译单元compilation unit）里定义的符号 C不阻止你随便声明符号的类型 但类型不匹配是undefined behavior 使用-Wl,--verbose可以将--verbose传递给ld 可以看到ld script //a.cint foo(int a , int b)&#123; return a + b;&#125; //b.cint x = 100;int y = 200; #include&lt;stdio.h&gt;extern int x,y;// extern int foo(int a, int b);extern int foo;可以看到int main()&#123; // printf(&quot;%d + %d = %d\\n&quot;, x, y, foo(x,y) ); printf(&quot;%x\\n&quot;, foo ); foo = 1;&#125; //MakefileCFLAGE := -Osa.out: a.o b.o main.o gcc -static -Wl,--verbose a.o b.o main.oa.o: a.c gcc $(CFLAGS) -c a.cb.o: b.c gcc $(CFLAGS) -c b.cmain.o: main.c gcc $(CFLAGS) -c main.cclean: rm -f *.o a.out 刚编译完程序的时候，main.o不能运行，因为其外部符号都“留空” objdump -d a.o （符号地址还没有解析，暂时是全零） 链接后objdump -d a.out | grep main 可以看到其外部符号已经被正确赋值 static load 静态ELF加载器：加载a.out时执行 ELF文件中有若干个ELF program header， 描述了文件到内存的映射。 静态ELF加载器根据ELF program header,将文件中指定部分移动到内存 遍历ELF中的各个program header，然后read/ mmap OS在execve时执行： OS在kernel mode调用mmap (进程还未准备好时，由内核直接执行系统调用) 映射好a.out代码、数据、堆、栈、vvar、vdso、vsyscall 加载完成后，静态链接的程序就从ELF entry开始执行： readelf -h a.out ELF Header: Magic: 7f 45 4c 46 02 01 01 03 00 00 00 00 00 00 00 00 Class: ELF64 Data: 2&#x27;s complement, little endian Version: 1 (current) OS/ABI: UNIX - GNU ABI Version: 0 Type: EXEC (Executable file) Machine: Advanced Micro Devices X86-64 Version: 0x1 Entry point address: 0x401670 //ELF entry 0x401670 Start of program headers: 64 (bytes into file) Start of section headers: 795088 (bytes into file) Flags: 0x0 Size of this header: 64 (bytes) Size of program headers: 56 (bytes) Number of program headers: 10 Size of section headers: 64 (bytes) Number of section headers: 32 Section header string table index: 31 gdb a.out ...(gdb) startiStarting program: /home/lyk/Documents/Hexo/LYK-love.github.io/source/_drafts/Test/a.out Program stopped.0x0000000000401670 in _start () //可以看到，确实是0x401670 查看该进程的地址空间： (gdb) info inferiors Num Description Connection Executable * 1 process 15776 1 (native) /home/lyk/Documents/Hexo/LYK-love.github.io/source/_drafts/Test/a.out (gdb) !cat /proc/15776/maps00400000-00401000 r--p 00000000 103:08 3802167 .../LYK-love.github.io/source/_drafts/Test/a.out00401000-00482000 r-xp 00001000 103:08 3802167 .../LYK-love.github.io/source/_drafts/Test/a.out00482000-004a9000 r--p 00082000 103:08 3802167 .../LYK-love.github.io/source/_drafts/Test/a.out004aa000-004b1000 rw-p 000a9000 103:08 3802167 .../LYK-love.github.io/source/_drafts/Test/a.out004b1000-004b2000 rw-p 00000000 00:00 0 [heap]7ffff7ff9000-7ffff7ffd000 r--p 00000000 00:00 0 [vvar]7ffff7ffd000-7ffff7fff000 r-xp 00000000 00:00 0 [vdso]7ffffffde000-7ffffffff000 rw-p 00000000 00:00 0 [stack]ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0 [vsyscall](gdb) 可看到的确有上述内容 静态链接缺点：导致复用性降低，磁盘过多被占用 dynamic link &amp; load 需求： 节约内存 Linux系统会共享动态链接库的一个副本（可以用size查看内存占用） 实现动态加载（代码）： 编译成Position Independent Code( PIC ) 引用代码全部使用PC相对寻址 x86已经是这样了 把代码mmap进进程的地址空间 实现动态加载（代码+数据+允许访问其他动态链接库导出的符号）： 编译成Position Independent Code( PIC ) 引用代码全部使用PC相对寻址 x86已经是这样了 对于其他动态链接库导出的符号，可以在数据区维护一张表，每次引用该符号时就查表。在运行时给相应的表项赋值 把代码mmap到进程的地址空间 ELF文件都有 Global Offset TableGOT， 即上述的“表” Lazy Symbol Resolution: 不一次性加载GOT的所有符号 Example 编写两个简单的程序（fred.c, bill.c），将其编译为目标文件，并分别生成静态库和动态库。再编写程序调用之，说明库的使用。 生成静态链接库 gcc -c h.c -o h.o ar cqs libh.a h.o：ar是生成库的命令，cqs是参数，libh.a是生成的静态链接库须以lib开头，h是库名，a表示是静态链接库，h.o是刚生成的目标文件 生成动态链接库 gcc -c h.c -o h.o gcc -shared -WI -o libh.so h.o：生成动态链接库使用gcc来完成，-shared -WI是参数，libh.so是刚生成的静态链接库，必须以lib开头，h是库名，so表示动态链接库，h.o是刚生成目标文件。 将生成的libh.a，libh.so拷贝到/usr/lib或/lib下 编译带静态链接库的程序 gcc -c test.c -o test.o gcc test.o -o test -WI -Bstatic -lh:-WI -Bstatic表示链接静态库，-lh中-l表示链接，h是库名即/usr/lib下的libh.a 编译带动态链接库的程序 gcc -c test.c -o test.o gcc test.o -o test -WI -Bdynamic -lh:-WI -Bdynamic表示链接动态库，-lh中-l表示链接，h是库名即/usr/lib下的libh.so 运行./test得到结果","categories":[{"name":"Technology","slug":"Technology","permalink":"http://lyk-love.cn/categories/Technology/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://lyk-love.cn/tags/Linux/"}]},{"title":"T1&T2 排序与分治","slug":"T1&T2 排序与分治","date":"2022-09-26T06:39:34.940Z","updated":"2022-09-26T06:39:34.941Z","comments":true,"path":"2022/09/26/T1&T2 排序与分治/","link":"","permalink":"http://lyk-love.cn/2022/09/26/T1&T2%20%E6%8E%92%E5%BA%8F%E4%B8%8E%E5%88%86%E6%B2%BB/","excerpt":"数学归纳法 QuickSort 堆","text":"数学归纳法 QuickSort 堆 数学归纳法 证明 逆否命题 + 良序公理 形式 例子 “所有马都是同色” 数学归纳法要求$P(1)$成立， 并且$P(1)$ 到 $P(2)$, $P(2)$ 到 $P(3)$等全部成立。 这里$P(1)$ 到$P(2)$不成立 公理化 算法中遇到的输入是“可数无穷多”的（ 《离散》 ） 自然数的定义 可以用集合定义自然数 可以用Lambda演算定义自然数 QuickSort 分析（指标随机变量） Input：$a_1,a_2,\\dots,a_n$, 它必定有一个唯一的排列 $z_1,z_2,\\dots,z_n$满足$z_1 &lt; z_2 &lt; \\dots &lt; z_n$​ 指标随机变量$x_{ij}:\\quad {z_i 比 z_j}$​ 一共有$\\sum\\limits_{1\\leq i\\leq j\\leq n}x_{ij}$次比较， 若两元素没有比较，则 $x_{ij} = 0$ 平均情况时间复杂度： $E[\\sum\\limits_{1\\leq i\\leq j\\leq n}x_{ij}] \\in \\Theta(nlogn)$​​ $E[x_{ij}] = \\frac{2}{j-i+1}$ 对于$z_i,z_{i+1},\\dots, z_{j-1},z_j$，只有$z_i$或$z_j$被选为pivot时，$z_i$和$z_j$才会发生比较 堆 堆的前k大元素： 只需搜索堆的前k层（规模从n缩小为k），而且堆的前k层还是堆 $\\sum h \\leq n-1$​ $\\sum h_L + \\sum h_R$ 用归纳法和堆的性质（左右子树必定有一棵是完美二叉树）","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[{"name":"Algorithm design and analysis","slug":"Algorithm-design-and-analysis","permalink":"http://lyk-love.cn/tags/Algorithm-design-and-analysis/"}]},{"title":"Shell Basic","slug":"Shell-Basic","date":"2022-09-26T06:39:34.939Z","updated":"2022-09-26T06:39:34.939Z","comments":true,"path":"2022/09/26/Shell-Basic/","link":"","permalink":"http://lyk-love.cn/2022/09/26/Shell-Basic/","excerpt":"Outline: Shell Basic Shell and Syscall User Environment Linux Shell Windows Shell","text":"Outline: Shell Basic Shell and Syscall User Environment Linux Shell Windows Shell Shell Basic Shell语言的文档详见拙著Shell Script， 本文只介绍Shell程序的基本概念 抽象的说，Shell泛指所有提供用户和操作系统的接口的程序，位于用户态的最底层。 可分为文本界面的命令行程序( Command LIne, aka CLI )和图形界面程序( Graphic User Interface, aka GUI )。 一般来说，“Shell”指的都是CLI The user interface program, shell or GUI, is the lowest level of user-mode software, and allows the user to start other programs --- 拙著 OS Introduction 从实现上讲，Shell程序就是一个解释器，它接受的语言就被称为Shell语言( Shell Script)。 用户将Shell语言输入给Shell， 后者执行命令。 注意，不同的Shell实现，也会形成一些自己的方言。不过一般不用考虑那么多，只要了解其中最标准的子集即可 Linux中有多种shell，默认使用的是Bash shell categories shell名称 描述 位置 ash 一个小的shell，但与bash完全兼容 /bin/ash ash.static 一个不依靠软件库的ash版本 /bin/ash.static bin ash的一个符号链接 /bin/ash bash “Bourne Again Shell”。由GNU开发，保持了对 sh shell 的(大部分)兼容性，是各种 Linux 发行版默认配置的 shell /bin/bash sh Bourne shell，是 UNIX 上的标准 shell /bin/sh ksh Korn Shell /bin/ksh csh C shell, 该shell的语法有点类似C语言 /bin/csh tcsh csh 的增强版，加入了命令补全功能，提供了更加强大的语法支持 /bin/tcsh 在现代的 Linux 上，sh 已经被 bash 代替。 sh在Linux中是bash的一个符号链接； 在mac中是一个独立的程序 当前 Linux 系统可用的 Shell 都记录在/etc/shells文件中： ❯ cat /etc/shells# /etc/shells: valid login shells/bin/sh/bin/bash/usr/bin/bash/bin/rbash/usr/bin/rbash Shell Session Shell session 是终端中当前的状态，在终端中只能有一个 session。当我们打开一个新的终端时，总会创建一个新的 shell session。 就进程间的关系来说，session 由一个或多个进程组组成。一般情况下，来自单个登录的所有进程都属于同一个 session。我们可以通过下图来理解进程、进程组和 session 之间的关系： 会话是由会话中的第一个进程创建的，一般情况下是打开终端时创建的 shell 进程。该进程也叫 session 的领头进程。Session 中领头进程的 PID 也就是 session 的 SID。我们可以通过下面的命令查看 SID： ps -o pid,ppid,pgid,sid,tty,comm PID PPID PGID SID TT COMMAND5745 5729 5745 5745 pts/4 zsh5785 1 5784 5745 pts/4 zsh5787 1 5784 5745 pts/4 zsh5789 1 5788 5745 pts/4 zsh5794 5789 5788 5745 pts/4 gitstatusd5844 5745 5844 5745 pts/4 ps Session 中的每个进程组被称为一个 job，有一个 job 会成为 session 的前台 job(foreground)，其它的 job 则是后台 job(background)。每个 session 连接一个控制终端(control terminal)，控制终端中的输入被发送给前台 job，从前台 job 产生的输出也被发送到控制终端上。同时由控制终端产生的信号，比如 ctrl + z 等都会传递给前台 job。 一般情况下 session 和终端是一对一的关系，当我们打开多个终端窗口时，实际上就创建了多个 session。 Session 的意义在于多个工作(job)在一个终端中运行，其中的一个为前台 job，它直接接收该终端的输入并把结果输出到该终端。其它的 job 则在后台运行。 Shell and Syscall 由于系统调用fork()和exec()的分离，程序可以在fork()之后，exec()之前运行代码，方便了shell的工作 shell workflow shell的workflow： 显示一个prompt,等待用户输入 用户进行输入，输入内容是一个命令，由一个可执行程序和若干参数组成 shell找到该可执行程序。调用fork()创建新进程 shell可能执行某些代码 调用exec()执行这个可执行程序 调用wait()等待该命令完成 子进程执行结束后，shell从wait()返回，继续步骤1 shell 重定向 shell实现重定向：对于 wc 5_2.c &gt; [filename].txt, wc的输出结果被重定向到[filename].txt 步骤解释： shell在fork()之后，exec()之前，会： 先用open打开文件[filename].txt, 给它分配一个文件描述符, 记为STDOUT_FILENO(一般是3，因为0，1，2都已被占用) 再关闭标准输出( fd = 1 ) shell使用dup/dup2来分配一个STDOUT_FILENO的复制，由于dup默认是分配未使用的最小的fd），此时fd=1已经关闭，所以就又分配了fd=1，它是STDOUT_FILENO的复制 shell执行指令，子进程准备输出，由于UNIX系统会从零开始寻找可用的fd(文件描述符)，因此STDOUT_FILENO会成为第一个可用的fd，作为子进程输出的目标 详见OS Persistence UNIX pipe也用类似方式实现，但使用pipe()系统调用,将前一个进程的输入作为后一个进程的输出：grep -o foo fole | wc -l Session 的创建和销毁 session的创建： 通常，新的 session 由系统登录程序创建，session 中的领头进程是运行用户登录 shell 的进程。新创建的每个进程都会属于一个进程组，当创建一个进程时，它和父进程在同一个进程组、session 中。 将进程放入不同 session 的惟一方法是使用 setsid 函数使其成为新 session 的领头进程。这还会将 session 领头进程放入一个新的进程组 session的销毁： 当 session 中的所有进程都结束时 session 也就消亡了。实际使用中比如网络断开了，session 肯定是要消亡的。 让 session 的领头进程退出。一般情况下 session 的领头进程是 shell 进程，如果它处于前台，我们可以使用 exit 命令或者是 ctrl + d 让它退出。或者我们可以直接通过 kill 命令杀死 session 的领头进程。 原理是：当系统检测到挂断(hangup)条件时，内核中的驱动会将 SIGHUP 信号发送到整个 session。通常情况下，这会杀死 session 中的所有进程 session 与终端的关系： 如果 session 关联的是伪终端，这个伪终端本身就是随着 session 的建立而创建的，session 结束，那么这个伪终端也会被销毁。 打开终端，会话开始；关闭终端，会话结束，会话内部的进程也会随之终止，不管有没有运行完。 一个典型的例子就是，SSH 登录远程计算机，打开一个远程终端执行命令。这时，网络突然断线，再次登录的时候，是找不回上一次执行的命令的。因为上一次 SSH 会话已经终止了，里面的进程也随之消失了 为了解决这个问题，会话与窗口可以&quot;解绑&quot;：窗口关闭时，会话并不终止，而是继续运行，等到以后需要的时候，再让会话&quot;绑定&quot;其他窗口（见“ Terminal Multiplexer”） 如果 session 关联的是 tty1-6，tty 则不会被销毁。因为该终端设备是在系统初始化的时候创建的，并不是依赖该会话建立的，所以当 session 退出，tty 仍然存在。只是 init 系统在 session 结束后，会重启 getty 来监听这个 tty User Environment .bash_profile, .bash_logout, .bashrc files .bash_profile: 用户登录时被读取，其中包含的命令被bash执行 .bashrc: 启动一个新的shell时读取并执行 .bash_logout: 登录退出时读取执行 Linux shell When you launch your terminal, you will see a prompt that often looks a little like this: missing:~$ 主机名: missing 当前工作目录: ~ (short for “home”). The $ tells you that you are not the root user \\ The most basic command is to execute a program with arguments: echo hello shell对命令的解析: 按空格分隔 第一个单词是命令名字,以后的每个单词都是命令的参数 If you want to provide an argument that contains spaces or other special characters (e.g., a directory named “My Photos”), you can either quote the argument with ' or &quot; (&quot;My Photos&quot;), or escape just the relevant characters with \\ (My\\ Photos) PATH shell变量分为用户变量和环境变量。 $PATH是特殊的环境变量， 当shell执行一条和其所有关键字都不匹配的命令时,它会查询 $PATH , 它列出了所有shell在查询命令时应查找的目录( 以 :分割 ) missing:~$ echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binmissing:~$ which echo/bin/echomissing:~$ /bin/echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin We can also bypass $PATH entirely by giving the path to the file we want to execute. Navigating in the shell 根目录: / is the “root” of the file system A path that starts with / is called an absolute path. Any other path is a relative path. Relative paths are relative to the current working directory, which we can see with the pwd command In a path, . refers to the current directory, and .. to its parent directory: Most commands accept flags and options (flags with values) that start with - to modify their behavior 文件权限： File的权限与人们的直觉相符， 而对于Directory: To enter a directory, a user must have “search” (represented by “execute”: x) To list its contents, a user must have read (r) permissions on that directory. Connecting programs &lt;, &gt;重定向（覆盖原文件内容） &lt;&lt;, &gt;&gt; append模式 root su： 切换到root sudo CMD： 以root权限执行CMD, 注意只有第一条CMD具有root权限 One thing you need to be root in order to do is writing to the sysfs file system mounted under /sys. sysfs exposes a number of kernel parameters as files, so that you can easily reconfigure the kernel on the fly without specialized tools. Note that sysfs does not exist on Windows or macOS. For example, the brightness of your laptop’s screen is exposed through a file called brightness under /sys/class/backlight By writing a value into that file, we can change the screen brightness. Your first instinct might be to do something like: $ sudo find -L /sys/class/backlight -maxdepth 2 -name &#x27;*brightness*&#x27;/sys/class/backlight/thinkpad_screen/brightness$ cd /sys/class/backlight/thinkpad_screen$ sudo echo 3 &gt; brightnessAn error occurred while redirecting file &#x27;brightness&#x27;open: Permission denied 注意：|, &gt;, and &lt; 这些操作符是由shell提供的， echo这些程序只接受输入、进行输出， 不知道这些操作符的存在， 因此shell首先用root权限运行echo 3， 再用默认权限（即shell的创建者）的权限打开brightness文件尝试写入， 而这需要root权限， 因此fail Using this knowledge, we can work around this: echo 3 | sudo tee brightness Command Parameters Linux Command的命令行参数可分为长参数（long option）和短参数（short option），长参数形式类似于--print-something， 以--开头，单词间以-分开；短参数形式类似于-p，以-开头，后面跟单词缩写 短参数 在短参数中，字母的大写效果是不同的，比如大写 T 和小写 t的含义通常不同 command -p 短参数赋值： command -p 10 多个短参数，可以用空格隔开: command -a -b -c -d 多个短参数也可以合并在一起： command -abcd 长参数 格式： command --parameter 长参数赋值： command --parameter=10 多个长参数，不能像多个短参数那样合并， 只能以空格隔开： command --parameter1 --parameter2 可以组合使用短参数和长参数 例如： command -abcd --parameter1 --parameter2 Windows shell windows中：命令和文件名(包括文件类型)不区分大小写. 即 cd 和 CD 一样， dir 和 DIR 一样。 Desktop 和 desktop 一样, *.pdf 和 *.PDF 一样 TAB键： 自动补完。 不区分大小写 cd ： change directory 改变目录 cd .. : go back to upper directory 回到上一级目录， 注意cd 和 .. 中间也可以没有空格 cd ../.. : 回到上两级目录 cd \\ : 回到根目录 exit: 退出 dir ： list the contents of current directory 浏览当前目录（结果按字母顺序排序）. 文件有代表它也是个目录 dir Desktop\\SE 查看SE的内容， 不会改变当前目录 dir Desktop\\SE\\*.pdf 遍历并列出SE中以.pdf结尾的文件 注意文件名是区分空格的，所以dir Desktop \\SE 或 dir Desktop\\ SE 找不到东西 dir /a: show hidden directories as well 注意dir 和 /a 中间也可以没有空格 cls： clear your screen ↑键 ：access your command history /? : to access help and options menu mkdir 制作目录 rmdir 删除一个空的目录 rmdir *** /s 删除一个有内容的目录 Home键： bring u to to the beginning of the command End键：反之 Ctrl+left : 一次左移一个词 —————————————————————————————————— echo not sweet &gt; apple.txt 向 apple.txt中添加not sweet（这会覆盖txt的原内容） 如果不想覆盖，只想append， 那就用echo not sweet &gt;&gt; apple.txt Desktop\\test&gt; dir &gt; apple.txt 向apple.txt中添加test的目录 总之 Desktop\\test&gt; *** &gt; apple.txt 星号部分是向apple.txt中添加的内容，可以是文字，可以是命令， 说到命令，当然·ttrib也可以，cls也可以。不过把cls append进去很蠢，啥都不会发生。 ————————————————————————————————— type 文件名 ： 在CMD中打印该文件的内容 remove 被移动的 移动到的 ： 顾名思义，当然，连空文件和空目录也会移动 rename 原名 新名字 ： 注意文件类型也在名字里，所以一个 XX.txt如果更名为YY(不带.txt)，那么XX会从一个文本文档变成一个文件夹 ————————————————————————————————— copy 要被复制的 复制到的 ： 文件复制 xcopy: 也是复制，但比copy更好（功能更多） 。 xcopy默认只会复制source中的文件，而不包括目录（copy也是如此）。 通过/S 可以让xcopy复制目录和子目录，不包括空目录。， 但copy没有/S的功能，也就是说copy无法复制目录。 要打开应用程序，要么转到对应的目录，然后 start ** .exe, 要么直接用双引号括住绝对路径 C:\\Users\\color 0B 数字是背景颜色，字母是foreground or text wmic logicaldisk get name : see all the available command drives tree关键字： 以树状列出目录及其内部目录… D: 转到D盘根路径","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[{"name":"shell","slug":"shell","permalink":"http://lyk-love.cn/tags/shell/"}]},{"title":"Python Basic","slug":"Python Basic","date":"2022-09-26T06:39:34.937Z","updated":"2022-09-26T06:39:34.937Z","comments":true,"path":"2022/09/26/Python Basic/","link":"","permalink":"http://lyk-love.cn/2022/09/26/Python%20Basic/","excerpt":"Outline: Basic Basic Data Types Function Advanced Features OOP Error Handling","text":"Outline: Basic Basic Data Types Function Advanced Features OOP Error Handling Basic ref: 廖雪峰的教程 Data Types Python3 中有六个标准的数据类型： Number（数字） String（字符串）, Python中没有字符类型 List（列表） Tuple（元组） Set（集合） Dictionary（字典） Python3 的六个标准数据类型中： 不可变数据类型：Number, String, Tuple 可变数据类型：List, Dictionary, Set string Python的字符串类型是str，是不可变的. 在内存中以Unicode表示，一个字符对应若干个字节. 如果要在网络上传输，或者保存到磁盘上，就需要把str变为以字节为单位的bytes Python对bytes类型的数据用带b前缀的单引号或双引号表示： x = b&#x27;ABC&#x27; 要注意区分'ABC'和b'ABC'，前者是str，后者虽然内容显示得和前者一样，但bytes的每个字符都只占用一个字节。 编码 char -&gt; ascii int: ord() ascii int -&gt; char: char() str -&gt; ascii: text = input(&quot;enter a string to convert into ascii values:&quot;)ascii_values = []for character in text: ascii_values.append(ord(character)) Or: 列表生成式 def to_ascii(text): ascii_values = [ord(character) for character in text] return ascii_values 对于单个字符的编码，Python提供了ord()函数获取字符的ascii整数表示，chr()函数把ascii编码转换为对应的字符： &gt;&gt;&gt; ord(&#x27;A&#x27;)65&gt;&gt;&gt; ord(&#x27;中&#x27;)20013&gt;&gt;&gt; chr(66)&#x27;B&#x27;&gt;&gt;&gt; chr(25991)&#x27;文&#x27; 字符串格式化 使用字符串的format()方法，它会用传入的参数依次替换字符串内的占位符&#123;0&#125;、&#123;1&#125;……，不过这种方式写起来比%要麻烦得多： &gt;&gt;&gt; &#x27;Hello, &#123;0&#125;, 成绩提升了 &#123;1:.1f&#125;%&#x27;.format(&#x27;小明&#x27;, 17.125)&#x27;Hello, 小明, 成绩提升了 17.1%&#x27; Number Python3 Number类型的子类型有: int, float, bool, complex( 复数 ) int Python不存在整数溢出, 如果数字过大就会自动转换成大整数类型计算. 非常方便 十六进制: hex(): 这个函数会把int转成它的十六进制表示, 类型是str float bool complex List 反转列表: list.reverse() 该方法没有返回值, 但是会对列表的元素进行反向排序 str -&gt; list: list(&quot;hahaha&quot;) list -&gt; str: &#x27;&#x27;.join(list, &#x27; &#x27;) Tuple Set Dictionary Operators Py内置了**运算符来求幂次 pow(a,n): 求a的n次幂, 如果a和n都是int,则返回int math.pow(a,n): 求a的n次幂, 但是会把参数转成浮点数进行运算, 浮点运算是会出错的. 对于比较大的数字, 肯定会出错, 所以不要用math.pow() &gt;&gt;&gt; import math&gt;&gt;&gt; 17 ** 127 % 120113&gt;&gt;&gt; math.pow(17,127) % 120 //出错了96.0&gt;&gt;&gt; pow(17,127) % 120113 Slice 取一个list或tuple的部分元素是非常常见的操作。比如，一个list如下： &gt;&gt;&gt; L = [&#x27;Michael&#x27;, &#x27;Sarah&#x27;, &#x27;Tracy&#x27;, &#x27;Bob&#x27;, &#x27;Jack&#x27;] 取前3个元素，应该怎么做？ 笨办法： &gt;&gt;&gt; [L[0], L[1], L[2]][&#x27;Michael&#x27;, &#x27;Sarah&#x27;, &#x27;Tracy&#x27;] 之所以是笨办法是因为扩展一下，取前N个元素就没辙了。 取前N个元素，也就是索引为0-(N-1)的元素，可以用循环： &gt;&gt;&gt; r = []&gt;&gt;&gt; n = 3&gt;&gt;&gt; for i in range(n):... r.append(L[i])... &gt;&gt;&gt; r[&#x27;Michael&#x27;, &#x27;Sarah&#x27;, &#x27;Tracy&#x27;] 对这种经常取指定索引范围的操作，用循环十分繁琐，因此，Python提供了切片（Slice）操作符，能大大简化这种操作。 对应上面的问题，取前3个元素，用一行代码就可以完成切片： &gt;&gt;&gt; L[0:3][&#x27;Michael&#x27;, &#x27;Sarah&#x27;, &#x27;Tracy&#x27;] L[0:3]表示，从索引0开始取，直到索引3为止，但不包括索引3。即索引0，1，2，正好是3个元素。 如果第一个索引是0，还可以省略： &gt;&gt;&gt; L[:3][&#x27;Michael&#x27;, &#x27;Sarah&#x27;, &#x27;Tracy&#x27;] 也可以从索引1开始，取出2个元素出来： &gt;&gt;&gt; L[1:3][&#x27;Sarah&#x27;, &#x27;Tracy&#x27;] 类似的，既然Python支持L[-1]取倒数第一个元素，那么它同样支持倒数切片，试试： &gt;&gt;&gt; L[-2:][&#x27;Bob&#x27;, &#x27;Jack&#x27;]&gt;&gt;&gt; L[-2:-1] // 不包括最后一个元素[&#x27;Bob&#x27;] 记住倒数第一个元素的索引是-1。 注: 由于倒数第一个元素索引为-1,因此正数第一个元素索引为-len(L),也就是说,L[-len(L):] 可以复制一个list,等价于L[0:len(L)],即L[:]. 注意,负数访问法最右边只到-1,意味着不能像正数访问一样通过不存在的下标len(L)来完整切片. 想要完整切片,只能用缺省的方式. 切片操作十分有用。我们先创建一个0-99的数列： &gt;&gt;&gt; L = list(range(100))&gt;&gt;&gt; L[0, 1, 2, 3, ..., 99] 可以通过切片轻松取出某一段数列。比如前10个数： &gt;&gt;&gt; L[:10][0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 后10个数： &gt;&gt;&gt; L[-10:][90, 91, 92, 93, 94, 95, 96, 97, 98, 99] 前11-20个数： &gt;&gt;&gt; L[10:20][10, 11, 12, 13, 14, 15, 16, 17, 18, 19] 前10个数，每两个取一个： &gt;&gt;&gt; L[:10:2][0, 2, 4, 6, 8] 所有数，每5个取一个： &gt;&gt;&gt; L[::5][0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95] 甚至什么都不写，只写[:]就可以原样复制一个list： &gt;&gt;&gt; L[:][0, 1, 2, 3, ..., 99] tuple也是一种list，唯一区别是tuple不可变。因此，tuple也可以用切片操作，只是操作的结果仍是tuple： &gt;&gt;&gt; (0, 1, 2, 3, 4, 5)[:3](0, 1, 2) 字符串'xxx'也可以看成是一种list，每个元素就是一个字符。因此，字符串也可以用切片操作，只是操作结果仍是字符串： &gt;&gt;&gt; &#x27;ABCDEFG&#x27;[:3]&#x27;ABC&#x27;&gt;&gt;&gt; &#x27;ABCDEFG&#x27;[::2]&#x27;ACEG&#x27; 在很多编程语言中，针对字符串提供了很多各种截取函数（例如，substring），其实目的就是对字符串切片。Python没有针对字符串的截取函数，只需要切片一个操作就可以完成，非常简单。 Tips: 反转list:l = [****] l = l[::-1] Encoding input DPI = int(input(&quot;DPI = &quot;) or 400 ) input()返回的是str, 对于数字要手动转成int Comments 单行注释： # 单行注释 多行注释: 用三引号( '或&quot;&quot; ) &#x27;&#x27;&#x27;使用三个连续的单/双引号分别作为注释的开头和结尾可以一次性注释多行内容或单行内容&#x27;&#x27;&#x27; 注意多行注释如果作为字符串出现，就应当看作字符串，而不是注释 条件判断 Python中, 空字符串, 空容器, 数字0都被判断为False age = 3if age &gt;= 18: print(&#x27;your age is&#x27;, age) print(&#x27;adult&#x27;)else: print(&#x27;your age is&#x27;, age) print(&#x27;teenager&#x27;) if还可以简写： if x: print(&#x27;True&#x27;) 循环 for: sum = 0for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: sum = sum + xprint(sum) range() range()生成一个整数序列, 再通过list()函数可以转换为list. 比如range(5)生成的序列是从0开始小于5的整数： &gt;&gt;&gt; list(range(5))[0, 1, 2, 3, 4] range(101)就可以生成0-100的整数序列 range( ) 可以倒序生成序列: # 从100加到1. 每次间隔-1. 也就是说100, 然后100-1 = 99, 然后99-1 = 98, 相当于递减for i in range(100,0,-1): print(i) Function 定义函数 在Python中，定义一个函数要使用def语句，依次写出函数名、括号、括号中的参数和冒号:，然后，在缩进块中编写函数体，函数的返回值用return语句返回。 我们以自定义一个求绝对值的my_abs函数为例 def my_abs(x):​ if x&gt;= 0:​ return x​ else:​ return -x 如果没有return语句，函数执行完毕后也会返回结果，只是结果为None。( Note that a return statement without a value is equivalent to return None )return None可以简写为return。 空函数 如果想定义一个什么事也不做的空函数，可以用pass语句： def nop(): pass pass语句什么都不做，那有什么用？实际上pass可以用来作为占位符，比如现在还没想好怎么写函数的代码，就可以先放一个pass，让代码能运行起来。 pass还可以用在其他语句里，比如： if age &gt;= 18: pass 缺少了pass，代码运行就会有语法错误。 参数检查 parameter形参, argument实参 调用函数时，如果参数个数不对，Python解释器会自动检查出来，并抛出TypeError： &gt;&gt;&gt; my_abs(1, 2)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: my_abs() takes 1 positional argument but 2 were given 但是如果参数类型不对，Python解释器就无法帮我们检查。试试my_abs和内置函数abs的差别： &gt;&gt;&gt; my_abs(&#x27;A&#x27;)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;&lt;stdin&gt;&quot;, line 2, in my_absTypeError: unorderable types: str() &gt;= int()&gt;&gt;&gt; abs(&#x27;A&#x27;)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: bad operand type for abs(): &#x27;str&#x27; 当传入了不恰当的参数时，内置函数abs会检查出参数错误，而我们定义的my_abs没有参数检查，会导致if语句出错，出错信息和abs不一样。所以，这个函数定义不够完善。 让我们修改一下my_abs的定义，对参数类型做检查，只允许整数和浮点数类型的参数。数据类型检查可以用内置函数isinstance()实现： def my_abs(x): if not isinstance(x, (int, float)): ## raise TypeError(&#x27;bad operand type&#x27;) if x &gt;= 0: return x else: return -x 注意isinstance() arg 2 must be a type or tuple of types,就是说(interesting,float)不能用[int,float]代替 添加了参数检查后，如果传入错误的参数类型，函数就可以抛出一个错误： &gt;&gt;&gt; my_abs(&#x27;A&#x27;)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;&lt;stdin&gt;&quot;, line 3, in my_absTypeError: bad operand type 错误和异常处理将在后续讲到。 The global statement If you want to assign a value to a name defined at the top level of the program (i.e. not inside any kind of scope such as functions or classes), then you have to tell Python that the name is not local, but it is global. We do this using the global statement. It is impossible to assign a value to a variable defined outside a function without the global statement. 在全局区域定义,在局部要使用的时候,要用global关键字 You can use the values of such variables defined outside the function (assuming there is no variable with the same name within the function). However, this is not encouraged and should be avoided since it becomes unclear to the reader of the program as to where that variable's definition is. Using the global statement makes it amply clear(显式地声明,不写会报错,解释器会将这个变量看做未定义) that the variable is defined in an outermost block. x = 50def func(): global x print(&#x27;x is&#x27;, x) x = 2 print(&#x27;Changed global x to&#x27;, x) func()print(&#x27;Value of x is&#x27;, x) The global statement is used to declare that x is a global variable - hence, when we assign a value to x inside the function, that change is reflected when we use the value of x in the main block. 在函数里改变了全局变量x,x的值是会更改的. You can specify more than one global variable using the same global statement e.g. global x, y, z . Default Argument Values Only those parameters which are at the end of the parameter list can be given default argument values i.e. you cannot have a parameter with a default argument value preceding a parameter without a default argument value in the function's parameter list. This is because the values are assigned to the parameters by position. For example, def func(a, b=5) is valid, but def func(a=5, b) is not valid. Keyword Arguments If you have some functions with many parameters and you want to specify only some of them, then you can give values for such parameters by naming them - this is called keyword arguments - we use the name (keyword) instead of the position (which we have been using all along) to specify the arguments to the function. There are two advantages - one, using the function is easier since we do not need to worry about the order of the arguments. Two, we can give values to only those parameters to which we want to, provided that the other parameters have default argument values. def func(a, b=5, c=10): print(&#x27;a is&#x27;, a, &#x27;and b is&#x27;, b, &#x27;and c is&#x27;, c)func(3, 7) func(25, c=24) func(c=50, a=100)Output:$ python function_keyword.py a is 3 and b is 7 and c is 10 a is 25 and b is 5 and c is 24 a is 100 and b is 5 and c is 50 How it works: In the second usage func(25, c=24) , the variable a gets the value of 25 due to theposition of the argument. Then, the parameter c gets the value of 24 due to naming i.e. keyword arguments. The variable b gets the default value of 5 . VarArgs parameters Sometimes you might want to define a function that can take any number of parameters, i.e. variable number of arguments, this can be achieved by using the stars. 可以传入任意数量的参数 def total(a=5, *numbers, **phonebook): p rint(&#x27;a&#x27;, a)#iterate through all the items in tuple for single_item in numbers: print(&#x27;single_item&#x27;, single_item)#iterate through all the items in dictionary for first_part, second_part in phonebook.items(): print(first_part,second_part) print(total(10,1,2,3,Jack=1123,John=2231,Inge=1560))Output:$ python function_varargs.py a 10single_item 1single_item 2 single_item 3 Inge 1560 John 2231 Jack 1123 None How it works When we declare a starred parameter such as *param , then all the positional arguments from that point till the end are collected as a tuple called 'param'. Similarly, when we declare a double-starred parameter such as **param , then all the keyword arguments from that point till the end are collected as a dictionary called 'param'. 解构赋值 Python的列表和元组支持解构赋值. def parse(): return 12,&#x27;me&#x27;x,y = parse() # 把parse()看作一个元组,默认按下标顺序赋值print(x,y)输出为: 12 me #---也可以这样:------------#x,y = parse[0],parse[1]print(x,y)输出为: 12 me 说明文档 在 Python 中可以通过 help()内置函数或者__doc__属性查看某个函数的说明文档： # 查看 print() 内置函数的说明文档print(&#x27;--- 用 help()内置函数查看说明文档 ---&#x27;)help(print)print(&#x27;--- 用 __doc__ 属性查看说明文档 ---&#x27;)print(print.__doc__) 函数的说明文档就是一段多行注释，位于函数内部、所有代码的最前面： # 定义一个比较数字大小的函数def num_max(num1, num2): &quot;&quot;&quot; 比较两个数字的大小 :param num1:形参1，数字1 :param num2:形参2，数字2 :return:大的数字，max_num = num1 if num1 &gt; num2 else num2 &quot;&quot;&quot; max_num = num1 if num1 &gt; num2 else num2 return max_num Advanced Features 迭代 如果给定一个list或tuple，我们可以通过for循环来遍历这个list或tuple，这种遍历我们称为迭代（Iteration）。 在Python中，迭代是通过for ... in来完成的，而很多语言比如C语言，迭代list是通过下标完成的，比如Java代码： for (i=0; i&lt;list.length; i++) &#123; n = list[i];&#125; 可以看出，Python的for循环抽象程度要高于C的for循环，因为Python的for循环不仅可以用在list或tuple上，还可以作用在其他可迭代对象上。 list这种数据类型虽然有下标，但很多其他数据类型是没有下标的，但是，只要是可迭代对象，无论有无下标，都可以迭代，比如dict就可以迭代： &gt;&gt;&gt; d = &#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 2, &#x27;c&#x27;: 3&#125;&gt;&gt;&gt; for key in d:... print(key)...acb 因为dict的存储不是按照list的方式顺序排列，所以，迭代出的结果顺序很可能不一样。 默认情况下，dict迭代的是key。如果要迭代value，可以用for value in d.values()，如果要同时迭代key和value，可以用for k, v in d.items()。 由于字符串也是可迭代对象，因此，也可以作用于for循环： &gt;&gt;&gt; for ch in &#x27;ABC&#x27;:... print(ch)...ABC 所以，当我们使用for循环时，只要作用于一个可迭代对象，for循环就可以正常运行，而我们不太关心该对象究竟是list还是其他数据类型。 那么，如何判断一个对象是可迭代对象呢？方法是通过collections模块的Iterable类型判断： &gt;&gt;&gt; from collections import Iterable&gt;&gt;&gt; isinstance(&#x27;abc&#x27;, Iterable) # str是否可迭代True&gt;&gt;&gt; isinstance([1,2,3], Iterable) # list是否可迭代True&gt;&gt;&gt; isinstance(123, Iterable) # 整数是否可迭代False 最后一个小问题，如果要对list实现类似Java那样的下标循环怎么办？Python内置的enumerate函数可以把一个list变成索引-元素对，这样就可以在for循环中同时迭代索引和元素本身： &gt;&gt;&gt; for i, value in enumerate([&#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;]):... print(i, value)...0 A1 B2 C 上面的for循环里，同时引用了两个变量，在Python里是很常见的，比如下面的代码： &gt;&gt;&gt; for x, y in [(1, 1), (2, 4), (3, 9)]:... print(x, y)...1 12 43 9 列表生成式 列表生成式即List Comprehensions，是Python内置的非常简单却强大的可以用来创建list的生成式。 举个例子，要生成list [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]可以用list(range(1, 11))： &gt;&gt;&gt; list(range(1, 11))[1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 但如果要生成[1x1, 2x2, 3x3, ..., 10x10]怎么做？方法一是循环： &gt;&gt;&gt; L = []&gt;&gt;&gt; for x in range(1, 11):... L.append(x * x)...&gt;&gt;&gt; L[1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 但是循环太繁琐，而列表生成式则可以用一行语句代替循环生成上面的list： &gt;&gt;&gt; [x * x for x in range(1, 11)][1, 4, 9, 16, 25, 36, 49, 64, 81, 100] 写列表生成式时，把要生成的元素x * x放到前面，后面跟for循环，就可以把list创建出来，十分有用，多写几次，很快就可以熟悉这种语法。 for循环后面还可以加上if判断，这样我们就可以筛选出仅偶数的平方： &gt;&gt;&gt; [x * x for x in range(1, 11) if x % 2 == 0][4, 16, 36, 64, 100] 还可以使用两层循环，可以生成全排列： &gt;&gt;&gt; [m + n for m in &#x27;ABC&#x27; for n in &#x27;XYZ&#x27;][&#x27;AX&#x27;, &#x27;AY&#x27;, &#x27;AZ&#x27;, &#x27;BX&#x27;, &#x27;BY&#x27;, &#x27;BZ&#x27;, &#x27;CX&#x27;, &#x27;CY&#x27;, &#x27;CZ&#x27;] 三层和三层以上的循环就很少用到了。 运用列表生成式，可以写出非常简洁的代码。例如，列出当前目录下的所有文件和目录名，可以通过一行代码实现： &gt;&gt;&gt; import os # 导入os模块，模块的概念后面讲到&gt;&gt;&gt; [d for d in os.listdir(&#x27;.&#x27;)] # os.listdir可以列出文件和目录[&#x27;.emacs.d&#x27;, &#x27;.ssh&#x27;, &#x27;.Trash&#x27;, &#x27;Adlm&#x27;, &#x27;Applications&#x27;, &#x27;Desktop&#x27;, &#x27;Documents&#x27;, &#x27;Downloads&#x27;, &#x27;Library&#x27;, &#x27;Movies&#x27;, &#x27;Music&#x27;, &#x27;Pictures&#x27;, &#x27;Public&#x27;, &#x27;VirtualBox VMs&#x27;, &#x27;Workspace&#x27;, &#x27;XCode&#x27;] for循环其实可以同时使用两个甚至多个变量，比如dict的items()可以同时迭代key和value： &gt;&gt;&gt; d = &#123;&#x27;x&#x27;: &#x27;A&#x27;, &#x27;y&#x27;: &#x27;B&#x27;, &#x27;z&#x27;: &#x27;C&#x27; &#125;&gt;&gt;&gt; for k, v in d.items():... print(k, &#x27;=&#x27;, v)...y = Bx = Az = C 因此，列表生成式也可以使用两个变量来生成list： &gt;&gt;&gt; d = &#123;&#x27;x&#x27;: &#x27;A&#x27;, &#x27;y&#x27;: &#x27;B&#x27;, &#x27;z&#x27;: &#x27;C&#x27; &#125;&gt;&gt;&gt; [k + &#x27;=&#x27; + v for k, v in d.items()][&#x27;y=B&#x27;, &#x27;x=A&#x27;, &#x27;z=C&#x27;] 最后把一个list中所有的字符串变成小写： &gt;&gt;&gt; L = [&#x27;Hello&#x27;, &#x27;World&#x27;, &#x27;IBM&#x27;, &#x27;Apple&#x27;]&gt;&gt;&gt; [s.lower() for s in L][&#x27;hello&#x27;, &#x27;world&#x27;, &#x27;ibm&#x27;, &#x27;apple&#x27;] if ... else 使用列表生成式的时候，有些童鞋经常搞不清楚if...else的用法。 例如，以下代码正常输出偶数： &gt;&gt;&gt; [x for x in range(1, 11) if x % 2 == 0][2, 4, 6, 8, 10] 但是，我们不能在最后的if加上else： &gt;&gt;&gt; [x for x in range(1, 11) if x % 2 == 0 else 0] File &quot;&lt;stdin&gt;&quot;, line 1 [x for x in range(1, 11) if x % 2 == 0 else 0] ^SyntaxError: invalid syntax 这是因为跟在for后面的if是一个筛选条件，不能带else，否则如何筛选？ 另一些童鞋发现把if写在for前面必须加else，否则报错： &gt;&gt;&gt; [x if x % 2 == 0 for x in range(1, 11)] File &quot;&lt;stdin&gt;&quot;, line 1 [x if x % 2 == 0 for x in range(1, 11)] ^SyntaxError: invalid syntax 这是因为for前面的部分是一个表达式，它必须根据x计算出一个结果。因此，考察表达式：x if x % 2 == 0，它无法根据x计算出结果，因为缺少else，必须加上else： &gt;&gt;&gt; [x if x % 2 == 0 else -x for x in range(1, 11)][-1, 2, -3, 4, -5, 6, -7, 8, -9, 10] 上述for前面的表达式x if x % 2 == 0 else -x才能根据x计算出确定的结果。 可见，在一个列表生成式中，for前面的if ... else是表达式，而for后面的if是过滤条件，不能带else。 练习 如果list中既包含字符串，又包含整数，由于非字符串类型没有lower()方法，所以列表生成式会报错： &gt;&gt;&gt; L = [&#x27;Hello&#x27;, &#x27;World&#x27;, 18, &#x27;Apple&#x27;, None]&gt;&gt;&gt; [s.lower() for s in L]Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt; File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;listcomp&gt;AttributeError: &#x27;int&#x27; object has no attribute &#x27;lower&#x27; 使用内建的isinstance函数可以判断一个变量是不是字符串： &gt;&gt;&gt; x = &#x27;abc&#x27;&gt;&gt;&gt; y = 123&gt;&gt;&gt; isinstance(x, str)True&gt;&gt;&gt; isinstance(y, str)False 生成器 通过列表生成式，我们可以直接创建一个列表。但是，受到内存限制，列表容量肯定是有限的。而且，创建一个包含100万个元素的列表，不仅占用很大的存储空间，如果我们仅仅需要访问前面几个元素，那后面绝大多数元素占用的空间都白白浪费了。 所以，如果列表元素可以按照某种算法推算出来，那我们是否可以在循环的过程中不断推算出后续的元素呢？这样就不必创建完整的list，从而节省大量的空间。在Python中，这种一边循环一边计算的机制，称为生成器：generator。 要创建一个generator，有很多种方法。第一种方法很简单，只要把一个列表生成式的[]改成()，就创建了一个generator： &gt;&gt;&gt; L = [x * x for x in range(10)]&gt;&gt;&gt; L[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]&gt;&gt;&gt; g = (x * x for x in range(10))&gt;&gt;&gt; g&lt;generator object &lt;genexpr&gt; at 0x1022ef630&gt; 创建L和g的区别仅在于最外层的[]和()，L是一个list，而g是一个generator。 我们可以直接打印出list的每一个元素，但我们怎么打印出generator的每一个元素呢？ 如果要一个一个打印出来，可以通过next()函数获得generator的下一个返回值： &gt;&gt;&gt; next(g)0&gt;&gt;&gt; next(g)1&gt;&gt;&gt; next(g)4&gt;&gt;&gt; next(g)9&gt;&gt;&gt; next(g)16&gt;&gt;&gt; next(g)25&gt;&gt;&gt; next(g)36&gt;&gt;&gt; next(g)49&gt;&gt;&gt; next(g)64&gt;&gt;&gt; next(g)81&gt;&gt;&gt; next(g)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;StopIteration 我们讲过，generator保存的是算法，每次调用next(g)，就计算出g的下一个元素的值，直到计算到最后一个元素，没有更多的元素时，抛出StopIteration的错误。 当然，上面这种不断调用next(g)实在是太变态了，正确的方法是使用for循环，因为generator也是可迭代对象： &gt;&gt;&gt; g = (x * x for x in range(10))&gt;&gt;&gt; for n in g:... print(n)... 0149162536496481 所以，我们创建了一个generator后，基本上永远不会调用next()，而是通过for循环来迭代它，并且不需要关心StopIteration的错误。 generator非常强大。如果推算的算法比较复杂，用类似列表生成式的for循环无法实现的时候，还可以用函数来实现。 比如，著名的斐波拉契数列（Fibonacci），除第一个和第二个数外，任意一个数都可由前两个数相加得到： 1, 1, 2, 3, 5, 8, 13, 21, 34, ... 斐波拉契数列用列表生成式写不出来，但是，用函数把它打印出来却很容易： def fib(max): n, a, b = 0, 0, 1 while n &lt; max: print(b) a, b = b, a + b n = n + 1 return &#x27;done&#x27; 注意，赋值语句： a, b = b, a + b 相当于： t = (b, a + b) # t是一个tuplea = t[0]b = t[1] 但不必显式写出临时变量t就可以赋值。 上面的函数可以输出斐波那契数列的前N个数： &gt;&gt;&gt; fib(6)112358&#x27;done&#x27; 仔细观察，可以看出，fib函数实际上是定义了斐波拉契数列的推算规则，可以从第一个元素开始，推算出后续任意的元素，这种逻辑其实非常类似generator。 也就是说，上面的函数和generator仅一步之遥。要把fib函数变成generator，只需要把print(b)改为yield b就可以了： def fib(max): n, a, b = 0, 0, 1 while n &lt; max: yield b a, b = b, a + b n = n + 1 return &#x27;done&#x27; 这就是定义generator的另一种方法。如果一个函数定义中包含yield关键字，那么这个函数就不再是一个普通函数，而是一个generator： &gt;&gt;&gt; f = fib(6)&gt;&gt;&gt; f&lt;generator object fib at 0x104feaaa0&gt; 这里，最难理解的就是generator和函数的执行流程不一样。函数是顺序执行，遇到return语句或者最后一行函数语句就返回。而变成generator的函数，在每次调用next()的时候执行，遇到yield语句返回，再次执行时从上次返回的yield语句处继续执行。 举个简单的例子，定义一个generator，依次返回数字1，3，5： def odd(): print(&#x27;step 1&#x27;) yield 1 print(&#x27;step 2&#x27;) yield(3) print(&#x27;step 3&#x27;) yield(5) 调用该generator时，首先要生成一个generator对象，然后用next()函数不断获得下一个返回值： &gt;&gt;&gt; o = odd()&gt;&gt;&gt; next(o)step 11&gt;&gt;&gt; next(o)step 23&gt;&gt;&gt; next(o)step 35&gt;&gt;&gt; next(o)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;StopIteration 可以看到，odd不是普通函数，而是generator，在执行过程中，遇到yield就中断，下次又继续执行。执行3次yield后，已经没有yield可以执行了，所以，第4次调用next(o)就报错。 回到fib的例子，我们在循环过程中不断调用yield，就会不断中断。当然要给循环设置一个条件来退出循环，不然就会产生一个无限数列出来。 同样的，把函数改成generator后，我们基本上从来不会用next()来获取下一个返回值，而是直接使用for循环来迭代： &gt;&gt;&gt; for n in fib(6):... print(n)...112358 但是用for循环调用generator时，发现拿不到generator的return语句的返回值。如果想要拿到返回值，必须捕获StopIteration错误，返回值包含在StopIteration的value中： &gt;&gt;&gt; g = fib(6)&gt;&gt;&gt; while True:... try:... x = next(g)... print(&#x27;g:&#x27;, x)... except StopIteration as e:... print(&#x27;Generator return value:&#x27;, e.value)... break...g: 1g: 1g: 2g: 3g: 5g: 8Generator return value: done 迭代器 我们已经知道，可以直接作用于for循环的数据类型有以下几种： 一类是集合数据类型，如list、tuple、dict、set、str等； 一类是generator，包括生成器和带yield的generator function。 这些可以直接作用于for循环的对象统称为可迭代对象：Iterable。 可以使用isinstance()判断一个对象是否是Iterable对象： &gt;&gt;&gt; from collections.abc import Iterable&gt;&gt;&gt; isinstance([], Iterable)True&gt;&gt;&gt; isinstance(&#123;&#125;, Iterable)True&gt;&gt;&gt; isinstance(&#x27;abc&#x27;, Iterable)True&gt;&gt;&gt; isinstance((x for x in range(10)), Iterable)True&gt;&gt;&gt; isinstance(100, Iterable)False 而生成器不但可以作用于for循环，还可以被next()函数不断调用并返回下一个值，直到最后抛出StopIteration错误表示无法继续返回下一个值了。 可以被next()函数调用并不断返回下一个值的对象称为迭代器：Iterator。 可以使用isinstance()判断一个对象是否是Iterator对象： &gt;&gt;&gt; from collections.abc import Iterator&gt;&gt;&gt; isinstance((x for x in range(10)), Iterator)True&gt;&gt;&gt; isinstance([], Iterator)False&gt;&gt;&gt; isinstance(&#123;&#125;, Iterator)False&gt;&gt;&gt; isinstance(&#x27;abc&#x27;, Iterator)False 生成器都是Iterator对象，但list、dict、str虽然是Iterable，却不是Iterator。 把list、dict、str等Iterable变成Iterator可以使用iter()函数： &gt;&gt;&gt; isinstance(iter([]), Iterator)True&gt;&gt;&gt; isinstance(iter(&#x27;abc&#x27;), Iterator)True 你可能会问，为什么list、dict、str等数据类型不是Iterator？ 这是因为Python的Iterator对象表示的是一个数据流，Iterator对象可以被next()函数调用并不断返回下一个数据，直到没有数据时抛出StopIteration错误。可以把这个数据流看做是一个有序序列，但我们却不能提前知道序列的长度，只能不断通过next()函数实现按需计算下一个数据，所以Iterator的计算是惰性的，只有在需要返回下一个数据时它才会计算。 Iterator甚至可以表示一个无限大的数据流，例如全体自然数。而使用list是永远不可能存储全体自然数的。 小结 凡是可作用于for循环的对象都是Iterable类型； 凡是可作用于next()函数的对象都是Iterator类型，它们表示一个惰性计算的序列； 集合数据类型如list、dict、str等是Iterable但不是Iterator，不过可以通过iter()函数获得一个Iterator对象。 Python的for循环本质上就是通过不断调用next()函数实现的，例如： for x in [1, 2, 3, 4, 5]: pass 实际上完全等价于： # 首先获得Iterator对象:it = iter([1, 2, 3, 4, 5])# 循环:while True: try: # 获得下一个值: x = next(it) except StopIteration: # 遇到StopIteration就退出循环 break OOP 面向对象编程——Object Oriented Programming，简称OOP，是一种程序设计思想。OOP把对象作为程序的基本单元，一个对象包含了数据和操作数据的函数。 面向过程的程序设计把计算机程序视为一系列的命令集合，即一组函数的顺序执行。为了简化程序设计，面向过程把函数继续切分为子函数，即把大块函数通过切割成小块函数来降低系统的复杂度。 而面向对象的程序设计把计算机程序视为一组对象的集合，而每个对象都可以接收其他对象发过来的消息，并处理这些消息，计算机程序的执行就是一系列消息在各个对象之间传递。 在Python中，所有数据类型都可以视为对象，当然也可以自定义对象。自定义的对象数据类型就是面向对象中的类（Class）的概念。 我们以一个例子来说明面向过程和面向对象在程序流程上的不同之处。 假设我们要处理学生的成绩表，为了表示一个学生的成绩，面向过程的程序可以用一个dict表示： std1 = &#123; &#x27;name&#x27;: &#x27;Michael&#x27;, &#x27;score&#x27;: 98 &#125;std2 = &#123; &#x27;name&#x27;: &#x27;Bob&#x27;, &#x27;score&#x27;: 81 &#125; 而处理学生成绩可以通过函数实现，比如打印学生的成绩： def print_score(std): print(&#x27;%s: %s&#x27; % (std[&#x27;name&#x27;], std[&#x27;score&#x27;])) 如果采用面向对象的程序设计思想，我们首选思考的不是程序的执行流程，而是Student这种数据类型应该被视为一个对象，这个对象拥有name和score这两个属性（Property）。如果要打印一个学生的成绩，首先必须创建出这个学生对应的对象，然后，给对象发一个print_score消息，让对象自己把自己的数据打印出来。 class Student(object): def __init__(self, name, score): self.name = name self.score = score def print_score(self): print(&#x27;%s: %s&#x27; % (self.name, self.score)) 给对象发消息实际上就是调用对象对应的关联函数，我们称之为对象的方法（Method）。面向对象的程序写出来就像这样： bart = Student(&#x27;Bart Simpson&#x27;, 59)lisa = Student(&#x27;Lisa Simpson&#x27;, 87)bart.print_score()lisa.print_score() 面向对象的设计思想是从自然界中来的，因为在自然界中，类（Class）和实例（Instance）的概念是很自然的。Class是一种抽象概念，比如我们定义的Class——Student，是指学生这个概念，而实例（Instance）则是一个个具体的Student，比如，Bart Simpson和Lisa Simpson是两个具体的Student。 所以，面向对象的设计思想是抽象出Class，根据Class创建Instance。 面向对象的抽象程度又比函数要高，因为一个Class既包含数据，又包含操作数据的方法。 类和实例 面向对象最重要的概念就是类（Class）和实例（Instance），必须牢记类是抽象的模板，比如Student类，而实例是根据类创建出来的一个个具体的“对象”，每个对象都拥有相同的方法，但各自的数据可能不同。 仍以Student类为例，在Python中，定义类是通过class关键字： class Student(object): pass class后面紧接着是类名，即Student，类名通常是大写开头的单词，紧接着是(object)，表示该类是从哪个类继承下来的，继承的概念我们后面再讲，通常，如果没有合适的继承类，就使用object类，这是所有类最终都会继承的类。 定义好了Student类，就可以根据Student类创建出Student的实例，创建实例是通过类名+()实现的： &gt;&gt;&gt; bart = Student()&gt;&gt;&gt; bart&lt;__main__.Student object at 0x10a67a590&gt;&gt;&gt;&gt; Student&lt;class &#x27;__main__.Student&#x27;&gt; 可以看到，变量bart指向的就是一个Student的实例，后面的0x10a67a590是内存地址，每个object的地址都不一样，而Student本身则是一个类。 可以自由地给一个实例变量绑定属性，比如，给实例bart绑定一个name属性： &gt;&gt;&gt; bart.name = &#x27;Bart Simpson&#x27;&gt;&gt;&gt; bart.name&#x27;Bart Simpson&#x27; 由于类可以起到模板的作用，因此，可以在创建实例的时候，把一些我们认为必须绑定的属性强制填写进去。通过定义一个特殊的__init__方法，在创建实例的时候，就把name，score等属性绑上去： class Student(object): def __init__(self, name, score): self.name = name self.score = score 注意：特殊方法“init”前后分别有两个下划线！！！ 注意到__init__方法的第一个参数永远是self，表示创建的实例本身，因此，在__init__方法内部，就可以把各种属性绑定到self，因为self就指向创建的实例本身。 有了__init__方法，在创建实例的时候，就不能传入空的参数了，必须传入与__init__方法匹配的参数，但self不需要传，Python解释器自己会把实例变量传进去： &gt;&gt;&gt; bart = Student(&#x27;Bart Simpson&#x27;, 59)&gt;&gt;&gt; bart.name&#x27;Bart Simpson&#x27;&gt;&gt;&gt; bart.score59 和普通的函数相比，在类中定义的函数只有一点不同，就是第一个参数永远是实例变量self，并且，调用时，不用传递该参数。除此之外，类的方法和普通函数没有什么区别，所以，你仍然可以用默认参数、可变参数、关键字参数和命名关键字参数。 数据封装 面向对象编程的一个重要特点就是数据封装。在上面的Student类中，每个实例就拥有各自的name和score这些数据。我们可以通过函数来访问这些数据，比如打印一个学生的成绩： &gt;&gt;&gt; def print_score(std):... print(&#x27;%s: %s&#x27; % (std.name, std.score))...&gt;&gt;&gt; print_score(bart)Bart Simpson: 59 但是，既然Student实例本身就拥有这些数据，要访问这些数据，就没有必要从外面的函数去访问，可以直接在Student类的内部定义访问数据的函数，这样，就把“数据”给封装起来了。这些封装数据的函数是和Student类本身是关联起来的，我们称之为类的方法： class Student(object): def __init__(self, name, score): self.name = name self.score = score def print_score(self): print(&#x27;%s: %s&#x27; % (self.name, self.score)) 要定义一个方法，除了第一个参数是self外，其他和普通函数一样。要调用一个方法，只需要在实例变量上直接调用，除了self不用传递，其他参数正常传入： &gt;&gt;&gt; bart.print_score()Bart Simpson: 59 这样一来，我们从外部看Student类，就只需要知道，创建实例需要给出name和score，而如何打印，都是在Student类的内部定义的，这些数据和逻辑被“封装”起来了，调用很容易，但却不用知道内部实现的细节。 封装的另一个好处是可以给Student类增加新的方法，比如get_grade： class Student(object): ... def get_grade(self): if self.score &gt;= 90: return &#x27;A&#x27; elif self.score &gt;= 60: return &#x27;B&#x27; else: return &#x27;C&#x27; 访问限制 在Class内部，可以有属性和方法，而外部代码可以通过直接调用实例变量的方法来操作数据，这样，就隐藏了内部的复杂逻辑。 但是，从前面Student类的定义来看，外部代码还是可以自由地修改一个实例的name、score属性： &gt;&gt;&gt; bart = Student(&#x27;Bart Simpson&#x27;, 59)&gt;&gt;&gt; bart.score59&gt;&gt;&gt; bart.score = 99&gt;&gt;&gt; bart.score99 如果要让内部属性不被外部访问，可以把属性的名称前加上两个下划线__，在Python中，实例的变量名如果以__开头，就变成了一个私有变量（private），只有内部可以访问，外部不能访问，所以，我们把Student类改一改： class Student(object): def __init__(self, name, score): self.__name = name self.__score = score def print_score(self): print(&#x27;%s: %s&#x27; % (self.__name, self.__score)) 改完后，对于外部代码来说，没什么变动，但是已经无法从外部访问实例变量.__name和实例变量.__score了： &gt;&gt;&gt; bart = Student(&#x27;Bart Simpson&#x27;, 59)&gt;&gt;&gt; bart.__nameTraceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: &#x27;Student&#x27; object has no attribute &#x27;__name&#x27; 这样就确保了外部代码不能随意修改对象内部的状态，这样通过访问限制的保护，代码更加健壮。 但是如果外部代码要获取name和score怎么办？可以给Student类增加get_name和get_score这样的方法： class Student(object): ... def get_name(self): return self.__name def get_score(self): return self.__score 如果又要允许外部代码修改score怎么办？可以再给Student类增加set_score方法： class Student(object): ... def set_score(self, score): self.__score = score 你也许会问，原先那种直接通过bart.score = 99也可以修改啊，为什么要定义一个方法大费周折？因为在方法中，可以对参数做检查，避免传入无效的参数： class Student(object): ... def set_score(self, score): if 0 &lt;= score &lt;= 100: self.__score = score else: raise ValueError(&#x27;bad score&#x27;) 需要注意的是，在Python中，变量名类似__xxx__的，也就是以双下划线开头，并且以双下划线结尾的，是特殊变量，特殊变量是可以直接访问的，不是private变量，所以，不能用__name__、__score__这样的变量名。 有些时候，你会看到以一个下划线开头的实例变量名，比如_name，这样的实例变量外部是可以访问的，但是，按照约定俗成的规定，当你看到这样的变量时，意思就是，“虽然我可以被访问，但是，请把我视为私有变量，不要随意访问”。 双下划线开头的实例变量是不是一定不能从外部访问呢？其实也不是。不能直接访问__name是因为Python解释器对外把__name变量改成了_Student__name，所以，仍然可以通过_Student__name来访问__name变量： &gt;&gt;&gt; bart._Student__name&#x27;Bart Simpson&#x27; 但是强烈建议你不要这么干，因为不同版本的Python解释器可能会把__name改成不同的变量名。 总的来说就是，Python本身没有任何机制阻止你干坏事，一切全靠自觉。 最后注意下面的这种错误写法： &gt;&gt;&gt; bart = Student(&#x27;Bart Simpson&#x27;, 59)&gt;&gt;&gt; bart.get_name()&#x27;Bart Simpson&#x27;&gt;&gt;&gt; bart.__name = &#x27;New Name&#x27; # 设置__name变量！&gt;&gt;&gt; bart.__name&#x27;New Name&#x27; 表面上看，外部代码“成功”地设置了__name变量，但实际上这个__name变量和class内部的__name变量不是一个变量！内部的__name变量已经被Python解释器自动改成了_Student__name，而外部代码给bart新增了一个__name变量。不信试试： &gt;&gt;&gt; bart.get_name() # get_name()内部返回self.__name&#x27;Bart Simpson&#x27; 继承和多态 在OOP程序设计中，当我们定义一个class的时候，可以从某个现有的class继承，新的class称为子类（Subclass），而被继承的class称为基类、父类或超类（Base class、Super class）。 比如，我们已经编写了一个名为Animal的class，有一个run()方法可以直接打印： class Animal(object): def run(self): print(&#x27;Animal is running...&#x27;) 当我们需要编写Dog和Cat类时，就可以直接从Animal类继承： class Dog(Animal): passclass Cat(Animal): pass 对于Dog来说，Animal就是它的父类，对于Animal来说，Dog就是它的子类。Cat和Dog类似。 继承有什么好处？最大的好处是子类获得了父类的全部功能。由于Animial实现了run()方法，因此，Dog和Cat作为它的子类，什么事也没干，就自动拥有了run()方法： dog = Dog()dog.run()cat = Cat()cat.run() 运行结果如下： Animal is running...Animal is running... 当然，也可以对子类增加一些方法，比如Dog类： class Dog(Animal): def run(self): print(&#x27;Dog is running...&#x27;) def eat(self): print(&#x27;Eating meat...&#x27;) 继承的第二个好处需要我们对代码做一点改进。你看到了，无论是Dog还是Cat，它们run()的时候，显示的都是Animal is running...，符合逻辑的做法是分别显示Dog is running...和Cat is running...，因此，对Dog和Cat类改进如下： class Dog(Animal): def run(self): print(&#x27;Dog is running...&#x27;)class Cat(Animal): def run(self): print(&#x27;Cat is running...&#x27;) 再次运行，结果如下： Dog is running...Cat is running... 当子类和父类都存在相同的run()方法时，我们说，子类的run()覆盖了父类的run()，在代码运行的时候，总是会调用子类的run()。这样，我们就获得了继承的另一个好处：多态。 要理解什么是多态，我们首先要对数据类型再作一点说明。当我们定义一个class的时候，我们实际上就定义了一种数据类型。我们定义的数据类型和Python自带的数据类型，比如str、list、dict没什么两样： a = list() # a是list类型b = Animal() # b是Animal类型c = Dog() # c是Dog类型 判断一个变量是否是某个类型可以用isinstance()判断： &gt;&gt;&gt; isinstance(a, list)True&gt;&gt;&gt; isinstance(b, Animal)True&gt;&gt;&gt; isinstance(c, Dog)True 看来a、b、c确实对应着list、Animal、Dog这3种类型。 但是等等，试试： &gt;&gt;&gt; isinstance(c, Animal)True 看来c不仅仅是Dog，c还是Animal！ 不过仔细想想，这是有道理的，因为Dog是从Animal继承下来的，当我们创建了一个Dog的实例c时，我们认为c的数据类型是Dog没错，但c同时也是Animal也没错，Dog本来就是Animal的一种！ 所以，在继承关系中，如果一个实例的数据类型是某个子类，那它的数据类型也可以被看做是父类。但是，反过来就不行： &gt;&gt;&gt; b = Animal()&gt;&gt;&gt; isinstance(b, Dog)False Dog可以看成Animal，但Animal不可以看成Dog。 要理解多态的好处，我们还需要再编写一个函数，这个函数接受一个Animal类型的变量： def run_twice(animal): animal.run() animal.run() 当我们传入Animal的实例时，run_twice()就打印出： &gt;&gt;&gt; run_twice(Animal())Animal is running...Animal is running... 当我们传入Dog的实例时，run_twice()就打印出： &gt;&gt;&gt; run_twice(Dog())Dog is running...Dog is running... 当我们传入Cat的实例时，run_twice()就打印出： &gt;&gt;&gt; run_twice(Cat())Cat is running...Cat is running... 看上去没啥意思，但是仔细想想，现在，如果我们再定义一个Tortoise类型，也从Animal派生： class Tortoise(Animal): def run(self): print(&#x27;Tortoise is running slowly...&#x27;) 当我们调用run_twice()时，传入Tortoise的实例： &gt;&gt;&gt; run_twice(Tortoise())Tortoise is running slowly...Tortoise is running slowly... 你会发现，新增一个Animal的子类，不必对run_twice()做任何修改，实际上，任何依赖Animal作为参数的函数或者方法都可以不加修改地正常运行，原因就在于多态。 多态的好处就是，当我们需要传入Dog、Cat、Tortoise……时，我们只需要接收Animal类型就可以了，因为Dog、Cat、Tortoise……都是Animal类型，然后，按照Animal类型进行操作即可。由于Animal类型有run()方法，因此，传入的任意类型，只要是Animal类或者子类，就会自动调用实际类型的run()方法，这就是多态的意思： 对于一个变量，我们只需要知道它是Animal类型，无需确切地知道它的子类型，就可以放心地调用run()方法，而具体调用的run()方法是作用在Animal、Dog、Cat还是Tortoise对象上，由运行时该对象的确切类型决定，这就是多态真正的威力：调用方只管调用，不管细节，而当我们新增一种Animal的子类时，只要确保run()方法编写正确，不用管原来的代码是如何调用的。这就是著名的“开闭”原则： 对扩展开放：允许新增Animal子类； 对修改封闭：不需要修改依赖Animal类型的run_twice()等函数。 继承还可以一级一级地继承下来，就好比从爷爷到爸爸、再到儿子这样的关系。而任何类，最终都可以追溯到根类object，这些继承关系看上去就像一颗倒着的树。比如如下的继承树： ┌───────────────┐ │ object │ └───────────────┘ │ ┌────────────┴────────────┐ │ │ ▼ ▼ ┌─────────────┐ ┌─────────────┐ │ Animal │ │ Plant │ └─────────────┘ └─────────────┘ │ │ ┌─────┴──────┐ ┌─────┴──────┐ │ │ │ │ ▼ ▼ ▼ ▼┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐│ Dog │ │ Cat │ │ Tree │ │ Flower │└─────────┘ └─────────┘ └─────────┘ └─────────┘ 静态语言 vs 动态语言 对于静态语言（例如Java）来说，如果需要传入Animal类型，则传入的对象必须是Animal类型或者它的子类，否则，将无法调用run()方法。 对于Python这样的动态语言来说，则不一定需要传入Animal类型。我们只需要保证传入的对象有一个run()方法就可以了： class Timer(object): def run(self): print(&#x27;Start...&#x27;) 这就是动态语言的“鸭子类型”，它并不要求严格的继承体系，一个对象只要“看起来像鸭子，走起路来像鸭子”，那它就可以被看做是鸭子。 Python的“file-like object“就是一种鸭子类型。对真正的文件对象，它有一个read()方法，返回其内容。但是，许多对象，只要有read()方法，都被视为“file-like object“。许多函数接收的参数就是“file-like object“，你不一定要传入真正的文件对象，完全可以传入任何实现了read()方法的对象。 获取对象信息 当我们拿到一个对象的引用时，如何知道这个对象是什么类型、有哪些方法呢？ 使用type() 首先，我们来判断对象类型，使用type()函数： 基本类型都可以用type()判断： &gt;&gt;&gt; type(123)&lt;class &#x27;int&#x27;&gt;&gt;&gt;&gt; type(&#x27;str&#x27;)&lt;class &#x27;str&#x27;&gt;&gt;&gt;&gt; type(None)&lt;type(None) &#x27;NoneType&#x27;&gt; 如果一个变量指向函数或者类，也可以用type()判断： &gt;&gt;&gt; type(abs)&lt;class &#x27;builtin_function_or_method&#x27;&gt;&gt;&gt;&gt; type(a)&lt;class &#x27;__main__.Animal&#x27;&gt; 但是type()函数返回的是什么类型呢？它返回对应的Class类型。如果我们要在if语句中判断，就需要比较两个变量的type类型是否相同： &gt;&gt;&gt; type(123)==type(456)True&gt;&gt;&gt; type(123)==intTrue&gt;&gt;&gt; type(&#x27;abc&#x27;)==type(&#x27;123&#x27;)True&gt;&gt;&gt; type(&#x27;abc&#x27;)==strTrue&gt;&gt;&gt; type(&#x27;abc&#x27;)==type(123)False 判断基本数据类型可以直接写int，str等，但如果要判断一个对象是否是函数怎么办？可以使用types模块中定义的常量： &gt;&gt;&gt; import types&gt;&gt;&gt; def fn():... pass...&gt;&gt;&gt; type(fn)==types.FunctionTypeTrue&gt;&gt;&gt; type(abs)==types.BuiltinFunctionTypeTrue&gt;&gt;&gt; type(lambda x: x)==types.LambdaTypeTrue&gt;&gt;&gt; type((x for x in range(10)))==types.GeneratorTypeTrue 使用isinstance() 对于class的继承关系来说，使用type()就很不方便。我们要判断class的类型，可以使用isinstance()函数。 我们回顾上次的例子，如果继承关系是： object -&gt; Animal -&gt; Dog -&gt; Husky 那么，isinstance()就可以告诉我们，一个对象是否是某种类型。先创建3种类型的对象： &gt;&gt;&gt; a = Animal()&gt;&gt;&gt; d = Dog()&gt;&gt;&gt; h = Husky() 然后，判断： &gt;&gt;&gt; isinstance(h, Husky)True 没有问题，因为h变量指向的就是Husky对象。 再判断： &gt;&gt;&gt; isinstance(h, Dog)True h虽然自身是Husky类型，但由于Husky是从Dog继承下来的，所以，h也还是Dog类型。换句话说，isinstance()判断的是一个对象是否是该类型本身，或者位于该类型的父继承链上。 因此，我们可以确信，h还是Animal类型： &gt;&gt;&gt; isinstance(h, Animal)True 同理，实际类型是Dog的d也是Animal类型： &gt;&gt;&gt; isinstance(d, Dog) and isinstance(d, Animal)True 但是，d不是Husky类型： &gt;&gt;&gt; isinstance(d, Husky)False 能用type()判断的基本类型也可以用isinstance()判断： &gt;&gt;&gt; isinstance(&#x27;a&#x27;, str)True&gt;&gt;&gt; isinstance(123, int)True&gt;&gt;&gt; isinstance(b&#x27;a&#x27;, bytes)True 并且还可以判断一个变量是否是某些类型中的一种，比如下面的代码就可以判断是否是list或者tuple： &gt;&gt;&gt; isinstance([1, 2, 3], (list, tuple))True&gt;&gt;&gt; isinstance((1, 2, 3), (list, tuple))True 总是优先使用isinstance()判断类型，可以将指定类型及其子类“一网打尽”。 使用dir() 如果要获得一个对象的所有属性和方法，可以使用dir()函数，它返回一个包含字符串的list，比如，获得一个str对象的所有属性和方法： &gt;&gt;&gt; dir(&#x27;ABC&#x27;)[&#x27;__add__&#x27;, &#x27;__class__&#x27;,..., &#x27;__subclasshook__&#x27;, &#x27;capitalize&#x27;, &#x27;casefold&#x27;,..., &#x27;zfill&#x27;] 类似__xxx__的属性和方法在Python中都是有特殊用途的，比如__len__方法返回长度。在Python中，如果你调用len()函数试图获取一个对象的长度，实际上，在len()函数内部，它自动去调用该对象的__len__()方法，所以，下面的代码是等价的： &gt;&gt;&gt; len(&#x27;ABC&#x27;)3&gt;&gt;&gt; &#x27;ABC&#x27;.__len__()3 我们自己写的类，如果也想用len(myObj)的话，就自己写一个__len__()方法： &gt;&gt;&gt; class MyDog(object):... def __len__(self):... return 100...&gt;&gt;&gt; dog = MyDog()&gt;&gt;&gt; len(dog)100 剩下的都是普通属性或方法，比如lower()返回小写的字符串： &gt;&gt;&gt; &#x27;ABC&#x27;.lower()&#x27;abc&#x27; 仅仅把属性和方法列出来是不够的，配合getattr()、setattr()以及hasattr()，我们可以直接操作一个对象的状态： &gt;&gt;&gt; class MyObject(object):... def __init__(self):... self.x = 9... def power(self):... return self.x * self.x...&gt;&gt;&gt; obj = MyObject() 紧接着，可以测试该对象的属性： &gt;&gt;&gt; hasattr(obj, &#x27;x&#x27;) # 有属性&#x27;x&#x27;吗？True&gt;&gt;&gt; obj.x9&gt;&gt;&gt; hasattr(obj, &#x27;y&#x27;) # 有属性&#x27;y&#x27;吗？False&gt;&gt;&gt; setattr(obj, &#x27;y&#x27;, 19) # 设置一个属性&#x27;y&#x27;&gt;&gt;&gt; hasattr(obj, &#x27;y&#x27;) # 有属性&#x27;y&#x27;吗？True&gt;&gt;&gt; getattr(obj, &#x27;y&#x27;) # 获取属性&#x27;y&#x27;19&gt;&gt;&gt; obj.y # 获取属性&#x27;y&#x27;19 如果试图获取不存在的属性，会抛出AttributeError的错误： &gt;&gt;&gt; getattr(obj, &#x27;z&#x27;) # 获取属性&#x27;z&#x27;Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: &#x27;MyObject&#x27; object has no attribute &#x27;z&#x27; 可以传入一个default参数，如果属性不存在，就返回默认值： &gt;&gt;&gt; getattr(obj, &#x27;z&#x27;, 404) # 获取属性&#x27;z&#x27;，如果不存在，返回默认值404404 也可以获得对象的方法： &gt;&gt;&gt; hasattr(obj, &#x27;power&#x27;) # 有属性&#x27;power&#x27;吗？True&gt;&gt;&gt; getattr(obj, &#x27;power&#x27;) # 获取属性&#x27;power&#x27;&lt;bound method MyObject.power of &lt;__main__.MyObject object at 0x10077a6a0&gt;&gt;&gt;&gt;&gt; fn = getattr(obj, &#x27;power&#x27;) # 获取属性&#x27;power&#x27;并赋值到变量fn&gt;&gt;&gt; fn # fn指向obj.power&lt;bound method MyObject.power of &lt;__main__.MyObject object at 0x10077a6a0&gt;&gt;&gt;&gt;&gt; fn() # 调用fn()与调用obj.power()是一样的81 实例属性和类属性 由于Python是动态语言，根据类创建的实例可以任意绑定属性。 给实例绑定属性的方法是通过实例变量，或者通过self变量： class Student(object): def __init__(self, name): self.name = names = Student(&#x27;Bob&#x27;)s.score = 90 但是，如果Student类本身需要绑定一个属性呢？可以直接在class中定义属性，这种属性是类属性，归Student类所有： class Student(object): name = &#x27;Student&#x27; 当我们定义了一个类属性后，这个属性虽然归类所有，但类的所有实例都可以访问到。来测试一下： &gt;&gt;&gt; class Student(object):... name = &#x27;Student&#x27;...&gt;&gt;&gt; s = Student() # 创建实例s&gt;&gt;&gt; print(s.name) # 打印name属性，因为实例并没有name属性，所以会继续查找class的name属性Student&gt;&gt;&gt; print(Student.name) # 打印类的name属性Student&gt;&gt;&gt; s.name = &#x27;Michael&#x27; # 给实例绑定name属性&gt;&gt;&gt; print(s.name) # 由于实例属性优先级比类属性高，因此，它会屏蔽掉类的name属性Michael&gt;&gt;&gt; print(Student.name) # 但是类属性并未消失，用Student.name仍然可以访问Student&gt;&gt;&gt; del s.name # 如果删除实例的name属性&gt;&gt;&gt; print(s.name) # 再次调用s.name，由于实例的name属性没有找到，类的name属性就显示出来了Student 从上面的例子可以看出，在编写程序的时候，千万不要对实例属性和类属性使用相同的名字，因为相同名称的实例属性将屏蔽掉类属性，但是当你删除实例属性后，再使用相同的名称，访问到的将是类属性。 Error Handling Python所有的错误都是从BaseException类派生的，常见的错误类型和继承关系看这里： https://docs.python.org/3/library/exceptions.html#exception-hierarchy try 让我们用一个例子来看看try的机制： try: print(&#x27;try...&#x27;) r = 10 / 0 print(&#x27;result:&#x27;, r)except ZeroDivisionError as e: print(&#x27;except:&#x27;, e)finally: print(&#x27;finally...&#x27;)print(&#x27;END&#x27;) 当我们认为某些代码可能会出错时，就可以用try来运行这段代码，如果执行出错，则后续代码不会继续执行，而是直接跳转至错误处理代码，即except语句块，执行完except后，如果有finally语句块，则执行finally语句块，至此，执行完毕。 上面的代码在计算10 / 0时会产生一个除法运算错误： try...except: division by zerofinally...END 从输出可以看到，当错误发生时，后续语句print('result:', r)不会被执行，except由于捕获到ZeroDivisionError，因此被执行。最后，finally语句被执行。然后，程序继续按照流程往下走。 如果把除数0改成2，则执行结果如下： try...result: 5finally...END 由于没有错误发生，所以except语句块不会被执行，但是finally如果有，则一定会被执行（可以没有finally语句）。 你还可以猜测，错误应该有很多种类，如果发生了不同类型的错误，应该由不同的except语句块处理。没错，可以有多个except来捕获不同类型的错误： try: print(&#x27;try...&#x27;) r = 10 / int(&#x27;a&#x27;) print(&#x27;result:&#x27;, r)except ValueError as e: print(&#x27;ValueError:&#x27;, e)except ZeroDivisionError as e: print(&#x27;ZeroDivisionError:&#x27;, e)finally: print(&#x27;finally...&#x27;)print(&#x27;END&#x27;) int()函数可能会抛出ValueError，所以我们用一个except捕获ValueError，用另一个except捕获ZeroDivisionError。 此外，如果没有错误发生，可以在except语句块后面加一个else，当没有错误发生时，会自动执行else语句： try: print(&#x27;try...&#x27;) r = 10 / int(&#x27;2&#x27;) print(&#x27;result:&#x27;, r)except ValueError as e: print(&#x27;ValueError:&#x27;, e)except ZeroDivisionError as e: print(&#x27;ZeroDivisionError:&#x27;, e)else: print(&#x27;no error!&#x27;)finally: print(&#x27;finally...&#x27;)print(&#x27;END&#x27;) Python的错误其实也是class，所有的错误类型都继承自BaseException，所以在使用except时需要注意的是，它不但捕获该类型的错误，还把其子类也“一网打尽”。比如： try: foo()except ValueError as e: print(&#x27;ValueError&#x27;)except UnicodeError as e: print(&#x27;UnicodeError&#x27;) 第二个except永远也捕获不到UnicodeError，因为UnicodeError是ValueError的子类，如果有，也被第一个except给捕获了。 raise 因为错误是class，捕获一个错误就是捕获到该class的一个实例。因此，错误并不是凭空产生的，而是有意创建并抛出的。Python的内置函数会抛出很多类型的错误，我们自己编写的函数也可以抛出错误。 如果要抛出错误，首先根据需要，可以定义一个错误的class，选择好继承关系，然后，用raise语句抛出一个错误的实例： # err_raise.pyclass FooError(ValueError): passdef foo(s): n = int(s) if n==0: raise FooError(&#x27;invalid value: %s&#x27; % s) return 10 / nfoo(&#x27;0&#x27;) 执行，可以最后跟踪到我们自己定义的错误： $ python3 err_raise.py Traceback (most recent call last): File &quot;err_throw.py&quot;, line 11, in &lt;module&gt; foo(&#x27;0&#x27;) File &quot;err_throw.py&quot;, line 8, in foo raise FooError(&#x27;invalid value: %s&#x27; % s)__main__.FooError: invalid value: 0 只有在必要的时候才定义我们自己的错误类型。如果可以选择Python已有的内置的错误类型（比如ValueError，TypeError），尽量使用Python内置的错误类型。 最后，我们来看另一种错误处理的方式： # err_reraise.pydef foo(s): n = int(s) if n==0: raise ValueError(&#x27;invalid value: %s&#x27; % s) return 10 / ndef bar(): try: foo(&#x27;0&#x27;) except ValueError as e: print(&#x27;ValueError!&#x27;) raisebar() 在bar()函数中，我们明明已经捕获了错误，但是，打印一个ValueError!后，又把错误通过raise语句抛出去了，这不有病么？ 其实这种错误处理方式不但没病，而且相当常见。捕获错误目的只是记录一下，便于后续追踪。但是，由于当前函数不知道应该怎么处理该错误，所以，最恰当的方式是继续往上抛，让顶层调用者去处理。好比一个员工处理不了一个问题时，就把问题抛给他的老板，如果他的老板也处理不了，就一直往上抛，最终会抛给CEO去处理。 raise语句如果不带参数，就会把当前错误原样抛出。此外，在except中raise一个Error，还可以把一种类型的错误转化成另一种类型： try: 10 / 0except ZeroDivisionError: raise ValueError(&#x27;input error!&#x27;) 只要是合理的转换逻辑就可以，但是，决不应该把一个IOError转换成毫不相干的ValueError。 assert 凡是用print()来辅助查看的地方，都可以用断言（assert）来替代： def foo(s): n = int(s) assert n != 0, &#x27;n is zero!&#x27; return 10 / ndef main(): foo(&#x27;0&#x27;) assert的意思是，表达式n != 0应该是True，否则，根据程序运行的逻辑，后面的代码肯定会出错。 如果断言失败，assert语句本身就会抛出AssertionError： $ python err.pyTraceback (most recent call last): ...AssertionError: n is zero! 程序中如果到处充斥着assert，和print()相比也好不到哪去。不过，启动Python解释器时可以用-O参数来关闭assert： $ python -O err.pyTraceback (most recent call last): ...ZeroDivisionError: division by zero 注意：断言的开关“-O”是英文大写字母O，不是数字0。 关闭后，你可以把所有的assert语句当成pass来看. 注意事项 文件路径 Python中的相对路径，是相对于当前被执行文件的路径， 举例来说， 如果main.py 中import了A.py, 而 A.py中有相对路径../path, 则在运行main.py时， 该路径实际上是相对于main.py的， 而如果单独运行A.py，该路径才是相对于A.py的 小技巧 矩阵 转置矩阵 def two_dimensional_array(m): row_nums = len(m) col_nums = len(m[0]) print(&quot;m = &quot;, m) print( &quot;row_nums = &#123;0&#125;, col_nums = &#123;1&#125;&quot;.format( row_nums, col_nums ) ) rows = [] for j in range( 0, col_nums ): tmp = [] for i in range( 0, row_nums ): tmp.append(m[i][j]) #print( m[i][j] ) if i == row_nums - 1: print(tmp) rows.append(tmp) res_m = [ rows[j] for j in range( 0, col_nums ) ] return res_m//输入 [[1, &#x27;j&#x27;], [2, &#x27;k&#x27;], [3, &#x27;l&#x27;], [4, &#x27;m&#x27;], [5, &#x27;n&#x27;]]//输出 [[1, 2, 3, 4, 5], [&#x27;j&#x27;, &#x27;k&#x27;, &#x27;l&#x27;, &#x27;m&#x27;, &#x27;n&#x27;]] 注意： python二维数组可以res_m = [ rows[j] for j in range( 0, col_nums ) ] 创建。也就是说，列表生成式可以由元素生成列表，而这个元素本身也可以是列表，所以最后会生成一个二维列表。 二维列表可以用列表生成式： l1 = [ 1,2,3,4,5 ]l2 = [ &quot;j&quot;,&quot;k&quot;,&quot;l&quot;,&quot;m&quot;,&quot;n&quot; ]l3 = [ l1,l2 ] // 方法一l4 = [ l3[i] for i in range(0,len(l3)) ] //方法二,列表生成式l5 = [].append(l1)l5.append(l1)l5.append(l2) // 方法三//注意, &quot;+&quot;会把列表拼接起来, 而append()会把参数整个当作一个元素加到列表末尾l6 = l1 + l2 // 结果是[ 1,2,3,4,5, &quot;j&quot;,&quot;k&quot;,&quot;l&quot;,&quot;m&quot;,&quot;n&quot; ] , 可以看到仍是一维的","categories":[{"name":"Language","slug":"Language","permalink":"http://lyk-love.cn/categories/Language/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://lyk-love.cn/tags/Python/"}]},{"title":"MySQL Data Manipulation","slug":"MySQL Data Manipulation","date":"2022-09-26T06:39:34.934Z","updated":"2022-09-26T06:39:34.934Z","comments":true,"path":"2022/09/26/MySQL Data Manipulation/","link":"","permalink":"http://lyk-love.cn/2022/09/26/MySQL%20Data%20Manipulation/","excerpt":"Outline: SELECT ORDER BY ... Mysql语法简介","text":"Outline: SELECT ORDER BY ... Mysql语法简介 MYSQL DATA MANIPULATION SELECT SELECT select_listFROM table_name; ;是可选的，表明一条语句的结束 SQL是大小写不敏感的，因此SELECT0，FROM这些关键字可大写可小写 SELECT的结果称为一个结果集 如果SELECT的参数是空行（连NULL都没有)，会返回NULL。 Returning n Random Records from a Table select ename, job from emporder by rand() limit 5 Transforming Nulls into Real Values Use the function COALESCE to substitute real values for nulls: select coalesce(comm, 0) from emp COALESCE 接受至少一个参数，返回第一个非空参数。 如果comm为null,则返回0 或者： select case when comm is not null then comm else 0 end from emp ORDER BY 默认是ASC SELECT select_listFROM table_nameORDER BY column1 [ASC|DESC], column2 [ASC|DESC], ...; ORDER BY的参数一般是字段，当然也可以是列号， 但后一种方式不推荐 SELECT CONCAT(firstName,&#x27; &#x27;,lastName) fullnameFROM employees UNION SELECT CONCAT(contactFirstName,&#x27; &#x27;,contactLastName)FROM customersORDER BY 1; # 采用列号（第一列） WHERE SELECT select_listFROM table_nameWHERE search_condition; The search_condition is a combination of one or more expressions using the logical operator AND, OR and NOT. In MySQL, a predicate is a Boolean expression that evaluates to TRUE, FALSE, or UNKNOWN. The SELECT statement will include any row that satisfies the search_condition in the result set. Besides the SELECT statement, you can use the WHERE clause in the UPDATE or DELETE statement to specify which rows to update or delete. When executing a SELECT statement with a WHERE clause, MySQL evaluates the WHERE clause after the FROM clause and before the SELECT and ORDER BY clauses: BETWEEN expression BETWEEN low AND high LIKE The LIKE operator evaluates to TRUE if a value matches a specified pattern. To form a pattern, you use the % and _ wildcards. The % wildcard matches any string of zero or more characters the _ wildcard matches any single character. The following query finds the employees whose last names end with the string 'son': SELECT firstName, lastNameFROM employeesWHERE lastName LIKE &#x27;%son&#x27;ORDER BY firstName; IN The IN operator returns TRUE if a value matches any value in a list. value IN (value1, value2,...) The following example uses the WHERE clause with the IN operator to find employees who locate in the office with office code 1. SELECT firstName, lastName, officeCodeFROM employeesWHERE officeCode IN (1 , 2, 3)ORDER BY officeCode; IS NULL To check if a value is NULL or not, you use the IS NULL operator, not the equal operator (=). The IS NULL operator returns TRUE if a value is NULL. value IS NULLCode language: SQL (Structured Query Language) (sql) In the database world, NULL is a marker that indicates that a value is missing or unknown. And NULL is not equivalent to the number 0 or an empty string. 对应的，还有IS NOT NULL LIMIT 注意： limit作用于结果集产生的最后，在order by之后，所以要先order by, 再 lmit The LIMIT clause is used in the SELECT statement to constrain the number of rows to return. SELECT select_listFROM table_nameLIMIT [offset,] row_count; ( OR LIMIT row_count OFFSET offset; ) The offset specifies the offset of the first row to return. OFFSET从0开始计数，缺省也为0 The row_count specifies the maximum number of rows to return. LIMIT子句对于超过其范围的行，会返回空行而不是NULL： select salary from `Employee` limit 1 offset 1# 如果Employee只有一行，则该查询返回空行：+--------+| salary |+--------++--------+# 不会返回null 可以和ORDER BY结合： SELECT select_listFROM table_nameORDER BY sort_expressionLIMIT offset, row_count; 示例： get the highest or lowest rows SELECT customerNumber, customerName, creditLimitFROM customersORDER BY creditLimit DESCLIMIT 5; First, the ORDER BY clause sorts the customers by credits in high to low. Then, the LIMIT clause returns the first 5 rows. IS NULL To test whether a value is NULL or not, you use the IS NULL operator. Here’s the basic syntax of the IS NULL operator: value IS NULLCode language: SQL (Structured Query Language) (sql) If the value is NULL, the expression returns true. Otherwise, it returns false. Note that MySQL does not have a built-in BOOLEAN type. It uses the TINYINT(1) to represent the BOOLEAN values i.e., true means 1 and false means 0. Because the IS NULL is a comparison operator, you can use it anywhere that an operator can be used e.g., in the SELECT or WHERE clause. See the following example: SELECT 1 IS NULL, -- 0 0 IS NULL, -- 0 NULL IS NULL; -- 1Code language: SQL (Structured Query Language) (sql) To check if a value is not NULL, you use IS NOT NULL operator: value IS NOT NULLCode language: SQL (Structured Query Language) (sql) This expression returns true (1) if the value is not NULL. Otherwise, it returns false (0). Consider the following example: SELECT 1 IS NOT NULL, -- 1 0 IS NOT NULL, -- 1 NULL IS NOT NULL; -- 0Code language: SQL (Structured Query Language) (sql) SELECT DISTINCT SELECT DISTINCT select_listFROM table_nameWHERE search_conditionORDER BY sort_expression; DISTINCT: 从选出的行中选择唯一的 如果DISTINCT后面跟多个字段，就会将这些字段作为一个整体来判断唯一性 MYSQL认为所有NULL都是相等的，因此多个NULL在DISTINCT后只剩下一个 Alias 在一次查询中重复引用一个表名会引发ERROR， 因此要定义别名 列别名： SELECT [column_1 | expression] [AS] descriptive_nameFROM table_name;//如果别名有空格，需要用引号括起来：‘descriptive name’ 表达式别名： SELECT CONCAT_WS(&#x27;, &#x27;, lastName, firstname) [AS] `Full name`FROM employees; 表别名： table_name [AS] table_alias JOIN A join is a method of linking data between one (self-join) or more tables based on values of the common column between the tables. MySQL supports the following types of joins: Inner join Left join Right join Cross join To join tables, you use the cross join, inner join, left join, or right join clause. The join clause is used in the SELECT statement appeared after the FROM clause. Note that MySQL hasn’t supported the FULL OUTER JOIN yet. Join会创建一个包含原本两方所有字段的新行，并加入结果集 INNER JOIN SELECT column_listFROM table_1INNER JOIN table_2 ON join_condition; The inner join clause compares each row from the first table with every row from the second table. 将左边的每一行与右边的所有行匹配 只有两边的行同时满足，才会被join 如果两张表中作为条件来比较的字段相同，那么可以用 USING来替代ON： using(id)# 等价于on a.id=b.id LEFT JOIN 对于左边的一行，如果右边的行都不满足条件，依然会创建出新行，只不过新行中，原来的右边的行的字段的对应值为NULL SELECT c.customerNumber, c.customerName, o.orderNumber, o.statusFROM customers cLEFT JOIN orders o ON c.customerNumber = o.customerNumberWHERE orderNumber IS NULL; RIGHT JOIN 从右边的行中选择，将右边的每一行与左边的所有行匹配。 如果左边的所有行都不满足条件，那么依然创建出新行，新行中左边行的字段值为NULL SELECT column_list FROM table_1 RIGHT JOIN table_2 ON join_condition; CROSS JOIN 没有查询条件，产生笛卡尔积 SELECT select_listFROM table_1CROSS JOIN table_2; Self Join 一个表和自己做join, 因为同一次查询中不能重复引用同一张表，因此必须定义别名 示例： SELECT CONCAT(m.lastName, &#x27;, &#x27;, m.firstName) AS Manager, CONCAT(e.lastName, &#x27;, &#x27;, e.firstName) AS &#x27;Direct report&#x27;FROM employees eINNER JOIN employees m ON m.employeeNumber = e.reportsToORDER BY Manager; GROUP BY The GROUP BY clause returns one row for each group. 可以和聚合函数结合。 不结合聚合函数的GROUP BY和SELECT DISTINCT是类似的 MYSQL8.0后不会对GROUP BY的结果集进行排序 The GROUP BY clause is often used with an aggregate function to perform calculations and return a single value for each subgroup. SELECT status, SUM(quantityOrdered * priceEach) AS amountFROM ordersINNER JOIN orderdetails USING (orderNumber)GROUP BY status; 可以按多个字段group by，用逗号分隔: select gender, university, count(device_id) as user_num, avg(active_days_within_30) as avg_active_days, avg(question_cnt) as avg_question_cntfrom user_profilegroup by gender, university # 多个字段 HAVING The HAVING clause is often used with the GROUP BY clause to filter groups based on a specified condition. If you omit the GROUP BY clause, the HAVING clause behaves like the WHERE clause. SELECT select_listFROM table_nameWHERE search_conditionGROUP BY group_by_expressionHAVING group_condition; ROLLUP Subquery 子查询必须用括号括起来， 可以作为表达式，用在任何需要表达式的地方 子查询如果用在FROM子句，则返回的结果集是一张临时表，被称为“导出表”， 导出表一定要有别名， 好作为一张表被引用 WHERE 子查询用在WHERE子句， 一般是和聚合函数的结合， 将聚合函数的结果返回， 然后在WHERE中进行条件运算： SELECT customerNumber, checkNumber, amountFROM paymentsWHERE amount = (SELECT MAX(amount) FROM payments); IN &amp; NOT IN 如果子查询返回不止一个结果（即不止一行）， 那还可以用来作为[NOT] IN的参数，在WHERE子句中进行条件运算 SELECT customerNameFROM customersWHERE customerNumber NOT IN (SELECT DISTINCT customerNumber FROM orders); FROM 子查询作为派生表，必须要有别名 SELECT MAX(items), MIN(items), FLOOR(AVG(items))FROM (SELECT orderNumber, COUNT(orderNumber) AS items FROM orderdetails GROUP BY orderNumber) AS lineitems; correlated subquery 子查询可以是独立的，一共只执行一次；也可以是关联的， 外层查询的每处理一行就执行一次： SELECT productname, buypriceFROM products p1WHERE buyprice &gt; (SELECT AVG(buyprice) FROM products WHERE productline = p1.productline) EXISTS &amp; NOT EXISTS 子查询就是表达式，因此用在[NOT] EXISTS 子句中时，子查询只会返回一个BOOL值： 只要子查询返回至少一行，就返回TRUE,否则返回FALSE 一般而言，作为EXISTS参数的子查询都是关联查询，这样才能和外层查询联系起来， 否则如果是独立查询， 每次计算时都返回固定的BOOL值，没什么意义 The following query finds sales orders whose total values are greater than 60K. SELECT orderNumber, SUM(priceEach * quantityOrdered) totalFROM orderdetails INNER JOIN orders USING (orderNumber)GROUP BY orderNumberHAVING SUM(priceEach * quantityOrdered) &gt; 60000; It returns 3 rows, meaning that there are three sales orders whose total values are greater than 60K. You can use the query above as a correlated subquery to find customers who placed at least one sales order with the total value greater than 60K by using the EXISTS operator: SELECT customerNumber, customerNameFROM customersWHERE EXISTS( SELECT orderNumber, SUM(priceEach * quantityOrdered) FROM orderdetails INNER JOIN orders USING (orderNumber) WHERE customerNumber = customers.customerNumber GROUP BY orderNumber HAVING SUM(priceEach * quantityOrdered) &gt; 60000); Derived Tables 派生表就是用在SELECT子句的子查询， 该子查询返回一张表（属于临时表）， 并一定具有别名以被引用 例子 Suppose you have to classify the customers who bought products in 2003 into 3 groups: platinum, gold, and silver. And you need to know the number of customers in each group with the following conditions: Platinum customers who have orders with the volume greater than 100K. Gold customers who have orders with the volume between 10K and 100K. Silver customers who have orders with the volume less than 10K. To form this query, you first need to put each customer into the respective group using CASE expression and GROUP BY clause as follows: SELECT customerNumber, ROUND(SUM(quantityOrdered * priceEach)) sales, (CASE WHEN SUM(quantityOrdered * priceEach) &lt; 10000 THEN &#x27;Silver&#x27; WHEN SUM(quantityOrdered * priceEach) BETWEEN 10000 AND 100000 THEN &#x27;Gold&#x27; WHEN SUM(quantityOrdered * priceEach) &gt; 100000 THEN &#x27;Platinum&#x27; END) customerGroupFROM orderdetails INNER JOIN orders USING (orderNumber)WHERE YEAR(shippedDate) = 2003GROUP BY customerNumber; The following is the output of the query: Then, you can use this query as the derived table and perform grouping as follows: SELECT customerGroup, COUNT(cg.customerGroup) AS groupCountFROM (SELECT customerNumber, ROUND(SUM(quantityOrdered * priceEach)) sales, (CASE WHEN SUM(quantityOrdered * priceEach) &lt; 10000 THEN &#x27;Silver&#x27; WHEN SUM(quantityOrdered * priceEach) BETWEEN 10000 AND 100000 THEN &#x27;Gold&#x27; WHEN SUM(quantityOrdered * priceEach) &gt; 100000 THEN &#x27;Platinum&#x27; END) customerGroup FROM orderdetails INNER JOIN orders USING (orderNumber) WHERE YEAR(shippedDate) = 2003 GROUP BY customerNumber) cgGROUP BY cg.customerGroup; The query returns the customer groups and the number of customers in each. EXISTS The EXISTS operator is a Boolean operator that returns either true or false. The EXISTS operator is often used to test for the existence of rows returned by the subquery. The following illustrates the basic syntax of the EXISTS operator: SELECT select_listFROM a_tableWHERE [NOT] EXISTS(subquery);Code language: SQL (Structured Query Language) (sql) If the subquery returns at least one row, the EXISTS operator returns true, otherwise, it returns false. In addition, the EXISTS operator terminates further processing immediately once it finds a matching row, which can help improve the performance of the query. The NOT operator negates the EXISTS operator. In other words, the NOT EXISTS returns true if the subquery returns no row, otherwise it returns false. Note that you can use SELECT *, SELECT column, SELECT a_constant, or anything in the subquery. The results are the same because MySQL ignores the select list appeared in the SELECT clause. UNION SELECT column_listUNION [DISTINCT | ALL]SELECT column_listUNION [DISTINCT | ALL]SELECT column_list UNION的参数也是一张表（由SELECT子句产生），但不是派生表，因此不需要别名 UNION将两张表垂直地并起来， 而JOIN将两张表水平地并起来： 要将多张表UNION起来，作为UNION主语的SELECT和宾语的SELECT子句的对应字段的数量和顺序必须相同， 类型也必须相容 比如： SELECT firstName, # 最终结果集的字段名，使用的是UNION主语的SELECT子句的字段名 lastNameFROM employees UNION SELECT contactFirstName, # 字段名不同无所谓，但是类型，顺序，数量必须相同 contactLastNameFROM customers; UNION默认是UNION DISTINCT； 要保留重复的行，需要使用UNION ALL UNION后产生的大表的字段名用的是UNION主语的SELECT的字段名 使用ORDER BY对产生的大表的行进行排序 注意， UNION宾语的那张子表里不需要排序 SELECT concat(firstName,&#x27; &#x27;,lastName) fullnameFROM employees UNION SELECT concat(contactFirstName,&#x27; &#x27;,contactLastName)FROM customersORDER BY fullname; MINUS SELECT select_list1 FROM table_name1MINUS SELECT select_list2 FROM table_name2; Unfortunately, MySQL does not support MINUS operator. However, you can use join to emulate it. To emulate the MINUS of two queries, you use the following syntax: SELECT select_listFROM table1LEFT JOIN table2 ON join_predicateWHERE table2.column_name IS NULL; INSERT INSERT INTO table(c1,c2,...)VALUES (v1,v2,...); The number of columns and values must be the same. In addition, the positions of columns must be corresponding with the positions of their values To insert multiple rows into a table using a single INSERT statement, you use the following syntax: INSERT INTO table(c1,c2,...)VALUES (v11,v12,...), (v21,v22,...), ... (vnn,vn2,...); insert using default value If you want to insert a default value into a column, you have two ways: Ignore both the column name and value in the INSERT statement. Specify the column name in the INSERT INTO clause and use the DEFAULT keyword in the VALUES clause. The following example demonstrates the second way: INSERT INTO tasks(title,priority)VALUES(&#x27;Understanding DEFAULT keyword in INSERT statement&#x27;,DEFAULT);Code language: SQL (Structured Query Language) (sql) In this example, we specified the priority column and the DEFAULT keyword. Because the default value for the column priority is 3 as declared in the table definition: priority TINYINT NOT NULL DEFAULT 3Code language: SQL (Structured Query Language) (sql) MySQL uses the number 3 to insert into the priority column. Inserting dates To insert a literal date value into a column, you use the following format: &#x27;YYYY-MM-DD&#x27;Code language: SQL (Structured Query Language) (sql) In this format: YYYY represents a four-digit year e.g., 2018. MM represents a two-digit month e.g., 01, 02, and 12. DD represents a two-digit day e.g., 01, 02, 30. The following statement inserts a new row to the tasks table with the start and due date values: INSERT INTO tasks(title, start_date, due_date)VALUES(&#x27;Insert date into table&#x27;,&#x27;2018-01-09&#x27;,&#x27;2018-09-15&#x27;);Code language: SQL (Structured Query Language) (sql) The following picture shows the contents of the tasks table after the insert: It is possible to use expressions in the VALUES clause. For example, the following statement adds a new task using the current date for start date and due date columns: INSERT INTO tasks(title,start_date,due_date)VALUES(&#x27;Use current date for the task&#x27;,CURRENT_DATE(),CURRENT_DATE())Code language: SQL (Structured Query Language) (sql) In this example, we used the CURRENT_DATE() function as the values for the start_date and due_date columns. Note that the CURRENT_DATE() function is a date function that returns the current system date. Insert Into Select In the previous tutorial, you learned how to insert one or more rows into a table using the INSERT statement with a list of column values specified in the VALUES clause. INSERT INTO table_name(c1,c2,...)VALUES(v1,v2,..);Code language: SQL (Structured Query Language) (sql) Besides using row values in the VALUES clause, you can use the result of a SELECT statement as the data source for the INSERT statement. The following illustrates the syntax of the INSERT INTO SELECT statement: INSERT INTO table_name(column_list)SELECT select_list FROM another_tableWHERE condition;Code language: SQL (Structured Query Language) (sql) In this syntax, instead of using the VALUES clause, you can use a SELECT statement. The SELECT statement can retrieve data from one or more tables. The INSERT INTO SELECT statement is very useful when you want to copy data from other tables to a table or to summary data from multiple tables into a table. UPDATE UPDATE salarySET sex = CASE sex WHEN &#x27;m&#x27; THEN &#x27;f&#x27; ELSE &#x27;m&#x27; END; CASE CASE返回一个字段值 case和when的参数字段都不要加别名， 别名要加在整个case子句后面，也就是END后面 Simple CASE expression CASE case_value WHEN when_value1 THEN statements WHEN when_value2 THEN statements ... [ELSE else-statements]END CASE; 如果没有任何一个WHEN子句被匹配，则进入ELSE子句。 ELSE子句是可选的， 如果省略了ELSE子句，则不匹配任何when时，直接返回NULL 例子： SELECT first_name, last_name, CASE WHEN salary &lt; 3000 THEN &#x27;Low&#x27; WHEN salary &gt;= 3000 AND salary &lt;= 5000 THEN &#x27;Average&#x27; WHEN salary &gt; 5000 THEN &#x27;High&#x27; END evaluationFROM employees; Searched CASE expression CASEWHEN boolean_expression_1 THEN result_1WHEN boolean_expression_2 THEN result_2WHEN boolean_expression_3 THEN result_3ELSE else_resultEND; 例子： SELECT first_name, last_name, CASE WHEN salary &lt; 3000 THEN &#x27;Low&#x27; WHEN salary &gt;= 3000 AND salary &lt;= 5000 THEN &#x27;Average&#x27; WHEN salary &gt; 5000 THEN &#x27;High&#x27; END evaluationFROM employees; DERIVED TABLE 派生表是从SELECT语句返回的虚拟表。派生表类似于临时表，但是在SELECT语句中使用派生表比临时表简单得多，因为它不需要创建临时表的步骤。 **术语:***派生表*和子查询通常可互换使用。当SELECT语句的FROM子句中使用独立子查询时，我们将其称为派生表。 以下说明了使用派生表的查询： 请注意，独立子查询是一个子查询，可独立于包含该语句的执行语句。 与子查询不同，派生表必须具有别名，以便稍后在查询中引用其名称。 如果派生表没有别名，MySQL将发出以下错误： Every derived table must have its own alias.Shell 以下说明了使用派生表的SQL语句： SELECT column_listFROM (SELECT column_list FROM table_1) derived_table_name;WHERE derived_table_name.c1 &gt; 0; aA12345678 FUNCTION ROUND ROUND(X,D) ​ 此函数返回x舍入到最接近的整数。如果第二个参数，D有提供，则函数返回x四舍五入至第D位小数点。D必须是正数 IFNULL MySQL IFNULL函数是MySQL控制流函数之一，它接受两个参数，如果不是NULL，则返回第一个参数。 否则，IFNULL函数返回第二个参数。 两个参数可以是文字值或表达式。 以下说明了IFNULL函数的语法： IFNULL(expression_1,expression_2);SQL 如果expression_1不为NULL，则IFNULL函数返回expression_1; 否则返回expression_2的结果。 IFNULL函数根据使用的上下文返回字符串或数字。 如果要返回基于TRUE或FALSE条件的值，而不是NULL，则应使用IF函数。 Aggregate Functions function_name(DISTINCT | ALL expression) use DISTINCT if you want to calculate based on distinct values or ALL in case you want to calculate all values including duplicates. The default is ALL The aggregate functions are often used with the GROUP BY clause to calculate an aggregate value for each group AVG() The AVG() function calculates the average value of a set of values. It ignores NULL in the calculation. AVG(expression) COUNT() The COUNT() function returns the number of the value in a set. For example, you can use the COUNT() function to get the number of products in the products table as shown in the following query: SELECT COUNT(*) AS totalFROM products; SUM() The SUM() function returns the sum of values in a set. The SUM() function ignores NULL. If no matching row found, the SUM() function returns NULL. To get the total order value of each product, you can use the SUM() function in conjunction with the GROUP BY clause as follows: SELECT productCode, SUM(priceEach * quantityOrdered) totalFROM orderDetailsGROUP BY productCodeORDER BY total DESC; MAX() The MAX() function returns the maximum value in a set. MAX(expression)Code language: SQL (Structured Query Language) (sql) For example, you can use the MAX() function to get the highest buy price from the products table as shown in the following query: SELECT MAX(buyPrice) highest_priceFROM products; CONSTRAINTS Primary Key When you define a primary key for a table, MySQL automatically creates an index called PRIMARY 1) Define a PRIMARY KEY constraint in CREATE TABLE Typically, you define the primary key for a table in the CREATE TABLE statement. If the primary key has one column, you can use the PRIMARY KEY constraint as a column constraint: CREATE TABLE table_name( primary_key_column datatype PRIMARY KEY, ...);Code language: SQL (Structured Query Language) (sql) When the primary key has more than one column, you must use the PRIMARY KEY constraint as a table constraint. CREATE TABLE table_name( primary_key_column1 datatype, primary_key_column2 datatype, ..., PRIMARY KEY(column_list));Code language: SQL (Structured Query Language) (sql) In this syntax, you separate columns in the column_list by commas (,). The PRIMARY KEY table constraint can be used when the primary key has one column: CREATE TABLE table_name ( primary_key_column datatype, ... , PRIMARY KEY(primary_key_column)); Foreign Key A foreign key is a column or group of columns in a table that links to a column or group of columns in another table. The foreign key places constraints on data in the related tables, which allows MySQL to maintain referential integrity. Once a foreign key constraint is in place, the foreign key columns from the child table must have the corresponding row in the parent key columns of the parent table or values in these foreign key column must be NULL (see the SET NULL action example below). Self-referencing foreign key Sometimes, the child and parent tables may refer to the same table. In this case, the foreign key references back to the primary key within the same table. See the following employees table from the sample database. The reportTo column is a foreign key that refers to the employeeNumber column which is the primary key of the employees table. This relationship allows the employees table to store the reporting structure between employees and managers. Each employee reports to zero or one employee and an employee can have zero or many subordinates. The foreign key on the column reportTo is known as a recursive or self-referencing foreign key. MySQL FOREIGN KEY syntax Here is the basic syntax of defining a foreign key constraint in the CREATE TABLE or ALTER TABLE statement: [CONSTRAINT constraint_name]FOREIGN KEY [foreign_key_name] (column_name, ...)REFERENCES parent_table(colunm_name,...)[ON DELETE reference_option][ON UPDATE reference_option]Code language: SQL (Structured Query Language) (sql) In this syntax: First, specify the name of foreign key constraint that you want to create after the CONSTRAINT keyword. If you omit the constraint name, MySQL automatically generates a name for the foreign key constraint. Second, specify a list of comma-separated foreign key columns after the FOREIGN KEY keywords. The foreign key name is also optional and is generated automatically if you skip it. Third, specify the parent table followed by a list of comma-separated columns to which the foreign key columns reference. Finally, specify how foreign key maintains the referential integrity between the child and parent tables by using the ON DELETE and ON UPDATE clauses. The reference_option determines action which MySQL will take when values in the parent key columns are deleted (ON DELETE) or updated (ON UPDATE). MySQL has five reference options: CASCADE, SET NULL, NO ACTION, RESTRICT, and SET DEFAULT. CASCADE: if a row from the parent table is deleted or updated, the values of the matching rows in the child table automatically deleted or updated. SET NULL: if a row from the parent table is deleted or updated, the values of the foreign key column (or columns) in the child table are set to NULL. RESTRICT: if a row from the parent table has a matching row in the child table, MySQL rejects deleting or updating rows in the parent table. NO ACTION: is the same as RESTRICT. SET DEFAULT: is recognized by the MySQL parser. However, this action is rejected by both InnoDB and NDB tables. In fact, MySQL fully supports three actions: RESTRICT, CASCADE and SET NULL. If you don’t specify the ON DELETE and ON UPDATE clause, the default action is RESTRICT. MySQL FOREIGN KEY examples Let’s create a new database called fkdemo for the demonstration. CREATE DATABASE fkdemo;USE fkdemo;Code language: SQL (Structured Query Language) (sql) RESTRICT &amp; NO ACTION actions Inside the fkdemo database, create two tables categories and products: CREATE TABLE categories( categoryId INT AUTO_INCREMENT PRIMARY KEY, categoryName VARCHAR(100) NOT NULL) ENGINE=INNODB;CREATE TABLE products( productId INT AUTO_INCREMENT PRIMARY KEY, productName varchar(100) not null, categoryId INT, CONSTRAINT fk_category FOREIGN KEY (categoryId) REFERENCES categories(categoryId)) ENGINE=INNODB;Code language: SQL (Structured Query Language) (sql) The categoryId in the products table is the foreign key column that refers to the categoryId column in the categories table. Because we don’t specify any ON UPDATE and ON DELETE clauses, the default action is RESTRICT for both update and delete operation. The following steps illustrate the RESTRICT action. \\1) Insert two rows into the categories table: INSERT INTO categories(categoryName)VALUES (&#x27;Smartphone&#x27;), (&#x27;Smartwatch&#x27;);Code language: SQL (Structured Query Language) (sql) \\2) Select data from the categories table: SELECT * FROM categories;Code language: SQL (Structured Query Language) (sql) \\3) Insert a new row into the products table: INSERT INTO products(productName, categoryId)VALUES(&#x27;iPhone&#x27;,1);Code language: SQL (Structured Query Language) (sql) It works because the categoryId 1 exists in the categories table. \\4) Attempt to insert a new row into the products table with a categoryId value does not exist in the categories table: INSERT INTO products(productName, categoryId)VALUES(&#x27;iPad&#x27;,3);Code language: SQL (Structured Query Language) (sql) MySQL issued the following error: Error Code: 1452. Cannot add or update a child row: a foreign key constraint fails (`fkdemo`.`products`, CONSTRAINT `fk_category` FOREIGN KEY (`categoryId`) REFERENCES `categories` (`categoryId`) ON DELETE RESTRICT ON UPDATE RESTRICT)Code language: JavaScript (javascript) \\5) Update the value in the categoryId column in the categories table to 100: UPDATE categoriesSET categoryId = 100WHERE categoryId = 1;Code language: SQL (Structured Query Language) (sql) MySQL issued this error: Error Code: 1451. Cannot delete or update a parent row: a foreign key constraint fails (`fkdemo`.`products`, CONSTRAINT `fk_category` FOREIGN KEY (`categoryId`) REFERENCES `categories` (`categoryId`) ON DELETE RESTRICT ON UPDATE RESTRICT)Code language: JavaScript (javascript) Because of the RESTRICT option, you cannot delete or update categoryId 1 since it is referenced by the productId 1 in the products table. CASCADE action These steps illustrate how ON UPDATE CASCADE and ON DELETE CASCADE actions work. \\1) Drop the products table: DROP TABLE products;Code language: SQL (Structured Query Language) (sql) \\2) Create the products table with the ON UPDATE CASCADE and ON DELETE CASCADE options for the foreign key: CREATE TABLE products( productId INT AUTO_INCREMENT PRIMARY KEY, productName varchar(100) not null, categoryId INT NOT NULL, CONSTRAINT fk_category FOREIGN KEY (categoryId) REFERENCES categories(categoryId) ON UPDATE CASCADE ON DELETE CASCADE) ENGINE=INNODB;Code language: SQL (Structured Query Language) (sql) \\3) Insert four rows into the products table: INSERT INTO products(productName, categoryId)VALUES (&#x27;iPhone&#x27;, 1), (&#x27;Galaxy Note&#x27;,1), (&#x27;Apple Watch&#x27;,2), (&#x27;Samsung Galary Watch&#x27;,2);Code language: SQL (Structured Query Language) (sql) \\4) Select data from the products table: SELECT * FROM products;Code language: SQL (Structured Query Language) (sql) \\5) Update categoryId 1 to 100 in the categories table: UPDATE categoriesSET categoryId = 100WHERE categoryId = 1;Code language: SQL (Structured Query Language) (sql) \\6) Verify the update: SELECT * FROM categories;Code language: SQL (Structured Query Language) (sql) \\7) Get data from the products table: SELECT * FROM products;Code language: SQL (Structured Query Language) (sql) As you can see, two rows with value 1 in the categoryId column of the products table were automatically updated to 100 because of the ON UPDATE CASCADE action. \\8) Delete categoryId 2 from the categories table: DELETE FROM categoriesWHERE categoryId = 2;Code language: SQL (Structured Query Language) (sql) \\9) Verify the deletion: SELECT * FROM categories;Code language: SQL (Structured Query Language) (sql) \\10) Check the products table: SELECT * FROM products;Code language: SQL (Structured Query Language) (sql) All products with categoryId 2 from the products table were automatically deleted because of the ON DELETE CASCADE action. SET NULL action These steps illustrate how the ON UPDATE SET NULL and ON DELETE SET NULL actions work. \\1) Drop both categories and products tables: DROP TABLE IF EXISTS categories;DROP TABLE IF EXISTS products;Code language: SQL (Structured Query Language) (sql) \\2) Create the categories and products tables: CREATE TABLE categories( categoryId INT AUTO_INCREMENT PRIMARY KEY, categoryName VARCHAR(100) NOT NULL)ENGINE=INNODB;CREATE TABLE products( productId INT AUTO_INCREMENT PRIMARY KEY, productName varchar(100) not null, categoryId INT, CONSTRAINT fk_category FOREIGN KEY (categoryId) REFERENCES categories(categoryId) ON UPDATE SET NULL ON DELETE SET NULL )ENGINE=INNODB;Code language: SQL (Structured Query Language) (sql) The foreign key in the products table changed to ON UPDATE SET NULL and ON DELETE SET NULL options. \\3) Insert rows into the categories table: INSERT INTO categories(categoryName)VALUES (&#x27;Smartphone&#x27;), (&#x27;Smartwatch&#x27;);Code language: SQL (Structured Query Language) (sql) \\4) Insert rows into the products table: INSERT INTO products(productName, categoryId)VALUES (&#x27;iPhone&#x27;, 1), (&#x27;Galaxy Note&#x27;,1), (&#x27;Apple Watch&#x27;,2), (&#x27;Samsung Galary Watch&#x27;,2);Code language: SQL (Structured Query Language) (sql) \\5) Update categoryId from 1 to 100 in the categories table: UPDATE categoriesSET categoryId = 100WHERE categoryId = 1;Code language: SQL (Structured Query Language) (sql) \\6) Verify the update: SELECT * FROM categories;Code language: SQL (Structured Query Language) (sql) \\7) Select data from the products table: The rows with the categoryId 1 in the products table were automatically set to NULL due to the ON UPDATE SET NULL action. \\8) Delete the categoryId 2 from the categories table: DELETE FROM categories WHERE categoryId = 2;Code language: SQL (Structured Query Language) (sql) \\9) Check the products table: SELECT * FROM products;Code language: SQL (Structured Query Language) (sql) The values in the categoryId column of the rows with categoryId 2 in the products table were automatically set to NULL due to the ON DELETE SET NULL action. Drop MySQL foreign key constraints To drop a foreign key constraint, you use the ALTER TABLE statement: ALTER TABLE table_name DROP FOREIGN KEY constraint_name;Code language: SQL (Structured Query Language) (sql) In this syntax: First, specify the name of the table from which you want to drop the foreign key after the ALTER TABLE keywords. Second, specify the constraint name after the DROP FOREIGN KEY keywords. Notice that constraint_name is the name of the foreign key constraint specified when you created or added the foreign key constraint to the table. To obtain the generated constraint name of a table, you use the SHOW CREATE TABLE statement: SHOW CREATE TABLE table_name;Code language: SQL (Structured Query Language) (sql) For example, to see the foreign keys of the products table, you use the following statement: SHOW CREATE TABLE products;Code language: SQL (Structured Query Language) (sql) The following is the output of the statement: As you can see clearly from the output, the table products table has one foreign key constraint: fk_category And this statement drops the foreign key constraint of the products table: ALTER TABLE products DROP FOREIGN KEY fk_category;Code language: SQL (Structured Query Language) (sql) To ensure that the foreign key constraint has been dropped, you can view the structure of the products table: SHOW CREATE TABLE products;Code language: SQL (Structured Query Language) (sql) Disabling foreign key checks Sometimes, it is very useful to disable foreign key checks e.g., when you import data from a CSV file into a table. If you don’t disable foreign key checks, you have to load data into a proper order i.e., you have to load data into parent tables first and then child tables, which can be tedious. However, if you disable the foreign key checks, you can load data into tables in any order. To disable foreign key checks, you use the following statement: SET foreign_key_checks = 0;Code language: SQL (Structured Query Language) (sql) And you can enable it by using the following statement: SET foreign_key_checks = 1;Code language: SQL (Structured Query Language) (sql) In this tutorial, you have learned about the MySQL foreign key and how to create a foreign key constraint with various reference options. DATA TYPES GLOBALIZARION IMPORT &amp; EXPORT Retrieving records","categories":[{"name":"Technology","slug":"Technology","permalink":"http://lyk-love.cn/categories/Technology/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://lyk-love.cn/tags/Database/"}]},{"title":"L8 logn search","slug":"L8-logn-search","date":"2022-09-26T06:39:34.933Z","updated":"2022-09-26T06:39:34.933Z","comments":true,"path":"2022/09/26/L8-logn-search/","link":"","permalink":"http://lyk-love.cn/2022/09/26/L8-logn-search/","excerpt":"Binary Search Generalized Outline： Red-Black Tree","text":"Binary Search Generalized Outline： Red-Black Tree Balanced Binary Search Tree binary search tree Def 2-Tree 左子树的所有值比根节点小，右子树的所有值比根节点大（如果properly drawn的话，会很清楚 ） Red-Black Tree Def (基于二叉搜索树，附加一些性质) If T is a binary search tree in which each node has a color, red or black, and all external nodes are black, then T is a red-black tree if and only if: [Color constraint] No red node has a red child [Black height constraint] The black length of all external paths from a given node u is the same(the black height of u) The root is black Almost-red-black tree(ARB tree) Root is red, satisfying the other constraints Recursive Definition of RBT (a red black tree of black height h is denoted as $RB_h$ Def: An external node is an $RB_0$​ tree, and the node is black. A binary tree is an $ARB_h$( $h \\ge 1$ )tree if: Its root is red,and Its left and right subtrees are each an $RB_{h-1}$ tree. A binary tree is an $RB_h$ tree if: Its root is black, and Its left and right subtrees are each either an $RB_{h-1}$​ tree or an $ARB_{h}$​ tree.​ Properties of Red-Black Tree The black height of any $RB_h$ tree or $ARB_h$ is well-defined and is h. Let T be an $RB_h$ tree, then: T has at most $2^h-1$ internal black nodes. T has at most $4^h-1$ in internal nodes. The depth of any black node is at most twice its black depth. Let A be an $ARB_h$ tree, then: A has at least $2^h-2$ internal black nodes. A has at most $\\frac{4^h}{2}-1$ internal nodes. The depth of any black node is at most twice its black depth. Bound on Depth of Node in RBT Let T be a red-black tree with n internal nodes. Then no node has black depth greater than $log(n+1)$, which means that the height of T in the usual sense is at most $2log(n+1)$​. Proof: Let h be the black height of T. The number of internal nodes, n, is at least the number of internal black nodes, which is at least $2^h-1$​, so $h \\le log(n+1)$​​. The node with greatest depth is some external node. All external nodes are with black depth h. So, the depth is at most $2h$​. Deletion Logical: 删除节点的内容 Structural：删除节点，整棵树做修复 Complexity of Operations on RBT With reasonable implementation A new node can be inserted correctly in a red-black tree with n nodes in $\\Theta(logn)$​ time in the worst-case Repairs for deletion do $O(1)$ structural changes, but may do $O(logn)$color changes","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[{"name":"Algorithm design and analysis","slug":"Algorithm-design-and-analysis","permalink":"http://lyk-love.cn/tags/Algorithm-design-and-analysis/"}]},{"title":"Love, Death and Robots","slug":"Love-Death-and-Robots","date":"2022-09-26T06:39:34.933Z","updated":"2022-09-26T06:39:34.933Z","comments":true,"path":"2022/09/26/Love-Death-and-Robots/","link":"","permalink":"http://lyk-love.cn/2022/09/26/Love-Death-and-Robots/","excerpt":"Sorted by episode","text":"Sorted by episode 桑尼的优势 打斗太帅。这种竞技场格斗，怪物绕着墙壁爬呀爬的画面好像在影视中一再出现。 比如暗黑3。 当然还有《刺杀小说家》，最后Boss战里，Boss也是打着打着爬墙上去了。 不过《刺杀小说家》是烂片，烂到我不想吐槽（它居然是路阳拍的，曾导演《绣春刀。。》。 看来中国又堕落了一个导演） 这集就是展现技术力的，画面很酷，最后的激情戏，主角做到一半头突然被刺穿的一幕挺惊悚的。 结局也能想到，在赛博时代，人的“身体”其实是个逻辑概念，“我”不一定在“我”这个身体里。 这种关于赛博时代人的肉体和思维分离的考虑也在爱死机的好几集中出现。 三个机器人 轻松的讽刺篇。 末日后三个机器人旅行人类城市的故事。 三“人”在篮球馆吵吵闹闹，背景中挂着一个吊死的女孩，这幕格外讽刺和戏谑。 只有人类会造出各种恐怖的玩意儿专门用来把同类弄死。 证人 循环叙事，技术力超强，美术也很好。 关于凶手在街上追逐一个裸体女人的故事，还有恋物癖性俱乐部的脱衣舞，未来式的香港城市。 其实画面已经隐喻了这是个梦 --- 歪曲的马路线（主角内心）、粉色的灯牌（女生嘛）、俱乐部中人们扭曲且模糊的脸（说明这些人在主角心中都一个样，主角对他们没什么印象） 、俱乐部墙上屏幕中的“Leash”字样（暗示主角讨厌俱乐部环境，她想要逃脱）。 拿了三项艾美奖哟，导演很厉害。 机动装甲 本以为是外星的植物大战僵尸，没想到是星际殖民的故事。人类---农夫们才是星球的殖民者，而怪物是原住民。 人类的殖民地（农庄）在这个星球上就像一个个脓包一样，绝妙的讽刺。 噬魂者 考古队挖到德古拉发现它怕猫，侥幸逃脱后发现自己来到德古拉老巢的故事，最后肯定团灭啦φ(゜▽゜*)♪ 有个很变态的肢解镜头。 德古拉怕小猫咪也挺好玩的。 如果硬说有什么教育意义的话，就是告诫人们不要自以为是，乱挖危险的坟墓。 否则尽管运气好有小猫咪，面对几百只德古拉也只能等死。 当酸奶统治世界 超好笑。 讽刺人统治世界还不如酸奶。 酸奶问总统要俄亥俄州，总统不给， 酸奶说你不给我们就去找中国，他们答应给我们一个省，总统听了马上就答应了。 笑死了哈哈哈。 人被酸奶统治后，发现日子过得反而越来越好，最后酸奶们只花了极短的时间就实现了科技跃迁，前往外太空了。 人类却担忧：“没有酸奶的统治我们该怎么办呀QAQ”，从统治到被统治的这一反转，讽刺了人类政府和人类种族的无能。 酸奶们的太空飞船是酸奶盒形状的哟。 裂缝之外 类似缸中之脑，不过外星人也是出于好心。 床戏很好看[]~(￣▽￣)~* 狩猎愉快 “我想狩猎，狩猎那些以为能够支配我们的男人，那些作奸犯科，却美其名曰进步的男人。” 讲外国对中国的殖民，借女权来讲中国的屈辱历史，毕竟女性的遭遇就是当时中国的一个缩影。 这片能上爱死机也反映了外国友人并非啥都不懂，人家也知道殖民主义是不对的。 夜幕下小狐狸的身影在楼宇间跳动，分不清它的躯体是机械还是血肉。 The Dump 垃圾堆成精了。 该片也讽刺了那些“上流人物”，他们瞧不起住在垃圾堆里的底层人民，殊不知自己才是制造垃圾的那方。 Shape Shifters 讲战争对人性的异化，那些能变身狼人而获得更强战力的士兵反而被其他士兵恐惧和排挤，认为他们是怪物，不是“自己人”。讽刺种族问题吧，能力再强也有的是办法排挤你。 变异士兵的处境和越战老兵很像，当初被派出去保家卫国，回来后还要被本国人骂“baby killer”，而主导这一切的政客高高在上。 Helping Hand 动画版《地心引力》，歌颂人的勇气和坚强的，看着太疼了ORZ Fish Night 既然人死后灵魂能在屋子里徘徊，那这些鱼啊水木啊死后，它们的灵魂能在这片昔日的大海存留吗？ 超浪漫的一集，话说大海死后有没有灵魂？ Lucky 13 外星战争，飞机有灵，完了。 Zima Blue 超爱的一集，美术很好看。 讲艺术的一集，和我的理解不谋而合。 艺术就是世界观，你在艺术中想描绘的世界就是你认知的世界。 起初我们是模仿世界，对现实模仿得越来越像；后来是认知世界，对世界的各方各面进行探索，无所不包；最后是解构世界----归根到底，我们的“世界”永远是我们“认知”的世界，客观的世界只有被认知成为主观的世界才能进入我们的心灵，因此到了这一阶段，艺术就开始注重于内心世界的建构了，外在世界只是为内心世界的建设提供原材料而已。 艺术家Zima起初只是个清洗泳池的小机器人，经过一代代主人的改造，才成为了拥有完备功能和智力的AI。 他的艺术的变化就是他世界观的变化，起初是模仿世界（画人像），后来不断探索世界（画星空），到后来回归自我，探索自己认知世界的方式（画蓝色块块，其实是泳池瓷砖的颜色，它作为泳池机器人时醒来第一眼就是这个）。最后Zima回归本源，有人理解是“生命的终点是回归起点”，我不太能理解这话是什么意思， 我对Zima此举的理解是它想要回到生命最初的状态，来体验那种纯粹地认知世界的过程。 Blind Spot 意识存档，可备份的意识还是主人吗？身为计算机学生好像不应该问这个问题。。。。毕竟意识就是一连串电波编码（我猜的，如有不对请指正）。 编码这种东西，当然不具备唯一性，所以完全可以存在两串一模一样的编码，不存在谁是主人的问题。 Ice Age 冰箱里有个微缩文明不断轮回。 小文明进行到现实世界的21世纪时突然爆发了核战争也是很讽刺，不过核战争后人类文明没有灭亡，反而进一步进化了，也算是对未来的美好祝愿吧。 其实我认为核战争早晚会爆发，一个文明想要不断进步就必须经历越来越惨烈的战争，否则科技无法跳跃式发展。 没有两次大战，我们说不定还活在蒸汽时代呢。 Alternative Histories 如果历史能被改变，我们会不会过上不一样的日子？ 显然不会，本片以戏谑的手法告诉人们，历史是不会以个人为转移的，哪怕你能以各种手法提前杀死希特勒，二战依然会爆发。 不过虽然历史大势不会改变，对于个人和某个团体而言，历史的细微变动确实能影响他们的际遇。 比如，如果希特勒是被俄国人杀死，俄国说不定能成为现在最强大的国家。 The Secret War 黑红军的，技术力挺好。","categories":[{"name":"Art","slug":"Art","permalink":"http://lyk-love.cn/categories/Art/"}],"tags":[{"name":"Movie","slug":"Movie","permalink":"http://lyk-love.cn/tags/Movie/"}]},{"title":"History of Western Music History","slug":"History of Western Music History","date":"2022-09-26T06:39:34.931Z","updated":"2022-09-26T06:39:34.931Z","comments":true,"path":"2022/09/26/History of Western Music History/","link":"","permalink":"http://lyk-love.cn/2022/09/26/History%20of%20Western%20Music%20History/","excerpt":"Ref: [西方音乐通论][http://elite.nju.edu.cn/jiaowu/student/elective/courseList.do?method=getCourseInfoM&amp;courseNumber=37100180&amp;classid=101708]","text":"Ref: [西方音乐通论][http://elite.nju.edu.cn/jiaowu/student/elective/courseList.do?method=getCourseInfoM&amp;courseNumber=37100180&amp;classid=101708] 西方音乐调式 调式以一个音为中心,其它音围绕它构建 分为大调和小调（文艺复兴时期发明） 每个音都有其功能性和倾向性 1 主音 2 上主音 3 中音 4 下属音 5 属音，解决1 6 下中音 7 导音 【1】主音 7是导音,西方音乐里认为7最不稳定,强烈地倾向于高音1 Major 大调式 明亮,稳定,辉煌 由12345671构成. 以 1 为中心. 大调式有三种形态: 自然大调: 12345671 中等 没有降调 &lt;国歌&gt; 和声大调: 12345 (6)71 最明亮, &lt;我的太阳&gt; 566716532153211 旋律大调: 12345(67)1 相对最柔和,但没有小调那么晦涩 Minor 小调式 柔和黯淡 自然小调:(6)(7)123456 没有降调 中等程度 &lt;喀秋莎&gt; 和声小调:(6)71234【5】6 最柔和,最忧伤 旋律小调: (6)(7)123【4】【5】6 **相对明亮一点 ** &lt;斯卡布罗&gt; 标志性的【4】 中国民族音乐调式 宫调式:以1为核心(主音),和西方的大调式很像 商调式:以2为核心(主音) 角调式:以3为核心(主音) 徵调式:以5为核心(主音). 以5结束. &lt;茉莉花&gt; 羽调式:以6为核心(主音). 称为五声调式。中国音乐里有5和7，但都会一带而过。 4叫清角,#4叫变徵,7叫变宫,#7叫闰 欧洲中世纪的音乐（4-6世纪） 米拉琴：西方音乐的一个象征 所有乐器里最古老的：弹拨乐。 有横弹和竖弹。 比如吉他，有西班牙吉他（横弹，一般都是这个）和夏威夷吉他（竖弹）两种。 欧洲中世纪的音乐不只是音乐本身。宗教是很大因素。公元一世纪基督教出现，基督教徒有两大重要活动：日课,圣咏。 395年，基督教随着罗马帝国分裂，分为东西两部 罗马教皇格里高利一世发现各地的圣咏的语言，内容，形式都不一样，于是他规定圣咏的表现形式：无伴奏（阿卡贝拉）、拉丁文、曲调平缓（对欧洲音乐的发展非常深远，意大利歌剧（诞生于巴洛克时期的美声唱法就是格里高利圣咏经过漫长演变而来的）、服装朴素、表情严肃。这对以后欧洲音乐的影响非常深远。 中世纪音乐享有崇高地位。属于七艺： 语文艺术(语法、逻辑、修辞) 数学艺术（算数、几何、天文、音乐） 中世纪音乐除了圣咏，还有经文歌曲、方言歌曲，基本上都是单音音乐（只有一个声部，两个声部叫复调），曲调比较平缓。乐器也分为两大类：弦乐器（主要是弹拨乐，出现得比较早）、管乐器。 此外还有一些音乐具有简单的复调，叫做奥尔加农 ref：《乡村骑士间奏曲》 圭多 复调 文艺复兴时期 帕列斯特里那 巴洛克音乐 巴洛克（Baroque）: 在音乐方面指1600 - 1750 左右的欧洲音乐。这个时期复调音乐获得了充分的发展，达到历史上的巅峰，也是巴洛克音乐的主要标志。 强调情感的表现和戏剧性对比，在细节上充满装饰性。艺术观念和手法大胆，有尽可能的综合各门艺术的倾向 最重要推动力：16C的人文主义思潮，它直接促进了单声歌曲（monody）和歌剧的诞生 时期： 早期：1600 - 1640，巴洛克风格的形成期，它发源于晚期文艺复兴的意大利 中期：1640-1690，定型期，音乐语言逐渐相通，风格开始传遍欧洲 1690-1750：盛期：不是创新，而是完善的时期，达到辉煌的顶点 当时的键盘乐器主要是管风琴和古钢琴。 协奏曲（concerto）：巴洛克最重要的器题材。（在巴洛克时期）强调在一个统一的形式中的突然对比。 指一件或多件独奏乐器与管弦乐队协同演奏 独奏协奏曲（solo concerto）：强调一件独奏乐器和整个乐队之间的对比和协调 大协奏曲（concerto grosso）：强调某一乐器组（如弦乐队）与整个乐队之间的对比 巴洛克时期,声乐和器乐并行发展 单声歌曲: monody, 通奏低音伴奏的宣叙风格的歌曲 * 歌剧 来源: 16C末意大利北部的宫廷娱乐活动和在这种活动中产生的幕间剧 第一部作为音乐完整保留下来的歌剧: Euridice 蒙特威尔第 意大利歌剧: 诞生于佛罗伦萨, 发展于威尼斯,成熟于那不勒斯. 那不勒斯也称为&quot;意大利歌剧的故乡&quot;. 歌剧有两种:宣叙调（唱），念白（说） 蒙特威尔第: 巴洛克早期最重要的音乐家,威尼斯圣马可大教堂的主教,且是唱诗班的带头人. 他创作了《奥菲欧》（L'Orfeo，被称为西方音乐史上第一部歌剧）、《奥丽安娜》。最早期，在佛罗伦萨诞生的歌剧只有简单的宣叙调，而《奥尔菲斯》有了真正的宣叙调。宣叙调（唱），念白（说） 意大利除了歌剧，还有戏剧,遵循罗马传统, 罗马歌剧和意大利戏剧是意大利的两大艺术. 意大利乐器:小提琴 阿马蒂，两个学生：斯特拉迪瓦里， 瓜尔内里 意大利的维奥尔(Viol)家族,以生产 小提琴( violin ), 中提琴( viola ), 大提琴( violocello ) 闻名 当时最好的小提琴制造师是艾尔玛蒂(Amati) 意大利是欧洲拉丁语系的国家, 拉丁语系国家还有法国, 法国的小提琴不如意大利,但是法国盛产钢琴 巴洛克时期音乐特征： 通奏低音 器乐的发展 奏鸣曲( sonata ): 独立的器乐作品, 通常是无标题的, 多乐章的室内乐合奏 教堂奏鸣曲 室内奏鸣曲 康塔塔( cantata ): 声乐作品 维塔利 Vitali, 1632 ~1692 《G小调恰空》 科雷利 阿尔坎杰罗·科雷利，A.Arcangelo Corelli (1653～1713) 意大利小提琴学派的奠基人 第一个完全用大小调体系进行创作的作曲家 《福利亚（La Folia）变奏曲》 维瓦尔第 1678 - 1741, “红发神父” 早年写了很多歌剧，活得很滋润，但是引来了天主教会的不满，被禁止在天主教地区演奏，晚年贫困而死。 曾在威尼斯的仁爱救济院任音乐教师 仁爱救济院：女童孤儿院。学习优秀的女孩会加入乐队，演奏水平被认为是威尼斯最高的，甚至超过了威尼斯当地的歌剧院。 在托雷利的基础上进一步发展了协奏曲。确立了协奏曲的“快 - 慢 - 快”的三部分形式，一直被沿用到浪漫主义时期。 三部分：Allegro（小快板） -- Adagio（柔板） -- Presto（急板） 协奏曲: 协奏曲（Concerto），是指一件或多件独奏乐器与乐队]协同演奏， 协奏曲一词来源于16世纪的意大利语concertare，意为“协调一致”；17世纪，协奏曲一词又产生了拉丁文涵义“竞争”等，接近于现代意义上的协奏曲 。 巴洛克晚期 1690 - 1750 法国著名音乐家: 拉莫 , 让·巴普蒂斯特·吕利。 德国音乐家: 拉莫 (Jean - Phiipre Rameau,1683-1764) 法国18C最重要的作曲家 《和声学》 斯卡拉蒂 Domenico Scarlatti （1685-1757） D大调奏鸣曲 亨德尔 亨德尔: 只写清唱剧（和歌剧） 亨德尔（Georg Friedrich Handel，1685—1759）出生于德国哈勒城的一个小市民家庭，是著名的英籍德国作曲家。亨德尔少年时期曾跟随当地风琴师、作曲家学习音乐，后来担任了哈雷礼拜堂的风琴师，并开始创作。 宗教清唱剧《弥赛亚》； 管弦乐《水上音乐》《皇家焰火音乐》《奥兰多》&quot;&quot; 首创用英语演唱清唱剧 他的清唱剧为剧场而不是教堂而写，因此具有强烈的戏剧性 巴赫 巴洛克音乐集大成的音乐家：巴赫，他在音乐上最早的突破是管风琴领域，音乐上最大的突破是复调。 约翰·塞巴斯蒂安·巴赫（Johann Sebastian Bach，1685年3月21日-1750年7月28日），出生于德国图林根州，他被称为“西方音乐之父” 在圣托马斯教堂工作了20多年 复调：最简单的形式是 奥尔加农,普通的叫复调,复杂的叫赋格. 两段或两段以上同时进行、相关但又有区别的声部所组成，这些声部各自独立，但又和谐地统一为一个整体，彼此形成和声关系，以对位法为主要创作技法。 不同旋律的同时结合叫做对比复调，同一旋律隔开一定时间的先后模仿称为模仿复调 半音阶幻想曲与赋格, 15首两部创意曲,。 第一个把十二平均律应用在钢琴领域( 明朝 zhuzaiyu证明了十二平均律, 可是没有应用在音乐实践上 )；200多首宗教contata（占作品总数的一半）, 23首世俗contata, 《马太受难曲》,《约翰受难曲》 《平均律钢琴曲集》，被誉为“钢琴家的旧约圣经”，共48首，每首包含一首前奏曲和赋格； 《赋格的艺术》，巴赫最后的作品，都是由一个主题发展变化而来，此作还未完成，巴赫就去世了，这是巴赫晚年对艺术最深层探索的思想和总结； 《勃兰登堡协奏曲》，大协奏曲，一共6首，每首的独奏乐器组都有不同，这组乐曲被瓦格纳称为“一切音乐中最惊人的奇迹” 献给勃兰登堡的路德维希侯爵，其宫廷位于普鲁士首都柏林，巴赫的目的是要在柏林找工作（但没成功） 将大协奏曲推向了最后的高峰 《哥德堡变奏曲》，巴赫最著名的变奏曲，全曲共32段，其中30段变奏，每3个为一组，以第16变奏为中心，速度分为前后两半，作曲结构非常精妙，让人叹为观止； 《意大利协奏曲》，是一首主调与复调交替进行的作品，是对意大利作曲家维瓦尔第小提琴协奏曲继承和发扬。 《马太受难乐》：《尼伯龙根的指环》之前最伟大的戏剧作品，“现存宗教音乐的巅峰” 由门德尔松发掘 巴赫预示着一个崭新时代的到来: 古典乐派. 代表人物: 海顿, 莫扎特, 贝多芬. 古典乐派(维也纳乐派) 1750（巴赫逝世） - 贝多芬 古典乐派讲究严谨的结构，完美的形式，和谐的音响与严密的逻辑。乐曲多以严肃，稳重为主，崇尚理性，而对感情的表现为含蓄内在。器乐曲多是无标题音乐。主要作曲家有：亨德尔、巴赫、海顿、莫扎特、贝多芬等人。 古典主义时期被分为两个阶段——前古典时期以及维也纳古典乐派时期 维也纳乐派三巨头： 海顿， 莫扎特， 贝多芬（ 贝多芬是德国人， 其余两个是奥地利人 ） 这里插入一下音乐学校作曲系考试流程: 乐理, 作曲 和声 艺术歌曲, 钢琴, 面试 前古典时期 维也纳古典时期 维也纳是奥地利哈布斯堡王朝的都城 海顿 1792 - 1809. 出身在奥匈边境的小镇（ 奥地利南部的罗劳 ），8岁到维也纳，成为Stephen教堂唱诗班成员。 后来被聘为宫廷乐队乐长服务了30年，在此阶段创作了一生中绝大部分作品 交响曲之父。 写了108部交响乐，26部歌剧，4部大型清唱剧， 68首弦乐四重奏。 交响乐： 《号角》、《告别》、 6首《巴黎》、《牛津》、 12首《伦敦》 清唱剧： 《创世纪》、《四季》 弦乐四重奏： 《云雀》、《皇帝》 海顿之前已经有交响乐，海顿把交响乐推进到了古典主义风格 爱乐 ：1839年，维也纳宫廷的乐师到民间演奏的时候，为了显示自己与民间音乐家水平的差距，自称为Philharmonic。 指“由离开宫廷的乐手们，为在民间演出而自发组成的演奏团体” 代表作：两部交响曲《创世纪》《四季》 莫扎特 莫扎特不像海顿一样生活依靠宫廷，他的生活依靠出版商，非常不稳定，他接触了很多阴暗面。 所以他的音乐尽管很华丽，但和声部分总是有无奈伤感的部分。“乐神” 11岁的时候写出了他的第一部歌剧《装疯卖傻》 贝多芬 德国人的骄傲，维也纳的骄傲，世界的骄傲。 “乐圣”。 出生于波恩。 单簧管：表现男性 双簧管：表现女性 长笛：表现孩子 《第六交响曲: 田园》：开创了浪漫主义时代 作品： 歌剧：《费德里奥》 庄严弥撒： D大调弥撒 歌曲： 《阿德拉伊德》、《致远方的爱人》 etc.： 《 第三交响曲 》、 《瓦尔德斯坦》、 《热情》、 《克莱采》、《拉祖莫夫斯四重奏》 “自由与进步” 浪漫主义（ 19 ~ 20 世纪 ） 浪漫主义音乐从德奥音乐开始. 德奥音乐 韦伯 卡尔·玛利亚·冯·韦伯（Carl Maria von Weber，1786—1826）出生在德国奥尔登堡的厄丁 1813年曾任布拉格歌剧院的院长。1817年担任德累斯顿宫廷剧院乐长。（ 德累斯顿是&quot;德国的佛罗伦萨&quot; ） 代表作品《自由射手》( 被认为是第一部真正的浪漫主义歌剧 ); 《欧丽安特》（ Euryanthe，1817—1821）和《奥伯龙》（Oberon，1825—1826）。 此人特别喜欢单簧管。写了两首单簧管协奏曲 钢琴曲阶梯: (拜厄, 汤普森) 599 → 849 → 299 → 740 → 肖邦 → 李斯特 → 斯特劳斯基 舒伯特 Franz Schubert，1797年1月31日—1828年11月19日 出生在维也纳郊区, 生活条件艰苦, 才华横溢, 音乐旋律性尤其出色. 他是贝多芬的粉丝. 严格来说,二人是师兄弟, 都受萨列里教导. 萨列里对二人的评价是: 贝多芬是天才, 而舒伯特是个好人. 舒伯特的旋律非常深邃. 因为生活贫困,他的很多歌曲都是在没有准备的情况下写的, 因此他写了很多即兴, 天马行空,非常浪漫. &quot;歌曲之王&quot;: 由于同时代有贝多芬,所以他只能在歌曲上下功夫. 著名作品： 艺术歌曲&lt;野玫瑰&gt;&lt;菩提树&gt;&lt;流浪者&gt;&lt;在海边&gt;&lt;魔王&gt;&lt;鳟鱼&gt;&lt;小夜曲&gt;&lt;摇篮曲&gt;&lt;天鹅之歌&gt;(14首) 声乐套曲《美丽的磨坊女》《冬之旅》 交响曲：重要的交响曲: 第五交响曲, 第九交响曲, 第八交响曲(未完成) 门德尔松 Mendelssohn,1809年2月3日—1847年11月4日 代表作： 第三交响曲《苏格兰交响曲》， 第四交响曲《意大利交响曲》 小提琴协奏曲有：《E小调小提琴协奏曲》 斯拉夫音乐 肖邦 F.F.Chopin，1810年3月1日—1849年10月17日 “钢琴诗人”，只写钢琴作品 当时，浪漫主义由西欧向东欧（斯拉夫民族）过渡。 肖邦的妈妈是波兰人，爸爸是法国人。他父亲教他法语。十几岁时，他就在华沙成名了，因为他的钢琴技巧，也因为他的大量的具有显著浪漫主义特征的钢琴作品。 肖邦的音乐不同于德奥音乐的严肃，他的作品轻盈优雅，如同诗歌。 肖邦在音乐学院毕业前，就写了《第一钢协》和《第二钢协》。肖邦认为他的钢琴会让人沉醉，于是故意不让钢琴太早出来，使前奏一拖再拖。《第一钢协》第一乐章的第一主题，钢琴一出来果真打动人心。第二乐章也很精彩，像是夜晚躺在床上看窗外的月光，想起童年的时光和妈妈的话语。 第一乐章：**庄严的快板，E小调。**形式是追随莫扎特确立的古典协奏曲形式，先由管弦乐合奏第一主题，第二主题由弦乐以E大调奏出。钢琴出现后，极尽技巧化而华丽地表现这两个主题，然后以管弦乐合奏结束呈示部。发展部为C大调，钢琴从处理第一主题后半段始，此后诸主题在多种转调中发展。再现部调子改为G大调，最后以钢琴灿烂技巧发挥的激动达到高潮。 **浪漫曲，甚缓板，E大调，有夜曲风格。**主题如歌性格，由两部分构成。其后半段以B大调开始，间奏后进入激动的中段，以强音出现升C小调略呈灰暗的新主题。此主题奏完后以升G大调回到仍装饰得很复杂的主题后半段。尾奏使用序奏部材料，以音阶与琶音三连音轻快的动态装饰，然后像烟雾消失般结束。 **回旋曲，甚快板，E大调。**钢琴谐谑地诱导出第一主题，产生妙趣横生的反复进行，插入的方式极有莫扎特的魅力，结尾是华丽的尾奏，以钢琴奏三连高音阶性乐流结束。 华沙被沙俄攻陷后，他逃到巴黎他父亲的家。由于他的作品和当时西欧的音乐都不一样，很快引起了轰动。一次聚会上，他认识了乔治桑，二人成为知己和恋人（尽管二人年龄相差悬殊），肖邦为她写了大量的马祖卡和《夜曲》。 肖邦把钢琴当作倾诉的工具，这不同于李斯特，后者把钢琴看作万能乐器 意大利音乐 罗西尼 Gioacchino Rossini，1792~1868 意大利歌剧三杰：罗西尼，威尔第，普契尼。罗西尼最早，属于浪漫主义早期和中期 罗西尼是19世纪上半叶意大利歌剧的最高峰。 代表作：《塞维利亚的理发师》《鹊贼》《威廉退尔》 他通过不同的乐器的音色的变化，和力度，和声，织体的变化，来表现人物形象，剧情发展，哲学思想。其实，浪漫主义的一方面，就是音乐家们对器乐的性能做的大量探索。 他的最后一部歌剧《威廉退尔》，一举奠定了他的历史地位. coda快的同时还把音乐放轻，造成了极大的紧张度 《鹊贼》讲的是经常偷窃人类物品的小鸟。罗西尼在观众席后面也安排了小军鼓，小鸟们就在辉煌的鼓声中雄赳赳气昂昂地出场了。 众鸟们看到目标后开始俯冲,一只小鸟成功偷到了东西,被众鸟视作英雄. 威尔第 Giuseppe Verdi，1813年—1901年 近50岁才有较成熟的歌剧作品。 父亲是旅馆老板，和教堂里一位谈管风琴的人学音乐，后考到米兰音乐学院，一度想放弃音乐，又因为对音乐的热爱坚持了下来。从《罗彻斯特》（第一部）到《法尔斯塔夫》（最后一部），他一共写了26部歌剧。 代表作：《纳布科》《伦巴底人》《弄臣》《游吟诗人》《茶花女》《奥赛罗》《假面舞会》《唐·卡洛斯》《命运之力》 不同于罗西尼的欢乐俏皮，威尔第的歌剧非常深邃有哲理。 普契尼 Giacomo Puccini,1858年12月22日-1924年11月29日 比威尔第更有浪漫主义精神。出生于浪漫主义音乐世家，第一部歌剧叫《群妖围舞》. 代表作《图兰朵》《玛侬·雷斯考特》《波希米亚人》 法国主义音乐 柏辽兹 Hector Louis Berlioz,1803年12月11日-1869年3月8日 法国浪漫主义音乐非常重要的音乐家 没有受过正规的音乐训练，他父亲是巴黎医学院毕业的著名医生，想让柏辽兹学医。 由于觉得医学院太枯燥，柏辽兹跑到了歌剧院，一开始当实习，后参加了1830年的罗马作曲大赛，获得大奖（ 非常厉害，那个比赛不是写歌，而是写器乐作品 ）。 此后当了全职作曲家。 著名作品《幻想交响曲》《安魂曲》《葬礼与凯旋》《浮士德的天空》《特洛伊人》。《配器法》（著作） 由于不是音乐学院毕业，所以柏辽兹被当时的人看不起。后来受帕格尼尼赞助，才走上了职业音乐家的道路 创立了标题音乐(幻想交响曲,每个乐章都有个名字,所以无形中创立了标题音乐) 法国音乐不同于德国的严谨，非常的自由奇妙。 典型的是柏辽兹的《浮士德的天空》： “下午夕阳高照，一队军人训练结束准备回营房了”---《浮士德的天空》 这一段描写军队行进，但是节拍非常飘，初学者很难打准（可见法军的战斗力不咋地） 一个军队的音乐居然写得这么好玩幽默轻巧，可见法国音乐之浪漫。 匈牙利音乐 这个国家非常非常漂亮. 虽然奥地利人施特劳斯写过《蓝色多瑙河》，奥地利人也把多瑙河称作母亲河，事实上奥地利境内的多瑙河并不咋地，匈牙利境内的多瑙河才叫漂亮。这个国家能歌善舞，出过很多艺术大师，而且民族众多。 音乐伴随着整个匈牙利民族 李斯特 Franz Liszt，1811年10月22日—1886年7月31日,出生于匈牙利雷汀 &quot;钢琴之王&quot;,&quot;音乐巨人&quot;. 此人的手非常非常大,单手就能跨两个八度. 11岁就声明远扬. 而且长得非常帅 首创&quot;交响诗&quot;( 单乐章交响曲 )。 著名的有《塔索》《山岳》《匈牙利》 创立了布达佩斯音乐学院,亲自任第一任院长. 对整个欧洲音乐的发展起了极大的推动作用. 代表作: 12首超技曲, 6首帕格尼尼练习曲, 19首匈牙利狂想曲. D小调奏鸣曲(单乐章). 降E大调钢协, A大调钢协. 五大钢协: 贝多芬的&lt;“皇帝”钢琴协奏曲&gt;(降E大调)、 柴可夫斯基的《降b小调第一钢琴协奏曲》、 拉赫玛尼诺夫的《C大调第二钢琴协奏曲》、 舒曼的《a小调钢琴协奏曲》、 李斯特的《第一钢琴协奏曲》 德国音乐( 19世纪 ) 瓦格纳 Richard Wagner，1813年5月22日—1883年2月13日 此人强的离谱 自学的音乐,学成之后就看不起别的音乐家了. 他考上了莱比锡大学,读法律,后来放弃了,去小歌剧院当指挥. 第一部作品《黎恩济》 19世纪最有影响力的德国作曲家 《漂泊的荷兰人》《汤豪瑟》《纽伦堡的名歌手》《罗恩格林》《尼伯龙根的指环》《特里斯坦与伊索尔德》《帕西法尔》 他的音乐煽动性极强 舒曼 舒曼 robert schumann，1810年6月8日—1856年7月29日。 诞生于德国东部莱比锡附近的小城茨维考。父亲是作家（西方国家的作家的地位不高）。大学进莱比锡大学读法律，不满足，就拜里希·维克为师学钢琴，把手练坏了，但是她爱上了老师的女儿小维克·克拉拉。 舒曼写了大量的歌曲。有了克拉拉的爱，他生命中有一段时期非常辉煌，《a小调钢琴协奏曲》是五大钢协之一。舒曼后来得了精神病，经常跳莱茵河。 继门德尔松之后，十九世纪上半叶德国音乐史上突出的人物 代表作： 钢琴：《蝴蝶》《维也纳狂欢节》《童年情景》 歌曲：《桃金娘》《诗人之恋》《妇女之恋》 代表作: 《蝴蝶》《维也纳狂欢节》 勃拉姆斯 Johannes Brahms，1833年5月7日—1897年4月3日.出生于德国汉堡，父亲是低音提琴手。后来学小提琴，并进行世界巡回演出，后来认识了李斯特和舒曼。 四部交响曲都充满了浪漫主义的古典风格（ 勃拉姆斯认为他是古典主义作曲家而不是浪漫主义， 但是古典主义的节奏是固定的 ， 浪漫主义的节奏是比较随性的， 所以勃拉姆斯依然是浪漫主义音乐家）。《D小调小提琴协奏曲》是四大小协之一。 《德意志安魂曲》《21首匈牙利舞曲》 我们一直以为音乐家的作品是很遥远的，直到勃拉姆斯的《匈牙利舞曲》，突然能发现音乐离我们这么近。勃拉姆斯为人不苟言笑，他的情感都蕴含在他的音乐里。有热烈的（《第一首》），诙谐的（《第三首》），辉煌（《第七首》），欢快的（《第八首》） 师生恋。 他发掘了德沃夏克。 民族音乐 ---俄罗斯音乐 格林卡：俄罗斯音乐之父 米哈伊尔·伊万诺维奇·格林卡（Михаил Иванович Глинка，1804—1857） 俄罗斯古典音乐之父, 俄罗斯交响乐之父 格林卡是第一个将俄罗斯民族音乐文化与西方音乐文化交融而使之达到先进水平的作曲家，被视为“俄罗斯古典音乐之父”。他的音乐以爱国主义的内容和质朴的民族形式相结合而著称，在歌剧、管弦乐、歌曲等重要领域为俄罗斯的音乐创作开辟了新路。他的歌剧，为俄罗斯的歌剧发展铺设了两条路线：一条是爱国主义、英雄主义的历史性题材，以《为沙皇献身》为代表；另一条是神话、传奇题材，以《鲁斯兰与柳德米拉》为代表。在器乐创作领域，还开创了标题性和民歌变奏性质的管弦乐道路。他的作品，风格清新，富于生活气息。而他的艺术歌曲，是建立在城市小调的基础上，使之具有浓郁的俄罗斯风格。此外，他还吸收异国情调和多民族音乐语言来丰富俄罗斯音乐形式，对日后的俄罗斯作曲家影响极大。 格林卡是俄罗斯民族音乐真正的奠基人，被尊称为“俄罗斯音乐之父”，他为俄罗斯民族音乐的发展开创了广阔的前景，对俄罗斯民族音乐的发展起到了举足轻重的作用。他之后，俄罗斯相继出现了一大批思想成熟，技法高超的民族音乐大师。 格林卡的两部歌剧不仅是俄罗斯歌剧宝库中的珍品，而且也是世界歌剧史上的杰作。他的代表作还有管弦乐序曲《马德里之夜》等 代表作： 歌剧：《伊凡·苏萨宁》和《鲁斯兰与柳德米拉》 借鉴了俄罗斯城市音乐的特征，吸收了俄罗斯 代表作： 歌剧《》《》； 西班牙戏曲《》；幻想交响曲《》 强力集团 巴拉基涅夫： 理论家 juyi： 卡萨客服 穆索尔斯基： 保罗丁： 民族音乐---捷克 母亲河： 沃尔塔瓦河（ 意思是河流汇聚的地方 ） 斯美塔那 代表作《我的祖国》 德沃夏克 出生于屠夫家庭。 音乐天才，他曾参加音乐比赛，评委勃拉姆斯说“第一名不给这个人，我就辞职”，于是德沃夏克得了第一名。为了感谢勃拉姆斯的知遇之恩，针对勃拉姆斯的21首匈牙利舞曲，德沃夏克写了16首斯拉夫舞曲 《自新大陆》 美国人把《第九交响曲》说成“我们的交响曲” 民族音乐 -- 芬兰 西贝柳斯 当时的芬兰被俄国占领了, 代表作：《芬兰颂》 规定赫尔辛基不许聚会,只要超过十个人聚会,就抓起来. 西贝柳斯很痛苦,就写了《芬兰颂》。 芬兰这个国家多雪多山，西贝柳斯把芬兰描绘成一个神圣伟大的国家。 民族音乐 --挪威 格里戈 民族音乐 --俄罗斯 柴可夫斯基 俄罗斯音乐史上极其伟大的音乐家。 他的创作覆盖几乎每一个领域。 《降B小调钢协》（五大钢协之一） 拉赫玛尼诺夫 大手怪，和李斯特有一拼 代表作： 《拉二》（五大钢协之一）， 《拉三》","categories":[],"tags":[{"name":"Music","slug":"Music","permalink":"http://lyk-love.cn/tags/Music/"}]},{"title":"Git","slug":"Git","date":"2022-09-26T06:39:34.930Z","updated":"2022-10-01T10:56:37.492Z","comments":true,"path":"2022/09/26/Git/","link":"","permalink":"http://lyk-love.cn/2022/09/26/Git/","excerpt":"全面介绍了Git的原理和使用","text":"全面介绍了Git的原理和使用 ref: 廖雪峰的教程 git使用技巧和笔记 Learn Git Branching Tools: Git Command Explorer Intro Git是Linus用C写的分布式版本管理系统 集中式: CVS[^1]:最早的开源且免费的集中式版本控制系统&lt;由于自身设计问题,会造成提交文件不完整,版本库莫名损坏的情况 SVN[^2]:同样开源且免费,修正了CVS的一些稳定性问题,是目前用得最多的集中式版本控制系统 ClearCase:IBM的,收费,又大又笨 分布式: Git [^1]: Concurrent Versions System [^2]:Apache Subversion 创建版本库 版本库:repository $ git initInitialized empty Git repository in /c/Users/陆昱宽/Desktop/DOC/.git/ 自动生成.git目录, 用于跟踪管理版本库; 以及在 project 和 每个 module 中生成一个 .gitgnore 文件 把文件添加到版本库 所有版本控制系统,只能跟踪文本文件的改动,比如txt,网页,代码等. 而图片,视频这些二进制文件,虽然也能由版本控制系统管理,但没法跟踪文件的变化,就是说知道改了,但不知道改了啥. 微软的Word格式是二进制格式,所以版本控制文件没法跟踪Word文件的改动. Windows自带的记事本在保存UTF-8的文件时,会自动在其开头添加0xefbbbf（十六进制）的字符,因此会有许多问题,建议使用Notepad++ ,编码设为UTF-8 without BOM 编写一个Git.md文件,放到Doc目录下(子目录也行) stage file: git add Git.md 执行上面的命令,没有任何反馈,OK了,说明添加成功 再commit $ git commit -m&#x27;1[master 45a4a38] 3 1 file changed, 46 insertions(+) create mode 100644 readme.txt 46 insertions：插入了46行内容 Q:如果输入git add Git.md，得到错误fatal: pathspec 'Git.md' did not match any files。 A：添加某个文件时，该文件必须在当前目录下存在，用ls或者dir命令查看当前目录的文件，看看文件是否存在，或者是否写错了文件名。 基本操作 git add 我们修改Git.md文件,运行git.status看看结果: $ git statusOn branch masterChanges not staged for commit: (use &quot;git add/rm &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) deleted: &quot;\\346\\227\\245\\350\\256\\260.md&quot;Untracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) Diary.md &quot;\\344\\271\\220\\350\\260\\261.md&quot;no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) git.status命令可以展示仓库当前的状态,上面的输出告诉我们,Git.md被修改过了,但还没有准备提交的修改. 用git diff Git.md能看到具体修改了什么内容: $ git diffdiff --git &quot;a/\\346\\227\\245\\350\\256\\260.md&quot; &quot;b/\\346\\227\\245\\350\\256\\260.md&quot;deleted file mode 100644index 4db3376..0000000--- &quot;a/\\346\\227\\245\\350\\256\\260.md&quot;+++ /dev/null@@ -1,179 +0,0 @@- # Diary--## 12 / 25-- 圣诞夜中了两棵树,一棵二叉搜索树,一棵AVL树,另一棵B树没有开工,忙碌的一天。---- ## 12/26-- 昨天凡人修仙传看到四点, 今天本打算写完微积分和计基的,但是没忍住看了一天小说 git diff顾名思义就是查看difference，显示的格式正是Unix通用的diff格式 知道了对Git.md作了什么修改后,再把它提交到仓库就放心多了. 提交修改和提交新文件一样是两步, git add $ git add Git.md 没有任何反应,在执行git commit之前,我们再运行git status 看看当前仓库的状态: $ git statusOn branch masterChanges to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) modified: Git.md git status告诉我们,将要被提交的修改包括Git.md ,下一步就能放心提交了: $ git commit -m&quot;4&quot;[master 4553e0a] 4 1 file changed, 77 insertions(+), 1 deletion(-) 提交后,再用git status看看仓库的当前状态: $ git statusOn branch masternothing to commit, working tree clean Git告诉我们当前没有需要提交的修改,且工作目录是干净(working tree clean)的. windows下commit的message千万不能有中文,否则会乱码 版本回退 先提交文件,message为&quot;origin&quot;;再输入&quot;初次修改&quot;并提交,message为&quot;chucixiugai&quot;;再输入&quot;再次修改&quot; 并提交,message为&quot;再次修改&quot;. 现在,Git.md一共有三个版本(origin之前的不算)被提交到repository里了, 版本1:origin (啥都没写) 版本2:chucixiugai 初次修改 版本3:zaicixiugai 再次修改 我们不可能记住一个文件每次都改了什么内容,版本控制系统肯定有某个命令可以告诉我们历史记录,Git中用git log命令查看: $ git logcommit 028637e47a974357debe623e8bb4d0ca5053db87 (HEAD -&gt; master)Author: 陆昱宽 &lt;191820133@ smail.nju.edu.cn&gt;Date: Sun Feb 28 23:41:08 2021 +0800 zaicixiugaicommit cdd92a630141982791273c123d60b3ce0ed09932Author: 陆昱宽 &lt;191820133@ smail.nju.edu.cn&gt;Date: Sun Feb 28 23:40:30 2021 +0800 chucixiugaicommit 914f04abce43f4517121ba10957d89bd9658432eAuthor: 陆昱宽 &lt;191820133@ smail.nju.edu.cn&gt;Date: Sun Feb 28 23:39:31 2021 +0800 origin git log命令显示最近到最远的全部提交日志,我们看最近三次,最近一次是zaicixiugai,上一次是chuxixiugai,再上一次是origin. 如果嫌输出信息太多不太好看,可以加上--pretty=oneline参数: $ git log --pretty=oneline028637e47a974357debe623e8bb4d0ca5053db87 (HEAD -&gt; master) zaicixiugaicdd92a630141982791273c123d60b3ce0ed09932 初次修改914f04abce43f4517121ba10957d89bd9658432e origin 我们看到许多类似028637e的commit id(版本号),这是一个SHA1计算出的非常大的数字,用十六进制表示. SVN的版本号是1,2,3,4递增,因为SVN是集中式. Git是分布式,号码容易不够,所以用这种方式 每提交一个新版本，实际上Git就会把它们自动串成一条时间线。如果使用可视化工具查看Git历史，就可以更清楚地看到提交历史的时间线 现在我们把Git.md回退到上个版本chucitijiao. 首先,Git必须知道当前版本是哪个版本,在Git中,用HEAD(小写也可以)表示当前版本,即最近的提交02863,上个版本就是**HEAD^,上上个版本就是HEAD^^,以此类推. 往前100个版本数不过来,可以写成HEAD~100**. 用 git reset命令: $ git reset --hard HEAD^HEAD is now at cdd92a6 chucitijiao ,--hard参数的意义之后再讲. OK,已经被还原了 现在用git log看看当前版本库的提交日志: $ git logcommit cdd92a630141982791273c123d60b3ce0ed09932 (HEAD -&gt; master)Author: 陆昱宽 &lt;191820133@ smail.nju.edu.cn&gt;Date: Sun Feb 28 23:40:30 2021 +0800 chucixiugai commit 914f04abce43f4517121ba10957d89bd9658432e Author: 陆昱宽 &lt;191820133@ smail.nju.edu.cn&gt;Date: Sun Feb 28 23:39:31 2021 +0800 origin 我们看不到最近的那个版本zaicixiugai了! 好比时间从21世纪穿越回了10世纪,当然看不到后面的历史了! 如果想回去,就要用commit id. 如果还没关掉GIt窗口,可以把窗口往上翻,找到那个zaicixiugai的commit id是028637e...,于是可以指定回到未来的某个版本: $ git reset --hard 0286HEAD is now at 028637e zaicixiugai 版本号没必要写全,写前几位. Git会自动去找. OK,已经回到未来了. 如果已经关掉窗口了,Git中有git reflog记录你的每一次命令: $ git reflog --pretty=oneline028637e (HEAD -&gt; master) HEAD@&#123;0&#125;: reset: moving to 0286914f04a HEAD@&#123;1&#125;: reset: moving to head^cdd92a6 HEAD@&#123;2&#125;: reset: moving to HEAD^028637e (HEAD -&gt; master) HEAD@&#123;3&#125;: commit: zaicixiugaicdd92a6 HEAD@&#123;4&#125;: commit: 初次修改914f04a HEAD@&#123;5&#125;: commit: origin 小结 HEAD指向的版本就是当前版本,这是个指针. 版本穿梭使用git reset --hard commit_id 穿梭前,用git log可以查看提交历史,以便确定要回退到哪个版本. 要回到未来,可以用git reflog来确定要回到未来的哪个版本. 如果commit（提交）比较多，git log 的内容就会比较多；当满屏放不下，就会显示冒号，回车（往下滚一行）、空格（往下滚一页）可以继续查看剩余内容；退出：英文状态下 按 q 可以退出git log 状态。 工作区和暂存区 Git和其他版本控制系统如SVN的一个不同之处就是有暂存区的概念. 工作区(Working directory) 就是在电脑里能看到的目录,如Doc文件夹就是一个工作区. 版本库(Eepository) 工作区有一个隐藏目录.git,它不算作工作区,而是Git的版本库. Git的版本库里存了许多东西,其中最重要的就是称为stage(或index)的暂存区,还有Git为我们自动创建的第一个分支master,以及指向master的一个指针叫HEAD (所以版本库(包括暂存区和分支们)都在工作区里面,只是不算作工作区) 分支和HEAD的概念暂且不表. 前面讲了我们把文件添加剂Git版本库时,是分两步的: 第一步用git add把文件修改添加到暂存区(stage) 第二步用git commit把暂存区的所有内容提交到当前分支 因为我们创建Git版本库时,Git为我们自动创建了一个master分支,所以,现在git commit就是往master分支上提交更改. 需要提交的文件统统放到暂存区,然后,一次性提交暂存区(stage)的所有修改到当前分支 举例,先建个readme.txt提交到版本库,再在工作区(即Doc这个文件夹)里新增一个LICENSE文本文件,再对readme.txt做些修改(内容都随意). 先用git status看看状态: $ git statusOn branch masterChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: readme.txtUntracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) LICENSE.txtno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) Git告诉我们,readme.txt被修改了,这个修改没有被添加到暂存区,遑论分支了( changes not staged for commit). 而LICENSE这个文件还从来没被添加(add)过,所以其状态是Untracked. 由于没有修改被提交到暂存区,暂存区就是空的( no changes added to be commit). 现在用两次git add把把readme.txt和LICENSE都添加后，用git status再查看一下： $ git statusOn branch masterChanges to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) new file: LICENSE.txt modified: readme.txt 现在,暂存区状态变成这样了: 所以,git add命令就是把要提交的所有修改放到暂存区(stage),然后,执行git commit可以一次性把暂存区所有修改提交到分支. (注意暂存区不是被**&quot;清空&quot;**,而是&quot;安静&quot;了.暂存区中永远保留着上次add的版本)因此 一旦提交(注意是commit) 后,如果你对工作区又没有做任何修改,那么工作区就是clean的: $ git statusOn branch masternothing to commit, working tree clean 关于git diff: 如果是输入git diff，查看到的是工作区和暂存区(上次git add 的内容)的不同，如果是git diff --cached，查看到的是暂存区和HEAD的不同。 撤销修改 在readme .txt中添加了一行,但还没有git add: $ cat readme.txtGit is a distributed version control system.Git is free software distributed under the GPL.Git has a mutable index called stage.Git tracks changes of files.My stupid boss still prefers SVN. 现在想撤销最后一行,可以用 git restore readme.txt, 有两种情况: read.me.txt自修改后还没有被放到暂存区,现在,撤销修改就回到和版本库一模一样的状态 . readme.txt已经添加到暂存区后,又做了修改. 现在,撤销修改就回到和添加到暂存区后的状态. 总之,就是让该文件回到最近一次add时的状态(即 暂存区里的状态) 现在,看看readme.txt的内容: $ cat readme.txtGit is a distributed version control system.Git is free software distributed under the GPL.Git has a mutable index called stage.Git tracks changes of files. 果然复原了. 如果我把那句话 git add到暂存区了呢? ( 但还没有commit) ,先用git status查看一下, $ git statusOn branch masterChanges to be committed: (use &quot;git restore --staged &lt;file&gt;...&quot; to unstage) modified: readme.txt git告诉我们,修改只是被添加到了暂存区,还没有被提交,可以用 git restore --staged readme.txt 把暂存区的修改撤销掉( unstage ),重新放回工作区: $ cat readme.txtGit is a distributed version control system.Git is free software distributed under the GPL.Git has a mutable index called stage.Git tracks changes of files.My stupid boss still prefers SVN. 再用git status查看一下,现在暂存区是干净的,工作区有修改: $ git statusOn branch masterChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: readme.txt 我们要丢弃工作区的修改,就回到了上上步: git restore readme.txt $ git restore readme.txt$ git statusOn branch masternothing to commit, working tree clean Over! 如果我不仅把那句话git add了,我还git commit到了版本库, 我们可以用版本回退. 但如果你还把它git push到了 远程版本库,那就完蛋了. 删除文件 情况一: 工作区文件删除,无其它操作 rm readme.txt 可用命令git restore readme.txt恢复文件 情况二: 工作区文件删除,版本库文件删除 rm readme.txt git rm readme.txt git commit -m&quot; remove readme.txt &quot; 可用命令git reset --hard HAED^恢复文件( 回到哪个头要看情况,如果当前分支内有这个文件,那就可以回到当前版本,不用去上个版本) rm是DOS命令,在各个shell都可以用. 用在git仓库中,是删除工作区的文件,用 git restore readme.txt可以还原. 而git rm readme.txt是删除工作区和暂存区的文件, 由于git restore的原理是将工作复原为暂存区中的版本,而暂存区中该文件也被删除了,所以恢复不了, 分支里还有这个文件,所以用版本回退git reset --hard HEAD^ git pull 1、将远程指定分支 拉取到 本地指定分支上： git pull origin &lt;远程分支名&gt;:&lt;本地分支名&gt; 2、将远程指定分支 拉取到 本地当前分支上： git pull origin &lt;远程分支名&gt; 3、将与本地当前分支同名的远程分支 拉取到 本地当前分支上(需先关联远程分支，方法见文章末尾) git pull 在克隆远程项目的时候，本地分支会自动与远程仓库建立追踪关系，可以使用默认的origin来替代远程仓库名 pull强制覆盖本地文件 ref： https://www.jianshu.com/p/1ac2e1f99166 重要提示：如果您有任何本地更改，将会丢失。无论是否有--hard选项，任何未被推送的本地提交都将丢失。 如果您有任何未被Git跟踪的文件(例如上传的用户内容)，这些文件将不会受到影响。 下面是正确的方法： git fetch --all 然后，你有两个选择： git reset --hard origin/master 或者如果你在其他分支上： git reset --hard origin/&lt;branch_name&gt; 说明： git fetch从远程下载最新的，而不尝试合并或rebase任何东西。 然后git reset将主分支重置为您刚刚获取的内容。 --hard选项更改工作树中的所有文件以匹配origin/master中的文件 在重置之前可以通过从master创建一个分支来维护当前的本地提交： git checkout mastergit branch new-branch-to-save-current-commitsgit fetch --allgit reset --hard origin/master 在此之后，所有旧的提交都将保存在new-branch-to-save-current-commits中。然而，没有提交的更改(即使staged)将会丢失。确保存储和提交任何你需要的东西。 git push 1、将本地当前分支 推送到 远程指定分支上： git push origin &lt;本地分支名&gt;:&lt;远程分支名&gt; 2、将本地当前分支 推送到 与本地当前分支同名的远程分支： git push origin &lt;本地分支名&gt; 3、将本地当前分支 推送到 与本地当前分支同名的远程分支上(需先关联远程分支]) git push 修改commit信息 https://blog.csdn.net/Muscleape/article/details/105637401 远程仓库 添加远程库 把已有的本地仓库与一个git仓库相关联( 建立绑定关系 ): $ git remote add origin https://github.com/LYK-love/Learning, 添加后,远程库的名字就是origin,这是Git默认的叫法,也可以改名,但没必要. 下一步,就可以把本地库的内容push到远程库上: $ git push -u origin masterEnumerating objects: 7, done.Counting objects: 100% (7/7), done.Delta compression using up to 12 threadsCompressing objects: 100% (7/7), done.Writing objects: 100% (7/7), 6.46 KiB | 6.46 MiB/s, done.Total 7 (delta 1), reused 0 (delta 0), pack-reused 0remote: Resolving deltas: 100% (1/1), done.To https://github.com/LYK-love/Learning.git * [new branch] master -&gt; masterBranch &#x27;master&#x27; set up to track remote branch &#x27;master&#x27; from &#x27;origin&#x27;. 把本地库内容推送到远程,用git push命令,实际上是把当前分支master(本地的那个)推送到远程库(名叫origin). 由于远程库是空的,我们第一次推送master分支时,加上了-u参数. Git不但会把本地的master分支内容推送到远程新的master分支( 现在远程也有一个叫master的分支了),还会把本地的master分支和远程的master分支关联起来,这样在以后的oush或oull时就可以简化命令. 从现在起,只要在本地作了git commit,就可以通过命令git push origin把本地的master分支的最新修改推送到Github,大功告成! 删除远程库 如果添加远程库的时候地址写错了,或者就是想删除远程库,可以用git remote rm &lt;name&gt;命令. 使用前建议先用git remote -v查看远程库信息: $ git remote -vorigin https://github.com/LYK-love/Learning.git (fetch)origin https://github.com/LYK-love/Learning.git (push) 然后,根据名字删除. 注意: 此处的*&quot;删除&quot;*其实是解除了本地和远程库的绑定关系,并没有物理上删除远程库,远程库本身没有任何改动( 也就是说远程库里的内容都还在 )要恢复绑定关系,可以再次用$ git remote add origin https://github.com/LYK-love/Learning , 然后用$ git push -u origin master来推送( -u参数能关联分支 ) origin就是你指向的远程库的名字,可以等价于https://github.com/LYK-love/Learning,它指向的是repository, 而master只是这个repository中默认创建的第一个branch. 当我们push的时候,因为origin和master是默认创建的,所以这二者可以省略,但这是个bad practice,因为如果我换一个branch再push的时候,这样就很纠结了. 当然origin这个名字来源于$ git remote add origin https://github.com/LYK-love/Learning,我们也可以把origin改成别的名字,比如阿猫阿狗,如 $ git remote add aMao https://github.com/LYK-love/Learning,那么推送的时候就可以用git push -u aMao master了, 如果你的本地版本库是从远程仓库git clone而来，git会默认把这个远程仓库的地址叫做origin. 这时候依旧可以通过 git remote add 把远程仓库的名称改成'aGou' 从远程库clone 随便哪个本地仓库,都可以用$ git clone git@github.com:LYK-love/Learning.git来clone. Github给出的地址不止一个,还可以用https://github.com/LYK-love/Learning. 实际上Git支持多种协议,默认的git://使用ssh( Secure Shell,安全外壳协议),但也可以使用http等其他协议. 使用https除了速度慢以外，还有个最大的麻烦是每次推送都必须输入口令，但是在某些只开放http端口的公司内部就无法使用ssh协议而只能用https。 补充: SSH Intro: SSH 为 [Secure Shell](https://baike.baidu.com/item/Secure Shell) 的缩写，由 IETF 的网络小组（Network Working Group）所制定；SSH 为建立在应用层基础上的安全协议。SSH 是较可靠，专为远程登录会话和其他网络服务提供安全性的协议。利用 SSH 协议可以有效防止远程管理过程中的信息泄露问题。 功能: 传统的网络服务程序，如：ftp、pop和telnet在本质上都是不安全的，因为它们在网络上用明文传送口令和数据，别有用心的人非常容易就可以截获这些口令和数据。而且，这些服务程序的安全验证方式也是有其弱点的， 就是很容易受到“中间人”（man-in-the-middle）这种方式的攻击。所谓“中间人”的攻击方式， 就是“中间人”冒充真正的服务器接收你传给服务器的数据，然后再冒充你把数据传给真正的服务器。服务器和你之间的数据传送被“中间人”一转手做了手脚之后，就会出现很严重的问题。通过使用SSH，你可以把所有传输的数据进行加密，这样&quot;中间人&quot;这种攻击方式就不可能实现了，而且也能够防止DNS欺骗和IP欺骗。使用SSH，还有一个额外的好处就是传输的数据是经过压缩的，所以可以加快传输的速度。SSH有很多功能，它既可以代替Telnet，又可以为FTP、PoP、甚至为PPP提供一个安全的&quot;通道&quot; 。 验证: 从客户端来看，SSH提供两种级别的安全验证。 第一种级别（基于口令的安全验证） 只要你知道自己帐号和口令，就可以登录到远程主机。所有传输的数据都会被加密，但是不能保证你正在连接的服务器就是你想连接的服务器。可能会有别的服务器在冒充真正的服务器，也就是受到“中间人”这种方式的攻击。 第二种级别（基于密匙的安全验证） 需要依靠密匙，也就是你必须为自己创建一对密匙，并把公用密匙放在需要访问的服务器上。如果你要连接到SSH服务器上，客户端软件就会向服务器发出请求，请求用你的密匙进行安全验证。服务器收到请求之后，先在该服务器上你的主目录下寻找你的公用密匙，然后把它和你发送过来的公用密匙进行比较。如果两个密匙一致，服务器就用公用密匙加密“质询”（challenge）并把它发送给客户端软件。客户端软件收到“质询”之后就可以用你的 私人密匙 解密再把它发送给服务器。 用这种方式，你必须知道自己密匙的口令。但是，与第一种级别相比，第二种级别不需要在网络上传送口令。 第二种级别不仅加密所有传送的数据，而且“中间人”这种攻击方式也是不可能的（因为他没有你的私人密匙）。但是整个登录的过程可能需要10秒 。 Branch 创建分支 创建dev分支, 然后切换到dev分支： $ git switch -c dev Switched to a new branch &#x27;dev&#x27; 切换分支 git switch dev 查看分支 查看本地所有分支： git branch 查看远程所有分支 git branch -r 查看本地及远程的所有分支: git branch -a 删除分支 删除远程分支: git push origin :br (origin 后面有空格) 删除本地分支: git branch -D br 关联分支 将本地分支与远程同名分支相关联 git branch --set-upstream-to=origin/develop develop 也可以在push时设置: lgit push --set-upstream origin &lt;本地分支名&gt;# 简写方式： # git push -u origin &lt;本地分支名&gt; pull和push同. 创建与合并分支 HEAD严格来说不是指向提交，而是指向master，master才是指向提交的，所以，HEAD指向的就是当前分支。 首先，我们创建dev分支，然后切换到dev分支： 然后，用git branch命令查看当前分支： 然后提交: $ git add readme.txt $ git commit -m &quot;branch test&quot;[dev b17d20e] branch test 1 file changed, 1 insertion(+) 现在，dev分支的工作完成，我们就可以切换回master分支： $ git switch masterSwitched to branch &#x27;master&#x27; 切换回master分支后，再查看一个readme.txt文件，刚才添加的内容不见了！( 工作区的不见了!!! )因为那个提交是在dev分支上，而master分支此刻的提交点并没有变： 现在，我们把dev分支的工作成果合并到master分支上： $ git merge devUpdating d46f35e..b17d20eFast-forward readme.txt | 1 + 1 file changed, 1 insertion(+) git merge命令用于合并指定分支到当前分支。合并后，再查看readme.txt的内容(工作区)，就可以看到，和dev分支的最新提交是完全一样的。 注意到上面的Fast-forward信息，Git告诉我们，这次合并是“快进模式”，也就是直接把master指向dev的当前提交，所以合并速度非常快。 当然，也不是每次合并都能Fast-forward，我们后面会讲其他方式的合并。 合并完成后，就可以放心地删除dev分支了： $ git branch -d devDeleted branch dev (was b17d20e). 删除后，查看branch，就只剩下master分支了： $ git branch* master 因为创建、合并和删除分支非常快，所以Git鼓励你使用分支完成某个任务，合并后再删掉分支，这和直接在master分支上工作效果是一样的，但过程更安全。 查看分支：git branch 创建分支：git branch &lt;name&gt; 切换分支：git checkout &lt;name&gt;或者git switch &lt;name&gt; 创建+切换分支：git checkout -b &lt;name&gt;或者git switch -c &lt;name&gt; 合并某分支到当前分支：git merge &lt;name&gt; 删除分支：git branch -d &lt;name&gt; 查看远程分支 查看远程与本地当前分支对应的分支： git branch -vv 查看本地和远程所有分支 git branch -a 解决冲突 如果master分支和feature1分支各自都分别有新的提交，变成了这样： 这种情况下，Git无法执行“快速合并”，只能试图把各自的修改合并起来，但这种合并就可能会有冲突，我们试试看： $ git merge feature1Auto-merging readme.txtCONFLICT (content): Merge conflict in readme.txtAutomatic merge failed; fix conflicts and then commit the result. 果然冲突了！Git告诉我们，readme.txt文件存在冲突，必须手动解决冲突后再提交。git status也可以告诉我们冲突的文件： $ git statusOn branch masterYour branch is ahead of &#x27;origin/master&#x27; by 2 commits. (use &quot;git push&quot; to publish your local commits)You have unmerged paths. (fix conflicts and run &quot;git commit&quot;) (use &quot;git merge --abort&quot; to abort the merge)Unmerged paths: (use &quot;git add &lt;file&gt;...&quot; to mark resolution) both modified: readme.txtno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) 打开工作区的readme.txt,我们可以直接查看readme.txt的内容： Git is a distributed version control system.Git is free software distributed under the GPL.Git has a mutable index called stage.Git tracks changes of files.&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADCreating a new branch is quick &amp; simple.=======Creating a new branch is quick AND simple.&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature1 Git用&lt;&lt;&lt;&lt;&lt;&lt;&lt;，=======，&gt;&gt;&gt;&gt;&gt;&gt;&gt;标记出不同分支的内容，我们修改如下后保存： Creating a new branch is quick and simple. 再提交： $ git add readme.txt $ git commit -m &quot;conflict fixed&quot;[master cf810e4] conflict fixed 现在，master分支和feature1分支变成了下图所示： 用带参数的git log也可以看到分支的合并情况： $ git log --graph --pretty=oneline --abbrev-commit* cf810e4 (HEAD -&gt; master) conflict fixed|\\ | * 14096d0 (feature1) AND simple* | 5dc6824 &amp; simple|/ * b17d20e branch test* d46f35e (origin/master) remove test.txt* b84166e add test.txt* 519219b git tracks changes* e43a48b understand how stage works* 1094adb append GPL* e475afc add distributed* eaadf4e wrote a readme file 最后，删除feature1分支： $ git branch -d feature1Deleted branch feature1 (was 14096d0). 工作完成。 当Git无法自动合并分支时，就必须首先解决冲突。解决冲突后，再提交，合并完成。 解决冲突就是把Git合并失败的文件手动编辑为我们希望的内容，再提交。 用git log --graph命令可以看到分支合并图。 分支管理策略 通常，合并分支时，如果可能，Git会用Fast forward模式，但这种模式下，删除分支后，会丢掉分支信息。 如果要强制禁用Fast forward模式，Git就会在merge时生成一个新的commit，这样，从分支历史上就可以看出分支信息。 下面我们实战一下--no-ff方式的git merge： 首先，仍然创建并切换dev分支： $ git switch -c devSwitched to a new branch &#x27;dev&#x27; 修改readme.txt文件，并提交一个新的commit： $ git add readme.txt $ git commit -m &quot;add merge&quot;[dev f52c633] add merge 1 file changed, 1 insertion(+) 现在，我们切换回master： $ git switch masterSwitched to branch &#x27;master&#x27; 准备合并dev分支，请注意--no-ff参数，表示禁用Fast forward： $ git merge --no-ff -m &quot;merge with no-ff&quot; devMerge made by the &#x27;recursive&#x27; strategy. readme.txt | 1 + 1 file changed, 1 insertion(+) 因为本次合并要创建一个新的commit，所以加上-m参数，把commit描述写进去。 合并后，我们用git log看看分支历史： $ git log --graph --pretty=oneline --abbrev-commit* e1e9c68 (HEAD -&gt; master) merge with no-ff|\\ | * f52c633 (dev) add merge|/ * cf810e4 conflict fixed... 可以看到，不使用Fast forward模式，merge后就像这样： 不用Fast forwa模式,提交图就像是 dev分支上做提交, master分支上做提交, 然后在master分支上手动解决冲突再提交 的图一样!. 合并分支时，加上--no-ff参数就可以用普通模式合并，合并后的历史有分支，能看出来曾经做过合并，而fast forward合并就看不出来曾经做过合并。 如果之前master和dev只想相同提交. 现在你在master分支, 对 readme.txt做了修改, 然后switch到了dev分支, add, commit, 现在dev指向新的提交了! 我们switch回到master分支, 打开readme.txt,发现里面的内容是没有修改过的, 这是因为分支是指向提交的, 尽管你在master分支做了修改,但没有提交, 提交是在dev分支上完成的. 因此master分支指向的是上一次提交, 也就是没有修改过的版本. Bug分支 软件开发中，bug就像家常便饭一样。有了bug就需要修复，在Git中，由于分支是如此的强大，所以，每个bug都可以通过一个新的临时分支来修复，修复后，合并分支，然后将临时分支删除。 当你接到一个修复一个代号101的bug的任务时，很自然地，你想创建一个分支issue-101来修复它，但是，等等，当前正在dev上进行的工作还没有提交： $ git statusOn branch devChanges to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) new file: hello.pyChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: readme.txt 并不是你不想提交，而是工作只进行到一半，还没法提交，预计完成还需1天时间。但是，必须在两个小时内修复该bug，怎么办？ 幸好，Git还提供了一个stash功能，可以把当前工作现场“储藏”起来，等以后恢复现场后继续工作： $ git stashSaved working directory and index state WIP on dev: f52c633 add merge 现在，用git status查看工作区，就是干净的（除非有没有被Git管理的文件），因此可以放心地创建分支来修复bug。 首先确定要在哪个分支上修复bug，假定需要在master分支上修复，就从master创建临时分支： $ git checkout masterSwitched to branch &#x27;master&#x27;Your branch is ahead of &#x27;origin/master&#x27; by 6 commits. (use &quot;git push&quot; to publish your local commits)$ git checkout -b issue-101Switched to a new branch &#x27;issue-101&#x27; 现在修复bug，需要把“Git is free software ...”改为“Git is a free software ...”，然后提交： $ git add readme.txt $ git commit -m &quot;fix bug 101&quot;[issue-101 4c805e2] fix bug 101 1 file changed, 1 insertion(+), 1 deletion(-) 修复完成后，切换到master分支，并完成合并，最后删除issue-101分支： $ git switch masterSwitched to branch &#x27;master&#x27;Your branch is ahead of &#x27;origin/master&#x27; by 6 commits. (use &quot;git push&quot; to publish your local commits)$ git merge --no-ff -m &quot;merged bug fix 101&quot; issue-101Merge made by the &#x27;recursive&#x27; strategy. readme.txt | 2 +- 1 file changed, 1 insertion(+), 1 deletion(-) 太棒了，原计划两个小时的bug修复只花了5分钟！现在，是时候接着回到dev分支干活了！ $ git switch devSwitched to branch &#x27;dev&#x27;$ git statusOn branch devnothing to commit, working tree clean 工作区是干净的，刚才的工作现场存到哪去了？用git stash list命令看看： $ git stash liststash@&#123;0&#125;: WIP on dev: f52c633 add merge 工作现场还在，Git把stash内容存在某个地方了，但是需要恢复一下，有两个办法： 一是用git stash apply恢复，但是恢复后，stash内容并不删除，你需要用git stash drop来删除； 另一种方式是用git stash pop，恢复的同时把stash内容也删了： $ git stash popOn branch devChanges to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) new file: hello.pyChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: readme.txtDropped refs/stash@&#123;0&#125; (5d677e2ee266f39ea296182fb2354265b91b3b2a) 再用git stash list查看，就看不到任何stash内容了： $ git stash list 你可以多次stash，恢复的时候，先用git stash list查看，然后恢复指定的stash，用命令： $ git stash apply stash@&#123;0&#125; 在master分支上修复了bug后，我们要想一想，dev分支是早期从master分支分出来的，所以，这个bug其实在当前dev分支上也存在。 那怎么在dev分支上修复同样的bug？重复操作一次，提交不就行了？ 有木有更简单的方法？ 有！ 同样的bug，要在dev上修复，我们只需要把4c805e2 fix bug 101这个提交所做的修改“复制”到dev分支。注意：我们只想复制4c805e2 fix bug 101这个提交所做的修改，并不是把整个master分支merge过来。 为了方便操作，Git专门提供了一个cherry-pick命令，让我们能复制一个特定的提交到当前分支： $ git branch* dev master$ git cherry-pick 4c805e2[master 1d4b803] fix bug 101 1 file changed, 1 insertion(+), 1 deletion(-) Git自动给dev分支做了一次提交，注意这次提交的commit是1d4b803，它并不同于master的4c805e2，因为这两个commit只是改动相同，但确实是两个不同的commit。用git cherry-pick，我们就不需要在dev分支上手动再把修bug的过程重复一遍。 修复bug时，我们会通过创建新的bug分支进行修复，然后合并，最后删除； 当手头工作没有完成时，先把工作现场git stash一下，然后去修复bug，修复后，再git stash pop，回到工作现场； 在master分支上修复的bug，想要合并到当前dev分支，可以用git cherry-pick &lt;commit&gt;命令，把bug提交的修改“复制”到当前分支，避免重复劳动。 Feature分支 软件开发中，总有无穷无尽的新的功能要不断添加进来。 添加一个新功能时，你肯定不希望因为一些实验性质的代码，把主分支搞乱了，所以，每添加一个新功能，最好新建一个feature分支，在上面开发，完成后，合并，最后，删除该feature分支。 现在，你终于接到了一个新任务：开发代号为Vulcan的新功能，该功能计划用于下一代星际飞船。 于是准备开发： $ git switch -c feature-vulcanSwitched to a new branch &#x27;feature-vulcan&#x27; 5分钟后，开发完毕： $ git add vulcan.py$ git statusOn branch feature-vulcanChanges to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) new file: vulcan.py$ git commit -m &quot;add feature vulcan&quot;[feature-vulcan 287773e] add feature vulcan 1 file changed, 2 insertions(+) create mode 100644 vulcan.py 切回dev，准备合并： $ git switch dev 一切顺利的话，feature分支和bug分支是类似的，合并，然后删除。 但是！ 就在此时，接到上级命令，因经费不足，新功能必须取消！ 虽然白干了，但是这个包含机密资料的分支还是必须就地销毁： $ git branch -d feature-vulcanerror: The branch &#x27;feature-vulcan&#x27; is not fully merged.If you are sure you want to delete it, run &#x27;git branch -D feature-vulcan&#x27;. 销毁失败。Git友情提醒，feature-vulcan分支还没有被合并，如果删除，将丢失掉修改，如果要强行删除，需要使用大写的-D参数。。 现在我们强行删除： $ git branch -D feature-vulcanDeleted branch feature-vulcan (was 287773e). 终于删除成功！ 开发一个新feature，最好新建一个分支； 如果要丢弃一个没有被合并过的分支，可以通过git branch -D &lt;name&gt;强行删除。 多人协作 master分支是主分支，因此要时刻与远程同步； dev分支是开发分支，团队所有成员都需要在上面工作，所以也需要与远程同步； bug分支只用于在本地修复bug，就没必要推到远程了，除非老板要看看你每周到底修复了几个bug； feature分支是否推到远程，取决于你是否和你的小伙伴合作在上面开发。 首先，可以试图用git push origin &lt;branch-name&gt;推送自己的修改； 如果推送失败，则因为远程分支比你的本地更新，需要先用git pull试图合并； 如果合并有冲突，则解决冲突，并在本地提交； 没有冲突或者解决掉冲突后，再用git push origin &lt;branch-name&gt;推送就能成功 查看远程库信息，使用git remote -v； 比 git remote更详细 上面显示了可以抓取和推送的origin的地址。如果没有推送权限，就看不到push的地址。 本地新建的分支如果不推送到远程，对其他人就是不可见的； 从本地推送分支，使用git push origin branch-name，如果推送失败，先用git pull抓取远程的新提交； 在本地创建和远程分支对应的分支并关联起来（ 就是说不用 set-upstream ），使用git switch -c branch-name origin/branch-name，本地和远程分支的名称最好一致. 建立本地分支和远程分支的关联，使用git branch --set-upstream branch-name origin/branch-name； 从远程抓取分支，使用git pull，如果有冲突，要先处理冲突。 Rebase 在上一节我们看到了，多人在同一个分支上协作时，很容易出现冲突。即使没有冲突，后push的童鞋不得不先pull，在本地合并，然后才能push成功。 每次合并再push后，分支变成了这样： $ git log --graph --pretty=oneline --abbrev-commit* d1be385 (HEAD -&gt; master, origin/master) init hello* e5e69f1 Merge branch &#x27;dev&#x27;|\\ | * 57c53ab (origin/dev, dev) fix env conflict| |\\ | | * 7a5e5dd add env| * | 7bd91f1 add new env| |/ * | 12a631b merged bug fix 101|\\ \\ | * | 4c805e2 fix bug 101|/ / * | e1e9c68 merge with no-ff|\\ \\ | |/ | * f52c633 add merge|/ * cf810e4 conflict fixed 总之看上去很乱，有强迫症的童鞋会问：为什么Git的提交历史不能是一条干净的直线？ 其实是可以做到的！ Git有一种称为rebase的操作. 同步后，我们对hello.py这个文件做了两次提交。用git log命令看看： $ git log --graph --pretty=oneline --abbrev-commit* 582d922 (HEAD -&gt; master) add author* 8875536 add comment* d1be385 (origin/master) init hello* e5e69f1 Merge branch &#x27;dev&#x27;|\\ | * 57c53ab (origin/dev, dev) fix env conflict| |\\ | | * 7a5e5dd add env| * | 7bd91f1 add new env... 注意到Git用(HEAD -&gt; master)和(origin/master)标识出当前分支的HEAD和远程origin的位置分别是582d922 add author和d1be385 init hello，本地分支比远程分支快两个提交。 现在我们尝试推送本地分支： $ git push origin masterTo github.com:michaelliao/learngit.git ! [rejected] master -&gt; master (fetch first)error: failed to push some refs to &#x27;git@github.com:michaelliao/learngit.git&#x27;hint: Updates were rejected because the remote contains work that you dohint: not have locally. This is usually caused by another repository pushinghint: to the same ref. You may want to first integrate the remote changeshint: (e.g., &#x27;git pull ...&#x27;) before pushing again.hint: See the &#x27;Note about fast-forwards&#x27; in &#x27;git push --help&#x27; for details. 很不幸，失败了，这说明有人先于我们推送了远程分支。按照经验，先pull一下： $ git pullremote: Counting objects: 3, done.remote: Compressing objects: 100% (1/1), done.remote: Total 3 (delta 1), reused 3 (delta 1), pack-reused 0Unpacking objects: 100% (3/3), done.From github.com:michaelliao/learngit d1be385..f005ed4 master -&gt; origin/master * [new tag] v1.0 -&gt; v1.0Auto-merging hello.pyMerge made by the &#x27;recursive&#x27; strategy. hello.py | 1 + 1 file changed, 1 insertion(+) 再用git status看看状态： $ git statusOn branch masterYour branch is ahead of &#x27;origin/master&#x27; by 3 commits. (use &quot;git push&quot; to publish your local commits)nothing to commit, working tree clean 加上刚才合并的提交，现在我们本地分支比远程分支超前3个提交。 用git log看看： $ git log --graph --pretty=oneline --abbrev-commit* e0ea545 (HEAD -&gt; master) Merge branch &#x27;master&#x27; of github.com:michaelliao/learngit|\\ | * f005ed4 (origin/master) set exit=1* | 582d922 add author* | 8875536 add comment|/ * d1be385 init hello... 对强迫症童鞋来说，现在事情有点不对头，提交历史分叉了。如果现在把本地分支push到远程，有没有问题？ 有！ 什么问题？ 不好看！ 有没有解决方法？ 有！ 这个时候，rebase就派上了用场。我们输入命令git rebase试试： $ git rebaseFirst, rewinding head to replay your work on top of it...Applying: add commentUsing index info to reconstruct a base tree...M hello.pyFalling back to patching base and 3-way merge...Auto-merging hello.pyApplying: add authorUsing index info to reconstruct a base tree...M hello.pyFalling back to patching base and 3-way merge...Auto-merging hello.py 输出了一大堆操作，到底是啥效果？再用git log看看： $ git log --graph --pretty=oneline --abbrev-commit* 7e61ed4 (HEAD -&gt; master) add author* 3611cfe add comment* f005ed4 (origin/master) set exit=1* d1be385 init hello... 原本分叉的提交现在变成一条直线了！这种神奇的操作是怎么实现的？其实原理非常简单。我们注意观察，发现Git把我们本地的提交“挪动”了位置，放到了f005ed4 (origin/master) set exit=1之后，这样，整个提交历史就成了一条直线。rebase操作前后，最终的提交内容是一致的，但是，我们本地的commit修改内容已经变化了，它们的修改不再基于d1be385 init hello，而是基于f005ed4 (origin/master) set exit=1，但最后的提交7e61ed4内容是一致的。 这就是rebase操作的特点：把分叉的提交历史“整理”成一条直线，看上去更直观。缺点是本地的分叉提交已经被修改过了。 最后，通过push操作把本地分支推送到远程： Mac:~/learngit michael$ git push origin masterCounting objects: 6, done.Delta compression using up to 4 threads.Compressing objects: 100% (5/5), done.Writing objects: 100% (6/6), 576 bytes | 576.00 KiB/s, done.Total 6 (delta 2), reused 0 (delta 0)remote: Resolving deltas: 100% (2/2), completed with 1 local object.To github.com:michaelliao/learngit.git f005ed4..7e61ed4 master -&gt; master 再用git log看看效果： $ git log --graph --pretty=oneline --abbrev-commit* 7e61ed4 (HEAD -&gt; master, origin/master) add author* 3611cfe add comment* f005ed4 set exit=1* d1be385 init hello... 远程分支的提交历史也是一条直线。 rebase操作可以把本地未push的分叉提交历史整理成直线； rebase的目的是使得我们在查看历史提交的变化时更容易，因为分叉的提交需要三方对比。 合并远程分支 假设你本地在使用的分支为a, 需要合并的远程分支为b 新建和远程分支对应的本地分支: git switch -c b origin/b 将远程代码pull到本地: git pull origin b 返回到你的分支a git switch a 合并分支a与分支b git merge b 标签管理 发布一个版本时，我们通常先在版本库中打一个标签（tag），这样，就唯一确定了打标签时刻的版本。将来无论什么时候，取某个标签的版本，就是把那个打标签的时刻的历史版本取出来。所以，标签也是版本库的一个快照。 Git的标签虽然是版本库的快照，但其实它就是指向某个commit的指针（跟分支很像对不对？但是分支可以移动，标签不能移动），所以，创建和删除标签都是瞬间完成的。 Git有commit，为什么还要引入tag？ “请把上周一的那个版本打包发布，commit号是6a5819e...” “一串乱七八糟的数字不好找！” 如果换一个办法： “请把上周一的那个版本打包发布，版本号是v1.2” “好的，按照tag v1.2查找commit就行！” 所以，tag就是一个让人容易记住的有意义的名字，它跟某个commit绑在一起。 创建标签 在Git中打标签非常简单，首先，切换到需要打标签的分支上： $ git branch* dev master$ git checkout masterSwitched to branch &#x27;master&#x27; 然后，敲命令git tag &lt;name&gt;就可以打一个新标签： $ git tag v1.0 可以用命令git tag查看所有标签： $ git tagv1.0 默认标签是打在最新提交的commit上的。有时候，如果忘了打标签，比如，现在已经是周五了，但应该在周一打的标签没有打，怎么办？ 方法是找到历史提交的commit id，然后打上就可以了： $ git log --pretty=oneline --abbrev-commit12a631b (HEAD -&gt; master, tag: v1.0, origin/master) merged bug fix 1014c805e2 fix bug 101e1e9c68 merge with no-fff52c633 add mergecf810e4 conflict fixed5dc6824 &amp; simple14096d0 AND simpleb17d20e branch testd46f35e remove test.txtb84166e add test.txt519219b git tracks changese43a48b understand how stage works1094adb append GPLe475afc add distributedeaadf4e wrote a readme file 比方说要对add merge这次提交打标签，它对应的commit id是f52c633，敲入命令： $ git tag v0.9 f52c633 再用命令git tag查看标签： $ git tagv0.9v1.0 注意，标签不是按时间顺序列出，而是按字母排序的。可以用git show &lt;tagname&gt;查看标签信息： $ git show v0.9commit f52c63349bc3c1593499807e5c8e972b82c8f286 (tag: v0.9)Author: Michael Liao &lt;askxuefeng@gmail.com&gt;Date: Fri May 18 21:56:54 2018 +0800 add mergediff --git a/readme.txt b/readme.txt... 可以看到，v0.9确实打在add merge这次提交上。 还可以创建带有说明的标签，用-a指定标签名，-m指定说明文字： $ git tag -a v0.1 -m &quot;version 0.1 released&quot; 1094adb 用命令git show &lt;tagname&gt;可以看到说明文字： $ git show v0.1tag v0.1Tagger: Michael Liao &lt;askxuefeng@gmail.com&gt;Date: Fri May 18 22:48:43 2018 +0800version 0.1 releasedcommit 1094adb7b9b3807259d8cb349e7df1d4d6477073 (tag: v0.1)Author: Michael Liao &lt;askxuefeng@gmail.com&gt;Date: Fri May 18 21:06:15 2018 +0800 append GPLdiff --git a/readme.txt b/readme.txt... 操作标签 如果标签打错了，也可以删除： $ git tag -d v0.1Deleted tag &#x27;v0.1&#x27; (was f15b0dd) 因为创建的标签都只存储在本地，不会自动推送到远程。所以，打错的标签可以在本地安全删除。 如果要推送某个标签到远程，使用命令git push origin &lt;tagname&gt;： $ git push origin v1.0Total 0 (delta 0), reused 0 (delta 0)To github.com:michaelliao/learngit.git * [new tag] v1.0 -&gt; v1.0 或者，一次性推送全部尚未推送到远程的本地标签： $ git push origin --tagsTotal 0 (delta 0), reused 0 (delta 0)To github.com:michaelliao/learngit.git * [new tag] v0.9 -&gt; v0.9 如果标签已经推送到远程，要删除远程标签就麻烦一点，先从本地删除： $ git tag -d v0.9Deleted tag &#x27;v0.9&#x27; (was f52c633) 然后，从远程删除。删除命令也是push，但是格式如下： $ git push origin :refs/tags/v0.9To github.com:michaelliao/learngit.git - [deleted] v0.9 要看看是否真的从远程库删除了标签，可以登陆GitHub查看。 命令git push origin &lt;tagname&gt;可以推送一个本地标签； 命令git push origin --tags可以推送全部未推送过的本地标签； 命令git tag -d &lt;tagname&gt;可以删除一个本地标签； 命令git push origin :refs/tags/&lt;tagname&gt;可以删除一个远程标签 .gitignore Tutor 现成的配置 当远程仓库或者缓存区已经存在被忽略文件的情况下，这个时候相应的忽略规则是不起作用的. 如果先commit了, 再写.gitignore文件, 则后者不起作用, 需要: 已经add了: 文件较少时： git rm --cached &lt;filename&gt; 文件较多时： git rm -r --cached . 已经commit了: git reset &lt;commit_id&gt; //这个命令, 把 commit 历史撤销，对应缓存区内容也撤销，工作区内容不变 git add . git commit -m &#x27;XXXX&#x27; 已经push了: 只能手动把远程的也删了 验证 验证.gitignore是否生效,可以尝试: git add &lt;filename&gt; 添加被忽略的文件，查看是否能添加成功, 如果.gitignore已经生效, 则不会添加成功 还可以使用: git check-ignore -v &lt;filename&gt; 定位到对应规则 在 .gitignore 文件中的具体位置 Example What Kind of Files Should You Ignore? Log files Files with API keys/secrets, credentials, or sensitive information Useless system files like .DS_Store on macOS Generated files like dist folders Dependencies which can be downloaded from a package manager And there might be other reasons (maybe you make little todo.md files) You can get an idea for what sort of files to ignore on gitignore.io, by selecting your operating system, text editor or IDE, languages, and frameworks. Grammar 空行或是以#开头的行即注释行将被忽略。 可以在前面添加正斜杠/来避免递归,下面的例子中可以很明白的看出来与下一条的区别。 可以在后面添加正斜杠/来忽略文件夹，例如build/即忽略build文件夹。 可以使用!来否定忽略，即比如在前面用了*.apk，然后使用!a.apk，则这个a.apk不会被忽略。 *用来匹配零个或多个字符，如*.[oa]忽略所有以&quot;.o&quot;或&quot;.a&quot;结尾，*~忽略所有以~结尾的文件（这种文件通常被许多编辑器标记为临时文件）；[]用来匹配括号内的任一字符，如[abc]，也可以在括号内加连接符，如[0-9]匹配0至9的数；?用来匹配单个字符。 看了这么多，还是应该来个栗子： # 忽略 .a 文件*.a# 但否定忽略 lib.a, 尽管已经在前面忽略了 .a 文件!lib.a# 仅在当前目录下忽略 TODO 文件， 但不包括子目录下的 subdir/TODO/TODO# 忽略 build/ 文件夹下的所有文件build/# 忽略 doc/notes.txt, 不包括 doc/server/arch.txtdoc/*.txt# 忽略所有的 .pdf 文件 在 doc/ directory 下的doc/**/*.pdf","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://lyk-love.cn/tags/Git/"}]},{"title":"Design Pattern","slug":"Design-Pattern","date":"2022-09-26T06:39:34.928Z","updated":"2022-09-26T06:39:34.929Z","comments":true,"path":"2022/09/26/Design-Pattern/","link":"","permalink":"http://lyk-love.cn/2022/09/26/Design-Pattern/","excerpt":"Outline: 类之间的关系 组合 &gt; 继承 Design Pattern 设计模式：创建型 设计模式： 结构型 设计模式：行为型 ref： 软件系统设计 --- 潘敏学； Design Pattern Turotial","text":"Outline: 类之间的关系 组合 &gt; 继承 Design Pattern 设计模式：创建型 设计模式： 结构型 设计模式：行为型 ref： 软件系统设计 --- 潘敏学； Design Pattern Turotial 类之间的关系 ref: https://www.jianshu.com/p/d1d45efcfac1 泛化(Generalization） 泛化表示一个更泛化的元素和一个更具体的元素之间的关系。泛化是用于对继承进行建模的UML元素。在Java中，用extends关键字表示。 **表示方式：**用实线空心箭头表示。 实现(Realization) 实现是一种类与接口的关系，表示类是接口所有特征和行为的实现，在程序中一般通过类实现接口来描述。 **表示方式：**用虚线空心三角形箭头表示，实现类指向接口。 依赖(Dependency) 是一种使用的关系，即一个类的实现需要另一个类的协助。在Java中，方法参数需要传入另一个类的对象，就表示依赖这个类。 **表示方式：**虚线箭头。 **注意：**类A需要用到类B，类A指向类B。 public class Student &#123; public void read(Book book)&#123; System.out.println(&quot;读的书是：&quot; + book.getName()); &#125; public void eat(Food food)&#123; System.out.println(&quot;吃的是：&quot; + food.getName()); &#125;&#125; 关联(Association) 表示类与类之间的联接,它使一个类知道另一个类的属性和方法，这种关系比依赖更强、不存在依赖关系的偶然性、关系也不是临时性的，一般是长期性的。通常是将一个类的对象作为另一个类的成员变量. 在Java中，一个类的全局变量引用了另一个类，就表示关联了这个类 **表示方式：**实线箭头。 **注意：**类A中用到类B属性，类A指向类B。 聚合(Aggregation) 概念：聚合是关联关系的一种，是强的关联关系。聚合是整体和个体之间的关系，即has-a的关系，整体与个体可以具有各自的生命周期，部分可以属于多个整体对象，也可以为多个整体对象共享。聚合和关联关系在语法上是一致的，只能从语义来区分 **表示方式：**尾部为空心菱形的实线箭头（也可以没箭头） **注意：**类A中用到类B属性，类A指向类B。 public class People&#123; private Student student; private Worker worker; private Farmer farmer; public People(Student student, Worker worker, Farmer farmer)&#123; this.student = student; this.worker = worker; this.farmer = farmer; &#125;&#125; 组合(Composition) 组合也是关联关系的一种。组合是一种整体与部分的关系，即contains-a的关系，比聚合更强。部分与整体的生命周期一致，整体的生命周期结束也就意味着部分的生命周期结束，组合关系不能共享。程序中组合和关联关系是一致的，只能从语义级别来区分。 **表示方式：**尾部为实心菱形的实现箭头（也可以没箭头）。 **注意：**Head是Dog的一部分，Dog指向Head。 public class Bird&#123; private Wing wing; public People()&#123; wing = new Wing();&#125; 组合 &gt; 继承 Expert designers know not to solve every problem from first principles, they reuse solutions : &quot;不要造轮子&quot; HAS-A can be better than IS-A Composition gives you a lot more flexibility. Not only does it let you encapsulate a family of algorithms into their own set of classes, but it also lets you change behavior at runtime. 与继承关系相比，组合的主要优势在于不会破坏类的封装性，而且继承是一种耦合度较大的静态关系，无法在程序运行时动态扩展。在软件开发阶段，组合关系虽然不会比继承关系减少编码量，但是到了软件维护阶段，由于关联关系使系统具有较好的松耦合性，因此使得系统更加容易维护。当然，组合的缺点是比继承关系要创建更多的对象 Design Pattern 设计模式(Design Pattern)是一套被反复使用、多数人知晓的、经过分类编目的、代码设计经验的总结 基本要素： 模式名称 (Pattern name) 问题 (Problem) 解决方案 (Solution) 效果 (Consequences) Types 根据其目的(模式是用来做什么的)可分为创建型(Creational)，结构型(Structural)和行为型(Behavioral)三种： 创建型模式主要用于创建对象。 结构型模式主要用于处理类或对象的组合。 行为型模式主要用于描述对类或对象怎样交互和怎样分配职责。 根据范围，即模式主要是用于处理类之间关系还是处理对象之间的关系，可分为类模式和对象模式两种： 类模式处理类和子类之间的关系，这些关系通过继承建立，在编译时刻就被确定下来，是静态的 对象模式处理对象间的关系，这些关系在运行时刻变化，是动态的 注： 模板方法为什么是行为性模式? 答案：子类控制父类的行为 Creationnal Patterns Singleton 太简单，不介绍了 Simple Factory 策略模式解决行为问题, 而工厂模式解决对象创建问题 模式结构 Factory： 工厂角色 Product：抽象产品 ConcreteProduct： 具体产品角色 示例 简单工厂模式的缺点 当有新产品要加入到系统中时,必须修改工厂类,加入必要的处理逻辑,这违背了“开闭原则” Factory Method 在工厂方法模式中,核心的工厂类不再负责所有产品的创建,而是将具体创建工作交给子类去做。这个核心类仅仅负责给出具体工厂必须实现的接口,而不负责哪一个产品类被实例化这种细节,这使得工厂方法模式可以在不修改工厂角色的情况下引进新产品 code 抽象工厂类代码: public abstract class PayMethodFactory&#123; public abstract AbstractPay getPayMethod();&#125; 具体工厂类代码： public class CashPayFactory extends PayMethodFactory&#123; public AbstractPay getPayMethod() &#123; return new CashPay(); &#125;&#125; 客户类代码片段: PayMethodFactory factory;AbstractPay payMethod; //在定义工厂和产品时都必须使用抽象层factory=new CashPayFactory();payMethod =factory.getPayMethod();payMethod.pay(); 为了提高系统的可扩展性和灵活性,在定义工厂和产品时都必须使用抽象层,如果需要更换产品类,只需要更换对应的工厂即可,其他代码不需要进行任何修改。 示例 优点 很好地符合了“开闭原则” 缺点 添加新产品时，不仅要添加新的产品类，还要添加对应的具体工厂类，增加系统的复杂度 Abstract Factory 抽象工厂模式(Abstract Factory Pattern):提供一个创建一系列相关或相互依赖对象的接口,而无须指定它们具体的类。抽象工厂模式又称为Kit模式,属于对象创建型模式 概念 产品等级结构： 即产品的继承结构。 如抽象类是电视机， 而子类有海信电视机， TCL电视机， 则抽象电视机与具体品牌的电视机构成了一个产品等级结构 产品族：由同一个工厂生产的，位于不同产品等级结构中的一组产品。 如海尔是一个产品族， 海尔电视机属于电视机产品结构， 电冰箱位于电冰箱产品结构 模式结构 AbstractFactory AbstractProduct Product 模式分析 抽象工厂模式与工厂方法模式的区别：工厂方法模式针对的是一个产品等级结构，而抽象工厂模式则需要面对多个产品等级结构 code 抽象工厂类： public abstract class AbstractFactory&#123; public abstract AbstractProductA createProductA(); public abstract AbstractProductB createProductB();&#125; 具体工厂类： public class ConcreteFactory1 extends AbstractFactory&#123; public AbstractProductA createProductA() &#123; return new ConcreteProductA1(); &#125; public AbstractProductB createProductB() &#123; return new ConcreteProductB1(); &#125;&#125; 示例 产品结构为： 抽象工厂模式： 优点 增加或者替换产品族比较方便 缺点： 增加新的产品等级结构需要修改抽象工厂和所有的具体工厂类，对“开闭原则”的支持呈现倾斜性 Builder 建造者模式可以将一个复杂对象的构建与它的表示分离,使得同样的构建过程可以创建不同的表示.将部件和其组装过程分开,一步一步创建一个复杂的对象 是一种对象创建型模式 模式结构 Builder:抽象建造者 ConcreteBuilder:具体建造者 Directo: :指挥者. 一方面它隔离了客户与生产过程;另一方面它负责控制产品的生成过程 Product:产品角色 code 抽象建造者: public abstract class Builder&#123; protected Product product=new Product(); public abstract void buildPartA(); public abstract void buildPartB(); public abstract void buildPartC();&#125;public Product getResult()&#123; return product;&#125; Director: public class Director&#123; private Builder builder;&#125;public Director(Builder builder)&#123; this.builder=builder;&#125;public void setBuilder(Builder builder)&#123; this.builder=builer;&#125;public Product construct()&#123; builder.buildPartA(); builder.buildPartB(); builder.buildPartC(); return builder.getResult();&#125; client: Builder builder = new ConcreteBuilder();Director director = new Director(builder);Product product = director.construct(); 在客户端代码中,无须关心产品对象的具体组装过程， 只需确定具体建造者的类型,建造者模式将复杂对象的构建与对象的表现分离开来,这样使得同样的构建过程可以创建出不同的表现 示例 KFC套餐,这里省略了抽象建造者 优点 client不必知道产品的创建过程，将产品本身与产品的创建过程解耦 可以更加精细地控制产品的创建过程 增加新的具体建造者无须修改原有类库的代码，指挥者类针对抽象建造者类编程，系统扩展方便，符合“开闭原则” 缺点 建造者模式所创建的产品一般具有较多的共同点,其组成部分相似,如果产品之间的差异性很大,则不适合使用建造者模式,因此其使用范围受到一定的限制 如果产品的内部变化复杂,可能会导致需要定义很多具体建造者类来实现这种变化,导致系统变得很庞大。 模式简化 省略抽象建造者角色: 如果系统中只需要一个具体建造者的话,可以省略掉抽象建造者。 省略指挥者角色: 在具体建造者只有一个的情况下,如果抽象建造者角色已经被省略掉,那么还可以省略指挥者角色,让Builder角色扮演指挥者与建造者双重角色。 Prototype 原型模式(Prototype Pattern): 用原型实例指定创建对象的种类,并且通过复制这些原型创建新的对象。原型模式允许一个对象再创建另外一个可定制的对象,无须知道任何创建的细节 是一种对象创建型模式 模式结构 Prototype:抽象原型类 ConcretePrototype:具体原型类 Client 一般而言,clone()方法满足: 对任何的对象x,都有x.clone() !=x,即克隆对象与原对象不是同一个对象。 对任何的对象x,都有x.clone().getClass()==x.getClass(),即克隆对象与原对象的类型一样。 如果对象x的equals()方法定义恰当,那么x.clone().equals(x)应该成立 Java提供的clone()方法默认是浅拷贝，可以override该方法实现不完全的深拷贝 要实现彻底的深拷贝是非常困难的 优点 可以快速创建很多相同或相似的对象，简化对象的创建过程，还可以保存对象的一些中间状态； 缺点 需要为每一个编写clone()，因此对已有类进行改造时，需要修改其源代码，违反了开闭原则 在实现深克隆时需要编写较为复杂的代码 事实上几乎不存在完全的深拷贝，这要考虑到底层对象复杂的引用关系 适用环境 创建新对象成本较大，新的对象可以通过原型模式对已有对象进行复制来获得； 系统要保存对象的状态，而对象的状态变化很小； 需要避免使用分层次的工厂类来创建分层次的对象，并且类的实例对象只有一个或很少的几个组合状态，通过复制原型对象得到新实例可能比使用构造函数创建一个新实例更加方便 Structural Patterns Adatpter 适配器模式(Adapter Pattern) :将一个接口转换成客户希望的另一个接口,适配器模式使接口不兼容的那些类可以一起工作,其别名为包装器(Wrapper)。适配器模式既可以作为类结构型模式,也可以作为对象结构型模式 在适配器模式中可以定义一个包装类,包装不兼容接口的对象,这个包装类指的就是适配器(Adapter),它所包装的对象就是适配者 (Adaptee),即被适配的类 适配器提供客户类需要的接口,适配器的实现就是把客户类的请求转化为对适配者的相应接口的调用。也就是说:当客户类调用适配器的方法时,在适配器类的内部将调用适配者类的方法,而这个过程对客户类是透明的,客户类并不直接访问适配者类。因此,适配器可以使由于接口不兼容而不能交互的类可以一起工作。这就是适配器模式的模式动机 模式结构： Target: 目标抽象类， 即 Adapter： 适配器类 Adaptee： 适配者类 Client 类适配器 Adapter多继承， 同时继承Target和Adaptee, 这需要语言支持多继承。 如果语言不支持多继承， 那么如果Target是接口的话，也可以选择继承Adaptee,实现Target public class Adapter extends Adaptee implements Target&#123; public void request() &#123; specificRequest(); &#125;&#125; 对象适配器 不是继承Adaptee类，而是持有Adaptee的实例，以解决兼容性的问题。即持有Adaptee类，实现Target接口，完成Adaptee—&gt;Target的适配 缺点是难以置换Aaptee的方法 public class Adapter extends Target&#123; private Adaptee adaptee; public Adapter(Adaptee adaptee) &#123; this.adaptee=adaptee; &#125; public void request() &#123; adaptee.specificRequest(); &#125;&#125; 优点 将目标类和适配者类解耦，通过引入一个适配器类来重用现有的适配者类，而无须修改原有代码。 增加了类的透明性和复用性，将具体的实现封装在适配者类中，对于客户端类来说是透明的，而且提高了适配者的复用性。 灵活性和扩展性都非常好，通过使用配置文件，可以很方便地更换适配器，也可以在不修改原有代码的基础上增加新的适配器类，完全符合“开闭原则’ 由于适配器类是适配者类的子类，因此可以在适配器类中置换一些适配者的方法，使得适配器的灵活性更强 缺点 对于Java、C#等不支持多重继承的语言，使用有局限 默认适配器模式 默认适配器模式(Default Adapter Pattern)或缺省适配器模式: 当不需要全部实现接口提供的方法时，可先设计一个抽象类实现接口，并为该接口中每个方法提供一个默认实现(空方法)，那么该抽象类的子类可有选择地覆盖父类的某些方法来实现需求，它适用于一个接口不想使用其所有的方法的情况。因此也称为单接口适配器模式 双向适配器模式 在对象适配器的使用过程中，如果在适配器中同时包含对目标类和适配者类的引用，适配者可以通过它调用目标类中的方法，目标类也可以通过它调用适配者类中的方法，那么该适配器就是一个双向适配器 Composite 组合模式(Composite Pattern): 组合多个对象形成树形结构以表示“整体部分”的结构层次。组合模式对单个对象(即叶子对象)和组合对象(即容器对象)的使用具有一致性 又称为“整体部分”(Part-Whole)模式， 属于对象的结构模式，它将对象组织到树结构中,可以用来描述整体与部分的关系 模式动机 对于树形结构，当容器对象(如文件夹)的某一个方法被调用时，将遍历整个树形结构，寻找也包含这个方法的成员对象(可以是容器对象，也可以是叶子对象，如子文件夹和文件)并调用执行(递归调用) 由于容器对象和叶子对象在功能上的区别，在使用这些对象的客户端代码中必须有区别地对待容器对象和叶子对象，而实际上大多数情况下客户端希望一致地处理它们，因为对于这些对象的区别对待将会使得程序非常复杂 组合模式描述了如何将容器对象和叶子对象进行递归组合，使得用户在使用时无须对它们进行区分，可以一致地对待容器对象和叶子对象 事实上，Leaf类不可能实现Component类的operation()之外的方法，这样毫无意义。 因此组合模式分为两种： 透明组合模式：把Leaf和Composite看成一个东西，Component类的所有方法都是抽象方法， 则Leaf必须实现所有这些方法 好处：对Client而言， Leaf和Composite没有区别，可以直接针对Component抽象类编程 坏处：Leaf的operation()之外的方法全都是无用方法，Leaf对这些方法的实现基本就是抛出异常。Client“透明”地处理Component时，（因为有Leaf存在）要么处理异常， 要么就用RTTI判断对象的实际类型，没有安全性，事实上也不可能透明 因此，透明组合模式就是个笑话 安全组合模式：Component是一个接口，仅拥有抽象方法operation（）。 Composite和Leaf都实现Component接口，这样这两个子类的行为就是合理的 优点：Leaf和Composite的行为是合理的 缺点：Leaf和Composite是两个不同的类型，使用的时候无法面向Component编程，即对Client不够透明 推荐使用安全组合模式 模式结构 Component: 抽象构件 client针对Component进行编程，无须知道它到底表示的是叶子还是容器，可以对其进行统一处理 容器对象中既可以包含叶子，也可以包含容器，以此实现递归组合，形成一个树形结构 抽象父类 Component中，只有operation （）是抽象方法，其他方法都要提供实现， 否则Leaf类就要实现这些方法了（这是不可能的） Leaf:叶子构件 Composite:容器构件 Client:客户类 模式分析 组合模式的关键是定义了一个抽象构件类，它既可以代表叶子，又可以代表容器，而客户端针对该抽象构件类进行编程，无须知道它到底表示的是叶子还是容器，可以对其进行统一处理 同时容器对象与抽象构件类之间还建立了聚合关系，容器对象既可以包含叶子，也可以包含容器，以此实现递归组合，形成树形结构 透明组合模式 Component： public abstract class Component //Leaf类必须实现所有抽象方法&#123; public abstract void add(Component c); public abstract void remove(Component c); public abstract Component getChild(int i); public abstract void operation();&#125; Leaf: public class Leaf extends Component&#123; public void add(Component c) //这些方法都是无意义的，只能抛个异常 &#123; //异常处理或错误提示 &#125; public void remove(Component c) &#123; //异常处理或错误提示 &#125; public Component getChild(int i) &#123; //异常处理或错误提示 &#125; public void operation() &#123; //实现代码 &#125;&#125; 安全组合模式 在Java AWT中使用了安全组合模式 Component： public interface class Component //Leaf类只需要实现operation（）&#123; public abstract void operation();&#125; Leaf: public class Leaf extends Component&#123; public void operation() &#123; //实现代码 &#125;&#125; code 前面给出了Component和Leaf的代码，这里只需要给出Composite的代码 Composite: public class Composite extends Component&#123; private ArrayList list = new ArrayList(); public void add(Component c) &#123; list.add(c); &#125; public void remove(Component c) &#123; list.remove(c); &#125; public Component getChild(int i) &#123; (Component)list.get(i); &#125; public void operation() &#123; for( Object obj: list ) &#123; ((Component)obj).operation(); &#125; &#125;&#125; 示例( 透明组合模式 ) Bridge 将抽象部分与它的实现 部分分离，使它们都可以独立地变化 对象结构型模式 桥接模式将抽象部分与它的实现部分分离，使它们都可以独立地变化。它是一种对象结构型模式，又称为柄体(Handle and Body)模式或接口(Interface)模式。 桥接模式包含如下四个角色:抽象类中定义了一个实现类接口类型的对象并可以维护该对象;扩充抽象类扩充由抽象类定义的接口，它实现了在抽象类中定义的抽象业务方法，在扩充抽象类中可以调用在实现类接口中定义的业务方法;实现类接口定义了实现类的接口，实现类接口仅提供基本操作，而抽象类定义的接口可能会做更多更复杂的操作;具体实现类实现了实现类接口并且具体实现它，在不同的具体实现类中提供基本操作的不同实现，在程序运行时，具体实现类对象将替换其父类对象，提供给客户端具体的业务操作方法。 在桥接模式中，抽象化(Abstraction)与实现化(Implementation)脱耦， 它们可以沿着各自的维度独立变化。 桥接模式的主要优点是分离抽象接口及其实现部分，是比多继承方案更好的解决方法，桥接模式还提高了系统的可扩充性，在两个变化维度中任意扩展一个维度，都不需要修改原有系统，实现细节对客户透明，可以对用户隐藏实现细节;其主要缺点是增加系统的理解与设计难度，且识别出系统中两个独立变化的维度并不是一-件容易的事情。 桥接模式适用情况包括:需要在构件的抽象化角色和具体化角色之间增加更多的灵活性，避免在两个层次之间建立静态的继承联系;抽象化角色和实现化角色可以以继承的方式独立扩展而互不影响;一个类存在两个独立变化的维度，且这两个维度都需要进行扩展;设计要求需要独立管理抽象化角色和具体化角色;不希望使用继承或因为多层次继承导致系统类的个数急剧增加的系统。 模式动机 对于有多个变化维度的系统，采用桥接模式来进行设计系统中类的个数更少， 且系统扩展更为方便 桥接模式将继承关系转换为关联关系， 从而降低了类与类之间的耦合 模式结构 Abstraction RefinedAbstraction：扩充抽象类 Implementor： 扩充的维度类，这里用接口，也可以用抽象类 ConcreteImplementor：维度类的实现类。 例如，若Implementor是“颜色”，则ConcreteImplementor可以是“红”、“黄”、“蓝” 模式分析 理解桥接模式，重点需要理解如何将抽象化(Abstraction)与实现化(Implementation)解耦 抽象化：将对象的共同性质抽取出来形成类 比如，将不同颜色、形状的矩形抽象为“正方形类” 实现化：针对抽象化给出具体实现。 实现化产生的对象比抽象化更具体 比如，对“矩形类”进行实例化，赋予其不同的属性（颜色、大小...） 抽象化与实现化解耦：将抽象化和实现化之间的强关联转变成弱关联，将两个角色（维度类和抽象物体类）之间的继承关系改为关联关系 具体矩形类不需要继承“颜色类”，再继承“大小类”，这样会造成类的数量爆炸。 取而代之的是，使用抽象矩形类，与“颜色类”和“大小类”组合 cocde Implementor： public interface Implementor &#123; public void operationImpl(); &#125; Abstraction: public abstract class Abstraction &#123; //引入了新的维度， 比如该Abstraction类是矩形类，Implementor是颜色类， //引入Implementor即使得矩形多了一个“颜色”维度 protected Implementor impl; public void setImpl(Implementor impl) &#123; this.impl=impl; &#125; public abstract void operation(); &#125; RefinedAbstraction: public class RefinedAbstraction extends Abstraction &#123; public void operation() &#123; //代码 impl.operationImpl(); //代码 &#125;&#125; 示例 模拟毛笔 蜡笔和毛笔是传统继承模式与桥接模式的最好比喻 现需要提供大中小3种型号的画笔，能够绘制5种不同颜色 如果使用蜡笔，我们需要准备3*5=15支蜡笔，也就是说必须准备15个具体的蜡笔类 蜡笔相当于传统的使用继承体系来扩展对象， 每个具体蜡笔类继承自某个具体大小类，和具体的颜色类， 一共有15个具体蜡笔类 毛笔相当于桥接模式，使用组合来扩展对象。 只需要一个抽象毛笔类，它依赖大小类和颜色类。 只需要3 + 5个维度类 + 一个抽象毛笔类，使用时再使用具体的毛笔实现类 跨平台视频播放器 每个播放器都有两个维度的属性： 支持的视频格式， 支持的操作系统 通过Bridge模式，将两个维度与播放器类组合起来 优点 分离抽象接口及其实现部分 与多继承对比： 多继承违背了类的单一职责原则(“一个类只有一个变化的原因”， 即变化的维度)； 使用多继承，类的个数会过多 桥接模式提高了系统的可扩展性 在两个变化维度中任意扩展一个维度，都不需要修改原有系统。 可以对用户隐藏实现细节，使得实现细节对客户透明 缺点 引入桥接模式会增加系统的复杂度 桥接模式要求正确识别出系统中两个独立变化的维度，因此其使用范围具有一定的局限性 适用环境 系统需要在构件的抽象化角色和具体化角色之间增加更多的灵活性，避免在两个层次之间建立静态的继承关系， 通过桥接模式可以使它们在抽象层建立一个关联关系 抽象化角色和实现化角色可以以继承的方式独立扩展而互不影响，在程序运行时可以动态将一个抽象化子类的对象和一个实现化子类的对象进行组合 一个类存在多个独立变化的维度，且这些维度都需要进行扩展 也可以使用多继承，但会导致类的组合爆炸 不希望使用继承或因为多层次继承导致系统类的个数急剧增加的系统 模式扩展 适配器模式与桥接模式的联用 桥接模式和适配器模式用于设计的不同阶段,桥接模式用于系统的初步设计，对于存在两个独立变化维度的类可以将其分为抽象化和实现化两个角色，使它们可以分别进行变化;而在初步设计完成之后，当发现系统与已有类无法协同工作时，可以采用适配器模式。但有时候在设计初期也需要考虑适配器模式，特别是那些涉及到大量第三方应用接口的情况。 Decorator 装饰模式(Decorator Pattern) : 动态地给一个对象增加一些额外的职责(Responsibility), 就增加对象功能来说，装饰模式比生成子类实现更为灵活 别名称为包装器(Wrapper)，与适配器模式的别名相同，但它们适用于不同的场合。根据翻译的不同，装饰模式也有人称之为“油漆工模式” 是一种对象结构型模式。 装饰模式可分为： 透明装饰模式： 要求client完全面向抽象编程，client应该全部使用抽象构件类型 半透明装饰模式:允许client声明具体装饰者类型的对象，调用在具体装饰者中新增的方法 模式结构 Component ConcreteComponent Decorator：抽象装饰类 ConcreteDecorator 模式分析 使用装饰模式来实现扩展比继承更加灵活，它以对客户透明的方式动态地给一个对象附加更多的责任 装饰模式可以在不需要创造更多子类的情况下，将对象的功能加以扩展 装饰模式的简化：如果只有一个具体构件类而没有抽象构件类，那么抽象装饰类可以作为具体构件类的直接子类。 code 抽象装饰类： public class Decorator extends Component &#123; private Component component; public Decorator(Component component) &#123; this.component=component; &#125; public void operation() &#123; component.operation(); &#125;&#125; 具体装饰类： public class ConcreteDecorator extends Decorator &#123; public ConcreteDecorator(Component component) &#123; super(component); &#125; public void operation() &#123; super.operation(); addedBehavior(); &#125; public void addedBehavior() &#123; //新增方法 &#125;&#125; 半透明装饰模式 半透明(semi-transparent)的装饰模式允许用户在客户端声明具体装饰者类型的对象，调用在具体装饰者中新增的方法 Transform camaro;camaro=new Car();camaro.move();//这里依然属于面向抽象编程，move()是抽象的Transform类的方法Robot bumblebee=new Robot(camaro); bumblebee.move();bumblebee.say(); //这里使用了具体的Robot装饰类的方法，没有面向抽象编程 示例 变形金刚在变形之前是一辆汽车，它可以在陆地上移动。当它变成机器人之后除了能够在陆地上移动之 外，还可以说话;如果需要，它还可以变成飞机，除了在陆地.上移动还可以在天空中飞翔。 透明装饰模式 在透明装饰模式中，要求客户端完全针对抽象编程，装饰模式的透明性要求客户端程序不应该声明具体构件类型和具体装饰类型，而应该全部声明为抽象构件类型。 Cipher sc,cc,ac;sc=new SimpleCipher(); cc=new ComplexCipher(sc); ac=new AdvancedCipher(cc); 示例 存在多种字符串加密算法(Cipher) SImpleCipher: ComplexCipher AdvancedCipher 用户可以只选择一种加密算法，也可以多重加密 优点 装饰模式与继承关系的目的都是要扩展对象的功能，但是装饰模式可以提供比继承更多的灵活性。 具体构件类与具体装饰类可以独立变化，用户可以根据需要增加新的具体构件类和具体装饰类，在使用时再对其进行组合，原有代码无须改变，符合“开闭原则” 缺点 使用装饰模式进行系统设计时将产生很多小对象，这些对象的区别在于它们之间相互连接的方式有所不同，而不是它们的类或者属性值有所不同，同时还将产生很多具体装饰类。这些装饰类和小对象的产生将增加系统的复杂度，加大学习与理解的难度。 这种比继承更加灵活机动的特性，也意味着装饰模式比继承更加易于出错，错误排查也比较困难 适用环境 在不影响其他对象的情况下，以动态、透明的方式给单个对象添加职责 需要动态地给一个对象增加功能，这些功能也可以动态地被撤销 当不能采用继承的方式对系统进行扩充或者采用继承不利于系统扩展和维护时。 不能采用继承的情况主要有两类: 是系统中存在大量独立的扩展，为支持每一种组合将产生大量的子类，使得子类数目呈爆炸性增长; 类被定义为不能继承(如final类) Facade 外观模式(Facade Pattern):在外观模式中，外部与一个子系统的通信必须通过一个统一的外观对象进行，为子系统中的一组接口提供一组一致的更高层次的接口，使得子系统更容易使用 又称为门面模式 是一种对象结构型模式 模式结构 Facade SubSystem 模式分析 外观模式要求一个子系统的外部与其内部的通信通过一个统一的外观对象进行，外观类将客户端与子系统的内部复杂性分隔开，使得client只需要与外观对象打交道，而不需要与子系统内部的很多对象打交道 是“迪米特法则”的体现，也降低了client与子系统类的耦合 引入外观类可以降低系统的复杂度 降低了子系统间的通信和相互依赖关系 提高了客户端使用的便捷性，使得客户端无须关心子系统的工作细节，通过外观角色即可调用相关功能 code Facade： public class Facade &#123; private SubSystemA obj1 = new SubSystemA(); private SubSystemB obj2 = new SubSystemB(); private SubSystemC obj3 = new SubSystemC(); public void method() &#123; obj1.method(); obj2.method(); obj3.method(); &#125; &#125; 示例 一个电源总开关可以控制四盏灯、一个风扇、一台空调和一台电视机的启动和关闭 优点 对客户屏蔽子系统组件，减少了客户处理的对象数目并使得子系统使用起来更加容易 实现了子系统与客户之间的松耦合关系 降低了大型软件系统中的编译依赖性，并简化了系统在不同平台之间的移植过程 因为编译一个子系统一般不需要编译所有其他的子系统。一个子系统的修改对其他子系统没有任何影响，而且子系统内部变化也不会影响到外观对象 只是提供了一个访问子系统的统一入口，并不影响用户直接使用子系统类 缺点 不能很好地限制客户使用子系统类，如果对客户访问子系统类做太多的限制则减少了可变性和灵活性。 在不引入抽象外观类的情况下，当增加新的子系统或者移除子系统时需要修改外观类，违背了“开闭原则” 适用环境 当要为一个复杂子系统提供一个简单接口时可以使用外观模式。该接口可以满足大多数用户的需求，而且用户也可以越过外观类直接访问子系统 客户程序与多个子系统之间存在很大的依赖性。引入外观类将子系统与客户以及其他子系统解耦，可以提高子系统的独立性和可移植性 在层次化结构中，可以使用外观模式定义系统中每一层的入口，层与层之间不直接产生联系，而通过外观类建立联系，降低层之间的耦合度 模式扩展 不要试图通过外观类为子系统增加新行为，外观模式的用意是为子系统提供一个集中化和简化的沟通渠道，而不是向子系统加入新的行 Flyweight 享元模式： 通过共享，实现大量的细粒度对象的复用，以解决对象数量太多导致的性能问题 是一种对象结构性模式 分为复合享元模式和单纯享元模式 单纯享元模式： 就是所有享元对象都可以共享 复合享元模式： 存在复合享元对象，它是单纯和不可共享享元对象的组合，本身不能被共享。 但可以被分解为单纯享元对象，和一些不可共享的享元对象 默认是复合享元模式 复合享元模式： 模式结构 Flyweight:抽象享元类： 声明一个接口，通过它可以接受并作用于外部状态 ConcreteFlyweight: 具体享元类 UnsharedConcreteFlyweight:不可共享的具体享元类 FlyweightFactory:享元工厂类，用于创建并管理享元对象 模式分析 享元模式是一个考虑系统性能的设计模式，通过使用享元模式可以节约内存空间，提高系统的性能。 享元模式的核心在于享元工厂类，享元工厂类的作用在于提供一个用于存储享元对象的享元池，用户需要对象时，首先从享元池中获取，如果享元池中不存在,则创建一个新的享元对象返回给用户，并在享元池中保存该新增对象。 享元模式以共享的方式高效地支持大量的细粒度对象 享元对象具有内部状态(Internal State)和外部状态(External State) 内部状态是存储在享元对象内部并且不会随环境改变而改变的状态，因此可以共享。 外部状态是存储在client的，随环境改变而改变的、不可以共享的状态 client在享元对象被创建之后并在需要被使用时，将外部状态传入到享元对象内部 外部状态间相互独立 code FlyweightFactory: public class FlyweightFactory &#123; //持有享元对象的序列 private HashMap flyweights = new HashMap(); public Flyweight getFlyweight(String key) &#123; if(flyweights.containsKey(key)) &#123; return (Flyweight)flyweights.get(key); &#125; else &#123; Flyweight fw = new ConcreteFlyweight(); flyweights.put(key,fw); return fw; &#125; &#125; Flyweight public class Flyweight&#123; //内部状态作为成员属性 private String intrinsicState; public Flyweight(String intrinsicState) &#123; this.intrinsicState = intrinsicState; &#125; //接受外部状态，并可能改变外部状态 public void operation(String extrinsicState) &#123; ...... &#125;&#125; 优点 可以极大减少内存中对象的数量，使得相同对象或相似对象在内存中只保存一份 外部状态相对独立，而且不会影响其内部状态，从而使得享元对象可以在不同的环境中被共享。 缺点 享元模式使得系统更加复杂，需要分离出内部状态和外部状态，这使得程序的逻辑复杂化。 为了使对象可以共享，享元模式需要将享元对象的状态外部化，而读取外部状态使得运行时间变长。 适用环境 一个系统有大量相同或者相似的对象，由于这类对象的大量使用，造成内存的大量耗费。 对象的大部分状态都可以外部化，可以将这些外部状态传入对象中。 使用享元模式需要维护一个存储享元对象的享元池，而这需要耗费资源，因此，在多次重复使用享元对象时才值得使用享元模式。 模式扩展 享元模式与其他模式的联用 在享元模式的享元工厂类中通常提供一个静态的工厂方法用于返回享元对象，使用简单工厂模式来生成享元对象。在一个系统中，通常只有唯一一个享元工厂，因此享元工厂类可以使用单例模式进行设计。 享元模式可以结合组合模式形成复合享元模式，统- -对享元对象设置外部状态。 Proxy 代理模式(Proxy Pattern) :给某一个对象提供一个代理，并由代理对象控制对原对象的引用 是一种对象结构型模式 远程代理为一个位于不同的地址空间的对象提供一个本地的代表对象，它使得客户端可以访问在远程机器上的对象，远程机器可能具有更好的计算性能与处理速度，可以快速响应并处理客户端请求。 如果需要创建一个资源消耗较大的对象，先创建一个消耗相对较小的对象来表示，真实对象只在需要时才会被真正创建，这个小对象称为虚拟代理。虚拟代理通过使用一个小对象来代表一一个大对象，可以减少系统资源的消耗，对系统进行优化并提高运行速度 ---- 也就是延迟初始化 保护代理可以控制对一个对象的访问，可以给不同的用户提供不同级别的使用权限 模式结构 Subject:抽象主题角色 Proxy:代理主题角色 RealSubject:真实主题角色 Proxy和RealSubject都继承Subject， 使得可以面向抽象的Subject编程，而不必考虑具体的实现类是Proxy还是ReadSubject Proxy持有ReadSubject对象，进行相应操作 code public class Proxy implements Subject &#123; private RealSubject realSubject = new RealSubject(); public void preRequest() &#123;......&#125; public void request() &#123; preRequest(); realSubject.request(); postRequest(); &#125; public void postRequest() &#123;......&#125;&#125; 优点 代理模式能够协调调用者和被调用者，在一定程度上降低了系统的耦合度。 远程代理使得客户端可以访问在远程机器.上的对象，远程机器可能具有更好的计算性能与处理速度，可以快速响应并处理客户端请求。 虚拟代理通过使用一个小对象来代表一-个大对象，可以减少系统资源的消耗，对系统进行优化并提高运行速度。 保护代理可以控制对真实对象的使用权限 缺点 慢 复杂度高 模式细分 根据代理模式的使用目的，常见的代理模式有以下几种类型: 远程(Remote)代理: 为一个位于不同的地址空间的对象提供一个 本地的代理，对象，这个不同的地址空间可以是在同一台主机中，也可以在另一台主机中，远程代理又叫做大使(Ambassador) 虚拟(Virtual)代理:如果需要创建一个资源消耗较大的对象，先创建一个消耗相对较小的对象来表示，真实对象只在需要时才会被真正创建 使用示例： 大图浏览的控制 用户通过浏览器访问网页时先不加载真实的大图，而是通过代理对象的方法来进行处理 在代理对象的方法中，先使用一个线程向客户端浏览器加载一个小图片，然后在后台使用另一个线程来调用大图片的加载方法将大图片加载到客户端。 当需要浏览大图片时，再将大图片在新网页中显示。 如果用户在浏览大图时加载工作还没有完成，可以再启动一个线程来显示相应的提示信息。通过代理技术结合多线程编程将真实图片的加载放到后台来操作，不影响前台图片的浏览 Copy- on-Write代理: 是虚拟代理的一种，把复制(克隆)操作延迟到只有在客户端真正需要时才执行。一般来说，对象的深克隆是一个开销较大的操作，Copy-on-Write代理可以让这个操作延迟，只有对象被用到的时候才被克隆 保护(Protect or Access)代理: 给不同用户提供不同的使用权限，来控制对对象的访问 缓冲(Cache)代理: 为数据提供临时的存储空间，以便多个客户端可以共享数据。 防火墙(Firewall)代理: 保护对象不被恶意攻击 智能引用(Smart Reference)代理: 相当于智能指针，当一个对象被引用时，智能引用代理会进行一些额外的操作，比如，记录下此对象被调用的次数 Behavioral Patterns Strategy 策略模式： 定义一系列可替换的算法，由client决定使用哪个 模式结构 上下文（Context ) ： 拥有Strategy对象的一个引用； 被配置了具体策略 ConcreteStrategy 策略（Strategy）: 声明了所支持策略的接口。 Context利用这些被ConcreteStrategy定义的接口 具体策略（ConcreteStrategy）: 实现了Strategy声明 的接口，给出了具体的实现 模式逻辑： 上下文Context和Strategy的相互协作完成整个算法。 Context可能会通过提供方法让Strategy访问其数据；甚 至将自身的引用传给Strategy，供其访问其数据。 Strategy会在需要的时候访问Context的成员变量。 上下文Context将一些对他的请求转发给策略类来实现 ，客户（Client）通常创建ConcreteStrategy的对象，然 后传递给Context来灵活配置Strategy接口的具体实现； 这样Client就有可以拥有一个Strategy接口的策略族，其 中包含多种ConcreteStrategy的实现 适用环境 当很多相关类只在它们的行为的实现上不一样 当我们需要同一个行为的不同实现（变体）的时候 算法需要用到一些数据，而这些数据不应该被客户知道。我们可以通过策略模式隐藏复杂的算法和数据接口。 一个类定义了很多行为，这些行为作为一个switch选择语句的分支执行部分。策略模式可以消除这些分支选择 模式扩展 Strategy可以是接口，也可以是类。如果是类，则可以抽象所有具体算法中公共的实现部分。 当然，我们也可以直接通过Context的子类来实现不同的 Context实现。不过这样算法的具体实现，就和算法的利用的 实现项目交织在一起，不利于理解和维护. 策略模式消除了类似根据策略类型的Switch语句。 可以动态选择不同的策略 这需要客户必须提前知晓不同的策略 Context和Strategy之间的通信有代价 策略模式会创建出较多的对象 Command 命令模式(Command Pattern): 将一个请求封装为一个对象,从而可在请求client时传递参数; 对请求排队或者记录请求日志,以及支持可撤销的操作。 是一种对象行为型模式, aka动作(Action)模式或事务(Transaction)模式 client持有reciever和 invoker， 通过将receiver传给ConcreteCommand来构造command， 然后把command传给invoker， client只要调用invoker的invoke（） 模式结构 命令模式包含如下角色: Command: 抽象命令类 ConcreteCommand: 具体命令类 Invoker: 调用者 Receiver: 接收者 Client: 客户类 模式分析 命令模式的本质是对命令进行封装，将发出命令的责任和执行命令的责任分开 client负责发出命令。 client持有receiver和invoker，用receiver构造ConcreteCommand， 将command传给invoker，调用invoker.invoke()，发出命令 invoke负责执行命令。 invoker对象持有了command对象， command对象已经被注入了receiver对象。由 invoker执行command 每一个命令都是一个操作:请求的一方发出请求，要求执行一个操作; 接收的一方收到请求，并执行操作 Receiver有doSomething()方法，在command.exeute()中调用 命令模式使请求本身成为一个对象，这个对象和其他对象一样可以被存储和传递 通过引入抽象命令接口（Command），且client针对Command接口编程 命令模式允许请求的一方和接收的一方独立开来，使得请求的一方不必知道接收请求的一方的接口、请如何接收、操作是否被执行、何时被执行、怎么被执行的 等问题 code 通用Receiver类： public abstract class Receiver &#123; //抽象接收者，定义每个接收者都必须完成的业务 public abstract void doSomething(); &#125; 具体的Receiver类ConcreteReciver1、ConcreteReciver2： public class ConcreteReciver1 extends Receiver&#123; //每个接收者都必须处理一定的业务逻辑 public void doSomething()&#123; &#125; &#125;public class ConcreteReciver2 extends Receiver&#123; //每个接收者都必须处理一定的业务逻辑 public void doSomething()&#123; &#125;&#125; 抽象的Command类： public abstract class Command &#123; //每个命令类都必须有一个执行命令的方法 public abstract void execute(); &#125; 调用者Invoker类： public class Invoker &#123; private Command command; //受气包，接受命令 public void setCommand(Command _command)&#123; this.command = _command; &#125; //执行命令 public void invoke()&#123; this.command.execute(); &#125;&#125; 具体的Command类ConcreteCommand1、ConcreteCommand2： public class ConcreteCommand1 extends Command &#123; //进行命令处理的Receiver private Receiver receiver; //通过构造函数来注入receiver public ConcreteCommand1(Receiver _receiver)&#123; this.receiver = _receiver; &#125; //必须实现一个命令 public void execute() &#123; //业务处理 this.receiver.doSomething(); &#125; &#125;public class ConcreteCommand2 extends Command &#123; //进行命令处理的Receiver private Receiver receiver; //通过构造函数来注入receiver public ConcreteCommand2(Receiver _receiver)&#123; this.receiver = _receiver; &#125; //必须实现一个命令 public void execute() &#123; //业务处理 this.receiver.doSomething(); &#125; &#125; Client： public class Client &#123; public static void main(String[] args) &#123; //首先声明调用者Invoker Invoker invoker = new Invoker(); //定义接收者 Receiver receiver = new ConcreteReciver1(); //定义一个发送给接收者的命令 Command command = new ConcreteCommand1(receiver); //把命令交给调用者去执行 invoker.setCommand(command); invoker.invoke(); &#125; &#125; 示例 遥控器（Controller）是client， 电视机（Television）是接收者，有三个具体命令类 这个示例是简化版的命令模式，没有invoker，也没有动态注入Receiver 优点 命令模式的优点 降低系统耦合度 可扩展性强，很容易加入新的命令类 可以比较容易地设计命令队列和宏命令（组合命令） 可以方便地实现请求的Undo和Redo 缺点 可能导致系统具有过多的命令类 适用环境 系统需要将请求调用者和请求接收者解耦，使得调用者和接收者不直接交互 系统需要在不同的时间指定请求、将请求排队和执行请求 系统需要支持命令的撤销(Undo)操作和恢复(Redo)操作 系统需要将一 组操作组合在一起，即支持宏命令 Unix的SHell编程的宏命令功能，可以将多条命令封装在一个命令对象中，这使用了命令模式 State 状态模式(State Pattern) :允许一个对象在其内部状态改变时改变它的行为,对象看起来似乎修改了它的类。其别名为状态对象(Objects for States), 状态模式是一种对象行为型模式 状态模式描述了对象状态的变化以及对象如何在每一种状态下表现出不同的行为。 状态模式的关键是引入了一个抽象类来专门表示对象的状态,这个类我们叫做抽象状态类,而对象的每一种具体状态类都继承了该类,并在不同具体状态类中实现了不同状态的行为,包括各种状态之间的转换。 结构： Context 环境类： 是拥有状态的对象。 有时候可以充当状态管理器(State Manager)的角色,可以在环境类中对状态进行切换操作 State 抽象状态类（也可以是接口） : 可以将不同对象下的行为单独提取出来封装在具体的状态类中,使得环境类对象在其内部状态改变时可以改变它的行为,对象看起来似乎修改了它的类,而实际上是由于切换到不同的具体状态类实现的。 ConcreteState: 具体状态类 示例 缺点 状态模式的使用必然会增加系统类和对象的个数。 状态模式的结构与实现都较为复杂,如果使用不当将导致程序结构和代码的混乱。 状态模式对“开闭原则”的支持并不太好,对于可以切换状态的状态模式,增加新的状态类需要修改那些负责状态转换的源代码,否则无法切换到新增状态;而且修改某个状态类的行为也需修改对应类的源代码。 Observer 观察者模式(Observer Pattern): 通过建立对象间的一对多依赖关系, 使得每当一个对象状态发生改变时,其相关依赖对象皆得到通知并被自动更新 又叫做发布-订阅(Publish/Subscribe)模式、模型-视图(Model/View)模式、源-监听器(Source/Listener)模式或从属者(Dependents)模式 是一种对象行为型模式 模式结构 Subject: 目标 ConcreteSubject:具体目标 Observer:观察者 ConcreteObserver: 具体观察者 code 抽象目标类： import java.util.*;public abstract class Subject&#123; protected ArrayList observers = new ArrayList(); public abstract void attach(Observer observer); public abstract void detach(Observer observer); public abstract void notify();&#125; 具体目标类： public class ConcreteSubject extends Subject&#123; public void attach(Observer observer) &#123; observers.add(observer); &#125; public void detach(Observer observer) &#123; observers.remove(observer); &#125; public void notify() &#123; for(Object obs:observers) &#123; ((Observer)obs).update(); &#125; &#125;&#125; 抽象观察者： public interface Observer&#123; public void update();&#125; 具体观察者： public class ConcreteObserver implements Observer&#123; public void update() &#123; //具体更新代码 &#125;&#125; client代码: Subject subject = new ConcreteSubject();Observer observer = new ConcreteObserver();subject.attach(observer);subject.notify(); 示例 猫是老鼠和狗的观察目标，老鼠和狗是观察者，猫叫，则老鼠跑，狗也跟着叫 优点 优点： 实现了表示层和数据逻辑层的分离 实现了Observer和Subject的解耦 支持广播通信 符合“开闭原则” 缺点 通知到所有直接和间接的Observer，耗时较大 Observer和Subject可能发生循环依赖 Mediator 中介者模式(Mediator Pattern): 用一个中介对象来封装一系列的对象交互,中介者使各对象不需要显式地相互引用,从而使其松耦合,而且可以独立地改变它们之间的交互 又称为调停者模式 是一种对象行为型模式 模式结构 Medicator ConcreteMedicator Colleague ConcreteColleague code 抽象中介者类代码: public abstract class Mediator&#123; protected ArrayList colleagues; public void register(Colleague colleague) &#123; colleagues.add(colleague); &#125;&#125; 具体中介者类代码: public class ConcreteMediator extends Mediator&#123; public void operation() &#123; ...... ((Colleague)(colleagues.get(0))).method1(); ...... &#125;&#125; 抽象同事类代码： public abstract class Colleague&#123; protected Mediator mediator; public Colleague(Mediator mediator) &#123; this.mediator=mediator; &#125; public abstract void method1();&#125; public class ConcreteColleague extends Colleague&#123; public ConcreteColleague(Mediator mediator) &#123; super(mediator); &#125; public void method1() &#123; ...... &#125; public void method2() &#123; mediator.operation1(); &#125; &#125; 优点 中介者模式可以使对象之间的关系数量大大减少： 缺点 具体中介者类可能会非常复杂，事实上这就是一个God Class Template Method 模板方法模式：定义一个操作中算法的骨架,而将一些步骤延迟到子类中,模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤 是一种类行为型模式 模板方法模式是基于继承的代码复用基本技术 只有继承，没有关联 在模板方法模式中,可以将相同的代码放在父类中,而将不同的方法实现放在不同的子类中 具体子类的基本方法将覆盖父类中定义的基本方法, 子类的钩子方法也将覆盖父类的钩子方法, 从而可以通过在子类中实现的钩子方法对父类方法的执行进行约束, 实现子类对父类行为的反向控制 模板方法模式包含两个角色: 模板方法： 定义在抽象类中，是一系列基本方法的组合 基本方法： 实现算法某个具体步骤， 可分为： 抽象方法 具体方法 钩子方法 模式结构 AbstractClass ConcreteClass code 钩子方法： public void template()&#123; open(); display(); if(isPrint())//根据钩子方法的返回值来执行逻辑 &#123; print(); &#125;&#125;//不同的子类可以自定义这个函数的返回值，从而实现对template方法的反向控制public boolean isPrint()&#123; return true;&#125; 抽象类代码: public abstract class AbstractClass&#123; public void templateMethod() //模板方法，是一系列基本方法的组合 &#123; primitiveOperation1(); primitiveOperation2(); primitiveOperation3(); &#125; public void primitiveOperation1() //基本方法—具体方法 &#123; //实现代码 &#125; public abstract void primitiveOperation2(); //基本方法—抽象方法，在子类中实现 public void primitiveOperation3() //基本方法—钩子方法，由子类来定义其逻辑 &#123; &#125;&#125; 子类代码： public class ConcreteClass extends AbstractClass&#123; public void primitiveOperation2() &#123; //实现代码 &#125; public void primitiveOperation3() &#123; //实现代码 &#125;&#125; 示例 在银行办理业务( process() )时，一般都包含几个基本步骤： 首先需要取号排队: takeNumber() 然后办理具体业务：transact() 最后需要对银行工作人员进行评分evaluate() 无论具体业务是取款、存款还是转账，其基本流程都一样, 因此模板方法是process()，基本方法是takeNumber() + transact() + evaluate() 优点 模板方法模式在一个类中抽象地定义算法，而由它的子类实现细节的处理 代码复用 模板方法模式可以实现反向控制 通过钩子方法 通过对修改子类来修改行为，符合“开闭原则” 缺点 每个不同的实现都需要定义一个子类，这会导致类的个数增加，系统更加庞大，设计也更加抽象 但是也更加符合“单一职责原则”，类的内聚性更高 好莱坞原则 “不要给我们打电话， 我们会给你打电话(Don 't call us, we’11 call you) 好莱坞原则(Hollywood Principle)： 子类不显式调用父类的方法，而是通过覆盖父类的方法来实现某些具体的业务逻辑，父类控制对子类的调用 在模板方法模式中，好莱坞原则体现在:通过父类来调用子类， 子类不需要调用父类。 将某些步骤的实现写在子类中，由父类来控制整个过程","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://lyk-love.cn/categories/Software-Engineering/"}],"tags":[]},{"title":"Application Layer","slug":"Computer Networking-Application Layer","date":"2022-09-26T06:39:34.927Z","updated":"2022-09-26T06:39:34.927Z","comments":true,"path":"2022/09/26/Computer Networking-Application Layer/","link":"","permalink":"http://lyk-love.cn/2022/09/26/Computer%20Networking-Application%20Layer/","excerpt":"Outline: DNS P2P 文件传送协议 TELNET 万维网 Email DHCP","text":"Outline: DNS P2P 文件传送协议 TELNET 万维网 Email DHCP 域名系统DNS port：53 概述 把互联网上的主机名字转换为IP地址 DNS被设计为一个联机分布式数据库系统， 并采用CS模式。 DNS使大部分名字都在本地进行解析（resolve）, 仅少量解析需要在互联网上通信。 由于是分布式系统，单个计算机的故障不会妨碍整个DNS系统的运行 解析是通过域名服务器程序， 而运行它的机器称为域名服务器 解析过程 当某一个应用进程需要把主机名解析为IP地址时， 它就调用解析程序（ resolver）， 并成为DNS的一个客户， 把带解析的域名放在DNS请求报文中， 以UDP用户数据报的方式发给本地域名服务器( 使用UDP是为了减小开销 ) 本地域名服务器在查找域名(递归, 迭代, 详见下文) 后, 把对应的IP地址放在回答报文中返回 应用进程获得目的主机的IP地址后即可进行通讯 其他服务 主机别名 原名称为“规范主机名” 邮件服务器别名 负载分配：将一个IP地址集合（即服务器集合）映射到一个规范主机名。 每次client向集合中的一个元素请求时，DNS以整个集合进行响应（每次返回集合中的一个元素，而整个集合的次序在不断变化，这样就实现了负载分配） 互联网的域名结构 任何连接在互联网的主机和路由器, 都有一个唯一的层次结构的名字, 即域名(Domain name ), 域是一个可被管理的划分. 域可以被划分为子域, 再划分为子域的子域.... 每个域名都由一个 label 序列组成，各label间用 *.*隔开 如： 三级域名 . 二级域名 . 顶级域名 不区分大小写 只是个逻辑概念 域名服务器 一个服务器管辖的范围叫做区(zone). 每个区内的所有节点是连通的. 每个区设置相应的权限域名服务器( authoritative name server ), 用来保存该区中所有主机的域名到IP地址的映射 DNS查询报文用UDP DNS服务器的管辖范围以区为单位, 区小于等于域, 是域的子集 比如域名abc.com可以只设一个区abc.com, 这样，区和域就是一回事（如左图）； 但域名abc.com也可以划分两个区：abc.com 和 y.abc.com 这两个区都隶属于域 abc.com ，都各自设置了相应的权限域名服务器（如又图） DNS服务器层次 DNS服务器按层次安排, 分四种 根域名服务器( root name servevr ) 最高层次 直到所有的顶级域名服务器的域名和IP地址 本地域名服务器自己若无法解析,则首先求助于根域名服务器 **任播(anycast)**技术: 找到离DNS客户最近的一个根域名服务器 顶级域名服务器( top level domain name server TLD服务器 ) 权限域名服务器 负责一个区的域名服务器。当它不能给出最后的查询回答时，就会告诉发出查询请求的DNS客户，下一步应该找哪一个权限域名服务器 本地域名服务器（ local name server ） ( 默认域名服务器 ) 当一个主机发出DNS查询请求时，这个查询请求报文就发给LNS 也称为“默认域名服务器” 当所要查询的主机也属于同一个本地ISP时， 该本地域名服务器就能立即将所查询的主机名转换为IP地址 域名解析过程： 主机向本地域名服务器递归查询（ recursive query ） 本地域名服务器以DNS客户的身份，向其它根域名服务器发送查询报文 因此递归查询的返回结果要么是要查的IP地址，要么是报错，表示没查到 本地域名服务器向根域名服务器迭代查询（iterative query） 让LNS去查，根域名服务器通常把自己知道的顶级域名服务器的IP地址告诉LNS,让LNS再向顶级域名服务器查询。 顶级域名服务器收到LNS的查询请求后，要么给出所查询的IP地址，要么告诉LNS下一步应当向哪个权限域名服务器进行查询 DNS缓存 DNS广泛使用高速缓存 为维护缓存中的内容正确，还应给每项内容设一个计时器 主机也需要（DNS的）高速缓存，主机启动时从LNS中下载名字和地址的全部数据库，并且维护（DNS）存放自己最近使用的域名的高速缓存 DNS记录和报文 资源记录Resource Record: 所有DNS服务器共同实现了DNS分布式数据库，其条目就是资源记录 (Name, Value,Type,TTL) Name和Value值取决于Type: Type = A: Name = 主机名， Value = 主机名对应的IP地址 Type = NS：Name = 域名，Value = 知道如何获得该域中IP地址的权威DNS服务器的主机名 与Type=A记录结合，将权限DNS的主机名进一步映射到权限域名服务器的IP地址，方便迭代查询 Type=CNAME: Value = 别名为Name的主机的规范主机名 Type = MX： Value = 别名为Name的邮件服务器的规范主机名 使用MX记录使得邮件服务器可以和其他服务器使用相同的别名，DNS client得到别名后，只需请求一条MX记录，就能得到规范主机名 权威域名服务器包含其区内主机的Type A记录，非权威域名服务器包含的是包含主机名的域的Type = NS记录和Type=A记录，后者提供了NS记录对应的权限域名服务器的IP地址 例子：假如edu TLD服务器不是主机gaia.cs.umass.edu的权限域名服务器，则该服务器将包含： 一条包括 主机cs.umass.edu的域记录，如（umass.edu, dns.umass.edu, NS） 一条A记录，与NS记录配套： (dns.umass.edu, 128.119.40.111,A) 这样就实现了迭代查询 Tool： nslookup DNS报文： 略 向DNS数据库中插入记录： 要到注册登记机构register ,它负责验证域名的唯一性，将该域名输入DNS DataBase. 步骤为： 假设要开设网站，注册域名 LYK-love.cn,你需要提交权威DNS服务器的域名和地址， 假设：权威DNS服务器名为dns.LYK-love.cn, 其IP地址为212.212.212.1 对每个权威DNS。该register确保将一个NS和A记录输入TLD cn服务器： ( LYK-love.cn, dns.LYK-love.cn, NS )( dns.LYK-love.cn, 212.212.212.1,A ) 还可以输入MX记录，与网站使用相同的别名 ( LYK-love.cn, mail.LYK-love.cn, MX ) //别名使用LYK-love.cn P2P应用 CS体系依赖于服务器，P2P减少了对中心化的依赖 P2P文件分发 考虑一个应用，它从单一server向大量client（称为对等方peer）分发一个文件 CS：server负担大 P2P: 每个对等方能重新分发它所拥有的该文件的任何部分 最流行的P2P文件共享协议：BitTorrent P2P体系的扩展性 定义： $u_s$: server接入链路的上载速率 $u_i$:第$i$对等方接入链路的上载速率 $d_i$: 第$i$对等方接入链路的下载速率 $F$: 被分发的文件大小（bit） $N$: 要获得该文件副本的对等方的数量 $D$: 分发时间, 所有 $N$ 个对等方得到该文件的副本所需的时间 假设网络具有足够的带宽 CS模式的分发时间(下界)： $$ D_{cs} \\geq \\max{\\frac{NF}{u_s}, \\frac{F}{d_{\\min}} } $$ server必须向 $N$个对等方的每个传输该文件的一个副本。因此server必须传输$NF$ bit， 因为其上载速率是u_s, 分发时间必定至少为$\\frac{NF}{u_s}$ 令$d_{min}$表示具有最小下载速率的对等方的下载速率，后者获得该文件的 所有F bit的时间最少为$ \\frac{F}{d_{\\min}}$ P2P模式的分发时间（下界）： $$ D_{P2P} \\geq \\max{ \\frac{F}{u_s}, \\frac{F}{d_{\\min}}, \\frac{NF}{u_s + \\sum\\limits_{i=1}^{N}{u_i} } } $$ 这里只是最小分发时间的简单表示式 server必须向至少发送该文件的每个bit一次， $\\frac{F}{u_s}$ $ \\frac{F}{d_{\\min}}$与CS模式相同 系统整体上载能力 $u_{total} = u_s + u_1 + u_2 + \\dots + u_N$, 系统必须向这$N$个对等方的每个上载$F$比特， 因此总共上载$NF$ bit, 这不能以快于$u_{total}$的速率完成，因此，分发时间也至少是$\\frac{NF}{u_s + \\sum\\limits_{i=1}^{N}{u_i} }$ 比较两种模式的最小分发时间关于 $N$的函数，发现P2P的最小分发时间更小 BitTorrent 洪流 torrent: 参与一个特定文件分发的所有对等方的集合 块 chunk：在一个洪流中的对等方彼此下载等长度的文件块 256KB 任何对等方可能在任何时候加入或离开洪流 追踪器 tracker: 每个洪流一个，用于追踪洪流中的对等方 每当对等方加入一个洪流，就向其追踪器register自己，并周期性通知追踪器自己仍在该洪流中 workflow： Alice 加入洪流，追踪器随机从对等方的集合中选一个子集（e.g. 50 个 ），并将它们的IP地址发给Alice Alice持有这张列表，试图与表上所有对等方建立TCP连接 所有与Alice成功建立连接的对等方称为“邻近对等方”（ e.g. $L$个 ） 由于用户可随时加入、离开洪流，因此邻近对等方集合是动态的 在任何给定的时间，每个对等方将具有来自该文件的的块子集，Alice周期性地询问每个邻近对等方它们所具有的块列表，得到$L$个块列表，然后对她还没有的块发出请求 请求哪些块？ 最稀缺优先（rarest first）： （在Alice没有的块中）请求在邻居中副本数量最少的块 向哪些向她请求的块中发送？ “一报还一报”（tit for tat）: Alice 根据当前能够以最高速率向她提供数据的邻居，给出其优先权 “一报还一报”被证明可以被回避 分布式散列表 分布式散列表( Distributed Hash Table, DHT )： 一种分布式数据库，每个对等方仅保持总体数据库的一个子集 其条目是(key - value) pair 为每个对等方分配 $n$ 位的标识符和key 值域: $[0,2^{n}-1]$ 对于不是整数的key，用一个散列函数映射到该区间， 以后我们提起key，指的是它的散列值 最邻近后继： 将最邻近对等方定义为key的最邻近后继， key就放在其最邻近后继上 环形DHT 假设$n=4$, 区间为$[0,15]$, 因为对等方12是键11最邻近的后继， 因此将(11, Johny)存储在对等方12上 查询key时，如何确定最邻近对等方？ 需要特殊的对等方拓扑结构： 所有对等方相连： 这样每个对等方，每个查询仅需一个报文，但这样的系统难以维护 环形DHT： 如图，平均发送$\\frac N 2$ 个报文 “捷径对等方”：在环形基础上增加边 普遍采用，每个请求的报文数量能被优化到 $O(logN)$ 对等方扰动 P2P系统中，对等方可以自由加入、退出，上述的拓扑结构会被破坏 措施： 每个对等方联系其$n$个后继（和捷径对等方），这样一个节点消失或增加时，该链表能自己调整 文件传送协议 FTP概述 FTP( File Transfer Protocal )： 文件传送协议 文件共享协议的两大类： 复制整个文件， 特点是： 若要存取一个文件， 就要先获得一个本地的文件副本。 若要修改文件， 只能对文件的副本进行修改， 然后再将修改后的文件副本传回到原节点 两种： FTP: 基于TCP TFTP: 基于UDP的简单文件传送协议 联机访问 允许多个程序同时对一个系统进行存取 由操作系统提供对远地共享文件进行访问的服务（ 不需要调用特殊的进程 ）， 就如同对本地文件的访问一样 用户可以用远地文件作为输入和输出来运行任何应用程序， 而操作系统中的文件系统则提供对共享文件的透明存取， 其优点是： 将原来用于处理本地文件的应用程序用来处理远地文件时， 不需要对应用程序做明显的改动 类似云计算机 例子： 网络文件系统NFS（ Network File System ） FTP的基本工作原理 CS模式， 一个FTP服务器进程可以同时为多个客户进程提供服务 一个FTP服务器进程由两部分组成： 主进程：负责接收新的请求 若干个从属进程：负责处理单个请求 主进程的工作步骤： 打开熟知端口（21），使客户进程能连接上 等待客户进程发出连接请求 启动从属进程处理client process发来的请求。 从属进程对 client process 的请求处理完毕后即终止，但从属进程在运行期间还可能创建一些其他的子进程 回到等待状态，继续接受其他客户进程发来的请求。主进程与从属进程的处理是并发进行的 并发： 在一个芯片上时分复用 FTP工作步骤： 服务器有控制进程。整个会话期间，客户和服务器一直保持控制连接 当客户进程向服务器进程发出建立连接请求时，要寻找连接服务器进程的熟知端口21，同时告诉服务器进程自己的另一个端口号码，用于建立“数据连接”。 客户发送的传送请求，通过控制连接发送给服务器的控制进程 控制进程在接收到文件传输请求后就创建“数据传送进程”和“数据连接”。数据传送进程实际完成文件的传送（通过数据连接），在传送完成后关闭“数据连接”。 服务器进程用自己传输数据的熟知端口20与客户进程所提供的端口号建立数据传送连接 由于FTP将控制连接与数据连接分离，因此FTP的控制信息是带外( out of band )传送的. 简单文件传送协议TFTP 用于UDP 所占内存较小 远程终端协议TELNET 用户用TELNET就能通过TCP连接登录到远地的另一台主机（使用主机名或IP地址），这种服务是透明的， 因此TELNET也称为终端仿真协议 为适应硬件和OS的差异，TELNET定义了数据和命令在互联网中的传输格式，即网络虚拟终端NVT( Network Virtual Terminal )，数据在传输时（C To S ， S To C）都被转为NVT格式 万维网WWW 万维网（World Wide Web）并非一个特殊的计算机网络，而是一个大规模的、联机式的信息储藏所。 万维网用链接的方法从互联网的一个站点访问另一个站点 万维网是个分布式的超媒体（hypermedia）系统，它是超文本（hypertext）系统的扩充 超文本： 包含指向其它文档的链接的text，即，超文本由多个信息源链接成 超媒体：超文本文档只能包含文本信息，超媒体文档还能包含其他表示方式的信息，如图形、声音... 万维网以CS模式工作， 浏览器就是在用户主机上的万维网客户程序。 万维网文档所驻留的主机则运行服务器程序，该主机也称为万维网服务器。在一个客户程序主窗口上显示出的万维网文档称为页面（page） 统一资源定位符URL 对资源的位置提供了抽象的识别方法，并用它来给资源定位 &lt;协议&gt;://&lt;主机&gt;:&lt;端口&gt;/&lt;路径&gt; 端口和路径可以省略。 省略“路径”，则URL指到主页 有些浏览器可以把&quot;http://&quot;和主机名最前面的&quot;www&quot;省略，当然浏览器会自动把它们添上 超文本传送协议HTTP HTTP概述 面向事务的应用层协议，使用TCP HTTP本身是无连接的，即无需事先建立HTTP连接 HTTP是无状态的，不记得曾经的客户，也不记得客户访问了多少次 默认端口80 请求一个万维网文档所需的时间 = 该文档的传输时间 + 两倍RTT 一个RTT用于建立TCP连接，一个RTT用于请求和接收万维网文档 TCP三报文握手的第三个报文段中的数据，就是客户对万维网文档的请求报文。 服务器收到HTTP请求报文后，就把所请求的文档作为响应报文返回给客户 为避免两倍RTT开销，HTTP/1.1使用了持续连接，它有两种工作方式： 非流水线模式( without pipeling ): 客户收到前一个响应后才能发出下一个请求. 因此，在TCP连接已建立后访问一次对象就要用去一个RTT 流水线模式（ with pipelining ）： 客户在收到HTTP的响应报文之前就能够接着发送新的请求报文 代理服务器 又称“万维网高速缓存 client要向互联网上的server发送请求时，就先和proxy server建立TCP连接，并向其发送HTTP报文。若proxy server没找到所请求的对象，则由proxy server代表client与互联网上的源点服务器（origin server）建立TCP连接，并发送HTTP报文 HTTP报文结构 ref: https://blog.csdn.net/zephyr999/article/details/80055420 开始行 在请求报文中称为“请求行”request line 方法 + url + HTTP版本 在响应报文中称为“状态行”status line 服务器HTTP协议版本，响应状态码，状态码的文本描述 首部行（请求头部header） 首部行后空一行 实体主体entity body 该字段可能缺失 请求报文 请求行由三部分组成：请求方法，请求URL（不包括域名），HTTP协议版本 请求方法比较多：GET、POST、HEAD、PUT、DELETE、OPTIONS、TRACE、CONNECT 最常用的是GET和POST 首部行（请求头部， 报文头）：由 key/value 对组成，每行为一对，key 和 value 之间通过冒号(:)分割。请求头的作用主要用于通知服务端有关于客户端的请求信息 User-Agent：生成请求的浏览器类型 Accept：客户端可识别的响应内容类型列表；星号* 用于按范围将类型分组。*/*表示可接受全部类型，type/*表示可接受 type 类型的所有子类型。 Accept-Language: 客户端可接受的自然语言 Accept-Encoding: 客户端可接受的编码压缩格式 Accept-Charset： 可接受的字符集 Host: 请求的主机名，允许多个域名绑定同一 IP 地址 connection：连接方式（close 或 keepalive） Cookie: 存储在客户端的扩展字段 Content-Type:标识请求内容的类型 Content-Length:标识请求内容的长度 请求体（报文体）: 主要用于 POST 请求，与 POST 请求方法配套的请求头字段一般有 Content-Type和 Content-Length 常见的Content-Type： Content-Type 解释 text/html html格式 text/plain 纯文本格式 text/css CSS格式 text/javascript js格式 image/gif gif图片格式 image/jpeg jpg图片格式 image/png png图片格式 application/x-www-form-urlencoded POST专用：普通的表单提交默认是通过这种方式。form表单数据被编码为key/value格式发送到服务器。 application/json POST专用：用来告诉服务端消息主体是序列化后的 JSON 字符串 text/xml POST专用：发送xml数据 multipart/form-data POST专用：下面讲解 multipart/form-data 用以支持向服务器发送二进制数据，以便可以在 POST 请求中实现文件上传等功能 响应报文 由状态行、响应头、空行、响应内容四部分组成 状态行 状态行也由三部分组成：服务器HTTP协议版本，响应状态码，状态码的文本描述 格式：HTTP-Version Status-Code Reason-Phrase CRLF 比如：HTTP/1.1 200 OK 响应头： Location：服务器返回给客户端，用于重定向到新的位置 Server： 包含服务器用来处理请求的软件信息及版本信息Vary：标识不可缓存的请求头列表 Connection: 连接方式， close 是告诉服务端，断开连接，不用等待后续的请求了。 keep-alive 则是告诉服务端，在完成本次请求的响应后，保持连接 Keep-Alive: 300，期望服务端保持连接多长时间（秒） 空行：(CR or LF )， 位于响应头和响应内容之间 响应内容：服务端返回给请求端的文本信息 状态码 1XX：服务器已接收了客户端的请求，客户端可以继续发送请求 2XX：服务器已成功接收到请求并进行处理 200：OK 202：No Content 206：Partial Content 3XX：服务器要求客户端重定向， 这表明浏览器需要执行某些特殊的处理以正确处理请求。 301：Moved Permanently 永久性重定向。 该状态码表示请求的资源已被分配了新的 URI， 以后应使用资源现在所指的 URI。 302：Found 临时性重定向。 该状态码表示请求的资源已被分配了新的 URI， 希望用户（本次） 能使用新的 URI 访问。 303：See Other， 表示由于请求对应的资源存在着另一个URI，应使用GET方法定向获取请求的资源。303状态码明确表示客户端应当采用GET方法获取资源，这点与302状态码有区别 当301、302、303响应状态码返回时，几乎所有的浏览器都会把POST改成GET，并删除请求报文内的主体，之后请求会自动再次发送 304：Not Modified， 请求的资源没有修改过, 304 虽然被划分在 3XX 类别中， 但是和重定向没有关系 307：Temporary Redirect， 与 302 Found 有着相同的含义，但浏览器不会把307从POST变成GET 4XX 客户端错误 400 Bad Request：请求报文中存在语法错误； 401 Unauthorized：该状态码表示发送的请求需要有通过 HTTP 认证（BASIC 认证、DIGEST 认证） 的认证信息。 403 Forbidden：请求的资源被服务器拒绝； 404 Not Found：服务器上无法找到资源； 5XX服务器错误 5XX的响应结果表明服务器本身发生错误 500 Internal Server Error：服务器端在执行请求时发生了错误 502 网关错误 503 Service Unavailable ：服务器暂时处于超负载或正在进行停机维护， 现在无法处理请求 Cookie 为了避免HTTP无状态带来的不便，万维网站点可以用Cookie来跟踪用户 Cookie： 在HTTP server 和 client间传递的状态信息 Cookie工作步骤： 当用户A浏览某个使用Cookie的网站时，该网站的服务器就为A产生一个唯一的识别码， 并以此为索引在后端数据库建立一个项目。接着在给A的HTTP响应报文中添加一个叫做Set Cookie的首部行，其字段名为“Set Cookie”，值为识别码，如： Set-Cookie： 2134vsfva32ddf432 当A收到这个响应时，其浏览器就在它管理的特点Cookie文件中添加一行，其中包括这个识别码和server的主机名。当A继续浏览这个网站时，每发送一个HTTP请求报文，其浏览器就会从其Cookie文件中取出这个网站的识别码，放到HTTP请求报文的首部行中： Set-Cookie： 2134vsfva32ddf432 于是，网站就能够跟踪用户2134vsfva32ddf432（用户A） Web cache Web cache: 也称代理服务器（ proxy server ）, 被配置了 proxy server的浏览器的请求都会被定向到该proxy server proxy server会查询缓存，未命中则请求web server，将后者的内容缓存，并发给浏览器 递归式查询 proxy server既是server（对于浏览器）也是clinet（对于web server） 应用：内容分发网络（ Content Distribution Network, CDN ） Conditional GET proxy cache带来“陈旧缓存”问题，为此proxy cache可以向web server发送Conditional GET 具体而言， server每次都会向 proxy server发送 Last-Modified-Since：date字段。 一段时间后，当用户请求代理服务器查询该资源时， proxy server会对web server发送If-Modified-Since: date 字段 如果未修改，则只返回一个响应报文，不用返回对象（ 状态码304 Not Modified ） 万维网的文档 静态文档 文档放在服务器中，在用户浏览过程中，内容不会改变 动态文档 文档是在client访问server时才由应用程序动态创建 当请求到达时，server要运行另一个应用程序。server把client发来的数据交给这个进程，且server能解释这个进程的输出，以及这个输出结果该如何使用，这就需要通用网关接口CGI( Common Gateway Interface )， 它既指CGI标准，也指程序 活动文档 动态文档一旦生成，内容就固定了，无法及时刷新屏幕。为了屏幕的及时更新，有两种技术： 服务器推送（server push）: 将所有工作交给服务器，服务器不断运行与动态文档相关联的应用程序，定时更新信息，并发送更新过的文档 server要为每个client维持一个不释放的TCP连接 活动文档（active document）: 所有工作交给浏览器端。服务器返回一个程序，它在浏览器端运行 Java applet 服务器端的活动文档内容是不变的，从传送的角度看，两种技术都把活动文档看成静态文档 万维网的信息检索系统 搜索引擎： 万维网中用来进行搜索的工具。 分为全文检索和分类目录两种。现在还出现了垂直搜索引擎和元搜索引擎， 全文检索：爬虫，建立索引，从已建立的索引数据库中查询 分类目录：不主动采集网站的信息，由网站向搜索引擎主动提交信息，经人工审核编辑后，输入到分类目录的数据库中。 查询时不需要关键词，只需按照分类。 垂直搜索引擎：也是关键词搜索，但只针对某一领域、某一人群等 元搜索引擎：把请求发给多个搜索引擎，再把结果集中处理 电子邮件 电子邮件系统三要素： 用户代理、邮件服务器、邮件发送协议(SMTP)和邮件读取协议（POP3） 用户代理（UA）:就是电子邮件客户端软件 邮件发送协议：用于UA向邮件服务器发送邮件或在邮件服务器之间发送邮件 邮件读取协议： UA从邮件服务器读取邮件 SMTP, POP3, IMAP都用TCP 用户在浏览器中浏览信息需要HTTP. 因此浏览器和邮件服务器之间传送邮件时，用HTTP. 而各邮件服务器之间传送邮件时,仍然使用SMTP Email发送和接收步骤： 用户发送邮件，把工作全部交给UA. 后者把邮件用SMTP协议发给发送方邮件服务器 此时UA充当SMTP客户，发送方邮件服务器充当SMTP服务器 SMTP服务器（即发送方邮件服务器）收到邮件后，将其暂放在邮件缓存队列中 发送方邮件服务器与接收方邮件服务器建立TCP连接（不会中转），然后依次把邮件缓存队列的邮件发出去 接收方邮件服务器中的SMTP服务器进程收到邮件后，把邮件放到收件人的信箱中 收件人打算收信时，运行UA，使用POP3(or IMAP)协议拉取邮件 有些“”特快专递“服务能够让UA直接用SMTP发给接收方邮件服务器（ 不用发给发送方邮件服务器了 ） Email由信封和内容组成 信封上最重要的就是收件人的地址，电子邮件地址格式： 用户名 @ 邮件服务器的域名 内容分为首部和主体，后者用户自己撰写 首部包括一些关键字，最重要的有 To： 收件人的邮件地址 Subject：主题 Cc：抄送，即留下一个复写副本 简单邮件传送协议SMTP 不使用中间邮件服务器 本用于传输ASCII码而不是二进制文件，后来虽然有了MIME可以传输二进制数据，但效率不高，为此有了Extended SMTP 邮件读取协议POP3 POP3: UA必须允许POP3 client ， 而收件人所连接的ISP的邮件服务器中则运行POP3 server，当然，它还要运行SMTP server以收信 POP3 server需要用户输入鉴别信息（ 用户名和口令 ） 流程： 特许authorization，事务处理， 更新 特许：UA以明文发送用户名和口令 主要命令： user &lt;username&gt; pass &lt;passwd&gt;( 现在一般要输入授权码而不是密码 ) 事务处理：UA可以对邮箱做一些操作，如list, retr, dele, quit quit仅仅给邮件打上删除标记，并没有删除邮件 更新：客户发出quit之后，结束该pop3 session,并删除那些被标记为删除的报文 POP3用户将邮件从服务器下载到本地 telnet pop.qq.com 110 //qq官网给的端口是995,但我用995是无法访问的，不知道为什么Trying 120.241.186.196...Connected to pop.qq.com.Escape character is &#x27;^]&#x27;.+OK XMail POP3 Server v1.0 Service Ready(XMail v1.0)user 邮箱名+OKpass 授权吗+OK 邮件读取协议IMAP IMAP:用户在自己的计算机上就可以操纵邮件服务器上的邮箱，就像在本地操纵一样 允许UA只获取邮件的一部分 基于 web的电子邮件 UA就是浏览器 通用互联网邮件扩充MIME 新增了5个首部 定义了许多邮件内容的格式，对多媒体邮件的表示方法进行了标准化 定义了传送编码 动态主机配置协议DHCP DHCP步骤： 需要IP地址的主机在启动时就广播发送DHCP发现报文（DHCPDISCOVER） 广播是因为此时不知道DHCP服务器在哪 目的地址是全1 源地址是全0，因为此时主机没有IP地址 本地网络上所有主机都能收到这个报文，但只有DHCP服务器才能对它回答。（返回报文称为”提供报文“） DHCP服务器先在其数据库中查找该计算机的配置信息，若找到，则返回找到的信息；若没找到，则从其地址池（address pool）中取一个地址分配给主机 为每个网络都设置DHCP服务器代价太高，解决方案是每个网络至少有一个DHCP中继代理， 它配置了DHCP服务器的IP地址等信息 当中继代理收到主机的DHCP广播发现报文后，就以单播方式向DHCP服务器转发此报文，收到提供报文后，中继代理再把此提供报文发给主机 DHCP分配的地址是暂时的，称为“租用期” DHCO报文采用UDP 希望我更新的话，请（以各种手段）催更我哦q(≧▽≦q)","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"Computer Networking","slug":"Computer-Networking","permalink":"http://lyk-love.cn/tags/Computer-Networking/"}]},{"title":"C++ 命名空间","slug":"C++ 命名空间","date":"2022-09-26T06:39:34.925Z","updated":"2022-09-26T06:39:34.925Z","comments":true,"path":"2022/09/26/C++ 命名空间/","link":"","permalink":"http://lyk-love.cn/2022/09/26/C++%20%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4/","excerpt":"Outline: 命名空间的定义 使用命名空间成员 类、命名空间与作用域 重载与命名空间","text":"Outline: 命名空间的定义 使用命名空间成员 类、命名空间与作用域 重载与命名空间 命名空间的定义 namespace + 命名空间名字。随后是一系列由花括号后才的声明和定义 namespace xxx&#123; class Sales+data &#123; ... &#125;; namespace yyy&#123;&#125;&#125; //无须分号 每个命名空间都是一个作用域 命名空间中的名字可以被该空间内的其他成员直接访问，也可以被内嵌作用域中的任何单位访问 空间外的代码必须明确指出所用的名字属于哪个命名空间 命名空间可以不连续 namespace nsp&#123; ...&#125; // 如果nsp已存在，这个定义会打开已存在的命名空间并为其添加新的声明 通常不把#include放在命名空间内部 否则就会把头文件中所有的名字定义成该空间的成员 全局命名空间是隐式的，作用域运算符可以作用于全局作用域的成员，但是没用 ::member_name 内联命名空间： 其中的名字可以被外层命名空间直接使用 inline必须写在命名空间第一次定义的地方，后续地方可以不写 inline namespace FifthEd&#123; ...&#125; 未命名的命名空间（ unnamed namespace ）：其中的变量拥有静态生命周期。 未命名的命名空间可以在给定文件内部连续，但不能跨文件。 如果一个头文件包含了未命名的命名空间，则该命名空间中定义的名字将在每个包含了该头文件的文件中对应不同实体 未命名的命名空间中的名字可直接使用，也不能对其使用作用域运算符 未命名的命名空间中的名字的作用域与该命名空间所在的作用域相同。这意味着如果未命名的命名空间定义在文件的最外层作用域中，则该命名空间中的名字一定要与全局作用域中的名字有区别 int i;namesapce&#123;int i; //二义性&#125; 别名：namespace primer = cpluscplus_primer; 不能再命名空间名字还没定义时就定义别名 using声明：一次只引入命名空间的一个成员。作用域从其声明的地方开始，一直到using声明所在的作用域结束为止 using指示：using namespace xxx;所有名字都可见 头文件如果在其顶层作用域中含有using指示和 using声明，则会将名字注入到所有包含了该文件的头文件中 使用命名空间成员 类、命名空间与作用域 重载与命名空间","categories":[{"name":"Language","slug":"Language","permalink":"http://lyk-love.cn/categories/Language/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://lyk-love.cn/tags/C/"}]},{"title":"C++拷贝控制","slug":"C++拷贝控制","date":"2022-09-26T06:39:34.925Z","updated":"2022-09-26T06:39:34.925Z","comments":true,"path":"2022/09/26/C++拷贝控制/","link":"","permalink":"http://lyk-love.cn/2022/09/26/C++%E6%8B%B7%E8%B4%9D%E6%8E%A7%E5%88%B6/","excerpt":"Outline: 对象移动 //TODO","text":"Outline: 对象移动 //TODO 对象移动 右值引用 左值引用绑定到返回左值的表达式 右值引用或const的左值引用绑定到返回右值的表达式 右值要么是字面常量，要么是临时对象。右值引用只能绑定到右值，所以可以接管所引用对象的资源 所引用对象将要被销毁 该对象没有其它用户 变量是左值 不能把右值引用直接绑定到一个变量,即使这个变量是右值引用类型 int &amp;&amp;rr1 = 42;int &amp;&amp;rr2 = rr1; //错误: rr1是右值引用类型的变量,它是个左值 move()将左值转换为右值引用. int &amp;&amp; rr3 = std::move(rr1); 可以销毁移后源对象,可以赋予其新值,但不能使用移后源对象的值 对move不提供using声明, 直接调用std::move而非move 在头文件&lt;utility&gt;中 移动构造函数和移动赋值运算符 class X&#123; int a;public: X(int x) &#123; a = x; &#125; //委托构造 X() : x(42) &#123;&#125; &#125;;","categories":[{"name":"Language","slug":"Language","permalink":"http://lyk-love.cn/categories/Language/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://lyk-love.cn/tags/C/"}]},{"title":"C++ 泛型算法","slug":"C++ 泛型算法","date":"2022-09-26T06:39:34.925Z","updated":"2022-09-26T06:39:34.925Z","comments":true,"path":"2022/09/26/C++ 泛型算法/","link":"","permalink":"http://lyk-love.cn/2022/09/26/C++%20%E6%B3%9B%E5%9E%8B%E7%AE%97%E6%B3%95/","excerpt":"Outline： 概述 泛型算法概览 定制操作 再探迭代器 泛型算法结构 特定容器算法","text":"Outline： 概述 泛型算法概览 定制操作 再探迭代器 泛型算法结构 特定容器算法 概述 头文件&lt;algorithm&gt;, &lt;numeric&gt; 泛型算法： 对不同类型容器的通用算法 大多数情况下，泛型算法操作迭代器，而不是直接操作容器 泛型算法不依赖容器，但依赖于元素类型的操作，比如运算符重载 算法概览 只读算法 find accumulate 必须有合适的+运算符 string sum = accumulate(v.cbegin(), v.cend(), string(&quot;&quot;)); string sum = accumulate(v.cbegin(), v.cend(), &quot;&quot;);是错误的，因为const char*没有+运算符 equal 必须有合适的==运算符 写容器元素的算法 fill接受一对迭代器表示范围 fill(vec.cbegin(), vec.cend() + vec.size()/2, 10 ); ``fill_n( dest, n , val )` 接受一个迭代器来指出单独的目的位置 copy( begin, end, dest ) unique(begin, end) 重排输入范围，使得不重复的元素出现在容器的开始部分 返回指向不重复区域之后一个位置的迭代器， 若没有不重复区域，则返回尾后迭代器 与erase搭配使用，删除重复的元素 //按字典序排序words并删除重复单词void elimDups( vector&lt;string&gt; &amp;words )&#123; sort( words.begin(), words.end() ); auto end_unique = unique( words.begin(),words.end() ); words.erase( end_unique , words.end());&#125; 测试： int main()&#123; istream_iterator&lt;string&gt; in_iter(cin), eof; ostream_iterator&lt;string&gt;out_iter( cout, &quot; &quot; ); vector&lt;string&gt;vec(in_iter, eof); elimDups( vec ); copy(vec.begin(), vec.end(), out_iter); &#125; aaa bbb ddd ccc aaa //输入 aaa bbb ccc ddd //输出 算法假定： 很多算法操作两个序列， 它们接受接受第三个的迭代器来表示第二个序列的目标位置，这些算法都假定， 第二个序列至少与第一个序列一样长 如equal， copy 返回（递增后的）目的位置迭代器 一些算法接受一个单独的迭代器来指出一个单独的目的位置，这类算法不检查写操作，因此越界访问是undefined behavior 比如，不能在空容器上调用fill_n 可以用 fill_n(back_inserter(),10,0),每次通过此迭代器赋值时，赋值运算符被重载为调用push_back,这就不用担心越界访问 定制操作 谓词 定义： 返回值为bool的可调用对象 一元谓词： 只接受一个参数 二元谓词： 接受两个参数 可调用对象：可以对其使用调用运算符的对象或表达式 函数和函数指针 重载了函数调用运算符的类 lambda表达式 算法对接受的谓词有要求，为了绕过这个限制，可以使用lambda表达式 find_if() 接受一个一元谓词，但有时该谓词函数需要不止一个参数 lambda的应用 for_each(begin,end, callable)：接受一个可调用对象 再探迭代器 除了标准迭代器外，还有以下几种迭代器，头文件：&lt;iterator&gt; insert iterator stream iterator reverse iterator move iterator Insert Iterator 迭代器适配器, 接受一个容器，生成一个迭代器 调用容器操作向给定容器的指定位置插入一个元素 操作 解释 it = t 在it 指定的当前位置插入t. 假定 c是it绑定的容器，依赖于插入迭代器的不同种类， 此赋值会分别调用 push_back(t)，push_front(t)，insert(t,p)， 其中p为传递给inserter的迭代器位置 *it, ++it, it++ 空操作。 都返回it 迭代器适配器 功能 back_insert_iterator 创建一个使用push_back的迭代器（这意味着不会发生越界访问，容器大小永远足够），前提是提供有 push_back() 成员方法的容器（包括 vector、deque 和 list）。 front_insert_iterator 创建一个使用push_front的迭代器，前提是提供有 push_front() 成员方法的容器（包括 list、deque 和 forward_list）。 insert_iterator 在容器的指定位置之前插入新元素，前提是该容器必须提供有 insert() 成员方法。 当调用inserter(c, iter)时，得到一个迭代器，接下来使用它时，会将元素插入到iter原来所指向的元素之前的位置， 即，如果it是由inserter生成的迭代器，则： *it = val; 效果与下面的代码一样： it = c.inserter(it,val);//it指向新加入的元素++it; //递增it使它指向原来的元素 front_inserter 生成的迭代器与inserter生成的完全不同。当调用 front_inserter时，元素总是插入到容器第一个元素之前： list&lt;int&gt; lst = &#123;1,2,3,4&#125;;list&lt;int&gt; lst2, lst3 ; // empty listcopy(lst.cbegin(), lst.cend(), front_inserter(lst2)); //拷贝完成后， lst2包含4,3,2,1copy(lst.cbegin(), lst.cend(), inserter(lst3, lst3.begin()));//拷贝完成后， lst3包含1,2,3,4 当调用push_front(c)时，得到一个插入迭代器，接下来会调用push_front iostream Insrator itstream不是容器，但STL定义了可以用于这些IO类型对象的迭代器。istream_iterator读取输入流， ostream_iterator向一个输出流写数据。 这些迭代其将它们对应的流当作一个特定类型的元素序列来处理。 通过使用流迭代器，我们可以使用泛型算法从流对象读取数据以及向其写入数据。 可以为任何定义了&gt;&gt;和&lt;&lt;运算符的类型创建istream_iterator和ostream_iterator istream_iterator操作 当创建流迭代器时，必须指定迭代器将要读写的对象类型。 一个istream_iterator使用&gt;&gt;来读取流。 因此， istream_iterator要读取的类型必须定义了输入运算符。 当创建一个istream_iterator时， 我们可以将它绑定到一个流。 当然，我们还可以默认初始化迭代器，这样就创建了一个尾后迭代器 vector&lt;int&gt; vec;istream_iterator&lt;int&gt; in_iter(cin); //从cin读取intistream_iterator&lt;int&gt; eof; //istream尾后迭代器while( in_iter != eof )&#123; vec.push_back(*in_iter++); //先后缀递增，返回迭代器的旧值。 再对旧值接引用，得到从流读取的前一个值，即原来指向的值&#125; 该程序可以改写为： istream_iterator&lt;int&gt; in_iter(cin) , eof; //从cin读取intvector&lt;int&gt; vec(in_iter, eof); 可以用一对表示元素范围的迭代器构造vec 这两个迭代器是istream_iterator , 这意味着元素范围是通过从关联的流中读取数据获得的， 这个构造函数从cin中读取数据，直至遇到文件尾或者遇到一个不是int的数据位置。 使用算法操作流迭代器 算法使用迭代器，而流迭代器至少支持某些迭代器操作，因此至少可以用某些算法来操作流迭代器 #include&lt;numeric&gt;istream_iterator&lt;int&gt; in(cin), eof;cout &lt;&lt; accumulate(in, eof, 0) &lt;&lt; endl; istream_iterator允许使用懒惰求值 当istream_iterator绑定到一个流时，标准库并不保证迭代器立即从流中读取数据。 具体实现可以直到使用迭代器时才真正读取。 ostream_iterator 可以对任何具有&lt;&lt;运算符的类型定义ostream_iterator. 当创建ostream_iterator时，我们可以提供（可选的）第二个参数。它是一个C风格字符串， 在输出每个元素后都会打印此字符串 必须将ostream_iterator绑定带一个指定的流。 不允许空的或者表示尾后位置的ostream_iterator 操作 解释 ostream_iterator&lt;T&gt; out(os); out 将类型为 T的值写入输出流os中 ostream_iterator&lt;T&gt; out(os,d); out 将类型为 T的值写入输出流os中，每个值后面都输出一个d, d指向一个空字符结尾的字符数组 out = val 用&lt;&lt;运算符将val 写入到out所绑定的ostream中， val的类型必须与out可写的类型兼容（ 即为T ） *out ,++out, out++ 这些运算符存在，但不对out做任何改变。 均返回out 使用ostream_iterator输出值的序列 int main()&#123; istream_iterator&lt;int&gt; in_iter(cin), eof; vector&lt;int&gt;vec(in_iter, eof); ostream_iterator&lt;int&gt;out_iter( cout, &quot; &quot; );//使用ostream_iterator输出值的序列 for( auto e: vec ) &#123; *out_iter++ = e; &#125; cout &lt;&lt; endl; return 0;&#125; 事实上，*和++不对ostream_iterator对象做任何事，因此可以写成： for( auto e: vec ) &#123; out_iter = e; &#125; 但推荐前者，因为易于理解 当然，还可以通过copy来打印vector中的元素： copy(vec.begin(), vec.end(), out_iter); reverse_iterator 与普通迭代器一样，只是是反向的 除了forward_list之外，所有容器都支持反向迭代器 可以让算法透明地向前或向后处理容器： sort(vec.rbigin(). vec.rend()); 除了流迭代器，其余迭代器都支持递减运算 反向迭代器的base（）可以返回对应的正向迭代器 返回的正向迭代器的位置在原反向迭代器的后一位（按正序排列） 反向迭代器的删除： for( auto: rit: vec,begin(); vec.end(); )&#123; if( *tir == XX ) rit = decltype(rit)( erase( ++rit.base() ) ); else&#123; rit++; &#125;&#125; erase()只接受正向迭代器，因此要base（）转换 注意到decltype(rit)将正向迭代器转为反向迭代器时，会将位置往前移一位（与之前后移一位对应），避免了手动++rit 例子：打印最后一个逗号后的字符串 string line(&quot;HELLO, MIKE!&quot;);auto rcomma = find( line.crbegin(), line.crend(), &#x27;,&#x27; );cout &lt;&lt; string( line.crbegin(), rcomma ) &lt;&lt; endl; 输出为： !EKIM 想要正确输出MIKE!，要使用正向迭代器： cout &lt;&lt; string( rcomma.base(), line.cend() ) &lt;&lt; endl; 泛型算法结构 任何算法都对其迭代器提供的操作有要求，这里将迭代器分为五类： name 解释 例子 输入迭代器 只读，不写；单遍扫描，只能递增 istream_iterator 输出迭代器 只写，不读; 单遍扫描，只能递增 ostream_iterator 前向迭代器 可读写，多遍扫描，只能递增 forward_list上的迭代器 双向迭代器 可读写，多遍扫描，可递增递减 很多 随机访问迭代器 可读写，多遍扫描，支持全部迭代器运算 vector,string, deque . etc 算法形参模式 大多数算法具有如下参数规范之一： alg(beg,end,other args);alg(beg,end,dest, other args);alg(beg,end,beg2,other args);alg(beg,end,beg2,end2,other args); dest: 算法可以写入的目的位置的迭代器 算法假定：目标空间足够容纳写入的数据 dest经常被绑定到一个插入迭代器或ostream_iterator 算法命名规范 一些算法使用重载形式传递一个谓词 unique(beg,end);unique(beg,end,comp); // 使用comp比较元素 _if版本的算法 接受一个元素值的算法通常有一个不同名（因此非重载）的_if版本，它接受一个谓词来代替元素值： find( beg, end, val );find_if( beg, end, pred ); //查找使得pred返回非零值的元素 区分拷贝元素和不拷贝的版本 默认情况下，重排元素的算法将重排后的元素写回给定的输入序列中。 这些算法还提供另一个版本，将元素写入一个指定的输出目的位置， 这些算法都在名字后面附加一个_copy reverse(beg,end);// 翻转输入序列中元素的顺序reverse(cbeg,cend,dest);// 将元素按逆序拷贝到dest replace( lst.begin(), lst.end(), 0, 42 );//将序列中的所有0替换为42replace_copy( lst.cbegin(), lst.cend(),back_inserter(vec), 0, 42 );//lst自身不变，vec包含list的一份拷贝，只是其中所有0被替换为42 一些算法同时提供_copy和_if版本，接受一个dest和一个谓词： //从v1中删除奇数元素remove_if( v1.begin(), v1.end(), [](int i)&#123;return i%2;&#125;);//将偶数元素拷贝到v2,v1不变 remove_copy_if( v1.begin(), v1.end(), back_inserter(v2), [](int i)&#123;return i%2;&#125;); 特定容器算法 list和forward_list定义了几个成员函数形式的算法，对于这类容器，应当优先使用使用成员函数版本的算法而不是通用算法","categories":[{"name":"Language","slug":"Language","permalink":"http://lyk-love.cn/categories/Language/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://lyk-love.cn/tags/C/"}]},{"title":"C++面向对象","slug":"C++面向对象","date":"2022-09-26T06:39:34.925Z","updated":"2022-09-26T06:39:34.926Z","comments":true,"path":"2022/09/26/C++面向对象/","link":"","permalink":"http://lyk-love.cn/2022/09/26/C++%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/","excerpt":"Outline: Basic Idea 成员初始化表 析构函数 Const成员 静态成员 继承 虚函数","text":"Outline: Basic Idea 成员初始化表 析构函数 Const成员 静态成员 继承 虚函数 Basic Idea 面向对象的优势是可以设计出可复用性和可维护性更强的代码. OO和PO能做的事其实是一样的,OO甚至会更慢,因为多态必然造成性能的下降. OO只是设计层面的思想,和运行没有关系 弱耦合性: 代码更容易复用 容易维护,主要是因为继承和多态,不点的接口,多种行为,类的内部可以自由修改(只要不改接口) 例子: PO实现Stack: #include&lt;iostream&gt;using namespace std;#define STACK_SIZE 100struct Stack&#123; int top; int buffer[STACK_SIZE];&#125;;bool push( Stack &amp;s , int i )&#123; if( s.top == STACK_SIZE - 1 ) &#123; cout &lt;&lt; &quot;stack is overflow!&quot; &lt;&lt; endl; return false; &#125; else &#123; s.top++; s.buffer[s.top] = i; return true; &#125;&#125;bool pop( Stack &amp;s, int &amp;i )&#123; if( s.top == -1 ) &#123; cout &lt;&lt; &quot;stack is empty&quot; &lt;&lt; endl; return false; &#125; else &#123; i = s.buffer[s.top]; s.top--; return true; &#125;&#125;int main()&#123; Stack st1,st2; st1.top = -1; st2.top = -1; int x; push( st1,12 ); pop( st1,x ); cout &lt;&lt; x &lt;&lt; endl; return 0;&#125; OO实现Stack #include&lt;iostream&gt;using namespace std;#define STACK_SIZE 100class Stack&#123; private: int top; int buffer[STACK_SIZE]; public: Stack() &#123;top = -1 ;&#125; bool push( int i ); bool pop( int &amp;i );&#125;;bool Stack::push( int i )&#123; if( top == STACK_SIZE - 1 ) &#123; cout &lt;&lt; &quot;stack is overflow!&quot; &lt;&lt; endl; return false; &#125; else &#123; top++; buffer[top] = i; return true; &#125;&#125;bool Stack::pop( int &amp;i )&#123; if( top == -1 ) &#123; cout &lt;&lt; &quot;stack is empty&quot; &lt;&lt; endl; return false; &#125; else &#123; i = buffer[top]; top--; return true; &#125;&#125;int main( void )&#123; Stack st1,st2; int x; st1.push(12); st1.pop(x); cout &lt;&lt; x &lt;&lt; endl; return 0;&#125; C++ 成员函数都有一个隐含的T *const this,指向本对象( 也就是存储的是本对象的地址 ) getter和setter 可以在类定义时定义,这样它们就成为隐式内联函数 成员初始化表 构造函数的补充 执行 先于构造函数体 按类数据成员申明次序 Class A&#123; int x; const int y; int &amp;z; public: A():y(1),z(x),x(0) //先于构造函数体,按类数据成员声明顺序,所以x初始化为0,z引用x. 再x赋值为10,z也变为10. &#123; x = 100 &#125;&#125;; 成员初始化表: 构造函数在分配内存的时候直接用这个值来进行初始化 x = 100: 这是赋值,不是初始化. 构造函数先初始化x,然后复制为100. 在构造函数中尽量使用成员初始化取代赋值动作 const成员, reference成员, 对象成员 效率高 数据成员太多时,不采用本条准则 降低可维护性 例题 class CString&#123; char *p; int size;public:CString(int x): size(x), p(new char[size]) &#123;&#125;&#125;; 错了! 因为p初始化的时候,size还没有初始化! 应该把size声明提前 析构函数 ~&lt;类名&gt;() 对象消亡时,系统自动调用( 释放对象持有的非内存资源和不属于这个对象的内存 ) RAII vs GC: RAII: Resource Accuisition Is Installization 资源获取即初始化 获得了一个资源,就像对待对象一样对待它 public 可定义为private Const成员 const成员 const成员变量 class A&#123; const int x; public A( int i) :x( i ) &#123; ; &#125;&#125;; const成员的初始化放在构造函数的初始化表中进行 static const: 类静态常量,这个常量放在静态区,只能在类定义外部初始化(而不是在构造函数内, 因为它不从属于某个对象) 静态成员 问题:同一个类的不同对象如何共享变量 class A&#123; int x,y; static int shared; public: static void f();//静态成员函数,只能存取静态成员变量, 遵循类访问控制 void q();&#125;;int A::shared = 0; //在函数定义的时候不需要写static; 不在构造函数内初始化void A::f()&#123; ;&#125; 静态成员的使用 通过对象使用 A a; a.f(); 通过类使用(不像某些语言一样用A.f()) A::F() 单例: class Singleton&#123; protected: Singleton(); Singleton(const Singleton&amp;); public: static Singleton* instance(); static void destroy(); private: static Singleton *m_instance;&#125;;Singleton *Singleton::m_instance = nullptr;Singleton *Singleton::instance()&#123; return m_instance == nullptr? m_instance = new Singleton : m_instance;&#125;void Singleton::destroy()&#123; delete m_instance; m_instance = nullptr;&#125; 友元 在使用C++进行项目开发的过程中难免会使用友元及前置声明 下面就对它们进行讲解： 在此之前，先来了解下什么是友元函数？什么是友元类？什么是友元成员函数？为什么需要友元？ 友元函数是指某些虽然不是类成员的函数却能够访问类的所有成员。友元类同理，只是友元类与友元函数最主要的区别是：一个是将某个函数作为类的友元，一个则是将整个类（所有成员函数）都作为其他类的友元。而友元成员函数顾名思义就是将某个类的某个成员函数作为其他类的友元。一般情况下，非成员函数是无法直接从外部访问类的私有或保护部分的，但是在有些开发中又需要非成员函数从外部访问该类的私有或保护部分，而友元可以实现。 类的友元函数： class Base&#123;private: int Num;public: Base(int n = 0):Num(n)&#123;&#125; void setValue(int n)&#123;Num = n;&#125; void show()const&#123;std::cout &lt;&lt; &quot;BaseNum:&quot; &lt;&lt; Num &lt;&lt; std::endl;&#125; friend void setData(Base&amp;,int); //声明友元函数&#125;;void setData(Base&amp; s1,int n)&#123; s1.Num = n; //#1 可以直接访问&#125; 如果没有 friend void setData(Base&amp;,int);该行声明语句的话，类外部函数setData是无法访问类私有成员Num的，当有该行友元声明的话，setData函数将可以直接访问该类的私有部分Num成员，如代码中#1所示，这是编译器所允许的。 友元类： class Base1; //前置声明 因为Base1类在Base类后面定义的，而Base类提到了Base1类(声明友元类的时候) //所以必须让编译器知道有这个类 也可以省略该步 但是在声明友元类的时候应该这样写：friend class Base1;class Base&#123;private: int Num;public: Base(int n = 0):Num(n)&#123;&#125; void setValue(int n)&#123;Num = n;&#125; //设置一个自定义新值 void clearValue()&#123;Num = 0;&#125; //将值置为0 void show()const&#123;std::cout &lt;&lt; &quot;BaseNum:&quot; &lt;&lt; Num &lt;&lt; std::endl;&#125; friend Base1; //将Base1整个类作为该类(Base)的友元 即Base1的所有成员函数均是Base类的友元&#125;;class Base1&#123;public: void setData(Base&amp; s1,int n)&#123;s1.Num = n;&#125; //与Base::setValue函数功能相同 使用了友元特性 void clearData(Base&amp; s1)&#123;s1.clearValue();&#125; //与Base::clearValue函数功能相同 但没有使用到友元特性&#125;; 当需要将一个类的所有成员函数作为另一个类的友元的话，可以将这个类直接作为另一个类的友元，这样整个类的成员函数都将是另外一个类的友员。如上面代码所示，因此Base1类中的setData和clearData函数都是Base类的友元，都可以直接从外部对Base类的私有或保护成员进行操作。因为Base1类在Base类后面定义的，在编译friend Base1;这句代码的时候，编译器并不知道Base1是个什么东西，所以必须在将Base1类放在Base类前面定义或者在Base类前面进行前置声明，上面代码正是这么做的。class Base1;这行代码就是前置声明。那么对于上面代码有没有其他方法可以省略前置声明并实现同样效果呢？答案是肯定的，在这里可以省略前置声明，但是必须要将friend Base1;改为friend class Base1;这样编译器就知道将Base1是一个类，然后将它设为友元。 有人可能会问了，能不能将Base1放在Base前面定义，然后前置声明一个class Base;呢？可以，但是必须要将Base1的函数定义部分去掉。如下所示： class Base; //对Base的前置声明 class Base1&#123;public: void setData(Base&amp; s1,int n); //在这里不能定义函数 void clearData(Base&amp; s1); //同上&#125;;class Base&#123;private: int Num;public: Base(int n = 0):Num(n)&#123;&#125; void setValue(int n)&#123;Num = n;&#125; void clearValue()&#123;Num = 0;&#125; void show()const&#123;std::cout &lt;&lt; &quot;BaseNum:&quot; &lt;&lt; Num &lt;&lt; std::endl;&#125; friend Base1; //在此之前 编译器已经知道了Base1的完整定义 所以不用再对Base1进行前置声明&#125;;void Base1::setData(Base&amp; s1,int n)&#123; s1.Num = n;&#125;void Base1::clearData(Base&amp; s1)&#123; s1.clearValue();&#125; 为什么不能在类中定义该函数呢？因为如果在Base1中定义了setData和clearData函数，而函数体中对Base类的成员进行操作了，这样就必须事先知道Base类的完整定义(让编译器知道类内部情况)，不然编译器不知道Base类中有没有这些成员，所以不允许这么做。但是能不能在类中定义setData和clearData函数并在Base1类前面进行前置声明Base类解决这个问题呢？不行！因为前置声明顾名思义只是提前声明，前置声明class Base;只是让编译器知道，有这么一个Base类将在后面进行定义，但是编译器并不知道该类的内部情况，所以编译器只允许在知道该类的完整定义后，才让对该类成员进行操作(就是定义对这个类的成员进行操作的函数)，否则不允许。( 所以不能定义函数,只能声明函数 ) 到这里，相信聪明的你应该发现，Base1类中只有函数setData使用了友元特性，对Base类私有成员直接访问。而clearData函数的实现只是调用了Base类的公有方法clearValue，间接访问私有成员，但是这并不涉及到友元特性。所以这个函数没有必要成为Base类的友元。如果一个类中有几十个函数，而大部分都没有使用到友元特性，将他们都设置为友元的方法（即友元类）并不推荐，而只有当大部分成员函数都需要使用友元特性的时候，使用友元类将非常方便，而只有个别的成员函数涉及到友元特性的话，推荐使用下面这个友元方法，但是这种方法需要特别注意类的定义顺序。 友元成员函数： class Base; //对Base的前置声明 class Base1&#123;public: void setData(Base&amp; s1,int n); //在这里不能定义函数 void clearData(Base&amp; s1); //同上&#125;;class Base&#123;private: int Num;public: Base(int n = 0):Num(n)&#123;&#125; void setValue(int n)&#123;Num = n;&#125; void clearValue()&#123;Num = 0;&#125; void show()const&#123;std::cout &lt;&lt; &quot;BaseNum:&quot; &lt;&lt; Num &lt;&lt; std::endl;&#125; friend void Base1::setData(Base&amp; s1,int n); //声明友元成员函数&#125;;void Base1::setData(Base&amp; s1,int n)&#123; s1.Num = n;&#125;void Base1::clearData(Base&amp; s1)&#123; s1.clearValue();&#125; 在声明友元类的时候，只要不在类内部定义函数，顺序无关紧要，只要添加后面定义的类的前置声明就好了。而友元成员函数就不行，因为在使用friend void Base1::setData(Base&amp; s1,int n);这句代码进行声明友元成员函数的时候，提到了Base1类的成员函数，既然需要将这个类的setData函数设为友元，那么就必须提前知道Base1类的完整定义(了解类的内部情况)，那么就必须将Base1类放在Base类的前面进行定义，以便当编译器编译friend void Base1::setData(Base&amp; s1,int n);这行代码的时候就已经知道Base1类的内部情况了，所以定义的顺序也至关重要，而且Base1类中的函数不能在类中定义，因为定义了的话，就需要知道Base的内部情况(类完整定义)那么就需要将Base放在Base1前面定义，而Base又需要将Base1放在Base前面，这将相互矛盾，所以最友善的解决方法就是Base1的函数不在类中定义，这也是至关重要的。 如果**类B提到了类A的成员函数,**那么需要提前知道类A的完整定义. 如果类B的成员函数提到了类A,那么只需要前置声明A.而B的函数定义要写在A的类定义后面 另外，使用前置声明时，例如将Base1放在Base前面定义，并且使用前置声明class Base;那么在Base1类成员部分中不能实例化Base类对象，因为实例化也涉及到构造函数，需要让编译器知道Base类的完整定义，使用前置声明class Base;是不行的！要么Base1类中不进行实例化Base类，要么就在Base1类成员部分定义一个Base类的指针，并且在Base1类构造函数中对该指针使用new Base;方法实例化，这样是可以的，因为只定义Base类指针，不需要了解Base类内部情况，只需要知道Base是一个什么类型就好了，如前面的前置声明class Base;就让编译器知道了，Base是一个类，而Base*是一个Base类的指针。如下代码： class Base; //对Base的前置声明 class Base1&#123;public: void setData(Base&amp; s1,int n); //在这里不能定义函数 void clearData(Base&amp; s1); //同上 Base1(int n = 0);private:// Base temp; //error! 需要在此之前知道Base的完整定义 Base* pTemp; //OK! 只需要提前知道Base是什么类型就好了 前面class Base;已经告诉编译器&#125;;class Base&#123;private: int Num;public: Base(int n = 0):Num(n)&#123;&#125; void setValue(int n)&#123;Num = n;&#125; void clearValue()&#123;Num = 0;&#125; void show()const&#123;std::cout &lt;&lt; &quot;BaseNum:&quot; &lt;&lt; Num &lt;&lt; std::endl;&#125; friend void Base1::setData(Base&amp; s1,int n); //声明友元成员函数&#125;;void Base1::setData(Base&amp; s1,int n)&#123; s1.Num = n;&#125;void Base1::clearData(Base&amp; s1)&#123; s1.clearValue();&#125;Base1::Base1(int n)&#123; pTemp = new Base(n); //实例化Base类&#125;在C++中还有模版友元 这将在后面讲述。有些人可能会问了，C++友元会不会与面向对象思想相悖？不会！因为友元只能由类定义，例如需要将Base1声明为Base类的友元，那么就只能在Base类中进行声明，而不能在外部强加友情，因此，尽管友元被授予从外部访问类的私有部分和保护部分的权限，但他们并不与面向对象编程思想相悖，相反，他们提高了公有接口的灵活性。 继承 继承机制 基于目标代码的复用 对事物进行分类 派生类是基类的具体化 把事物(概念)以层次结构表示出来,有利于描述和解决问题 增量开发 构造函数 派生类对象的初始化 由基类和派生类共同完成 构造函数的执行次序 基类的构造函数 派生类对象成员类的构造函数 派生类的构造函数 析构函数的执行顺序 与构造函数相反 基类构造函数的调用 缺省执行基类默认构造函数 如果要执行基类的非默认构造函数,则必须在派生类构造函数的成员初始化表中指出 #include&lt;iostream&gt;using namespace std;class A&#123; int x;public: A()&#123; x = 0; &#125; A(int i)&#123; x = i; &#125;&#125;;class B: public A&#123; int y;public: B()&#123; y = 0; &#125; B( int i ) &#123; y = i &#125; B( int i, int j ): A(i)// 注意,实际上A的构造函数会先于B的构造函数执行 &#123; y = j; &#125;&#125;;int main()&#123; B b1;//A::A()和执行B::B() B b2(1);//A::A()和执行B::B(int) B b3(0,1); //执行A::A(int) 和 B::B(int，int) return 0;&#125; Overload（重载） 重载的概念最好理解，在同一个类声明范围中，定义了多个名称完全相同、参数（类型或者个数）不相同的函数，就称之为Overload（重载）。重载的特征如下： **（1）**相同的范围（在同一个类中）； **（2）**函数名字相同； **（3）**参数不同； **（4）**virtual 关键字可有可无。 Override（覆盖） 覆盖的概念其实是用来实现C++多态性的，即子类重新改写父类声明为virtual的函数。Override（覆盖）的特征如下： **（1）**不同的范围（分别位于派生类与基类）； **（2）**函数名字相同； **（3）**参数列表完全相同； **（4）**返回类型也必须一样 **（5）**基类函数必须有virtual 关键字。 Overwrite（改写） 改写是指派生类的函数屏蔽（或者称之为“隐藏”）了与其同名的基类函数。正是这个C++的隐藏规则使得问题的复杂性陡然增加，这里面分为两种情况讨论： **（1）**如果派生类的函数与基类的函数同名，但是参数不同。那么此时，不论有无virtual关键字，基类的函数将被隐藏（注意别与重载混淆）。 **（2）**如果派生类的函数与基类的函数同名，并且参数也相同，但是基类函数没有virtual关键字。那么此时，基类的函数被隐藏（注意别与覆盖混淆）。(编译器在当前类作用域内找到了找到了匹配的函数,于是不再去找基类的; 如果是虚函数,那么会查虚函数表 ) 单继承 class Base&#123;public: //对于C++类中定义的任何类型的函数，名字覆盖的规则都是一样的。 virtual void func1(int x)&#123;cout &lt;&lt; &quot;func1(int) in Base ...&quot; &lt;&lt; endl;&#125; void func2()&#123;cout &lt;&lt; &quot;func2() in Base ...&quot; &lt;&lt; endl;&#125; virtual void func3() = 0;&#125;; class Derived : public Base&#123;public: void func1() &#123;cout &lt;&lt; &quot;func1() in Derived...&quot; &lt;&lt; endl;&#125; void func2(int x) &#123;cout &lt;&lt; &quot;func2(int) in Derived...&quot; &lt;&lt; endl;&#125; void func3() &#123;cout &lt;&lt; &quot;func3() in Derived...&quot; &lt;&lt; endl;&#125; void func3(int ) &#123;cout &lt;&lt; &quot;func3() in Derived...&quot; &lt;&lt; endl;&#125;&#125;; int main()&#123; Derived d; d.func1(3); //调用失败。编译器在Derived的作用域中找到了该函数名字，但是发现调用不匹配，不能通过编译 d.func2(); return 0; 派生类只能通过基类的类来访问基类的protecteed 成员,而不能通过基类的对象来访问. 当你在派生类中定义一个基类函数的重载版本后,基类中所有的重载函数在子类中不可见. 这是因为编译器在面对函数调用时，首先是在作用域范围内查找该函数名（由内之外）， 如果找到了该函数名之后，编译器便停止查找，开始检查形参与实参的匹配是否合法， 如果不合法，不能通过编译。( 因此,编译器在子类中找到了这个函数名,但是发现调用不匹配,于是不能通过编译 ). 解决方法: 使用using &lt;基类&gt;:: &lt;成员名称&gt; 来将基类中的属性或者函数成员名称引入到子类中来, 也就是让基类中的函数名字在&quot;编译器查找子类&quot;作用域是可见的. 构造函数和析构函数不能被继承 派生类友元函数可以通过派生类对象访问基类的protected 成员, 但是不能通过基类对象访问基类的 protected 成员. 友元不可传递! 私有继承 在声明一个派生类时将基类的继承方式指定为private的，称为私有继承，用私有继承方式建立的派生类称为私有派生类(private derived class )， 其基类称为私有基类(private base class )。 私有基类的公用成员和保护成员在派生类中的访问属性相当于派生类中的私有成员，即派生类的成员函数能访问它们，而在派生类外不能访问它们。私有基类的私有成员在派生类中成为不可访问的成员，只有基类的成员函数可以引用它们。一个基类成员在基类中的访问属性和在派生类中的访问属性可能是不同的。私有基类的成员在私有派生类中的访问属性见表 虽然在派生类外不能通过派生类对象调用私有基类的公用成员函数，但在派生类外可以通过派生类的公共成员函数调用私有基类的公用成员函数(此时它是派生类中的私有成员函数，可以被派生类的任何成员函数调用)。这就是一种委托 私有基类中的成员 在私有派生类中的访问属性 私有成员 不可访问 公用成员 私有 保护成员 私有 私有继承类和基类的接口不一样,它们不是is-a关系,而是has-a关系 因此,私有继承不存在类型兼容和类型转换, 也就是不能用父类指向子类. class CHumanBeing&#123; ...&#125;;class CStudent: private CHumanBeing&#123; ...&#125;;CHumanBeing a; CStudent b;eat(a);eat(b); //Error 多继承 继承方式及访问控制的规定同单继承 派生类拥有所有基类的所有成员 多继承定义的时候是一个权限名对应一个基类，class derived:public base1, public base2. 不能是class derived:public base1,base2 基类的声明次序决定： 对基类构造函数/析构函数的调用次序 对基类数据成员的存储安排 名冲突 &lt;基类名&gt;：：&lt;基类名称名&gt; 虚基类 如果直接基类有公共的基类，则该公共基类中的成员变量在多继承的派生类中有多个副本 class A&#123; int x; ...&#125;;class B: A;class C: A;Class D: B,C; 类D拥有两个成员B:: x 和C:: x D调用B和C, B调用A, C调用A. 所以A的构造函数会被调用两次. 实际上A的成员会被拷贝给B和C, 拷贝到不同子类的基类成员之间是无关联的 虚基类 合并 class A;Class B: virtual public A;class C: virtual public A;class D: B,C; 设定为虚基类后，系统知道base1和base2都是由base派生出的，所以它就统一先构造base，调用base的构造函数。 再按照顺序调用base1和base2的构造函数，只不过在此时，大家在构造时操作的都是同一个成员 注意 虚基类的构造函数由最新派生出的类的构造函数调用 虚基类的构造函数优先非虚基类的构造函数执行 解释: 设B,C虚继承A, D继承B,C, 则D的构造函数会先执行虚基类A的构造函数,再执行B和C的构造函数; 而在普通多继承中,D会调用B,C,它们再分别调用A. 二者是不同的. 多继承形式下析构函数的执行顺序和构造函数的执行顺序相反。 多继承下的构造函数 多继承形式下的构造函数和单继承形式基本相同，只是要在派生类的构造函数中调用多个基类的构造函数。以上面的 A、B、C、D 类为例，D 类构造函数的写法为： D(形参列表): A(实参列表), B(实参列表), C(实参列表)&#123; //其他操作&#125; 基类构造函数的调用顺序和和它们在派生类构造函数中出现的顺序无关，而是和声明派生类时基类出现的顺序相同。仍然以上面的 A、B、C、D 类为例，即使将 D 类构造函数写作下面的形式： D(形参列表): B(实参列表), C(实参列表), A(实参列表)&#123; //其他操作&#125; 是先调用 A 类的构造函数，再调用 B 类构造函数，最后调用 C 类构造函数。 从运行结果中还可以发现， 命名冲突 当两个或多个基类中有同名的成员时，如果直接访问该成员，就会产生命名冲突，编译器不知道使用哪个基类的成员。这个时候需要在成员名字前面加上类名和域解析符::，以显式地指明到底使用哪个类的成员，消除二义性。 修改上面的代码，为 BaseA 和 BaseB 类添加 show() 函数，并将 Derived 类的 show() 函数更名为 display()： #include &lt;iostream&gt;using namespace std;//基类class BaseA&#123; public: BaseA(int a, int b); ~BaseA();public: void show(); protected: int m_a; int m_b;&#125;;BaseA::BaseA(int a, int b): m_a(a), m_b(b)&#123; cout&lt;&lt;&quot;BaseA constructor&quot;&lt;&lt;endl;&#125;BaseA::~BaseA()&#123; cout&lt;&lt;&quot;BaseA destructor&quot;&lt;&lt;endl;&#125;void BaseA::show()&#123; cout&lt;&lt;&quot;m_a = &quot;&lt;&lt;m_a&lt;&lt;endl; cout&lt;&lt;&quot;m_b = &quot;&lt;&lt;m_b&lt;&lt;endl;&#125;//基类class BaseB&#123; public: BaseB(int c, int d); ~BaseB(); void show(); protected: int m_c; int m_d;&#125;;BaseB::BaseB(int c, int d): m_c(c), m_d(d)&#123; cout&lt;&lt;&quot;BaseB constructor&quot;&lt;&lt;endl;&#125;BaseB::~BaseB()&#123; cout&lt;&lt;&quot;BaseB destructor&quot;&lt;&lt;endl;&#125;void BaseB::show()&#123; cout&lt;&lt;&quot;m_c = &quot;&lt;&lt;m_c&lt;&lt;endl; cout&lt;&lt;&quot;m_d = &quot;&lt;&lt;m_d&lt;&lt;endl;&#125;//派生类class Derived: public BaseA, public BaseB&#123; public: Derived(int a, int b, int c, int d, int e); ~Derived(); public: void display(); private: int m_e;&#125;;Derived::Derived(int a, int b, int c, int d, int e): BaseA(a, b), BaseB(c, d), m_e(e)&#123; cout&lt;&lt;&quot;Derived constructor&quot;&lt;&lt;endl;&#125;Derived::~Derived()&#123; cout&lt;&lt;&quot;Derived destructor&quot;&lt;&lt;endl;&#125;void Derived::display()&#123; BaseA::show(); //调用BaseA类的show()函数 BaseB::show(); //调用BaseB类的show()函数 cout&lt;&lt;&quot;m_e = &quot;&lt;&lt;m_e&lt;&lt;endl;&#125; int main() &#123; Derived obj(1, 2, 3, 4, 5); obj.display(); return 0; &#125; 显式地指明了要调用哪个基类的 show() 函数 虚函数 c++语言中，基类必须将它的两种成员函数区分开来：一种是基类希望其派生类进行覆盖的函数，另一种是基类希望派生类直接继承而不要改变的函数。对于前者，基类通常将其定义为虚函数（virtual）。当我们使用指针或者引用调用虚函数时，该调用将被动态绑定。根据引用或者指针所绑定的对象类型不同，该调用可能执行基类的版本也可能执行某个派生类的版本。在某些时候基类希望它的派生类有权访问该成员，同时禁止其他用户访问。我们用受保护的（protected）访问运算符来说明这样的成员。 基类通过在其成员函数声明语句之前加上关键字virtual使得改函数执行动态绑定。任何构造函数之外的非静态函数都可以是虚函数。关键字virtual只能出现在类内部声明语句之前而不能用于类外部的函数定义。如果基类把一个函数声明成虚函数，则该函数在派生类中隐式地也是虚函数。 虚函数的访问控制 编译器根据对象的静态类型来决定访问控制权限，并且进行形参的默认参数的赋值 虚函数是在运行时查虚函数表,而访问控制发生在编译期而不是运行期, 因此在派生类中更改虚函数的访问控制实际上没有意义 （1） 编译器在决定调用函数时，如果该函数是虚函数才会在运行时确定调用什么函数（动态绑定），如果不是虚函数，那么在编译阶段就已经确定了调用的函数类型（静态绑定）。 如下面的代码，基类与派生类都声明了函数f。但是在main函数的调用中编译器调用的是静态类型对应的函数，因为f函数并不是虚函数，虽然在基类与派生类中都声明了该函数。 class Base&#123;public: void f(int i=0) &#123;cout &lt;&lt; &quot;f() in Base...&quot; &lt;&lt; i &lt;&lt; endl;&#125;&#125;;class Derived:public Base&#123;private: void f(int i=1)&#123;cout &lt;&lt; &quot;f() in derived...&quot; &lt;&lt; i &lt;&lt; endl;&#125;&#125;;int main(void)&#123; Base *b = new Derived(); b-&gt;f(); return 0;&#125; （2）如下,基类定义虚函数为public，派生类覆盖了该虚函数，但是将其声明为private，这样当基类的指针绑定到派生类的对象时，使用该基类指针调用该虚函数时: class Base&#123;public: virtual void f(int i=0) &#123;cout &lt;&lt; &quot;f() in Base...&quot; &lt;&lt; i &lt;&lt; endl;&#125;&#125;;class Derived:public Base&#123;private: void f(int i=1)&#123;cout &lt;&lt; &quot;f() in derived...&quot; &lt;&lt; i &lt;&lt; endl;&#125;&#125;;int main(void)&#123; Base *b = new Derived(); b-&gt;f(); return 0;&#125;//输出为： f() in derived 0 【分析】首先分析为什么输出结果是f() in derived。 编译器在看到b对f进行调用时，此时编译器根据b的静态类型（也就是Base）来决定f函数是否可访问，并且进行形参的默认参数的赋值 由于f是虚函数，那么具体调用哪个函数是在运行时确定的，于是在运行时查找Derived的虚函数表，得到虚函数f（此时的f已经被Derived类覆盖，于是调用的就是派生类的版本。） 也就是说,在编译期间,编译器根据静态类型来决定函数的访问权限,并进行函数匹配,现在匹配到了f,且参数列表符合,则匹配成功,并缺省参数赋值( 这一步详见下文 &quot;绝对不要重新定义继承而来的缺省参数值&quot;), 由于该函数是虚函数,则在运行期间,编译器决定调用子类的f. 类型相容 类,类型 类型相容,赋值相容 问题: a, b是什么类型时, a = b 合法? A a; B b; class B: public A A a = b; 对象的身份发生变化 属于派生类的属性已不存在( 切片, 也可以看成是因为拷贝构造函数 ) B* pb; A* pa = p;b class B: public A 可以用父类的指针指向子类 B b; A &amp;a=b; class B: public A 可以用父类的引用引用子类 对象的身份没有变化 前期绑定( Early Binding ) 编译时刻 依据对象的静态类型 效率高, 灵活性差 动态绑定( Late Binding ) 运行时刻 依据对象的实际类型(动态) 灵活性高,效率低( 比静态绑定多一次寻址 ) 注重效率 C++默认前期绑定 后期绑定需显示指出 virtual 定义 virtual class A&#123; ...public: virtual void f();&#125;; 动态绑定 根据实际引用和指向的对象类型 方法重定义 如基类中被定义为虚成员函数, 则派生类中对其冲定义的成员函数均为虚函数. 限制 类的成员函数才可以是虚函数 虚函数表 基类对象包含一个虚表指针，指向基类的虚函数表 派生类对象也将包含一个虚表指针，指向派生类虚函数表 如果派生类重写了基类的虚方法，该派生类虚函数表将保存重写的虚函数的(入口)地址，而不是基类的虚函数地址 如果基类中的虚方法没有在派生类中重写，那么派生类将继承基类中的虚方法，而且派生类中虚函数表将保存基类中未被重写的虚函数的地址，但如果派生类中定义了新的虚方法，则该虚函数的地址也将被添加到派生类虚函数表中 final，override 1. override 重载 当你在父类中使用了虚函数时候，你可能需要在某个子类中对这个虚函数进行重写，以下方法都可以： class A&#123; virtual void foo();&#125;class B :public A&#123; void foo(); //OK void foo() override; //OK&#125; 如果不使用override，当你手一抖，将**foo()写成了f00()**会怎么样呢？结果是编译器并不会报错，因为它并不知道你的目的是重写虚函数，而是把它当成了新的函数。如果这个虚函数很重要的话，那就会对整个程序不利。 所以，override的作用就出来了，它指定了子类的这个虚函数是重写的父类的，如果你名字不小心打错了的话，编译器是不会编译通过的： class A&#123; virtual void foo();&#125;;class B :A&#123; virtual void f00(); //OK virtual void f0o()override; //Error &#125;; 显然, override 不能用来修饰非虚函数 2.final * 当不希望某个类被继承，或不希望某个虚函数被重写，可以在类名和虚函数后添加final关键字，添加final关键字后被继承或重写，编译器会报错。 final用于类或虚函数, 不能用来修饰非虚函数 例子如下： class Base&#123; virtual void foo();&#125;; class A : Base&#123; void foo() final; // foo 被override并且是最后一个override，在其子类中不可以重写 void bar() final; // Error: 父类中没有 bar虚函数可以被重写或final&#125;;class B final : A // 指明B是不可以被继承的&#123; void foo() override; // Error: 在A中已经被final了&#125;; class C : B // Error: B is final&#123;&#125;; PPT例子: struct B&#123; virtual void f1(int) const; virtual void f2(); void f3(); virtual void f5(int) final;&#125;;struct D: B //默认的继承访问权限。struct是public继承的&#123; void f1(int) const override; //正确，f1与基类中的f1匹配 void f2(int) override;//错误： B没有形如f2（int）的函数 void f3() override;//错误， f3不是虚函数 void f4() override;//错误： B没有名为f4的函数 void f5(int);//错误， B已经将f5声明成final；&#125; 纯虚函数和抽象类 纯虚函数 声明时在函数原型后面加上= 0 往往只给出函数声明,不给出实现 往往的意思是，对于纯虚析构函数,我们是要提供实现的 抽象类 至少包含一个纯虚函数 不能用于创建对象 为派生类提供框架,派生类提供抽象基类的所有成员函数的实现 虚析构函数 确定public inheritance,是真正意义的&quot; is a &quot; 关系 不要定义与继承而来的非虚成员函数同名的成员函数 明智地运用private Inheritance 见下文&quot;私有继承&quot; Implemented-in-term-of 需要使用基类的protected成员,或重载虚函数 不希望一个基类被客户使用 在设计层面无意义,只用于实现层面. 绝对不要重新定义继承而来的缺省参数值( 写了也没用 ) 静态绑定 效率 话说这个机制是历史遗留问题, 给我们的启示是:尽量少用缺省参数值 缺省参数值: 编译的时候,编译器如果看到这个函数没有参数,但有缺省参数值,当场就会把缺省参数值绑定到形参. 由于对象的vtable只会存虚函数的入口地址(不存缺省参数的原因是很少用,效率也差). 于是, 编译器会在编译期把缺省参数静态绑定上去. 因此,就会发生: 指向子类的父类引用调用了子类的虚函数(这是我们所期望的),但是其缺省参数值居然是父类的的情况. #include&lt;iostream&gt;using namespace std;class A &#123;public: virtual void f(int x = 0) &#123; cout &lt;&lt; &quot;class A: &quot; &lt;&lt; x &lt;&lt; endl; &#125;&#125;;class B: public A&#123; private: virtual void f(int x = 1) &#123; cout &lt;&lt; &quot;class B: &quot; &lt;&lt; x &lt;&lt; endl; &#125;&#125;;class C : public A&#123;private: virtual void f(int x) &#123; cout &lt;&lt; &quot;class C: &quot; &lt;&lt; x &lt;&lt; endl; &#125;&#125;;int main()&#123; A* p_a; B b; p_a = &amp;b; p_a-&gt;f(); A* p_a1; C c; p_a1 = &amp;c; p_a1-&gt;f(); //先找父类，再绑定缺省参数。 在运行期，再选中子类的函数。 return 0;&#125;//输出:class B: 0class C: 0 三种函数 纯虚函数 只有函数接口会被继承 子类必须继承函数接口 子类必须提供实现代码 一般虚函数 函数的接口及缺省实现代码 子类必须继承函数接口 可以继承缺省实现代码 非虚函数 函数的接口和其实现代码会被继承 必须同时继承接口和实现代码 纯虚析构函数 原文 纯虚析构函数和普通纯虚函数的区别在于，纯虚析构函数需要提供函数的实现，而一般纯虚函数不能有实现，这样的原因在于，纯虚析构函数最终需要被调用，以析构基类对象，虽然是抽象类没有实体。而如果不提供该析构函数的实现，将使得在析构过程中，析构无法完成而导致析构异常的问题 Class A&#123; public: A()&#123;&#125; virtual ~A()=0;&#125;A::~A()&#123;&#125; //提供了纯虚析构函数的实现ClassB:public A&#123;&#125;A *p =new B();*Delete p; Delete p;通过父类指针去析构子类对象, 分三种情况: 父类如A的析构函数不是虚函数，这种情况下，将只会调用A的析构函数而不会调用子类的析构函数，前面的文章中有提到过，非虚函数是通过类型来寻址的，这样的析构将会导致析构畸形 父类如A的析构函数是普通的虚函数，这种情况下，会很正常，从子类一直析构到基类，最后完成析构 父类如A的析构函数是纯虚析构函数，如本文所提，正是重点，在这种情况之下，由于析构函数首先是虚函数，所以会按2的方法从子类一直析构到父类，但是，又由于父类的析构函数是纯虚函数，没有实现体，所以，当析构到父类时，由于没有实现体，所以导致父类无法析构，最终也导致了析构畸形，因此，特殊的地方就在于这里，纯虚析构函数需要提供一个实现体，以完成对象的析构","categories":[{"name":"Language","slug":"Language","permalink":"http://lyk-love.cn/categories/Language/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://lyk-love.cn/tags/C/"}]},{"title":"C++结构化","slug":"C++结构化","date":"2022-09-26T06:39:34.925Z","updated":"2022-09-26T06:39:34.925Z","comments":true,"path":"2022/09/26/C++结构化/","link":"","permalink":"http://lyk-love.cn/2022/09/26/C++%E7%BB%93%E6%9E%84%E5%8C%96/","excerpt":"Outline: Preface Prigramming Data Type Variable ... ref: C++ Primer, Effective C++, More Effective C++, Effective Modern C++","text":"Outline: Preface Prigramming Data Type Variable ... ref: C++ Primer, Effective C++, More Effective C++, Effective Modern C++ Preface G = ( VN , VT , P , Z ) VN : non-terminal,非终结符号 VT:终结符号 P:规则 Z：target，目标 高级程序设计语言都是有理论保障的，否则不能保证得到正确结果。 Programming 看待程序设计的两个观点： The Science of programming: 从科学的角度。从这个角度看，许多bug是来自数据流而不是程序本身。 The Art of programming:从艺术的角度。程序设计要时刻注意所处的设计环境。 Von-Neumann structure: 计算器、存储器、控制器、IO Programming Paradigm Procedure: 最经典的就是 程序 = 数据结构 + 算法 Object-Oriented: OO是对人来说的， 依然在冯诺依曼架构里。 Functional: 典型的就是 f( g(x) ) = g( f( x ) ), 在java程序中，如果函数有副作用，那这个式子是不成立的，而函数式就要确保函数没有副作用，这就能满足数学上的表达。这种没有副作用的场景是非常多的，比如说分布式计算就依赖于此 Logical: 规则 + 条件 → automatic proof 基本数据类型 built-in datatype char,int , float , double Modifiers: long , short , signed , unsigned char 只能用signed,unsigned修饰 float不能被修饰 double只能用long修饰 int可以用四种修饰符组合修饰 省略表示 操作符sizeof ANSI C++: wcchar_t,bool typedef 为已有的类型定义一个同义词： typedef int INT32; typedef int A[8]; using也是类型别名 类型别名 不是宏替换！ typedef char *pstring;const pstring cstr = nullptr; //cstr是指向char的常量指针 pstring的基本数据类型是指针，（ *不是声明符的一部分 ），const修饰的是指针。 因此cstr是常量指针 生命周期 自动局部变量：在函数调用时产生，在函数终止时消亡。可以显示使用auto（写不写都一样）来强调。 如果未显式初始化，自动局部变量会包含以前使用的垃圾值 静态局部变量： 生命周期是整个程序的生命周期。 全局变量：生命周期同上。 如果未显式初始化， 全局变量和静态局部变量会被初始化为默认值 初始化方式 列表初始化： 当用于内置类型的变量时，如果使用列表初始化且初始值存在丢失信息的风险，则编译器将报错（ 即无法窄化转换 ） long double ld = 3.141325242;int a&#123; ld &#125;, b = &#123; ld &#125;; //编译错误int c(ld), d = ld; //只会warning 复合类型 复合类型指基于其他类型定义的类型 一条声明语句由一个基本数据类型和一个声明符列表（ declarator ）组成，每个声明符命名了一个变量并指定该变量为与基本数据类型有关的某个类型 *和&amp;都是类型修饰符,属于声明符的一部分,不会修饰本次定义的全部变量 int *p1,p2; //p1是指针,p2是int 一条声明语句只能有一个基本类型 引用必须初始化，而且无法重新绑定 const限定符 const对象一旦创建后其值就不能再改变,因此const对象必须初始化 const对象的常量特征仅仅在执行改变其值的时候才会起作用,因此const对象可以执行不改变内容的操作(如类型转换) 默认状态下,const对象仅在文件内有效. 当多个文件内出现同名的const变量时, 实际等同于在不同文件中分别定义了独立的变量 如果要在多个文件中共享const对象,则必须在变量定义之前添加extern. 即: 无论声明和定义都添加extern,这样只需定义一次 //file1.cppextern const int bufSize = fcn();//file1.hextern const int bufSize; //这个声明使用了extern,指明 bufSize并非本文件独有,它的定义将在别处出现 初始化和对constd引用 引用的类型必须与所引用对象的类型一致，但初始化常量引用时允许用任意表达式作初始值，只要该表达式的结果能转换成引用的类型即可。尤其，允许为一个常量引用绑定非常量的对象、字面值，甚至是个一般表达式 原因：常量引用绑定到另一种类型上时，编译器会将代码改成绑定到临时量对象。而对于非常量引用，既然要使用非常量引用，就肯定想通过它改变绑定对象的值，可是引用绑定的是临时量，改变它是没有意义的，因此C++视这种行为为非法 临时量：当编译器需要一个空间来暂存表达式的求职结果时临时创建的一个未命名的对象 double dval = 3.14; const int &amp;ri = dval;//编译器会将代码改成： const int tmp = dval; //tmp是临时量 const int &amp;r1 = tmp 顶层、底层const 对常量的引用（reference to const）:引用一个常量 常量指针（const pointer）:指针本身是一个常量 意味着必须初始化 顶层（ top-level ）const: 表示任意的对象是一个常量，这对任何数据结构都适用 算术类型、类、指针等 底层（low-level） const: 与指针、引用等复合类型的基本类型（base type）部分有关 指针既可以是底层又可以是顶层 当执行拷贝时，顶层const不受影响，而拷入和拷出对象必须有相同的底层const之歌，或者两个对象的数据类型必须能够转换 constexpr 常量表达式：值不会改变且在编译期间就能得到计算结果的表达式 字面值、用常量表达式初始化的const对象... 一个对象（or 表达式）是不是constexpr由其数据类型和初始值共同决定 初始值为constexpr, 数据类型为const const int max_files = 20;const int limit = max_files + 1;//以下两个不是int staff_size = 27; //数据类型不是constconst int sz = get_size(); // 值直到运行期才能获取 允许将变量声明为constexpr 以便由编译器检查其值是否为constexpr. 声明为constexpr的变量一定是一个常量，且必须用constexpr初始化 尽管引用和指针都能定义成constexpr，它们的初始值却受到严格限制。一个constexpr指针的初始值必须nullptr 或0， 或者是存储与某个固定地址的对象 constexpr指针的constexpr只对指针有效，与指针所指对象无关。也就是说，constexpr把它所定义的对象置为了顶层const auto auto让编译器通过初始值来推断变量的类型，因此，auto定义的变量必须有初始值 使用引用实际上是使用引用的对象，因此引用类型被作为初始值时，真正参与初始化的是引用对象的值。此时auto的类型是引用对象的类型 auto忽略顶层const, 保留底层const int i = 0;const int ci=i, &amp;cr=ci;auto b = ci; //b是整数（ ci的顶层const被忽略 ）auto c = cr; //c是正是整数（ cr是ci的别名，后者是顶层const ）auto d = &amp;i; //d是整形指针auto e = &amp;ci; //e是底层const（ 对常数对象取地址是一种底层const ） 如果希望推断出类型是顶层const，需要明确指出 const auto f =ci decltype 选择并返回操作数的数据类型，编译器只分析表达式得到类型，不实际计算其值 返回表达式的类型，包括顶层const和引用 引用从来都作为所指对象的同义词，只有在此处是例外 表达式 组成 operand operator others 求值 优先级 结合性 类型转换约定: 所有计算,规定的都是同类型计算. 对于混合运算,我们采取类型转换.有默认类型转换( Type Coercion , 可以查文档) , 也可以强制地改变类型转换约定( Type Casting )( 这可以让你从宽的往窄的地方转, compiler会给一个warning,结果你自己负责 ). 我们也可以自己定义各种混合类型的计算. 类型转换是按照计算顺序逐个进行的 类型转换精度: 浮点数不能精确表达整数,会有精度损失. 因此int默认转成double(一定能得到正确答案) 而不是float 求值次序 取决于Compiler,不是嘴上说说的 token:具有独立意义的最小语法单位 代码由编译器翻译为机器码,而编译器中一个重要的功能就是Optimization,因此 x = 1 + 2 + 3; 和 a = 1 + 2 ; x = a + 3 性能是一样的,所以没必要为了性能而写出很晦涩的表达式,其实性能都一样 overflow: 加法: 判断结果是否为0或者负数 减法: 看作加法 ,但是要注意补码特征的问题,如 min (10000...)取反加一之后还是 min 乘法: 也要注意补码特征问题 除法: 不能除以0. -min / -1有问题,别的都没问题 种类 注意表达式不是语句. x=1是表达式,加上分号之后x=1;才是表达式语句. 1是表达式, 1;是表达式语句 算数 关系和逻辑 赋值 逗号 字位运算符 操作符可重载 ​ 增加语言灵活性. ​ 不是所有操作符都能重载,比如逗号 ​ 重载后的操作符不能与原来语义相违背,比如&amp;&amp;重载后会失去短路效果,所以一般不重载&amp;&amp; 赋值表达式 左值 = 右值表达式 左值: 可以出现在赋值表达式左部的表达式,具有存放数据的确定地址 类型不同时,先计算右值表达式的值,再转换为左值类型,如double d = 5/2,右边会先计算出2,然后转为2.0 算术表达式 增量和简练操作符 前增量(前减量) ++a // 前增量的结果是左值 后增量(后减量) --a 提高编译后的执行效率 条件运算符表达式 &lt;exp1&gt;?&lt;exp2&gt;:&lt;exp3&gt; 唯一的三目运算符 只计算一个分量 如果和的值类型相同,且均为左值,则该条件运算符表达式为左值表达式 可嵌套 sign(x) x &gt; 0 ? 1 : x==0? 0 : -1 就近原则 逗号表达式 &lt;exp1&gt;,&lt;exp2&gt;,&lt;exp3&gt;...&lt;expn&gt; &lt;expn&gt;的值作为该逗号表达式的值 int a,b,c; d = ( a = 1 , b = a + 2 , c = b + 3);cout &lt;&lt; d &lt;&lt; endl; // 6 如&lt;expn&gt;为左值,则该逗号表达式为左值表达式 字位运算符表达式 对整型数二进制位(bit)的操作,将整型数看作二进制序列 与同一个对象异或两次,结果不变( 结合性 ) 移位运算符表达式 语句 表达式语句 IO语句 cin,cout &gt;&gt;,&lt;&lt;可重载 . 现在它们变成操作符了. cin&lt;&lt;x是一个表达式 控制流语句 顺序,选择,重复 switch是顺序执行的，所以写成case 1: ... ; case 2:... ;并没有什么意义,因为1和2只是个标记,编译器看不懂, 要是写成case 2:... ;case 1: ... ,就会先执行2,再执行1. switch的实现与优化: Table_Driven. 实现一个哈希表(用枚举),这是一个指针数组。实际上value（就是每条case的地址）是不连续的。但是数组中顺序地存了指向它们的指针，所以逻辑上是连续的。比如两条case的代码地址分别是08 04 87 f5 08 04 87 f5,这是不连续的,设数组起始地址为08 04 89 e4,它们分别是前二个元素,那么数组元素的地址分别为08 04 89 e4 08 04 89 e8,我们访问数组的元素,就能连续地间接访问case的代码. 表驱动的思想在很多地方都有. Framework,在C++中是用宏完成的(因为这属于编辑edit的工作),后来为了简便,用IDE封装了操作 函数也是用表驱动的,每个函数都在表里面.注意实现取决于编译器.C的编译器不支持重载,所以void f(int)和void f()在表中是一个东西,没法作出区分; C++编译器生成的表是把函数签名(而非单纯的函数名)作为表项的(要经过一系列转换),所以上述两个函数是两个不同的表项(签名不同),也就是支持重载. 程序中的指令放在EIP里. 这是有意义的,因为无论是代码还是数据还是堆区栈区其实都是二进制的,都可以被机器执行,有些恶意代码可以跳转到栈区去,让程序崩溃.因此用了EIP,不在其中的指令不能执行,后来有了偷代码的攻击,于是我们把R+X也封了,这样小偷就不知道要偷那里的代码. 函数 原则 定义不允许嵌套 先定义后使用 函数的执行 建立被调用函数的栈空间 参数传递 值传递( call by value ) 引用传递( call by reference): reference是别名,所以改变宽宽当然会改变陆昱宽 保存调用函数的运行状态 将控制转交被调函数 所有局部变量一定要初始化才能用,因为栈区不会清零 函数原型 遵循先定义后使用原则 自由安排函数定义位置 语句 只需参数类型,无需参数名称 编译器检查 函数-重载 原则 名同,参数不同(个数,类型,顺序) 返回值类型不作为区别重载函数的依据 匹配原则 严格 内部转换 用户定义的转换 void f(long); void f(double) 当产生问题的时候,有可能你的程序逻辑没问题,而是你的相互使用的库有问题 String String的相关操作(0) --- IO 读入，以空白字符或 EOF 作为结束标志 cin &gt;&gt; s; 读入一行，以换行符或指定的字符作为结束标志，丢弃定界符（delimiter） getline(cin, s); // 以换行符为结束标志getline(cin, s, &#x27;,&#x27;); // 以 , 为结束标志 String的相关操作(1) --- 长度: str.size() 和 str.length()，含义相同 str.capacity() 表示分配的存储空间的大小 str.empty() 判断 str 是否为空字符串 &quot;&quot; string 的相关操作 (2) – 获取 char str[index] 0 ≤ index ≤ str.length() index == str.length() 返回末尾的 \\0，不应该修改！ str.at(int index) 0 ≤ index ＜ str.length() str.front() str.back() string 的相关操作 (3) – 连接 s1 = s2 + s3 s1.append(s2) 或 s1 += s2 string 的相关操作 (4) – 其他 查找 str.find(&quot;ab&quot;); // 从前向后的第一个 ab str.find(&quot;ab&quot;, 2); // 从下标 2 开始的第一个 ab str.rfind(&quot;ab&quot;); // 从后向前的第一个 ab str.rfind(&quot;ab&quot;, 2); // 从下标 2 开始从后向前第一次找到 ab 如果找不到，会返回 string::npos string 的相关操作 (5) – 与数值互转 子串 string s2 = s.substr(pos, n); // 与 Java **不同：**从 pos 开始取 n 个字符 比较 &lt;、&lt;=、&gt;、&gt;=、==、!= s1.compare(s2) 相等时返回 0；s1 &lt; s2 时返回 -1；s1 &gt; s2 时返回 1 字符串转换为 int int v = std::stoi(str); 字符串转换为 long、long long、float 和 double 分别为 stol、stoll、stof 和 stod #include &lt;iostream&gt; #include &lt;sstream&gt; int main() &#123; std::string str = &quot;668&quot;; int num = 0; std::istringstream ss(str); ss &gt;&gt; num; std::cout &lt;&lt; num; return 0; &#125; * * ```C++ #include &lt;iostream&gt; #include &lt;stdlib.h&gt; int main() &#123; std::string str = &quot;668&quot;; std::cout &lt;&lt; atoi(str.c_str()); return 0; &#125; 数值转换为字符串 string s = std::to_string(42); string 的相关操作 (6) – split std::vector&lt;std::string&gt; split(const std::string &amp;s, const char delimiter) &#123; std::vector&lt;std::string&gt; ans; std::istringstream iss(s); std::string token; while (std::getline(iss, token, delimiter)) &#123; ans.push_back(token); &#125; return ans;&#125; std::vector&lt;std::string&gt; split(const std::string &amp;s, const std::string &amp;delim) &#123; std::vector&lt;std::string&gt; ans; int begin = 0, end = std::string::npos; do &#123; int end = s.find(delim, begin); if (end != std::string::npos) &#123; ans.push_back(s.substr(begin)); &#125; else &#123; ans.push_back(s.substr(begin, end - begin)); begin = end + delim.length(); &#125; &#125; while (end != std::string::pos); return ans;&#125; 注意! 跟 Java 不同，C++ 的 string 几乎是一个字节容器 string s = &quot;中国&quot;;cout &lt;&lt; s.length() &lt;&lt; endl; 输出是4,因为里面是四个字节 '\\0'会特殊对待 char bytes[] = &#123; &#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;\\0’, &#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;&#125;;string s(bytes, 0, 7);cout &lt;&lt; s.length() &lt;&lt; endl;cout &lt;&lt; s &lt;&lt; endl; 长度是3,输出是abc,也就是遇到\\0会截断 程序组织 逻辑结构 物理结构 多个源文件组成 main唯一 工程文件 外部文件 外部变量 namespace 编译预处理 与作用域, 类型, 接口等概念格格不入 潜伏于环境 穿透于作用域 #include make interface definitions available compose source text #define Symbolic constants const Open subroutines inline Generic subroutines template //这三种复用都是源代码层次的，即 可以看到细节的 Generic &quot;types&quot; template Renaming namespace // 以上五点，C++中已经有了替换的方法，而下面三点，C++中还没有合适的替代方法： String concatenation Special purpose syntax General macro processing # define MAX(x,y) x &gt;= y ? x : y //不加括号的后果12 * MAX(2,3) 12 * 2 &gt;= 3 ? 2 : 3# define MIN(x,y) ((x) &lt; (y) ? (x) : ( y));//这样可以实现泛型，前提是类类里面重载了“&lt;”运算符 缺点： 重复计算，因为宏不是函数 没有类型检查，太宽松了 注意： #define ADD(x,y) &#123;x+y;&#125;int x=0,y=1;if( x == 0 ) ADD(x,y); //会报错,因为花括号后面有分号else cout &lt;&lt; &quot;HI&quot;;//但如果写成:#define ADD(x,y) x+y;cout &lt;&lt; ADD(x+y) &lt;&lt; endl; //会报错//只能写成:#define ADD(x,y) do&#123; x + y ; &#125; while(0); 选择性编译 version control # ifndef MY_PRINT_VERSION #define MY_PRINT_VERSION 1# endif# if MY_PRINTF_VERSION == 1void printf( char *str )&#123; ... &#125;#elif MY_PRINTF_VERSION == 2int printf( char *fmt , char *args,... )&#123; ... &#125;#endif Commenting out code #pragma Control of layout Informing the compiler 数组 特征 相同类型 连续存储 一维数组 类型定义 函数接口 元素个数须通过参数显式给出, 不能通过sizeof取得. 传入的len不能保证真的是数组长度,C++是允许数组越界的 字符串 void f( int a[16] )&#123; int len = sizeof(a); //这是错误的&#125;//必须写成:void f( int a[16] , int size) char s1[] = &quot;abc&quot;; // == char s1 = &#123;&#x27;a&#x27;,&#x27;b&#x27;, &#x27;c&#x27;,&#x27;\\0&#x27;&#125;char s2[] = &#123;&#x27;a&#x27;,&#x27;b&#x27;, &#x27;c&#x27; &#125;; //找到arr中的第一个负数#include&lt;iterator&gt;int arr[] = &#123;0,1,2,3,4,5.-1&#125;;int *pbeg = begin(arr), *pend = end(arr);while( pbeg!= pend &amp;&amp; *pend &gt;= 0) ++pbeg; 删除所有数组所有元素 int remove(int * arr, int target, int n)&#123; int front = 0, back = 0, targetCnt = 0; for(; back &lt; n; back++)&#123; if(arr[back] == target)&#123; targetCnt++; &#125;else&#123; arr[front] = arr[back]; front++; &#125; &#125; return targetCnt;&#125;int main(int argc, char const *argv[])&#123; int a1[] = &#123;1, 2, 3, 4, 0, 6, 6, 6, 7, 8, 6, 4, 5&#125;; int l1 = 1; l1 -= remove(a1, 6, l1); for(int i = 0; i &lt; l1; i++)&#123; std::cout &lt;&lt; a1[i] &lt;&lt; &quot; &quot;; &#125; std::cout &lt;&lt; std::endl; system(&quot;pause&quot;); return 0;&#125; 多维数组 定义 存储组织,C++中一定要知道数组的layout(内存布局). C++中的多维数组只是一维数组的不断迭代,和java中的容器等是不一样的 参数传递 缺省第一维( C++里没有ragged array) int arr[][3] = &#123;&#123;1&#125; , &#123;2,3&#125; , &#123;4,5,6&#125; &#125;cout &lt;&lt; arr[0][1] &lt;&lt; endl; //java中这会报错,但是C++中这是允许的,会得到一个未定义的值. 1的后面是两个未定义的值,再往后是2,3和一个未定义的值,然后是4,5,6. 也就是说真的有3 * 3 = 9 个元素. 这和java中的ragged array不同. 升/降维处理 Struct 赋值 --- 同类型 alignment. 契合硬件,提升效率 参数传递 struct B&#123; char b; //1 int a; //4 short c; //2&#125;;cout &lt;&lt; sizeof(B) &lt;&lt; endl; //答案是12,因为会对齐 union 共享存储空间 union C&#123; char b; //1 int a; //4 short c; //2&#125;;cout &lt;&lt; sizeof(C) &lt;&lt; endl; //答案是4 union Matrix&#123; struct &#123; double a11, a12, a13; double a21, a22, a23; double a31, a32, a33; &#125;; double _element[3][3];&#125;; Matrix m; int i, j; for (i = 0; i &lt; 3; i++) for (j = 0; j &lt; 3; j++) m._element[i][j] = (i + 1) * (j + 1); m.a11 = 0; m.a22 = 0; m.a33 = 0; for (i = 0; i &lt; 3; i++) for (j = 0; j &lt; 3; j++) cout &lt;&lt; m._element[i][j] &lt;&lt; endl; 指针 管理地址信息 调用代码 指针定义与基本操作 用typedef 定义一个指针类型 typedef int* Pointer; pointer p,q // p, q均为指针变量 注意: 指针其实不是一个类型,所以 int *p,q中, q不是指针 赋值 int *p = (int*) 0xKN231232 现在没人用这个了 操作符 &amp;取地址 *间接取内容 注意: C++中, NULL是0; ANSI C: #define NULL( (void*) 0 ) (用 ((void*)0)来置换NULL,这种方式有很大漏洞 ) C++: #define NULL 0 ( 即用 0 去置换NULL ) 空指针并不一定用与整数0同样的二进制模式表示,可由实现者采用任何选定的方式表示 现在空指针都用nullptr赋值 同类型指针比较 输出 特例: 原因是操作符重载 char *p = &quot;ABCD&quot;;cout &lt;&lt; p; //p指向的字符串,即&quot;ABCD&quot;cout &lt;&lt; *p; // p指向的字符,即&#x27;A&#x27;//对于int*之类的就不会这样 例: 将某块内存清零 void memset( void *pointer, unsigned size )&#123; char *p = (char*) pointer;//必须要类型转换,不然p是void的指针,无法进行操作 for( int k = 0 ; k &lt; size ; k++ ) *p++ = 0; &#125; 常量指针与指针常量 常量指针, 即 指针指向一个常量. ( 也可以指向一个变量, 但指针不知道.所以常量指针就是一个只读的指针,可以消除函数副作用 ) const int c = 0;const int *cp;int y = 1;int *q;cp = &amp;c;q = &amp;y;*cp = 1; // 不行,指针还没有绑定*q = 2;cp = &amp;y; //可以,常量指针可以指向一个变量,这样的用法非常常见,不能通过指针来修改y,只能用y来修改yq = &amp;c;// 不可以. 变量指针不能指向一个常量//如果硬要让变量指针指向常量,可以用 const_cast&lt;int&gt;,这种方法不太好://假设你有一个 const A a = 8; 你需要把a传入一个 // void fun( A *x) 中,如果你确保fun不会更改a的话,那可以在传参的时候写 fun( const_cast&lt;A *&gt;&amp;a ).//这是在函数给定的情况下的被逼无耐之举, 有条件的话,应该把fun写成 void fun( const A *x)// 没事干不要用 const_cast&lt;&gt; 常量替换在编译期间发生,类似于宏替换. const int a = 5;int *p = (int *)&amp;a; //让p指向与a相同的内存空间cout &lt;&lt; &amp;a &lt;&lt; &quot;, &quot; &lt;&lt; p &lt;&lt; endl;*p = 10; // 照说a的值也应该改变，实际却没有,这就是常量折叠.cout &lt;&lt; a &lt;&lt; &quot;, &quot; &lt;&lt; *p &lt;&lt; endl;// 这个&quot;常量折叠&quot;就是在编译器进行语法分析的时候，将常量表达式计算求值，并用求得的值来替换表达式，放入常量表。可以算作一种编译优化。// 我只是改了这个地址内容,但是a还是5,// 因为编译器在优化的过程中，会把碰见的const全部以内容替换掉// （跟宏似的: #define PI 3.1415,用到PI时就用.1415代替），// 这个出现在预编译阶段；但是在运行阶段，它的内存里存的东西确实改变了!!!// 简单来说就是, 常量a的内存空间里面的内容在运行期间会被更改为10, 但是编译器在预编译阶段,已经用a的值(这个时候a的值还是5) 来替换a这个名字,类似于宏替换. 所以a这个名字的值永远是5,但它代表的内存空间的值可能被改变.输出是:0x61fe14, 0x61fe145, 10 常量指针,就是说指针本身是一个常量, 因此必须在定义的时候初始化. &lt;类型&gt; * const &lt;指针变量&gt; 数组的最后一个元素的后一个元素的指针是空指针nullptr 指针数组 main函数: int main( int argc, char* argv[], char* env[] )&#123; cout &lt;&lt; argc &lt;&lt; endl; for( int i=0; i != argc ; i++ ) &#123; cout &lt;&lt; argv[i] &lt;&lt; endl; &#125; cout &lt;&lt; env &lt;&lt; endl; return 0; &#125; argc:参数个数 argv:命令行参数 env:环境参数 多级指针 编写一个函数，交换两个字符串 int main( int argc, char* argv[], char* env[] )&#123; char *p1 = &quot;abcd&quot;; char *p2 = &quot;1234&quot;; cout &lt;&lt; p1 &lt;&lt; &quot; &quot; &lt;&lt; p2 &lt;&lt; endl; swap( &amp;p1, &amp;p2 ); cout &lt;&lt; p1 &lt;&lt; &quot; &quot; &lt;&lt; p2 &lt;&lt; endl; return 0; &#125; 动态变量 申请(当然是堆区) new &lt;类型名&gt; new &lt;类型名&gt; [ &lt;整形表达式&gt; ] new和malloc都能申请堆区内存. 当为类的对象分配内存时,new会执行构造函数,而malloc只会申请内存,不会执行构造函数. 因此C++中尽量用new而不是malloc 申请内存也有可能失败. 因此对于内存申请,一定要判断是否成功,即异常处理 归还 操作符 new --- delete 会调用析构函数 delete [] 申请的指针,不要改变它的值,这是因为申请空间使用的是cookie的方式. 如果硬要改变,那得创建一个副本,然后更改副本. 总之,不能更改申请的指针","categories":[{"name":"Language","slug":"Language","permalink":"http://lyk-love.cn/categories/Language/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://lyk-love.cn/tags/C/"}]},{"title":"C++ I/O","slug":"C++ IO","date":"2022-09-26T06:39:34.925Z","updated":"2022-09-26T06:39:34.925Z","comments":true,"path":"2022/09/26/C++ IO/","link":"","permalink":"http://lyk-love.cn/2022/09/26/C++%20IO/","excerpt":"Outline： IO 控制台对象 文件流 字符串流 重定向","text":"Outline： IO 控制台对象 文件流 字符串流 重定向 IO 类的层次结构： 层次结构的顶部是ios类，它充当虚基类。不能被实例化 其他类： 控制台类：istream,ostream,iostream 它们用于将程序连接到控制台 文件流：ifstream,ofstream,fstream 用于将程序连接到文件 字符串流：istringstream,ostringstream,stringstream C++ 标准输入输出包含在头文件 中，使用输入输出流库需要引入此头文件 标准库中有 4 个 I/O 相关对象： 处理输入的 istream对象 cin 处理输出的 ostream 对象 cout 另外两个 ostream 对象 cerr 和 clog 也可以通过引入头文件 &lt;cstdio&gt; 或 &lt;stdio.h&gt; 使用 printf 和 scanf 控制台对象 istream对象： cin istream类无法实例化，但是系统已经创建了此类的一个名为cin的对象，存储中 流提取符 &gt;&gt; ,以空白字符或输入结束字符作为终止 输入结束（End-Of-File，EOF）字符：在 Windows 的命令行中，用 Ctrl + Z 表示，在类 UNIX 系统的命令行中，用 Ctrl (Command) + D 表示 读入一个字符 char ch ;cin.get(ch); 放回一个字符,可能会有问题 cin.unget(); 删除连续的空白字符: cin &gt;&gt; std::ws; 忽略一行中剩余的字符，丢弃定界符（delimiter） cin.ignore( std::numeric_limits&lt;std::streamsize&gt;::max(), &#x27;\\n&#x27;); 示例: 输入十个数字: int nums[10];for( int i = 0 ; i &lt; 10 ; i++ )&#123; cin &gt;&gt; nums[i];&#125; 输入未知个数的数字并求和: int sum = 0;while( cin &gt;&gt; num )&#123;sum += num;&#125; int stoi(string) : 把字符串转换成整数 用gets()输入字符数组 （方便，常用） char ch[100];gets(ch); 读入一个完整的行（从标准输入读，一直读到遇到换行符），把读到的内容存入括号中指定的字符数组里，并用空字符’\\0’取代行尾的换行符’\\n’。读入时不需要考虑换行符。 ostream对象： cout、cerr和clog ostream类无法实例化，但是系统已经创建了此类的三个对象：cout、cerr和clog，存储于 cout绑定到cin,意味着每次通过cin输入数据时，cout都会被刷新（清空） cerr和clog都被设计为向console发送错误，但cerr每次操作后立即刷新其内容，clog手机错误消息，当程序终止或显示刷新时才刷新 流插入符&lt;&lt; 文件流 &lt; fstream &gt; 要在C++进行文件处理,需要引入头文件&lt;fstream&gt; &lt; ifstream &gt; 读取文件 ifstream fin(file_path);if (!fin.is_open()) &#123; cerr &lt;&lt; &quot;file not found&quot; &lt;&lt; endl;&#125; ifstream 的构造函数还可以传入一个 mode 参数，包括但不限于（不同的 mode 可以用按位或运算符 | 组合在一起）： ios_base::binary 以二进制方式读取文件 ios_base::app 在文件末尾追加 ios_base::trunc 丢弃文件中原有的内容 &lt; ofstream &gt; 写入文件 ofstream fout(file_path);if (!fin.is_open()) &#123; cerr &lt;&lt; &quot;file not found&quot; &lt;&lt; endl;&#125; ofstream 的构造函数还可以传入一个 mode 参数，包括但不限于（不同的 mode 可以用按位或运算符 | 组合在一起） ： ios_base::app 在文件末尾追加 ios_base::trunc 丢弃文件中原有的内容 &lt; fstream &gt; 兼具 ifstream 和 ofstream 的功能 字符串流 使用 string 需要引入头文件 字符串流在&lt;sstream&gt; 跟 Java 不同，string 是可修改内容的 重定向 rdbuf() rdbuf() 函数定义在&lt;ios&gt;头文件中，专门用于实现 C++ 输入输出流的重定向。 值得一提的是，ios 作为 istream 和 ostream 类的基类，rdbuf() 函数也被继承，因此 cin 和 cout 可以直接调用该函数实现重定向。 rdbuf() 函数的语法格式有 2 种，分别为： streambuf * rdbuf() const;streambuf * rdbuf(streambuf * sb); streambuf 是 C++ 标准库中用于表示缓冲区的类，该类的指针对象用于代指某个具体的流缓冲区。 其中，第一种语法格式仅是返回一个指向当前流缓冲区的指针；第二种语法格式用于将 sb 指向的缓冲区设置为当前流的新缓冲区，并返回一个指向旧缓冲区的对象。 举个例子： #include &lt;iostream&gt;#include &lt;fstream&gt;using namespace std;int main()&#123; //打开 in.txt 文件，等待读取 ifstream fin(&quot;in.txt&quot;); //打开 out.txt 文件，等待写入 ofstream fout(&quot;out.txt&quot;); streambuf *oldcin; streambuf *oldcout; char a[100]; //用 rdbuf() 重新定向，返回旧输入流缓冲区指针 oldcin = cin.rdbuf(fin.rdbuf()); //从input.txt文件读入 cin &gt;&gt; a; //用 rdbuf() 重新定向，返回旧输出流缓冲区指针 oldcout = cout.rdbuf(fout.rdbuf()); //写入 out.txt cout &lt;&lt; a &lt;&lt; endl; //还原标准输入输出流 cin.rdbuf(oldcin); // 恢复键盘输入 cout.rdbuf(oldcout); //恢复屏幕输出 //打开的文件，最终需要手动关闭 fin.close(); fout.close(); return 0;&#125; 仍以前面创建好的 in.txt 文件为例，执行此程序后，控制台不会输出任何数据，而是会在该项目的目录下生成一个 out.txt 文件，其中就存有该程序的执行结果： C++ http://c.biancheng.net/cplus/ 命名空间 cin 和 cout 是 C++ 标准库内置对象而不是关键字 标准库的所有标识符都在命名空间 std 中 using namespace std; // 直接使用 cin、coutusing std::cin; // 直接使用 cin、cout，而来自标准库的其他符号需要加上 std:: 前缀using std::cout; 格式化 cout格式化输出 需要引入头文件 示例: 输出不同进制 cout &lt;&lt; showbase &lt;&lt; hex &lt;&lt; 26 &lt;&lt; &#x27; &#x27; &lt;&lt; oct &lt;&lt; 26;// 输出：0x1a 032 浮点数输出指定精度 cout &lt;&lt; setprecision(5) &lt;&lt; 3.1415926535;// 输出：3.1416 输出指定宽度,右对齐 cout &lt;&lt; setw(6) &lt;&lt; right &lt;&lt; 10; 输出年月日 int year = 2021, month = 3, day = 26;cout &lt;&lt; year &lt;&lt; &#x27;-&#x27; &lt;&lt; setw(2) &lt;&lt; setfill(&#x27;0&#x27;) &lt;&lt; month &lt;&lt; &#x27;-&#x27; &lt;&lt; setw(2) &lt;&lt; setfill(&#x27;0&#x27;) &lt;&lt; day;// 输出：2021-03-26 资源和工具 •资源 •https://en.cppreference.com/w/ •https://zh.cppreference.com/w/首页 •工具 •http://cpp.sh/","categories":[{"name":"Language","slug":"Language","permalink":"http://lyk-love.cn/categories/Language/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://lyk-love.cn/tags/C/"}]},{"title":"Automated Testing","slug":" Automated Testing","date":"2022-09-26T06:39:34.924Z","updated":"2022-09-26T06:39:34.924Z","comments":true,"path":"2022/09/26/ Automated Testing/","link":"","permalink":"http://lyk-love.cn/2022/09/26/%20Automated%20Testing/","excerpt":"Outline： 源码测试 移动应用 AI测试 Thanks to 191250025 and other classmates This document is used to the Final Review of this course","text":"Outline： 源码测试 移动应用 AI测试 Thanks to 191250025 and other classmates This document is used to the Final Review of this course 源码测试 回归测试 基本概念 版本迭代后，重新测试用例 部分代码修改会影响接口，导致测试用例失效。 新需求需要新用例 可有效保证代码修改的正确性并避免代码修改对被测程序其他模块产生的副作用。 优化方法 测试用例优先级（Test Case Prioritization，TCP） 测试用例集约减（Test Suite Reduction，TSR） 不考，没记 测试用例选择（Test Case Selection，TCS） 差分测试 通过将同一测试用例运行到一系列相似功能的应用中观察执行差异来检测bug。(对拍) 蜕变测试 依据被测软件的领域知识和软件的实现方法建立的蜕变关系来生成新的测试用例,通过验证蜕变关系是否被保持来决定测试是否通过。( $sin x=sin(\\pi-x)$ ) 蜕变关系(Metamorphic Relation, MR) 是指多次执行目标程序时,输入与输出之间期望遵循的关系 变异测试 通过定义好的变异操作来对源码进行修改,以此来帮助测试者定位测试数据的弱点,避免执行测试的弱点 测试用例优先级 简介： 依照某种策略赋予每个测试用例的不同优先级,以提高测试套件的故障检测率。 测试优先级排序技术采用特定的(启发式)算法计划测试用例,使得优先级较高的用例能够先于优先级低的用例执行。 类型： 通用测试排序:为有利于后续版本的测试用例赋予更高的优先级。 特定于版本的测试排序:根据不同软件版本的特性为测试用例分配优先级。 主要算法的流程及复杂度 基于贪心的TCP 全局贪心策略 每轮优先挑选覆盖最多代码单元的测试用例。 多个用例相同随机选择。 增量贪心策略 每轮优先挑选覆盖最多，且未被已选择用例覆盖代码单元的测试用例。 所有代码单元均已被覆盖则重置排序过程 多个用例相同随机选择 基于相似性的TCP 基本定义：每轮优先与已选择测试用例集差异性最大的测试用例。让测试用例均匀地分布在输入域中。 （类似PRIM算法） 排序步骤： 测试用例之间的距离计算：假设$U(t_1)$ 和 $U(t_2)$为测试用例$t_1$和$t_2$所覆盖的代码单元集合，那么这两个用例之间的距离计算如下： $$ Jaccard(t_1,t_2)=1 - \\frac { | U(t_1) \\cap U(t_2)|}{| U(t_1) \\cup U(t2) |} $$ 用例与测试用例集之间的距离计算：分别使用最小距离、平均距离和最大距离度量方式计算待选择用例$t_c$与已选择用例集S的距离： $$ D(tc,S)=\\left{ \\begin{aligned} \\max \\left{ \\min\\limits_{0\\leq i \\leq |S|} \\left{ Jaccard(t_c, t_i)\\right} \\right} \\ \\max \\left{ \\underset{0 \\leq i \\leq|S|}{\\operatorname{avg}} \\left{ Jaccard(t_c, t_i)\\right} \\right} \\ \\max \\left{ \\max\\limits_{0\\leq i \\leq |S|} \\left{ Jaccard(t_c, t_i)\\right} \\right} \\ \\end{aligned} \\right. $$ $t_c$是待选用例。这里是计算tc跟已选测试用例集每个ti的距离。假设已选k个用例，则有k个距离，min就是取k个距离中最小的距离作为tc到已选集（包含k个测试用例）的距离。 基于搜索的TCP 基本定义：探索用例排序组合的状态空间，以此找到检测错误更快的用例序列。 排序步骤： 种群构造：生成N个测试用例序列，之后随机生成切割点，互相交换两个用例序列切割点后部分的片段，仅交换相同测试用例的部分；同时以一定概率选择测试用例，并随机生成两个测试用例位置，进行互换，产生新的测试用例序列。 评估值计算：以语句覆盖为例，给定程序包含m个语句$M = {s_1,s_2,...,s_m}$和n个测试用例$T={t_1,t_2,...,t_n}$，$T'$为某一次搜索中$T$的一个排序序列，$TS_i$为该测试用例序列$T'$中第一个覆盖语句$s_i$的测试用例下标，那么其适应度计算为： $$ APSC = 1 - \\frac {TF_1+TF_2+ \\dots + TF_m}{n * m} + \\frac {1}{2n} $$ 基于机器学习的TCP策略 基本定义：基于测试分布特征，预测表现最佳的排序技术。 排序步骤： 测试分布特征提取：给定被测程序，提取每个测试用例覆盖单元数；执行时间与单元时间内覆盖单元数。 模型生成：由于三种特征取值范围不同，使用min-max正则化，最后使用XGBoost学习特征进行预测。 APFD计算 APFD：(Average Percentage of Faults Detected )平均故障检测百分比 给定程序包含m个故障$F={f_1,f_2,...,f_m}$和n个测试用例$T={t_1,t_2,...t_n}$，$T'$为$T$的一个排序序列，$TF_i$为该测试用例序列$T'$中第一个检测到故障$f_i$ 的测试用例下标，则该排序序列$T'$的APFD值计算公式为 $$ APFD = 1 - \\frac {TF_1+TF_2+ \\dots + TF_m}{n * m} + \\frac {1}{2n} $$ 测试用例选择 简介： 回归测试用例选择可以通过重新运行原始测试套件的一个子集，验证某些变更是否对当前软件版本的功能造成了影响。 优点： 降低回归测试的开销 最大化缺陷探测能力 流程： 给定修改前的程序$P$，对应的测试用例集$T$，和修改后的程序$P'$ 寻找$T$的子集$T'$对$P'$进行测试，并且$T'$中的任意测试用例均是可以检测代码修改的测试用例。 主要方法 最小化测试用例选择 从$T$中找出最小的子集$T_{min}$，$T_{min}$能够覆盖$P$中所有本次修改的、或者受本次修改影响的部分。 每一条新增的或者被修改的语句都能够被至少一个来自$T$的测试用例执行。 安全测试用例选择 从$T$中选出能够暴露$P'$中的一个或多个缺陷的所有测试用例，构成安全回归测试集$T_S$ $T_S$中的每个测试都能够满足以下条件之一 执行至少一条在$P'$中被删除的语句 执行至少一条在$P'$中新增的语句 基于数据流和覆盖的测试用例选择 变更后的代码$P'$中使数据交互变化的语句构成语句集合$S_I$。从原本的测试用例集$T$中选取出所有覆盖到$S_I$中某条语句的测试用例，组成测试集$T_D$ $T_D$中的每个测试都能够满足以下条件之一 执行至少一个在$P'$中被删除的Define-Use对 执行至少一个在$P'$中新增的Define-Use对 特制/随机测试用例选择 规定测试用例的选取数量为m，测试人员随机地从原本的测试用例集$T$中选出m个测试用例，组成随机回归测试集$T_R$ 面向剖面测试用例选择：与AOP有关，从$T$中选出与某个剖面a有关的测试用例k，组成回归测试集$T_a$ 基于程序分析的测试用例选择 简介： 通过程序分析技术计算出测试代码（方法、用例或套件）与生产代码之间的依赖关系，并在后者发生变更时，利用这些依赖关系将所有受到变更影响的测试代码（Change-Impacted Tests）自动选取出来，组成回归测试集 一般被认为是一种安全测试用例选择技术 分类：静态，动态 粒度： 基本块级、方法级(细) 类级、项目级(粗) 阶段: A Phase –分析阶段:分析代码变更、计算测试依赖 E Phase –执行阶段:运行选中测试 C Phase –收集阶段:收集测试信息 静态 在没有实际执行程序的情况下对计算机软件程序进行自动化分析的技术（手动分析一般被称为程序理解或代码审查）。 大多数情况下，分析的材料为源语言代码，少部分静态分析会针对目标语言代码进行 例如：分析Java的字节码 动态 通过在真实或虚拟处理器上执行程序来完成对程序行为的分析。 为了使动态程序分析有效，必须使用足够的测试输入来执行目标程序，以尽可能覆盖程序所有的输出。进行动态分析时一般需要注意最小化插桩对目标程序的影响 类的防火墙算法 假设在继承层级(Inheritance Hierarchy)中,类A是类B的子类。当有且仅有B发生变动时,为了保证 测试充分,除B之外,A也应该重新进行单元测试。 假设在聚合层级(Aggregation Hierarchy)中,类A是类B的一个聚合类。当有且仅有B发生变动时,为 了保证测试充分,除B之外,A也应该重新进行单元测试。 假设在关联层级(Association Hierarchy)中,类A与类B的关系满足下列条件之一: 类A访问了类B的数据成员; 类A需要向B传递信息 (简言之: A依赖B) 当B发生变动时,为了保证测试充分,除B之外,A也应该重新进行测试。同时,类A还应该与类B进行重新集成。 动态 vs 静态 特征 动态 静态 总体 好 依赖信息 多 开销 小 过拟合 有 插桩 需要 不需要 运行测试阶段 好 测试用例优先级 V.S. 测试用例选择 优先级技术是对测试用例集进行排序,以最快的速度找到缺陷,提高测试用例集的故障检测率。 选择技术是取测试用例集的子集,能覆盖修改过的代码,降低回归测试的开销并最大化缺陷探测能力 移动应用 基于图像理解的移动应用自动化测试 各个任务的难点 测试输入生成 任务难点: 运行环境多样化:网络多样化、平台多样化、设备多样化 环境碎片化 开发平台快速演化 需要考虑系统级事件 能够论述各个任务的解决方法 ( 只针对测试输入生成 ) 基于随机:Monkey 基于模型:Stout 基于机器学习:Q-testing 核心思想 测试输入生成即针对给定应用进行测试探索 移动应用结构可以抽象为图结构 方法步骤 (Stout) 模型构建 UI页面层次结构 – 事件识别 随机有限状态机(动态分析) 系统事件分析 事件执行频率 – 初始概率 模型变异、测试生成与执行 随机变异事件转换概率 基于概率生成测试 随机注入系统事件 测试执行 输出度量(覆盖率) Gibbs取样(是否继续执行) 缺陷诊断 基于群智协同的众包测试 众包测试树 众包的难点 大量重复报告 大量不完善报告 测试力度分布不均 不能充分利用用户合作,验证问题 整理归类报告困难,相似报告分散,浪费时间精力 审核人员不能专注于质量控制,交付的报告质量较低 基本机制 协同推荐：众测系统上发布测试任务（任务发布者） 质量保证：众包工人参与测试任务，协作方式完成BUG报告（众包工人） 聚合交付：识别并剔除恶意众包工人，审核报告并交付缺陷列表（管理者） 解决方法 协同推荐 信息共享,用户提交报告时进行实时相似报告推荐,避免重复报告提交 任务分配:用户在提交报告后对其进行审核报告推荐和测试页面推荐。 协作方式:用户对于相似报告和审核报告的结果,可以点赞点踩,验证报告有效性。 质量保证 众包工人: 竞争式提交(独立测试提交报告,贡献归个人所有)、 协作式提交(借鉴他人报告,修改他人报告后生成子报告)、 审核(对他人报告点赞点踩) 质量控制系统: bug报告有效性模块 bug报告自动评估模块 反馈与监控模块 bug报告审核模块 聚合交付 过程： 聚合阶段： 相似报告聚合 融合阶段 ：融合相似报告，提高可读性 围绕报告融合构建审核业务流程：使审核人员聚焦质量控制 目标： 构建一套基于报告融合的测试报告处理流程 AI测试 AI测试概述 与传统测试的区别 决策逻辑: 传统软件：程序代码控制决策逻辑 智能软件:深度学习模型的结构、训练后得到的权重节点 系统程序特征: 传统软件：控制流和数据流构建的业务处理 智能软件:数据驱动构建的参数化数值计算 智能软件系统的缺陷往往不是显式的代码或参数错误 测试的难点 数据量不够、低质量数据、数据分布不均、不充分测试 数据驱动的测试 需求分析 数据采集 数据标注 模型结构设计 模型训练 模型测试 模型部署 模糊测试 基本流程 通过异常的输入自动化发现待测程序缺陷 预期输入：变异数据 预期输出：断言失败、无效输入、异常崩溃、错误输出 图像扩增 通过轻微变换现有数据或创建新的合成图像来得到新数据的技术。应用领域有图像扩增、文本扩增、雷达扩增...... 目的：增加数据量、丰富数据多样性、提高模型的泛化能力。 扩增的原则 不能引入无关的数据 扩增总是基于先验知识的,对于不同的任务和场景,数据扩增的策略也会不同。 扩增后的标签保持不变 常用扩增方法 原样本扩增 单样本扩增 两阶段：找到可能包含物体的区域 -&gt; 对该区域进行分类 单阶段：图片缩放划分等分网络，并且卷积后过滤获得最后预测框。 特征合成：基于规则、基于分割确定合成区域 多样本扩增 样本级合成：标签a的特征 + 标签b的特征 = 标签a 特征级合成：将不同特征在同一张图上面重新排列组合 标签降级：变成不同的特征 医疗图像扩增的特点 图像中不同形式的细微结构可能代表某种病变 不能保证扩增的质量 不同疾病要使用不同的扩增方法 医疗图像扩增的难点 患者隐私保护,医学影像匮乏 共享临床数据困难 影像质量参差不齐 需要专家手动贴标签","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://lyk-love.cn/categories/Software-Engineering/"}],"tags":[{"name":"Software Testing","slug":"Software-Testing","permalink":"http://lyk-love.cn/tags/Software-Testing/"}]},{"title":"Big Data Tools","slug":"Big Data Tools","date":"2022-09-26T06:39:34.924Z","updated":"2022-09-26T06:39:34.924Z","comments":true,"path":"2022/09/26/Big Data Tools/","link":"","permalink":"http://lyk-love.cn/2022/09/26/Big%20Data%20Tools/","excerpt":"Outline： 集群简介 Hadoop Spark Hadoop + Spark For Manjaro Hadoop + Spark For Mac Clickhouse Flink Kafka( //TODO )","text":"Outline： 集群简介 Hadoop Spark Hadoop + Spark For Manjaro Hadoop + Spark For Mac Clickhouse Flink Kafka( //TODO ) Hadoop + Spark 集群报告 这是hadoop+spark集群搭建的报告，当然集群太卡了用不了， 所以实际做作业我采用了单机( on Mac M1 )形式hadoop 2.7.4 + spark 2.3.3 + scala 2.11.12 + jdk8 这只是hadoop+spark集群搭建的报告， 不包括Clickhouse、Flink的内容； 可以作为hadoop+spark单机搭建的参考 hadoop+spark集群搭建完毕 hadoop, spark低版本和高版本没有任何区别，只是高版本的spark，hadoop的worker(s)文件， 在低版本中名为slaves 单机和集群也没有什么区别，只是不需要配置worker的DNS了，由于maser和worker都是本机，就直接让localhost做唯一的worker。 这意味着hadoop、spark的worker文件不需要做任何更改（里面默认值就是localhost） 主机只需配置master的DNS hdfs-site.xml中的dfs.replication数量应该设为1，因为只有本机自己一个worker 集群部署 云服务器配置如下： 拥有者 ip role lyk 124.222.135.47 master sgf 81.69.174.80 slave01 lss 47.93.158.241 slave02 xmt 175.27.136.106 slave03 方便的做法是所有节点均适用root账户，hadoop安装在/usr/local，但是，严谨的做法是使用用户账户，此时为了避免权限问题，hadoop需要安装在~ 所有节点均使用用户lyk 使用/home/lyk/.bashrc（或者.zshrc）来配置环境变量 由于所有服务器都不在同一局域网，因此都采用公网通信。 实际上十分不推荐公网通信，太慢了 集群网络 对于master和slave节点，配置其DNS表(./etc/hosts )， 其中增加: 127.0.0.1 localhost //如果本来就有这条就不用加了&lt;master的内网ip&gt; master //对于master而言，这里直接填127.0.0.1&lt;slave01的公网ip&gt; slave01&lt;slave02的公网ip&gt; slave02&lt;slave03的公网ip&gt; slave03 配置master自己到自己的ssh免密登陆 这里增加master后，之后的命令会调用ssh lyk@master， 因此需要配置好本机到master（也是本机）的免密登陆， 是的，本机到本机也需要配置免密登陆！！！ 配置master对所有slave的ssh免密登陆 不需要配置shave对其他节点的免密登陆 master需要开放相应端口： hadoop: 8099 50070 9000 spark: 7077 18080 集群用户 所有节点采用lyk用户 集群环境 这里的环境是集群的环境，与我做作业时单机配置的版本不同 注意，hadoop+spark+scala的版本管理非常混乱，因此要严格按照文档里的版本来安装（要么用下面集群这套( hadoop3.2.3... )， 要么用上面单机那套（hadoop2.7.4...）） 除了jdk和scala， 其他用户软件都安装在lyk用户目录下 JDK：usr/lib/jvm/java-8-openjdk-amd64 sudo apt-get install openjdk-8-jdk scala: /usr/local/scala scala-2.12.15 hadoop: /home/lyk/hadoop hadoop-3.2.3 spark: /home/lyk/spark spark-3.1.3 spark和hadoop均是先在master上安装，并进行一些配置，最后打包发给slave spark是scala实现，hadoop是java实现，二者都运行在JVM上， 因此都可以使用JVM进程查看工具jps进行查看 环境变量 /home/lyk/.bashrc中一共需要配置如下环境变量： # JAVA_HOMEexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=./:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib# SCALA_HOMEexport SCALA_HOME=/usr/local/scalaexport PATH=$PATH:$&#123;JAVA_HOME&#125;/bin:$SCALA_HOME/bin#HADOOP_HOMEexport HADOOP_HOME=/home/lyk/hadoopexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin hadoop安装 下载hadoop安装包，传到服务器上，改名，配置环境变量 下载hadoop安装包 将hadoop-3.2.3.tar.gz拷贝到master服务器上 在master上将hadoop压缩包解压并改名 tar -zxvf hadoop-3.2.3.tar.gz -C ~cd ~mv ./hadoop-3.2.3 ./hadoop #修改文件夹名称为hadoop 添加Hadoop的环境变量 vim ~/.bashrc # 实际我用的是～/.zshrc 将以下内容添加到末尾： export HADOOP_HOME=/home/lyk/hadoopexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 查看hadoop版本 hadoop version 这一命令对所有节点都有效(包括slave)，可以查看hadoop是否被正确安装 spark安装 spark会预装scala，位于其jar文件夹内，但我没有使用内置的scala，而是自己下载了一个 注意到Spark3预装了Scala2.12， 而Spark 3.2+预装了Scala 2.13 我的集群hadoop版本是3.2.3, 与Sprak3.1.3配套， 因此就选择Sprak3.1.3, 而后者对应Scala版本是2.12 由于已经安装了hadoop，就选择安装不带hadoop的Spark，即spark-3.1.3-bin-without-hadoop.tgz 下载spark-3.1.3-bin-without-hadoop.tgz 将压缩包拷贝到master( 这里我拷贝到了/ ) 解压并改名 tar -zxvf spark-3.1.3-bin-without-hadoop.tgz -C /usr/local/ cd /home/lyk mv ./spark-3.1.2-bin-without-hadoop/ ./spark scala 下载安装包 解压到对应目录（我放在/usr/local）并改名为scala 配置环境变量$SCALA_HOME, 这一步在上文“环境变量”已经做好了 JDK 强烈建议通过安装包的方式安装java，因为这样可以指定目录， 因为JAVA环境变量是写在hadoop-env.sh里的，所有节点拷贝一份。 如果指定了java目录，那么所有节点只需把jdk安装在相同目录即可。 （我采用了直接apt-get install对方式） 由于我所有节点都是ubuntu20.04， 直接sudo apt-get install openjdk-8-jdk 安装到目录/usr/lib/jvm/java-8-openjdk-amd64, 所有节点都这样（安装到相同的目录）， 因此只要所有节点都通过该指令安装jdk，也可以做到环境变量的同步 下面介绍安装包的方式： 将jdk-8u301-linux-x64.tar.gz保存在根目录 运行如下命令 mkdir /usr/local/javatar -zxvf /jdk-8u301-linux-x64.tar.gz -C /usr/local/java 添加环境变量, vim ～/.bashrc 将下面的内容添加至末尾 export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=./:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$PATH:$&#123;JAVA_HOME&#125;/bin 退出vim，运行如下命令来使得配置生效 source ~/.bashrc 使用如下命令来确认是否已经安装完成 java -version ​ 应该看到如下输出： java version &quot;1.8.0_301&quot;Java(TM) SE Runtime Environment (build 1.8.0_301-b09)Java HotSpot(TM) 64-Bit Server VM (build 25.301-b09, mixed mode) 集群页面 正确启动hadoop和spark后，应该能看到二者的webUI页面： hadoop：&lt;master-ip&gt;:50070 spark: &lt;master-ip&gt;:18080 Hadoop 配置（ on master） 所有配置文件都位于目录~/hadoop/etc/hadoop/中 修改workers为： slave01slave02slave03 workers文件默认内容是localhost,这里把它删除了。 如果用单机版hadoop，则保留localhost,且不需要添加slave 修改core-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/home/lyk/hadoop/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 请注意属性hadoop.tmp.dir的值,需要创建该目录（这里就需要创建/home/lyk/hadoop/tmp） 修改hdfs-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/home/lyk/hadoop/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/home/lyk/hadoop/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.blocksize&lt;/name&gt; &lt;value&gt;134217728&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;description&gt;必须将dfs.webhdfs.enabled属性设置为true，否则就不能使用webhdfs的LISTSTATUS、LISTFILESTATUS等需要列出文件、文件夹状态的命令，因为这些信息都是由namenode来保存的&lt;/description&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.http.address&lt;/name&gt; &lt;value&gt;master:50070&lt;/value&gt; &lt;description&gt;hadoop的webUI访问页面&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 注意， replicatoin数量即worker数量，这里有3个worker； 如果是单机，那么只有1个worker（就是本机） 由于采用lyk用户，所以namenode和datanode目录都设在lyk用户目录下（ /home/lyk/hadoop/dfs ） 修改mapred-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 在hadoop2.X中，该文件内容名为mapred-site.xml.template，需要先改名为mapred-site.xml，再编辑 修改yarn-site.xml &lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;master:8099&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn.resourcemanager.webapp.address是hadoop的master的管理页面 在hadoop-env.sh中配置JAVA_HOME, 先找到本机的JAVA_HOME， 这里是/usr/lib/jvm/java-8-openjdk-amd64，在hadoop-env.sh中添加： export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64export HDFS_NAMENODE_USER=lykexport HDFS_DATANODE_USER=lykexport HDFS_SECONDARYNAMENODE_USER=lykexport YARN_RESOURCEMANAGER_USER=lykexport YARN_NODEMANAGER_USER=lyk 注意，该文件内本来有一个export JAVA_HOME=$&#123;JAVA_HOME&#125;， 但是该配置不知道为什么无效，所以需要换成显式的JAVA_HOME 由于用户是lyk，所以这里是lyk 将配置好的hadoop打包 cd ~rm -rf ./hadoop/tmprm -rf ./hadoop/logs/*tar -zcvf hadoop.master.tar.gz ./hadoop 将打包好的hdoop发送给slave scp hadoop.master.tar.gz lyk@slave01:~ scp hadoop.master.tar.gz lyk@slave02:~ scp hadoop.master.tar.gz lyk@slave03:~ 如果传输较慢，建议用CMD &amp;， 将命令放在后台执行 如果是首次ssh连接的话， 还需要对ssh公钥确认输入yes，所以这个命令就不能直接放入后台执行，需要先确认一次。 之后再使用该命令都不需要确认，也就可以放到后台了 安装（ on slave ） 只要把master的hadoop文件夹发给slave就行了，这里我采用压缩包方式发送。注意需要先删除slave上原有的hadoop（如果有的话) : 运行如下命令, 删除原有的hadoop根目录( 如果有的话 )并将新的压缩包（master发来的）解压 rm -rf /usr/local/hadooptar -zxvf ~/hadoop.master.tar.gz -C ~ 然后改名为hadoop 启动Hadoop 格式化HDFS:注意在首次使用时使用，若重复格式化，将无法开启datanode cd ~/hadoop/binhdfs namenode -format 初始化之前需要删掉配置文件的tmp目录下的所有内容 如果不初始化，则启动时无法启动namenode 最终会输出SHUTTING DOWN.... 在master上输入以下命令启动Hadoop cd ~/hadoop ./sbin/start-all.sh 查看Hadoop是否启动 检验hadoop是否已经启动 jps 在master上应该有类似如下输出: 28848 NameNode29122 ResourceManager29234 Jps29016 SecondaryNameNode28921 DataNode29199 NodeManager 主要关注NameNode、ResourceManager、SecondaryNameNode这三条是否存在，如果不存在则应该去~/hadoop/logs/目录下寻找相应地日志查看错误信息 在slave上输入jps应该有类似如下输出 4406 Jps1914 NodeManager1787 DataNode 主要关注NodeManager、DataNode这两条是否存在，如果不存在同样去~/hadoop/logs/目录下寻找相应地日志查看错误信息 我发现某个slave的datanode没有启动，查看其hadoop-lyk-datanode-VM-4-7-ubuntu.log发现，原来是xml配置文件写错了。。 查看hadoop webUI： &lt;master-ip&gt;:50070 在hdfs-site.xml中配置 关闭hadoop 如果关闭Hadoop(~/hadoop/sbin/stop-all.sh ),则所有节点的jps都不会有与hadoop关联的输出 Bugs ERROR: Invalid HADOOP_HDFS_HOME 网上有说需要配置HADOOP_HDFS_HOME这个环境变量，其实根本不需要。 发生这个问题，大概率是你的hadoop安装出错了， 启动hadoop实际上会执行类似/home/lyk/hadoop/share/hadoop/common/hadoop-common-3.2.3.jar的jar包， 进到该目录看看有没有这个jar包就行了 Spark Spark是一个分布式的大数据计算引擎，可以执行你的任务（jar包） 配置 改名配置文件： cd ~/spark/ cp ./conf/spark-env.sh.template ./conf/spark-env.shcp ./conf/workers.template ./conf/workers 修改workers文件的内容，将原来的内容替换成： masterslave01slave02slave03 在spark-env.sh的末尾添加如下内容 export SPARK_DIST_CLASSPATH=$(/home/lyk/hadoop/bin/hadoop classpath)export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64export SCALA_HOME=/usr/local/scalaexport HADOOP_HOME=/home/lyk/hadoopexport HADOOP_COMMON_HOME=$HADOOP_HOME# Spark# SPARK_MASTER_PORT是7077# SPARK_MASTER_WEBUI_PORT用于在网页上访问spark管理页面，默认的端口是8080, 这是个常用端口， 因此替换成了18080export SPARK_MASTER_HOST=masterexport SPARK_MASTER_PORT=7077export SPARK_MASTER_WEBUI_PORT=18080 export SPARK_DIST_CLASSPATH=$($(HOME)/hadoop/bin/hadoop classpath)export HADOOP_CONF_DIR=$(HOME)/hadoop/etc/hadoop#export SCALA_HOME=/usr/lib/scala/scala-2.13.3 export HADOOP_HOME=$(HOME)/hadoopexport SPARK_MASTER_IP=&lt;master的公网ip&gt; #指定 Spark 集群 Master 节点的 IP 地址 export SPARK_MASTER_PORT=7077export SPARK_MASTER_HOST=masterexport SPARK_EXECUTOR_MEMORY=4096m #大小看虚拟机内存export SPARK_LOCAL_IP=localhostexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 注意，由于我并没有安装scala所以不添加SCALA_HOME，而是添加JAVA_HOME。此外，由于出现了类似于如下的worker日志报错，我添加了SPARK_LOCAL_IP 这里填hadoop的目录， 我在hadoop-env.sh中指定了 使用如下命令将配置好的spark打包发送给slave cd /usr/local/tar -zcvf /spark.master.tar.gz ./sparkscp /spark.master.tar.gz slave01:/scp /spark.master.tar.gz slave02:/scp /spark.master.tar.gz slave03:/ 安装（slave）： 使用如下命令在slave上安装spark # rm -rf /usr/local/spark/# tar -zxvf /spark.master.tar.gz -C /usr/local 启动spark ==在启动之前先修改master的spark-env.sh文件，将其中的SPARK_LOCAL注释掉，不然就只能从服务器内网的localhost来访问spark的webui了（我也不知道为啥。。。）== 先按照之前的步骤启动Hadoop 启动spark cd ~/spark ./sbin/start-all.sh 查看spark是否启动 执行jps，此时除了hadoop的进程输出外，还能看到spark的进程： master： 多了Master和Worker(加入指定该master也作为worker的话) worker： 多了Worker 提交任务 spark-submit [path-to-jar] Bugs jps没有输出 jps是查看java进程的工具，java程序启动以后，会在/tmp目录下生成一个hsperfdata_[username]的文件夹，其中的文件以java进程的pid命名。因此使用jps查看当前进程，其实就是把/tmp/hsperfdata_username中的文件名遍历一遍之后输出。 情况1: 如果/tmp/hsperfdata_[username]的文件所有者和文件所属用户组与启动进程的用户不一致的话，在进程启动之后，就没有权限写/tmp/hsperfdata_[username]，所以/tmp/hsperfdata_[username]是一个空文件，理所当然jps也就没有任何显示。 情况2: 不知道为啥，重启服务器就好了。。。 nodemanager running as process 6410. Stop it first. 进程已经在运行中了，先执行stop-all.sh下]，再执行start-all.sh Permission denied 文件所有权的问题， 如果以root身份安装Hadoop（比如一开始把hadoop放在/usr/local），然后又想用普通用户来使用hadoop（比如之后把hadoop放在~)，就会发生此问题，只需要更改hadoop的所有权: chown -R lyk:lyk ~/hadoop Hadoop + Spark For Manjaro 这里演示一下Manjaro/arch用户安装hadoop的流程，只开个头。剩余的hadoop配置和spark安装及配置也大同小异。主要是对于manjaro/arch用户来说，安装这类软件会有一些小坑 sudo pacman -Syu 安装jdk：yay -S jdk 安装openssh：yay -S openssh manjaro默认不安装openssh 如果你要用hadoop用户，而非lyk用户的话，还得做以下步骤 创建hadooop用户并且更改hadoop用户密码 //创建hadoop用户sudo adduser hadoop//更改hadoop用户密码sudo passwd hadoop manjaro默认不安装adduser命令，要使用useradd 如果yay -S adduser,这个安装的命令默认是在/etc内创建用户，而不是（如centos中自带的adduser）在root目录下，因此路径会不一样，在配置环境变量时需要注意。 以上的配置完成之后，使用hadoop用户 su - hadoop 切换成hadoop之后，设置ssh免密登录 ssh-keygen -t rsa cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys chmod 640 ~/.ssh/authorized_keys 然后试试ssh localhost，查看是否能够免密登录 如果遇到22端口被拒绝的情况，很有可能是ssh服务并没有开启。 切换成exit退出当前用户，使用主用户 输入sudo service ssh start进行服务的开启。如果显示没有该服务，那么可以确定系统并没有安装openssh, 去安装。 Archlinux或者manjaro开启ssh服务命令: systemctl enable sshd.service 开机启动 systemctl start sshd.service 立即启动 systemctl restart sshd.service 立即重启 用户hadoop使用su可能会报错： hadoop is not in the sudoers file. This incident will be reported.，需要在/etc/sudoers文件里给该用户添加权限 （ref） Hadoop + Spark For Mac 实际做作业时，我用的就是mac m1单机hadoop+spark，亲测没问题 mac和linux的区别主要还是文件路径不同，需要修改一些环境变量，其余步骤完全相同 由于mac的用户目录位于/Users/lyk, 各种配置中的环境变量也需要更改（而不是linux的/home/lyk） 同理， mac的jdk位置也可能不一样，需要做更改 因为spark和hadoop都是由高级语言java/scala编写的，因此没有跨平台问题。我的m1能正常安装、运行集群 Clickhouse Clickhouse是一个单机数据库，对机器学习的支持比较好 环境 mac（m1）没法装clickhouse，只能用服务器，即在服务器上裸机安装（后来为了服务高可用性，改为在服务器上docker安装 ） 搭建clickhouse 官网文档 系统要求 ClickHouse可以在任何具有x86_64，AArch64或PowerPC64LE CPU架构的Linux，FreeBSD或Mac OS X上运行。 官方预构建的二进制文件通常针对x86_64进行编译，并利用SSE 4.2指令集，因此，除非另有说明，支持它的CPU使用将成为额外的系统需求。下面是检查当前CPU是否支持SSE 4.2的命令: $ grep -q sse4_2 /proc/cpuinfo &amp;&amp; echo &quot;SSE 4.2 supported&quot; || echo &quot;SSE 4.2 not supported&quot; 要在不支持SSE 4.2或AArch64，PowerPC64LE架构的处理器上运行ClickHouse，您应该通过适当的配置调整从源代码构建ClickHouse 注意，m1无法安装用软件包clickhouse，只能手动编译，clickhouse官网有教程。 然而m1默认的编译器是apple clang，而clickhouse只推荐clang编译(教程也是用的clang)，因此m1用户还得把默认编译器换成clang， 由于特别麻烦，因此我在服务器上搭建了clickhouse，没有用mac本机 [Apple clang无法编译clickhouse，只能用clang](Clickhouse installation for mac fails: &quot;AppleClang is not supported, you should install clang from brew.&quot;) Installing LLVM/Clang on OS X DEB软件包安装 建议使用Debian或Ubuntu的官方预编译deb软件包。运行以下命令来安装包: sudo apt-get install -y apt-transport-https ca-certificates dirmngrsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 8919F6BD2B48D754echo &quot;deb https://packages.clickhouse.com/deb stable main&quot; | sudo tee \\ /etc/apt/sources.list.d/clickhouse.listsudo apt-get updatesudo apt-get install -y clickhouse-server clickhouse-clientsudo service clickhouse-server startclickhouse-client # or &quot;clickhouse-client --password&quot; if you&#x27;ve set up a password. docker安装 强烈建议安装docker版本的clickhouse，文档 pull镜像： docker pull clickhouse/clickhouse-server 运行server容器，并进行端口映射： docker run -d -p 8123:8123 -p9000:9000 --name some-clickhouse-server --ulimit nofile=262144:262144 clickhouse/clickhouse-server 这里将主机端口9000， 8123 映射到容器的9000， 8123， 和裸机版的clickhouse一样 这样就可以进行公网通信了 运行client容器，连接到server容器： docker run -it --rm --link lyk-clickhouse-server:clickhouse-server clickhouse/clickhouse-client --host clickhouse-server 其余功能，如挂载卷，指定容器使用某个配置文件启动等，都参见文档 配置clickhosue clickhouse系统配置文件:/etc/clickhouse-server/config.xml clickhouse用户配置文件:/etc/clickhouse-server/users.xml clickhouse日志文件所在目录: /var/log/clickhouse-server 如果clickhouse-server 没有找到任何有用的信息或根本没有任何日志，您可以使用命令查看 system.d : sudo journalctl -u clickhouse-server 允许远程连接 ClickHouse server默认只监听环回地址, 无法用公网通信： root@ubuntu:/var/lib/clickhouse/# lsof -i :8123COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEclickhous 653 clickhouse 41u IPv6 32544 0t0 TCP ip6-localhost:8123 (LISTEN)clickhous 653 clickhouse 44u IPv4 32547 0t0 TCP localhost:8123 (LISTEN) 需要修改系统配置文件，使其监听公网地址： vim /etc/clickhouse-server/config.xml 把注释掉的&lt;listen_host&gt;::&lt;/listen_host&gt;取消注释，然后重启服务： service clickhouse-server restart 现在的端口监听情况： root@ubuntu:/var/lib/clickhouse/data/# lsof -i :8123COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEclickhous 9188 clickhouse 32u IPv6 61573 0t0 TCP *:8123 (LISTEN) 配置用户名和密码 clickhouse默认的登录账号是default， 没有密码 教程 可以修改用户配置文件，添加密码： vim /etc/clickhouse-server/users.xml 在对应用户的标签中添加密码： &lt;password&gt;123&lt;/password&gt; 这里是明文密码，还可以配置加密密码，这里不介绍了 当password标签为空时，代表免密码登录 登陆 clickhouse-client -u default --passord 123 -u 为指定使用哪个账号进行登录，如不指定, 默认使用default Bugs clickhouse有时会自动崩溃，客户端启动时报错： clickhouse-clientClickHouse client version 21.2.4.6 (official build).Connecting to localhost:9000 as user default.Code: 210. DB::NetException: Connection refused (localhost:9000) 并且，一旦出现一次崩溃，之后的clickhouse都无法通过systemctl启动(sudo service clickhouse-server start)，只能手动启动(sudo clickhouse start ) 在网上找了各种教程，都没能解决。 我的服务器是ubuntu20.04，换了新服务器（相同OS），重装，依然有这个问题。 最后无奈使用docker的clickohouse，崩溃就崩溃，重启容器就好了 clickhouse的文件的用户/组都为clickhouse，如果发现权限问题，需要chown 例如，新建了日志文件，需要手动更改其权限: sudo chown clickhouse:clickhouse /var/log/clickhouse-server/clickhouse-server.log 删除clickhouse 查看系统已安装的包： apt list --installed sudo apt remove -y clickhouse-common-static sudo apt remove -y clickhouse-server-common sudo rm -rf /var/lib/clickhousesudo rm -rf /etc/clickhouse-*sudo rm -rf /var/log/clickhouse-server Flink Flink也是一个计算引擎，可以执行你提交的jar包 搭建FLink 环境：Mac单机搭建， Flink Version: 1.13.5-bin-scala-2.11 参考教程 下载jar包：https://archive.apache.org/dist/flink/flink-1.13.5/ 解压： sudo tar -zxf flink-1.13.5-bin-scala_2.11.tgz -C /usr/local 修改文件名字，并设置权限 cd /usr/localsudo mv ./flink-*/ ./flinksudo chown -R hadoop:hadoop ./flink 添加环境变量: vim ~/.zshrcexport FLINK_HOME=/usr/local/flinkexport PATH=$FLINK_HOME/bin:$PATH 更改配置文件( [flink位置]/conf/flink-conf.yaml ): # The number of task slots that each TaskManager offers. Each slot runs one parallel pipeline.taskmanager.numberOfTaskSlots: 4# The parallelism used for programs that did not specify and other parallelism.parallelism.default: 1 启动Flink # 启动Flink,因为FLINK_HOME已经写入了环境变量，因此可以直接执行脚本：start-cluster.sh 可以通过观察logs目录下的日志来检测系统是否正在运行了 tail log/flink--jobmanager-.log JobManager同时会在8081端口上启动一个web前端，通过http://localhost:8081来访问 关闭Flink stop-cluster.sh 提交任务 教程 Flink可以直接在Flink的webUI上提交jar包 也可以命令行提交： run -c Flink -p 2 [path-to-jar] Kafka 没学，不会，不想学","categories":[{"name":"Technology","slug":"Technology","permalink":"http://lyk-love.cn/categories/Technology/"}],"tags":[{"name":"Cloud Computation","slug":"Cloud-Computation","permalink":"http://lyk-love.cn/tags/Cloud-Computation/"}]},{"title":"VSCode","slug":"VSCode","date":"2022-09-13T15:43:12.000Z","updated":"2022-09-26T06:39:34.942Z","comments":true,"path":"2022/09/13/VSCode/","link":"","permalink":"http://lyk-love.cn/2022/09/13/VSCode/","excerpt":"Outline: Install Config","text":"Outline: Install Config Config To open the User settings: Open the command palette (either with F1 or Ctrl+Shift+P) Type &quot;open settings&quot; You are presented with two options, choose Open Settings (JSON) Which, depending on platform, is one of: Windows %APPDATA%\\Code\\User\\settings.json macOS $HOME/Library/Application\\ Support/Code/User/settings.json Linux $HOME/.config/Code/User/settings.json The Workspace settings will be in a &#123;workspaceName&#125;.code-workspace file where you saved it, and the Folder settings will be in a .vscode folder if and when it has been created. Using Clang in Visual Studio Code https://code.visualstudio.com/docs/cpp/config-clang-mac Visual Studio Code on macOS https://code.visualstudio.com/docs/setup/mac Using C++ on Linux in VS Code https://code.visualstudio.com/docs/cpp/config-linux","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://lyk-love.cn/tags/Linux/"},{"name":"Editor","slug":"Editor","permalink":"http://lyk-love.cn/tags/Editor/"}]},{"title":"Database Toolkit","slug":"Database-Toolkit","date":"2022-09-13T15:20:30.000Z","updated":"2022-10-01T10:48:43.221Z","comments":true,"path":"2022/09/13/Database-Toolkit/","link":"","permalink":"http://lyk-love.cn/2022/09/13/Database-Toolkit/","excerpt":"Outline: Mysql Mongodb Database Tools","text":"Outline: Mysql Mongodb Database Tools Mysql Install manjaro: yay -S mysql mac: brew install mysql Ubuntu:这里只针对Ubuntu 20.04. Ubuntu 源仓库中最新的 MySQL 版本号是 MySQL 8.0 sudo apt updatesudo apt install mysql-server Start mysql server: sudo systemctl start mysql Check status of mysql-server: systemctl status mysql Unintall on Ubuntu: First, remove already installed mysql-server using-- sudo apt-get remove --purge mysql-server mysql-client mysql-common Then clean all files sudo apt-get autoremove Config mysql允许远程访问 mysql默认绑定本地环回地址（127.0.0.1），无法公网通信(即使你的用户是允许远程登录的用户(如%): tcp 0 0 127.0.0.1:3306 0.0.0.0:* LISTEN 3783/mysqld 需要修改mysql的配置文件, 该文件的位置依平台的不同而变化. 它在ubuntu20.04中位于: /etc/mysql/mysql.conf.d/mysqld.cnf 我们需要修改bind-address字段: 可以在其后面添加可访问mysql服务器的ip，用空格隔开: bind-address=127.0.0.1 139.196.197.138 &lt;other-ip&gt; 或者直接把该字段注释掉 重启mysql服务: service mysql restart Connection 远程连接: mysql -h 192.168.5.116 -P 3306 -u root -p123456 简单来说，locahost 一般意味着使用的是 Unix Domain Socket ，此时是不会经过网络防火墙的。 如果 skip_name_resolve 配置关闭，那么 127.0.0.1 这类地址也会被解析成 locahost 如果不指定主机或者使用 -hlocalhost ，实际上会有限尝试使用 Unix Domain Socket 连接 (实际还需要保证没有使用 --protocol=TCP 参数)，默认的是 /var/lib/mysql/mysql.sock ，如果 Sock 地址修改了，也可以通过 -S PATH 参数指定。 也就是说，在通过 mysql 客户端访问数据库时，如果指定了 -h&lt;IP&gt; 参数，那么会通过 TCP/IP 方式连接数据库。 最简单的，如果要要强制使用 TCP/IP 连接到本地服务器，那就使用 IP 地址 127.0.0.1 而不是主机名 localhost 通过 TCP/IP 方式进来的连接，MySQL 服务器接收到的来源主机是 127.0.0.1；如果采用的是 UNIX Domain Socket 方式，那么 MySQL 服务器接收到的来源主机是 localhost 。 另外，对于 TCP/IP 方式来说，如果关闭了 skip_name_resolve 选项，那么会尝试将获取到的 IP 地址解析成域名。 skip_name_resolve 参数在调优时，一般建议开启，也就是说禁止域名解析，可以通过如下命令查看是否开启。 SHOW VARIABLES LIKE &#x27;%skip_name_resolve%&#x27;; 修改时，可以直接在 my.cnf 配置文件的 [mysqld] 字段中添加 skip-name-resolve 即可。 在 MySQL 中 localhost 一般是用来标示 Unix Domain Socket ，如果将 skip_name_resolve 关闭，那么 127.0.0.1 也可能会被反解析成 localhost 。 Problems 空密码无法用Datagrip连接 首选当然是设一个密码 也可以将Datagrip配置中的密码的保存方式更改为Never Mongodb Install 由于 MongoDB 修改了软件授权协议， 官方仓库已经删除了此软件包。请注意从代码编译 mongodb 需要 ~160GB 磁盘空间，需要花费几个小时时间。因此我们最好安装-bin版本 yay -S mongodb 注意： 只有先启动mongod， 才能正确启动mongo 报错 可能有报错： &gt; mongoconnecting to: mongodb://127.0.0.1:270172018-09-27T21:11:14.779+0800 W NETWORK [main] Failed to connect to 127.0.0.1:27017, reason: Connection refused2018-09-27T21:11:14.780+0800 E QUERY [main] Error: couldn&#x27;t connect to server 127.0.0.1:27017, connection attempt failed :connect@src/mongo/shell/mongo.js:234:13@(connect):1:6exception: connect failed 出现以上原因，可能是data目录下的mongod.lock文件的问题。可以使用命令修复： [root@localhost mongodb]# ./bin/mongod --repair 然后重新打开一个窗口，启动mongodb Config 配置文件路径： \\etc\\mongodb.conf. 其中dbpath为数据库的路径 Commands 启动 $ systemctl start mongodb.service 查看状态: sudo systemctl status mongodb 重启: sudo systemctl restart mongodb 停止: sudo systemctl stop mongodb 重载服务: sudo systemctl daemon-reload 设置开机启动: sudo systemctl enable mongodb Database Tools mycli 好用的命令行mysql界面,项目地址 # 通用pip install -U mycli or # on Archyay -S mycli or # Only on macOSbrew update &amp;&amp; brew install mycli or # Only on debian or ubuntusudo apt-get install mycli","categories":[],"tags":[]},{"title":"Clang/LLVM Toolkit","slug":"Clang-LLVM-Toolkit","date":"2022-09-13T15:16:26.000Z","updated":"2022-09-26T06:39:34.926Z","comments":true,"path":"2022/09/13/Clang-LLVM-Toolkit/","link":"","permalink":"http://lyk-love.cn/2022/09/13/Clang-LLVM-Toolkit/","excerpt":"Outline: LLVM","text":"Outline: LLVM LLVM Install 这里讲一下Mac OSX怎么安装llvm/clang. 也可以手动编译LLVM/clang: Doc OSX自带了LLVM, 但是是苹果版的, 和开源版本不太一样, 我们希望使用开源版本的LLVM. 使用brew安装: brew upgrade &amp;&amp; brew install llvm 这会安装开源版本的LLVM. 注意, 主机中实际是两个版本的LLVM共存的, 我们不要使用系统的LLVM, 只用brew下载的LLVM. Linux中一般直接安装到/usr/local了. 但是由于Max也有一个LLVM, 不能把二者混淆, brew就会把LLVM装到别的位置 查看brew安装的LLVM位置: cat $(brew --prefix llvm) 这显示的是符号链接的位置(例如, /opt/homebrew/opt/llvm), 真实的llvm被安装在形如/opt/homebrew/Cellar/llvm/14.0.6_1的位置. 不过无关紧要 把LLVM添加到PATH: # LLVM on MAC, mac已经自带了llvm, 但是位置很奇怪 Not Committed YetMAC_LOCAL_LLVM_VERSION=14.0.6_1export LLVM_MAC_LOCAL_HOME=/opt/homebrew/Cellar/llvm/$MAC_LOCAL_LLVM_VERSION# brew目录下的llvm实际上是指向Mac local LLVM的符号链接, 为了方便, 还是用符号链接的路径吧export LLVM_HOME=/opt/homebrew/opt/llvmexport PATH=$PATH:$LLVM_HOME/bin 再添加两个环境变量, 让clang能找到LLVM: export LDFLAGS=&quot;-L($LLVM_HOME)/lib -Wl,-rpath,($LLVM_HOME)/lib&quot;export CPPFLAGS=&quot;-I($LLVM_HOME)/include -I($LLVM_HOME)/include/c++/v1/&quot; 再把Clang的环境变量设置一下: export CC := /usr/local/opt/llvm/bin/clangexport CXX := $(CC)++ 更好的办法是添加到PATH. 我这样设置, 命令行需要用$(CC)来调用clang 测试安装是否成功: llvm-dis --version","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://lyk-love.cn/tags/Linux/"},{"name":"Compiler","slug":"Compiler","permalink":"http://lyk-love.cn/tags/Compiler/"}]},{"title":"Java Toolkit","slug":"Java-Toolkit","date":"2022-09-13T15:13:52.000Z","updated":"2022-09-26T06:39:34.931Z","comments":true,"path":"2022/09/13/Java-Toolkit/","link":"","permalink":"http://lyk-love.cn/2022/09/13/Java-Toolkit/","excerpt":"Outline: JDK Maven","text":"Outline: JDK Maven JDK 安装jdk mac: mac建议到oracle官网下载jdk linux一般用命令行安装，因此推荐openjdk Ubuntu: 查找合适的openjdk版本: # ubuntuapt-cache search openjdk 安装 sudo apt-get install openjdk-8-jdk 如果search和install都没反应，应该先更新软件源 Manjaro: 查找合适的openjdk版本: yay search jdk yay install openjdk-8-jdk (3) 配置环境变量, 编辑如下文件: vim ~/.bashrc 在最后一行加: export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 查找JDK 查看jdk版本： java -version 查找jdk: whereis java 输出为 java: /usr/bin/java /usr/share/java /usr/share/man/man1/java.1.gz 查看jdk真实位置（上面的都是软链接）： ls -l /usr/bin/java 输出为： lrwxrwxrwx 1 root root 22 Mar 30 16:07 /usr/bin/java -&gt; /etc/alternatives/java ls -l /etc/alternatives/java 输出为： lrwxrwxrwx 1 root root 46 Mar 30 16:07 /etc/alternatives/java -&gt; /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java# 以上就是jdk位置 配置jdk环境变量 linux 对于linux: 编辑/etc/profile, 或 ~/.bashrc, .zshrc等： vim /etc/profile # 这里可以选择任意的shell配置文件， export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 # 这里填jdk位置export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=./:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$PATH:$&#123;JAVA_HOME&#125;/bin 使配置文件生效： source /etc/profile mac mac的jdk安装位置和linux不同 查询当前的java的安装版本 cd /Library/Java/JavaVirtualMachinesls 配置.zshrc # jdk 版本切换， on mac# jdk-17.0.2.jdk jdk1.8.0_321.jdk jdk-11.0.14.jdk export JAVA_8_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_321.jdk/Contents/Homeexport JAVA_11_HOME=/Library/Java/JavaVirtualMachines/jdk-11.0.14.jdk/Contents/Homeexport JAVA_17_HOME=/Library/Java/JavaVirtualMachines/jdk-17.0.2.jdk/Contents/Homealias jdk8=&#x27;export JAVA_HOME=$JAVA_8_HOME&#x27;alias jdk11=&#x27;export JAVA_HOME=$JAVA_11_HOME&#x27;alias jdk17=&#x27;export JAVA_HOME=$JAVA_17_HOME&#x27; source ~/.bash_profi 切换jdk版本 jdk11java -version Maven maven配置文件位置： ～/.m2/settings.xml 加入阿里云仓库： &lt;!-- 阿里云仓库 --&gt;&lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/repositories/central/&lt;/url&gt;&lt;/mirror&gt;","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://lyk-love.cn/tags/Linux/"},{"name":"Java","slug":"Java","permalink":"http://lyk-love.cn/tags/Java/"}]},{"title":"Cloud Native Toolkit","slug":"Cloud-Native-Toolkit","date":"2022-09-13T15:10:42.000Z","updated":"2022-09-26T06:39:34.926Z","comments":true,"path":"2022/09/13/Cloud-Native-Toolkit/","link":"","permalink":"http://lyk-love.cn/2022/09/13/Cloud-Native-Toolkit/","excerpt":"Outline: Docker Kubernetes","text":"Outline: Docker Kubernetes Docker 参见《Docker Intro》 install: sudo apt install docker.io 最好使用非root用户来使用Docker,此时需要添加非root用户到本地Docker Unix组： sudo usermod -aG docker [user_name] 如果当前登陆用户就是要添加进组的用户的话，需要重新登陆才能生效 换源阿里云,需要去阿里云“容器镜像服务” --&gt; “镜像加速器” 生成镜像url： sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-&#x27;EOF&#x27;&#123; &quot;registry-mirrors&quot;: [&quot;https://zz1b9pta.mirror.aliyuncs.com&quot;]&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart docker *mac用户可在setting -- docker engine中修改daemon.json文件 终端输入docker info查看是否配置成功 &gt; docker info...Registry Mirrors:https://zz1b9pta.mirror.aliyuncs.com/ 将docker设置为开机自启： systemctl enable docker kubernetes Install ubuntu20.04下安装k8s 注意，k8s的安装和集群初始化都需要root用户 安装docker 安装依赖 sudo apt-get updatesudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common 直接在/etc/apt/sources.list里添加https://mirrors.aliyun.com/kubernetes/apt/是不行的，因为这个阿里镜像站使用的ssl进行传输的，所以要先安装apt-transport-https并下载镜像站的密钥才可以进行下载 安装GPG证书: curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add - 写入软件源信息: cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.listdeb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial mainEOF 然后更新源列表： apt-get update 查看可安装版本： apt-cache madison kubelet 安装指定版本: apt-get install -y kubelet=1.18.4-00 kubeadm=1.18.4-00 kubectl=1.18.4-00 如果想要安装最新版本，就直接： apt-get install -y kubelet kubeadm kubectl 但是很不推荐，因为最新的k8s 1.24.* 的集群初始化有bug，不如用老版本 设置开机启动: sudo systemctl enable kubelet &amp;&amp; sudo systemctl start kubelet 查看所需镜像,以刚才安装的1.18.4版本为例: kubeadm config images list --kubernetes-version=v1.18.4k8s.gcr.io/kube-apiserver:v1.18.4k8s.gcr.io/kube-controller-manager:v1.18.4k8s.gcr.io/kube-scheduler:v1.18.4k8s.gcr.io/kube-proxy:v1.18.4k8s.gcr.io/pause:3.2k8s.gcr.io/etcd:3.4.3-0k8s.gcr.io/coredns:1.6.7 上面的镜像是Google的，国内无法访问，需要用阿里云的镜像来替换: docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.18.4docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.18.4docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.18.4docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.18.4docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.2docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.4.3-0docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.6.7 为镜像重新打tag： docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.18.4 k8s.gcr.io/kube-apiserver:v1.18.4docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.18.4 k8s.gcr.io/kube-controller-manager:v1.18.4docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.18.4 k8s.gcr.io/kube-scheduler:v1.18.4docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.18.4 k8s.gcr.io/kube-proxy:v1.18.4docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.2 k8s.gcr.io/pause:3.2docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.4.3-0 k8s.gcr.io/etcd:3.4.3-0docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.6.7 k8s.gcr.io/coredns:1.6.7 执行下面命令，测试安装是否正常 kubeadm init k8s集群在初始化时建议使用root用户，如果以普通用户执行kubeadm init， 会得到报错: [ERROR IsPrivilegedUser]: user is not running as root [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...` 查看k8s版本: kubeadm version kubeadm version: &amp;version.Info&#123;Major:&quot;1&quot;, Minor:&quot;24&quot;, GitVersion:&quot;v1.24.1&quot;, GitCommit:&quot;3ddd0f45aa91e2f30c70734b175631bec5b5825a&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2022-05-24T12:24:38Z&quot;, GoVersion:&quot;go1.18.2&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125; ``GitVersion:&quot;v1.24.1&quot;&quot;` 即为版本号 Uninstall sudo apt remove -y kubelet kubeadm kubectlrm -rf ~/.kube/rm -rf /etc/kubernetes/rm -rf /etc/systemd/system/kubelet.service.drm -rf /etc/systemd/system/kubelet.servicerm -rf /usr/bin/kube*rm -rf /etc/cnirm -rf /opt/cnirm -rf /var/lib/etcdrm -rf /var/etcd 配置管理节点 配置主机 增加主机名: 单独为每个服务器增加主机名，格式为hostnamectl set-hostname hostname。一般主节点取名master，从节点取名node. hostnamectl set-hostname [k8s-master] 在主节点配置所有k8s服务器的host，并且host名字跟各服务器的主机名对应。这里配置的是公网IP cat &gt;&gt; /etc/hosts &lt;&lt; EOF &lt;主节点公网IP&gt; k8s-master&lt;工作节点公网IP&gt; k8s-node1EOF echo &quot;使host配置生效&quot; &gt; /dev/null/etc/init.d/network restart 修改cgroup Kubernetes cgroup driver was set to systems but docker was set to systemd. So I created /etc/docker/daemon.json and added below: &#123; &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]&#125; Then sudo systemctl daemon-reloadsudo systemctl restart dockersudo systemctl restart kubelet Then： docker info | grep Cgroup 如果输出为： Cgroup Driver: systemedCgroup Version: 1 即成功 Cluster Config 初始化master节点 如果初始化过程中出现错误，要使用kubeadm reset -f清除之前的配置 kubeadm init \\--apiserver-advertise-address=121.36.247.134 \\--kubernetes-version=v1.18.4 \\--pod-network-cidr=10.244.0.0/16 \\--service-cidr=10.96.0.0/12 \\--ignore-preflight-errors=Swap kubeadm init \\--apiserver-advertise-address=121.36.247.134 \\--image-repository registry.aliyuncs.com/google_containers \\--pod-network-cidr=10.244.0.0/16 这里介绍一下一些常用参数的含义： --apiserver-advertise-address: k8s 的apiserver的部署地址，填自己的管理节点 ip( 如果公网通信，那就是公网ip ) --image-repository: 拉取的 docker 镜像源，因为初始化的时候kubeadm会去拉 k8s 的很多组件来进行部署，所以需要指定国内镜像源，下不然会拉取不到镜像。 --pod-network-cidr: 这个是 k8s 采用的节点网络，因为我们将要使用flannel作为 k8s 的网络，所以这里填10.244.0.0/16就好 --kubernetes-version: 这个是用来指定你要部署的 k8s 版本的，一般不用填，不过如果初始化过程中出现了因为版本不对导致的安装错误的话，可以用这个参数手动指定。 --ignore-preflight-errors: 忽略初始化时遇到的错误，比如说我想忽略 cpu 数量不够 2 核引起的错误，就可以用--ignore-preflight-errors=CpuNum。错误名称在初始化错误时会给出来。 配置 kubectl 工具 mkdir -p /root/.kube &amp;&amp; \\cp /etc/kubernetes/admin.conf /root/.kube/config 执行完成后并不会刷新出什么信息，可以通过下面两条命令测试 kubectl是否可用： # 查看已加入的节点kubectl get nodes# 查看集群状态kubectl get cs 部署 flannel 网络 flannel是一个专门为 k8s 设置的网络规划服务，可以让集群中的不同节点主机创建的 docker 容器都具有全集群唯一的虚拟IP地址 kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 检查节点是否部署完成 kubectl get nodes","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://lyk-love.cn/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"http://lyk-love.cn/tags/Docker/"},{"name":"K8s","slug":"K8s","permalink":"http://lyk-love.cn/tags/K8s/"}]},{"title":"Traditional Unix CLI Tools","slug":"Traditional-Unix-CLI-Tools","date":"2022-09-13T15:05:52.000Z","updated":"2022-09-26T06:39:34.941Z","comments":true,"path":"2022/09/13/Traditional-Unix-CLI-Tools/","link":"","permalink":"http://lyk-love.cn/2022/09/13/Traditional-Unix-CLI-Tools/","excerpt":"传统的Unix 命令行工具, 已被大部分Linux发行版所包括, 有时候需要手动下载","text":"传统的Unix 命令行工具, 已被大部分Linux发行版所包括, 有时候需要手动下载 Networking ping: apt-get install iputils-ping traceroute: apt-get install traceroute","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://lyk-love.cn/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"http://lyk-love.cn/tags/Shell/"}]},{"title":"Terminal","slug":"Terminal","date":"2022-09-13T14:58:22.000Z","updated":"2022-09-26T06:39:34.941Z","comments":true,"path":"2022/09/13/Terminal/","link":"","permalink":"http://lyk-love.cn/2022/09/13/Terminal/","excerpt":"Outline: Terminal Emulators Zsh Ohmyzsh Zsh Theme General Terminal Emulater Config","text":"Outline: Terminal Emulators Zsh Ohmyzsh Zsh Theme General Terminal Emulater Config Terminal Emulators MacOS常用的终端模拟器是iTerms2, 而Linux KDE使用Konsole. 这两个都不能跨平台, 并且iTerms2相比Konsole很垃圾. 因此, 可以把终端换成跨平台的Alacritty Konsole Linux上无敌, 可惜MacOS不能用 iTerms2 和MacOS无缝集成, 开箱即用. 但是它也不能跨平台, 所以没必要对它进行过度的配置. Alacritty Github Repo Installation Configuration 作者的Blog Features 跨平台 可定制性强, 并且由于是跨平台的, 不用担心换平台之后, Alacritty用不了, 定制打水漂的问题 使用配置文件来配置, 意味着可以使用版本管理, 换台电脑clone一下就能用原来的配置了 可以在MacOS的OpenInTerminal里配置, 太爽了! 一键打开! 我就是因为Warp不能这么做才放弃了Warp uses GPU acceleration (OpenGL in Rust), Performance天下第一 specifically written to correctly render applications like Vim, Alacritty + Nvim是天然搭档 Config Github Configuration Official Doc 配置文件位置: Alacritty doesn't create the config file for you, but it looks for one in the following locations: $XDG_CONFIG_HOME/alacritty/alacritty.yml $XDG_CONFIG_HOME/alacritty.yml $HOME/.config/alacritty/alacritty.yml: 我用这个 $HOME/.alacritty.yml 字体: 由于我的Zsh主题是p10k, 使用的默认字体是Meslo Nerd Font, 而Alacritty默认使用的不是Meslo Nerd Font, 因此Alacritty中很多图标无法正常显示. 所以需要[安装字体](# p10k Fonts) 这一步我之前的[p10k字体配置]里 配色: 使用下文所述的[Dracula](# Color: Dracula), 配色文件是一个符号链接, 指向dotfile, 纳入了版本管理 ln -s /Users/lyk/Projects/MyOfficialProjects/dotfiles/dracula.yml ~/.config/alacritty/dracula.yml 当然还可以用别的方案 Alacritty 本身不提供窗口拆分、Session 管理等功能, 所以需要安装[Tmux](# Terminal Multiplexer). 并且对于MacOS来说, 每次使用&lt;Ctrl+b&gt;实在是太繁琐了. 可以使用改键, 改成Cmd, 具体参考 Josh Medeski 的这篇『macOS Keyboard Shortcuts for tmux』: # alacritty.ymlkey_bindings: - &#123; key: T, mods: Command, chars: &quot;\\x02\\x63&quot; &#125; \\x02\\x63就是&lt;Ctrl+b&gt; + c的十六进制表示, 可以用[xxd -ps](# xxd )验证 I have also been playing with Zellij, a full terminal workspace manager that combines the best of tmux with native tabs and scrollback. Zsh Install 安装zsh shell sudo apt install zsh 切换shell chsh -s /bin/zsh Shell Switch 检查当前可用的shell: cat /etc/shells or: chsh -l 查看当前使用的shell: echo $SHELL set one shell as default for your user: chsh -s full-path-to-shell Ohmyzsh clone mirror: github安装oh-my-zsh（很慢） sh -c &quot;$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot; gitee安装oh-my-zsh: (国内用这个 ) wget https://gitee.com/mirrors/oh-my-zsh/raw/master/tools/install.sh 执行install.sh 如果发现install很慢，可以修改为gitee： vim install.sh 找到以下部分： # Default settingsZSH=$&#123;ZSH:-~/.oh-my-zsh&#125;REPO=$&#123;REPO:-ohmyzsh/ohmyzsh&#125;REMOTE=$&#123;REMOTE:-https://github.com/$&#123;REPO&#125;.git&#125;BRANCH=$&#123;BRANCH:-master&#125; 将中间两行改为： REPO=$&#123;REPO:-mirrors/oh-my-zsh&#125;REMOTE=$&#123;REMOTE:-https://gitee.com/$&#123;REPO&#125;.git&#125; Plugins Installation 如果使用brew, yay等包管理工具安装插件, 会把插件安装在$ZSH/plugins,这些插件被称为&quot;&quot;standard plugins&quot; 如果使用git clone安装插件, 则这些插件被称为&quot;custom plugins&quot;, 需要被clone到$ZSH_CUSTOM/plugins (默认位置是 ~/.oh-my-zsh/custom/plugins) 所有插件在安装完毕后,都需要在~/.zshrc的plugins中配置: plugins=( [custom plugins...] zsh-syntax-highlighting autosuggestion) 修改完配置文件后记得source ~/.zshrc, 然后重新启动shell Plugins autosuggestion &amp;&amp; highlighting 安装 autosuggestion：( github非常慢，改用gitee ) (这里直接用环境变量+重定向，不需要在指定目录下clone了) git clone https://gitee.com/phpxxo/zsh-autosuggestions.git $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-autosuggestions 安装syntax-highlighting： git clone https://github.com/zsh-users/zsh-syntax-highlighting.git $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-syn tax-highlighting fasd &amp; autojump 安装fasd: 它将文件和目录按最近最常用算法排序, 并提供各种操作 yay -S fasd 安装后需要配置~/.zshrc: alias a=&#x27;fasd -a&#x27; # anyalias s=&#x27;fasd -si&#x27; # show / search / selectalias d=&#x27;fasd -d&#x27; # directoryalias f=&#x27;fasd -f&#x27; # filealias sd=&#x27;fasd -sid&#x27; # interactive directory selectionalias sf=&#x27;fasd -sif&#x27; # interactive file selectionalias z=&#x27;fasd_cd -d&#x27; # cd, same functionality as j in autojumpalias zz=&#x27;fasd_cd -d -i&#x27; # cd with interactive selection alias使用: a foo 列出最近操作的路径匹配 &#x27;foo&#x27; 的文件与目录f foo 列出最近操作的路径匹配 &#x27;foo&#x27; 的文件d foo 列出最近操作的路径匹配 &#x27;foo&#x27; 的目录s foo 列出最近操作的路径匹配 &#x27;foo&#x27; 的文件与目录，并可以通过序号选择sf foo 列出最近操作的路径匹配&#x27;foo&#x27;的文件，并可以通过序号选择sd foo 列出最近操作的路径匹配&#x27;foo&#x27;的目录，并可以通过序号选择z foo cd到最近操作的匹配&#x27;foo&#x27;并且得分最高的目录zz foo 列出最近操作的路径匹配&#x27;foo&#x27;的目录，通过序号选择，然后cd进目录 e.g. If you often go to ~/files/cool_project you can simply use z cool to jump there. autojump提供了j命令, 效果和fasd的z命令一样, 可以作为fasd的替代品: yay -S autojump Zsh Theme 目前用powlevel10k， 主页上给了中国大陆的下载方式, oh-my-zsh下： Clone the repository: git clone --depth=1 https://gitee.com/romkatv/powerlevel10k.git $&#123;ZSHz Set ZSH_THEME=&quot;powerlevel10k/powerlevel10k&quot; in ~/.zshrc source一下配置文件 开启主题配置：p10k configure 后续可以继续用p10k configure重新开始配置， 或者手动更改配置文件：~/.p10k.zsh 注意, 我使用了符号链接. ~/.p10k.zsh实际是指向/Users/lyk/Projects/MyOfficialProjects/dotfiles/.p10k.zsh的符号链接. 因此每次p10k configure后, 都不能正确地在~/.p10k.zsh生成配置文件,需要手动修改. p10k Fonts For Details Go To Github Powerlevel10k doesn't require custom fonts but can take advantage of them if they are available. It works well with Nerd Fonts, Source Code Pro, Font Awesome, Powerline, and even the default system fonts. The full choice of style options is available only when using Nerd Fonts. 👇 Recommended font: Meslo Nerd Font patched for Powerlevel10k. 👇 If you are using iTerm2 or Termux, p10k configure can install the recommended font for you. Simply answer Yes when asked whether to install Meslo Nerd Font. Manual font installation Download these four ttf files: [MesloLGS NF Regular.ttf](https://github.com/romkatv/powerlevel10k-media/raw/master/MesloLGS NF Regular.ttf) [MesloLGS NF Bold.ttf](https://github.com/romkatv/powerlevel10k-media/raw/master/MesloLGS NF Bold.ttf) [MesloLGS NF Italic.ttf](https://github.com/romkatv/powerlevel10k-media/raw/master/MesloLGS NF Italic.ttf) [MesloLGS NF Bold Italic.ttf](https://github.com/romkatv/powerlevel10k-media/raw/master/MesloLGS NF Bold Italic.ttf) Double-click on each file and click &quot;Install&quot;. This will make MesloLGS NF font available to all applications on your system. Configure your terminal to use this font: iTerm2: Type p10k configure and answer Yes when asked whether to install Meslo Nerd Font. Alternatively, open iTerm2 → Preferences → Profiles → Text and set Font to MesloLGS NF. Visual Studio Code: Open File → Preferences → Settings (PC) or Code → Preferences → Settings (Mac), enter terminal.integrated.fontFamily in the search box at the top of Settings tab and set the value below to MesloLGS NF. Consult this screenshot to see how it should look like or see this issue for extra information. Alacritty: Create or open ~/.config/alacritty/alacritty.yml and add the following section to it: font: normal: family: &quot;MesloLGS NF&quot; Notes Linux上默认配置不开启username@hostname 的显示，参加官方文档的“How do I add username and/or hostname to prompt?”. 也可以将这行注释掉： # Don&#x27;t show context unless running with privileges or in SSH.typeset -g POWERLEVEL9K_CONTEXT_&#123;DEFAULT,SUDO&#125;_&#123;CONTENT,VISUAL_IDENTIFIER&#125;_EXPANSION= General Terminal Emulater Config Color: Dracula Dracula: 该网站收集了很多终端模拟器的Dracula主题配色方案 iTerms2: Install: git clone https://github.com/dracula/iterm.git 启用该配色： iTerm2 &gt; Preferences &gt; Profiles &gt; Colors Tab Open the Color Presets... drop-down in the bottom right corner Select Import... from the list Select the Dracula.itermcolors file Select the Dracula from Color Presets... Alacritty: Install: Download using the GitHub .zip download option. You just have to import dracula.yml in ~/.config/alacritty/alacritty.yml. ## Dracula Color Theme## It&#x27;s a symlink to my dotfileimport: - ~/.config/alacritty/dracula.yml","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://lyk-love.cn/tags/Linux/"},{"name":"Terminal","slug":"Terminal","permalink":"http://lyk-love.cn/tags/Terminal/"}]},{"title":"Tmux","slug":"Tmux","date":"2022-09-13T14:58:13.000Z","updated":"2022-09-28T10:00:54.348Z","comments":true,"path":"2022/09/13/Tmux/","link":"","permalink":"http://lyk-love.cn/2022/09/13/Tmux/","excerpt":"Outline: Intro Config Hierarchy Commands Session Handling","text":"Outline: Intro Config Hierarchy Commands Session Handling Intro tmux 是最常用的Terminal Multiplexer tmux expects you to know its keybindings, and they all have the form &lt;C-b&gt; x wheretmux has the following hierarchy of objects: All commands in tmux are triggered by a prefix key followed by a command key (quite similar to emacs).By default, tmux uses C-b as prefix key which means press Ctrl+b release Ctrl+b press command key 默认的session_name和window_name都是0开头的 Config 配置文件位于: ~/. tmux.conf , 没有则自己创建 我使用了符号链接+dotfile: 参考这个人的配置 参考 也就是说, Hierarchy Sessions a session is an independent workspace with one or more windows tmux starts a new session. tmux new -s NAME starts it with that name. tmux ls lists the current sessions Within tmux typing &lt;C-b&gt; d detaches the current session tmux a attaches the last session. You can use -t flag to specify which Windows Equivalent to tabs in editors or browsers, they are visually separate parts of the same session &lt;C-b&gt; c Creates a new window. To close it you can just terminate the shells doing &lt;C-d&gt; &lt;C-b&gt; [window_name] Go to the specified window. Note they are numbered &lt;C-b&gt; p Goes to the previous window &lt;C-b&gt; n Goes to the next window &lt;C-b&gt; , Rename the current window &lt;C-b&gt; w List current windows Panes - Like vim splits, panes let you have multiple shells in the same visual display. &lt;C-b&gt; &quot; Split the current pane horizontally &lt;C-b&gt; % Split the current pane vertically &lt;C-b&gt; &lt;direction&gt; Move to the pane in the specified direction. Direction here means arrow keys. &lt;C-b&gt; z Toggle zoom for the current pane &lt;C-b&gt; [ Start scrollback. You can then press &lt;space&gt; to start a selection and &lt;enter&gt; to copy that selection. &lt;C-b&gt; &lt;space&gt; Cycle through pane arrangements. &lt;C-b&gt; x: close the pane Commands Some common commands： C-b z: make a pane go full screen. Hit C-b z again to shrink it back to its previous size C-b C-&lt;arrow key&gt;: Resize pane in direction of 4 C-b ,: Rename the current window For further reading, here is a quick tutorial on tmux and this has a more detailed explanation that covers the original screen command. You might also want to familiarize yourself with screen, since it comes installed in most UNIX systems. Session Handling &lt;C-b&gt; d : detach current session &lt;C-b&gt; D: choose which of your sessions you want to detach. tmux ls: List existing sessions tmux attach -t [session_name]: attach to the session you specified -t : 指定attach的session renaming session: 新建一个session,指定其名字 tmux new -s [session_name] rename your existing session: tmux rename-session -t 0 database","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://lyk-love.cn/tags/Linux/"},{"name":"Terminal","slug":"Terminal","permalink":"http://lyk-love.cn/tags/Terminal/"}]},{"title":"Package Manager","slug":"Package-Manager","date":"2022-09-13T14:58:06.000Z","updated":"2022-09-26T06:39:34.937Z","comments":true,"path":"2022/09/13/Package-Manager/","link":"","permalink":"http://lyk-love.cn/2022/09/13/Package-Manager/","excerpt":"Outline: For Linux For OSX For Languages","text":"Outline: For Linux For OSX For Languages For Linux apt 更新软件源 apt-get updateapt-get upgrade 查看系统已安装的包： apt list --installed pacman 同步存储库数据库，并且更新系统的所有软件包，但不包括不在软件库中的“本地安装的”包： pacman -Syu S 代表同步 y 代表更新本地存储库 u 代表系统更新 conda 推荐miniconda， 直接去NJU MIRROR下载： wget https://mirror.nju.edu.cn/anaconda/miniconda/Miniconda3-py39_4.9.2-Linux-x86_64.sh 清华源 然后安装： bash /path/to/miniconda 重启终端，检查安装是否成功： conda -V 换源 conda换源建议用nju源 （清华源早就不行了，建议别用）， 具体指导 先执行 conda config --set show_channel_urls yes 生成用户目录下的 .condarc 文件 编辑该文件： channels: - defaultsshow_channel_urls: truedefault_channels: - https://mirror.nju.edu.cn/anaconda/pkgs/main - https://mirror.nju.edu.cn/anaconda/pkgs/r - https://mirror.nju.edu.cn/anaconda/pkgs/msys2custom_channels: conda-forge: https://mirror.nju.edu.cn/anaconda/cloud msys2: https://mirror.nju.edu.cn/anaconda/cloud bioconda: https://mirror.nju.edu.cn/anaconda/cloud menpo: https://mirror.nju.edu.cn/anaconda/cloud pytorch: https://mirror.nju.edu.cn/anaconda/cloud simpleitk: https://mirror.nju.edu.cn/anaconda/cloud 即可添加 Anaconda Python 免费仓库。 运行 conda clean -i 清除索引缓存，保证用的是镜像站提供的索引。 运行 conda create -n myenv numpy 测试一下吧 For OSX brew brew安装的软件位置千奇百怪, 但都会在/opt/homebrew/opt留下软链接 事实上, brew可能把 安装： /bin/zsh -c &quot;$(curl -fsSL https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh)&quot; 查看brew版本: brew --version 查看安装的软件版本: brew info [name] 查看某软件的可安装版本: brew search [name] brew可以对它安装的软件进行版本切换. 在最新的3.x版本使用link: brew unlink go &amp;&amp; brew link go@1.13 低版本的brew使用的是switch, here is Doc 查看brew安装的符号路径: 一般都位于/opt/homebrew/opt brew --prefix &lt;package&gt; 查看brew安装的软件的实际路径: brew list &lt;package&gt; For Languages pip3 pip是python的一个包管理工具，python2:使用pip, python3使用pip3 ( Python3下使用pip 默认用的是pip3 ) 安装 yay -S pip3 mac用户：brew intall pip3相当卡， 因此要用： curl bootstrap.pypa.io/get-pip.py | python3 检查安装是否成功： pip3 --version 换源 在 pip 命令中使用 -i 参数来指定镜像地址 pip3 install numpy -i https://mirrors.aliyun.com/pypi/simple/ 如果需要配置全局的镜像地址，需要修改配置文件: Linux/Mac os 环境中，配置文件在 ~/.pip/pip.conf（如不存在创建该目录和文件）： mkdir ~/.pip 打开配置文件 ~/.pip/pip.conf，修改如下： [global]index-url = https://mirrors.aliyun.com/pypi/simple/[install]trusted-host = https://mirrors.aliyun.com 查看镜像地址： pip3 config list npm 换源 国内优秀npm镜像： 淘宝npm镜像 搜索地址：http://npm.taobao.org/ registry地址：http://registry.npm.taobao.org/ cnpmjs镜像 搜索地址：http://cnpmjs.org/ registry地址：http://r.cnpmjs.org/ 1.临时使用 npm install express --registry https://registry.npm.taobao.org 2.持久使用 npm config set registry https://registry.npm.taobao.org 配置后可通过下面方式来验证是否成功: npm config get registry","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://lyk-love.cn/tags/Linux/"}]},{"title":"Unix CLI Tools","slug":"Unix-CLI-Tools","date":"2022-09-13T14:57:44.000Z","updated":"2022-09-26T06:39:34.942Z","comments":true,"path":"2022/09/13/Unix-CLI-Tools/","link":"","permalink":"http://lyk-love.cn/2022/09/13/Unix-CLI-Tools/","excerpt":"一些常用的命令行工具，和它们的现代替代 ref: MIT lesson, Unix命令的现代替代","text":"一些常用的命令行工具，和它们的现代替代 ref: MIT lesson, Unix命令的现代替代 Easy Installation 对于传统UnixCLi工具,都已经被加入到Unix标准工具集中,不需要手动下载了. 而很多新工具需要手动下载. 以下列出的所有新工具均可以用包管理器( yay for Arch, brew for OSX )下载. 为了方便, 我还提供了依赖列表来批量下载. 依赖列表说明 文件名格式为[XX_Packages]_[Platform[_Advanced]]].txt: [xx_packages]: 就是包的类型, 比如Resource_Monitoring_Packages等等 [Platform]:包兼容的平台 Linux: 故名思义,就是Linux下可以安装的包 Mac: Mac OSX下可以安装的包. 总体来说和linux下没有大的区别 General:Linux和Mac同时兼容的包 [XX]_[Platform[_Advanced]: 如果一个列表出现了有和无Advanced的两个版本, 说明该列表的类型内有些包用处不大, 不需要安装, 也就没有包括在无Advanced的列表里面. 而Advanced列表中包含了全部包. 比如同为终端文件浏览器的ranger和broot,一般情况下使用前者就够了, 因此 File&amp;Oir_Operations_Packages_General.txt只包含了ranger, 而File&amp;Oir_Operations_Packages_Anvanced.txt包含了后者 依赖列表使用 Mac: brew install $(cat [packages].txt) for other platform just replace brew with your correspondding package manager File and Dir File&amp;Dir_Operations_Packages_General.txt: 没安装broot exa ranger bat diff-so-fancy fd ripgrep File&amp;Dir_Operations_Packages_Advanced.txt: 把broot加上了 exa ranger bat diff-so-fancy fd ripgrep broot 列出目录: exa exa是ls命令的替代品，可以彩色输出目录内容，更容易辨识: exa --long --git 参数--git表示输出文件的 Git 状态。 N：新文件 M：文件有变动 I：该文件被忽略 除了平铺显示，exa还支持目录的树状显示。 exa --tree --level=2 终端文件浏览器: ranger / broot ranger: 终端文件浏览器, 支持vim语法 broot: 类似ranger的终端文件浏览器， 不支持vim语法，但是能浏览媒体文件 Usage: 列出当前目录的文件信息。 br -sdph -s：显示文件体积 -d：显示文件时间 -p：显示文件权限 -h：显示隐藏文件 在broot界面中，可以完成各种文件操作： 搜索：输入搜索词 复制：输入:cp &lt;复制后的文件名&gt; 删除：输入:rm 改名：输入:mv &lt;新文件名&gt; 退出：输入:q 还可以预览文件： 打开外部关联程序： 通过上下箭头选中文件，然后按下右箭头→ 在当前窗口预览文件： ctrl + → bat &amp; fx &amp; hexyl bat : cat命令的替代品，输出文本文件的内容，并且带有cat所没有的代码高亮和行号 yay -S bat usage: bat README.md fx 或者 jq: 浏览JSON 文件 yay -S fx usage: fx data.json hexyl: 浏览二进制文件 yay -S hexyl usage: hexyl example.jpg diff-so-fancy diff-so-fancy : 对diff进行了改进, 使得命令行的 diff 操作具有类似 GitHub 的页面效果. 可以用来替代git diff和diff yay -S diff-so-fancy 安装后, 参考官方文档进行配置: With diff Use -u with diff for unified output, and pipe the output to diff-so-fancy: diff -u file_a file_b | diff-so-fancy find/fd One of the most common repetitive tasks that every programmer faces is finding files or directories. All UNIX-like systems come packaged with find, a great shell tool to find files. find will recursively search for files matching some criteria. Some examples: # Find all directories named srcfind . -name src -type d# Find all python files that have a folder named test in their pathfind . -path &#x27;*/test/*.py&#x27; -type f# Find all files modified in the last dayfind . -mtime -1# Find all zip files with size in range 500k to 10Mfind . -size +500k -size -10M -name &#x27;*.tar.gz&#x27; Beyond listing files, find can also perform actions over files that match your query. This property can be incredibly helpful to simplify what could be fairly monotonous tasks. # Delete all files with .tmp extensionfind . -name &#x27;*.tmp&#x27; -exec rm &#123;&#125; \\;# Find all PNG files and convert them to JPGfind . -name &#x27;*.png&#x27; -exec convert &#123;&#125; &#123;&#125;.jpg \\; Despite find’s ubiquitousness, its syntax can sometimes be tricky to remember. For instance, to simply find files that match some pattern PATTERN you have to execute find -name '*PATTERN*' (or -iname if you want the pattern matching to be case insensitive). You could start building aliases for those scenarios, but part of the shell philosophy is that it is good to explore alternatives. Remember, one of the best properties of the shell is that you are just calling programs, so you can find (or even write yourself) replacements for some. For instance, fd is a simple, fast, and user-friendly alternative to find. It offers some nice defaults like colorized output, default regex matching, and Unicode support. It also has, in my opinion, a more intuitive syntax. For example, the syntax to find a pattern PATTERN is fd PATTERN. Most would agree that find and fd are good, but some of you might be wondering about the efficiency of looking for files every time versus compiling some sort of index or database for quickly searching. That is what locate is for. locate uses a database that is updated using updatedb. In most systems, updatedb is updated daily via cron. Therefore one trade-off between the two is speed vs freshness. Moreover find and similar tools can also find files using attributes such as file size, modification time, or file permissions, while locate just uses the file name. A more in-depth comparison can be found here. grep &amp; ripgrep For now, know that grep has many flags that make it a very versatile tool. Some I frequently use are -C for getting Context around the matching line and -v for inverting the match, i.e. print all lines that do not match the pattern. For example, grep -C 5 will print 5 lines before and after the match. When it comes to quickly searching through many files, you want to use -R since it will Recursively go into directories and look for files for the matching string. But grep -R can be improved in many ways, such as ignoring .git folders, using multi CPU support, &amp;c. Many grep alternatives have been developed, including ack, ag and rg. All of them are fantastic and pretty much provide the same functionality. For now I am sticking with ripgrep (rg), given how fast and intuitive it is. Some examples: # Find all python files where I used the requests libraryrg -t py &#x27;import requests&#x27;# Find all files (including hidden files) without a shebang linerg -u --files-without-match &quot;^#!&quot;# Find all matches of foo and print the following 5 linesrg foo -A 5# Print statistics of matches (# of matched lines and files )rg --stats PATTERN Note that as with find/fd, it is important that you know that these problems can be quickly solved using one of these tools, while the specific tools you use are not as important. ripgrep 是grep命令的替代品，用来搜索文件内容, 默认带有行号和搜索词高亮，速度也更快: 它跟grep的用法类似。下面例子是搜索当前目录里面，所有内容包含字符串foo的 Markdown 文件。 # grep 的写法$ grep foo *.md# ripgrep 的写法$ rg foo *.md Finding Shell History Finding_Shell_History_Packages_General.txt: fzf mcfly history : 在STDOUT上打印shell命令记录, 可以用 history | grep find 进行检索 Ctrl+R + type a substring you want to match for commands in your history来检索历史命令. This can also be enabled with the UP/DOWN arrows in zsh fzf &amp;&amp; mcfly fzf : GO 语言编写的交互式的 Unix 命令行工具。可以用来查找任何 列表内容，文件、Git 分支、进程等。所有的命令行工具可以生成列表输出的都可以再通过管道 pipe 到 fzf 上进行搜索和查找. yay -S fzf 这个命令比较复杂, 见教程 McFly 是ctrl-r的替代品，安装以后，只要按下ctrl-r，就会出现一个更好用的搜索界面 yay -S mcfly Version Control Configure git: diff-so-fancy Configure git to use diff-so-fancy for all diff output: git config --global core.pager &quot;diff-so-fancy | less --tabs=4 -RFX&quot;git config --global interactive.diffFilter &quot;diff-so-fancy --patch&quot; Improved colors for the highlighted bits The default Git colors are not optimal. The colors used for the screenshot above were: git config --global color.ui truegit config --global color.diff-highlight.oldNormal &quot;red bold&quot;git config --global color.diff-highlight.oldHighlight &quot;red bold 52&quot;git config --global color.diff-highlight.newNormal &quot;green bold&quot;git config --global color.diff-highlight.newHighlight &quot;green bold 22&quot;git config --global color.diff.meta &quot;11&quot;git config --global color.diff.frag &quot;magenta bold&quot;git config --global color.diff.func &quot;146 bold&quot;git config --global color.diff.commit &quot;yellow bold&quot;git config --global color.diff.old &quot;red bold&quot;git config --global color.diff.new &quot;green bold&quot;git config --global color.diff.whitespace &quot;red reverse&quot; 以后每次执行git diff，就会输出它的执行结果 gitupdate gitupdate是一个工具, 用于Commit and push updated files with file names as commit message: Install: go install github.com/nikitavoloboev/gitupdate@latest Usage: You can either use it by passing it a file path (with git repo) that you want to commit. i.e. gitupdate /Users/nikivi/src/cli/gitupdate Or if you are already in the git directory you want to commit, run: gitupdate . This will add all files that have changed since last commit and will include all the file names (without extension) as the commit message. Example use. If you want to only consider top level folders for the commit message, use the --top (or -t for short) flag. Text Operation Text_Operations_Packages_General.txt choose jq choose choose 是cut命令的替代品，用来选中指定的栏位。 它的优势主要是语法更简单，比如输出文件的第一列。 # cut 的写法$ cat data.txt | cut -d &quot; &quot; -f 1# or$ cut -d &quot; &quot; -f 1 data.txt# choose 的写法$ cat data.txt | choose 0# or$ choose 0 -i data.txt jq jq: sed for JSON data. Resource Monitoring Resource_Monitoring_Packages_General.txt: htop bottom iotop procs duf ncdu free lsof 可以酌情添加glances和dstat General Monitor: htop &amp;&amp; bottom &amp;&amp; iotop top: 传统的资源监控工具 htop: htop has a myriad of options and keybinds, some useful ones are: &lt;F6&gt; to sort processes, t to show tree hierarchy and h to toggle threads. glances: alternative for htop. with a great UI. For , dstat : alternative for htop. with aggregate measures across all processes. It can computes real-time resource metrics for lots of different subsystems like I/O, networking, CPU utilization, context switches, &amp;c. bottom: Yet another cross-platform graphical process/system monitor. You can run bottom using btm. For help on flags, use btm -h for a quick overview or btm --help for more details. For info on key and mouse bindings, press ? inside bottom or refer to the documentation. iotop displays live I/O usage information and is handy to check if a process is doing heavy I/O disk operations Process Info:procs ps： 显示进程信息 procs: ps的现代替代 Disk Usage: duf, ncdu df : 显示磁盘占用情况 du 显示当前目录下每个文件占用的磁盘空间 -h ： print with human readable format. duf： df的现代替代 ncdu： du的现代替代 Memory Usage: free free displays the total amount of free and used memory in the system. Memory is also displayed in tools like htop. Open Files: lsof lsof lists file information about files opened by processes. It can be quite useful for checking which process has opened a specific file. Networking networking_packages_Linux.txt: ss iproute2 nethlogs iftop gping dog Networking_Packages_Mac.txt: 没有ss, iproute2改成了iproute2mac iproute2mac nethlogs iftop gping dog Network Package Monitor: ss ss: short for &quot;socket statistics&quot;. ss lets you monitor incoming and outgoing network packets statistics as well as interface statistics. A common use case of ss is figuring out what process is using a given port in a machine. 当服务器的socket连接数量变得非常大时，无论是使用netstat命令还是直接cat /proc/net/tcp，执行速度都会很慢, 用ss才是节省时间。 教程 该工具似乎没有mac版 Network Config: ip ip: 用于操作路由,网络设备和网络接口. 是netstat and ifconfig 的现代替代, 后二者已经停止维护了，可能不会被包含在新的Linux发行版中. ip命令位于iproute2工具包中. Linux: apt install iproute2 Mac版本: brew install iproute2mac Usage 也可以手动在/etc/network/interfaces内永久地添加路由信息 查看路由表 ip route 注意，物理上的路由表和转发表，在Linux内核中被抽象成了一个东西，不作区分，所以统称为路由表 如果用netstat: netstat -nr Add a new default gateway route: add default via &lt;gateway_IP&gt; Add/Delete an IP address to an interface: sudo ip route add &lt;network_ip&gt;/&lt;cidr&gt; via &lt;gateway_ip&gt; [ dev &lt;network_card_name&gt; ] 如果不指定网络设备，则会默认使用第一块网卡( 不会使用环回地址) 也可以添加默认路由: 把&lt;network_ip&gt;/&lt;cidr&gt;换成default Modify an existing route using ip route command sudo ip route change Clear routes with flush using ip route command sudo ip route flush [ip] Clear all the routes from the routing table using ip route command sudo ip route flush table main Network Usage: nethogs, iftop nethogs and iftop are good interactive CLI tools for monitoring network usage. If you want to test these tools you can also artificially impose loads on the machine using the stress command. Network Connections: gping gping: ping, but with a graph. NetworkRequests: httpie httpie 是 curl 的替代品，用来发出 HTTP 请求。 它的特点是语法更简单，并且服务器的返回内容会格式化高亮显示。 下面是发出 PUT 请求的例子。 # curl 的写法$ curl -X PUT -d hello=world example.com# httpie 的写法$ http PUT example.com hello=world 同类项目还有 curlie、xh。 DNS: dog dog : 是dig命令的替代品，用来发出 DNS 查询。它的用法比dig简单多了。 # 默认查询 A 记录dog example.net# 查询多个记录dog example.net A NS MX Benchmarking Benchmarking_Packages_General.txt: hyperfine hyperfine hyperfine: A command-line benchmarking tool. Calculating Calculating_Packages_General.txt scal sc sc: short for: scal Efficiency tldr yay -S tldr 或者用man/info，后者的信息存放在/usr/info","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://lyk-love.cn/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"http://lyk-love.cn/tags/Shell/"}]},{"title":"Lexical Analysis","slug":"Lexical-Analysis","date":"2022-09-12T16:30:57.000Z","updated":"2022-09-26T06:39:34.933Z","comments":true,"path":"2022/09/13/Lexical-Analysis/","link":"","permalink":"http://lyk-love.cn/2022/09/13/Lexical-Analysis/","excerpt":"Outline: Intro Language Pattern Token","text":"Outline: Intro Language Pattern Token Intro lexical analysis: 词法分析器( Lexical Analyser, or Scanner )将源程序按字符流读入, 按照模式匹配映射成一个个 lexeme( 词素 ), 再将lexeme转化成如下形式的token( 词法单元 ): &lt; token-name, attribute-value &gt; 当一串字符能模式匹配多个词素时, 必须通过属性来传递附加的信息。 token-name 表示某种词法单位的抽象符号. 语法分析器通过token-name即可确定token sequence的结构 attribute-value 指向符号表的一项, 用于语义分析和代码生成 Scanner由Parser调用, 需要的时候不断读取读入并生成Token Language 字母表: 一个有限的符号集合 二进制 {0, 1} ASCII Unicode 典型的字母表包括字母、数位和标点符号 串: 字母表中符号组成的一个有穷序列 $|s|$: 串s的长度 $\\epsilon$: 长度为0的串, 空串 语言:给定字母表上一个任意的可数的串的集合. 语法正确的C程序的集合, 英语, 汉语 和串有关的术语(banana) 前缀: 从串的尾部删除0个或多个符号后得到的串 (ban、banana、 ε) 后缀: 从串的开始处删除0个或多个符号后得到的串 (nana、banana、ε) 子串: 删除串的某个前缀和某个后缀得到的串 (banana、nan、 ε) 真前缀, 真后缀, 真子串:既不等于原串，也不等于空 串的前缀、后缀、子串 子序列: 从原串中删除0个或者多个符号后得到的串 (baan) 串的运算: 连接(concatenation): x和y的连接的就是把y附加到x的后面形成的串, 记作xy x=dog, y=house, xy=doghouse 指数运算(幂运算): $s_0=\\epsilon$ , $s_1=s$ , $s_i=s^{i-1}s$ $x$=dog , $x^0$=ε , $x^1$=dog , $x^3$=dogdogdog 语言上的运算: Operation Def L和M的并 $L \\cup M = { s \\ L和M的连接 $LM = {st \\ L的Kleene闭包 $L^* = \\cup_{i=0}^\\infty L^i$ L的正闭包 $L^+ = \\cup_{i=1}^\\infty L^i$ Pattern Parser对输入的字符串进行模式匹配, 得到lexeme 模式( Pattern )可以用正则表达式( Regular Expression )来表示. 面对复杂的语言时, 正则也会变得极其复杂, 为此可以用NFA或DFA来表示模式. 可以证明, 正则, NFA, DFA是等价的, 可以相互转换 保留字和标识符的识别 在很多程序设计语言中，保留字也符合标识符的模式, 识别标识符的状态转换图也会识别保留字. 解决方法: 在符号表中预先填写保留字，并指明它们不是普通标识符 为关键字/保留字建立单独的状态转换图. 并设定保留字的优先级高于标识符 Token Token Description Lexeme Example if 字符i, f if else 字符e, l, s, e else comparison &lt;, &gt;, &lt;=, !=等比较运算符 &lt;=, != id 字母开头的字母/数字串 Po, score, D2 number 任何数字常量 3.1415, 0, 6.02e42 literal 在&quot;&quot;之间, 除&quot;&quot;以外的任何字符 &quot;core dumped&quot; Example 字符串E = M * C ** 2对应的Token: &lt;id, 指向符号表中E的条目的指针&gt; &lt;assign_op&gt; &lt;id, 指向符号表中M的条目的指针&gt; &lt;mult_op&gt; &lt;id, 指向符号表中C的条目的指针&gt; &lt;exp_op&gt; &lt;number, 整数值2&gt;","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"Compilers","slug":"Compilers","permalink":"http://lyk-love.cn/tags/Compilers/"}]},{"title":"Programming Language Basic","slug":"Programming-Language-Basic","date":"2022-09-12T16:30:33.000Z","updated":"2022-09-26T06:39:34.937Z","comments":true,"path":"2022/09/13/Programming-Language-Basic/","link":"","permalink":"http://lyk-love.cn/2022/09/13/Programming-Language-Basic/","excerpt":"Outline: The Static/Dynamic Distinction Environments and States Name Variable ...","text":"Outline: The Static/Dynamic Distinction Environments and States Name Variable ... The Static/Dynamic Distinction If a language uses a policy that allows the compiler to decide an issue then we say that the language uses a static policy or that the issue can be decided at compile time. static: 某件事在编译期就被决定 dynamic: 在运行期就被决定 以上是抽象的static/dynamic概念, 一个语言可以在一个方面是static的, 另一个方面又是dynamic的. 例如: 对于Type System, 静态类型语言的类型在编译期就决定, 而动态类型语言的类型在运行期才能决定. 事实上, 由于类型系统最受关注, 我们叙述一门语言特性的时候, 经常省略掉定语“类型系统”, 直接说“XX语言是静态/动态的”. 对于变量的内存分配, 如果一个变量是静态变量( class variable in Java ): public class Foo&#123; public static int x;&#125; 变量x的内存分配已经确定了, 所有Foo的instance都共享这一个变量. Environments and States environment: 从name到variable的映射, 来区分同一作用域内同名的不同变量. 例如: int x = 1;void f()&#123; int x = 3; cout &lt;&lt; x &lt;&lt; endl;&#125; f()中的变量名x被environment映射到变量x = 3. 而f外的变量名x被environment映射到变量x = 31. 当然, 变量variable, 或者说左值lvalue, 就是一块具有值的内存的location, 所以“映射到变量”, 其实就是“映射到location” 绝大多数PL的environment是动态的, 否则也无法区分同一作用域内同名的不同变量了. state: 从location到value的映射. 即lvalue到rvalue的映射 大部分PL的state是动态的, 因为只有程序运行时才能得到value. 但也有编译期决定的值, 也就是静态的state, 比如const: const a = 1000 这里a的值在编译期就决定了, 是1000. 当然, Macro的值也是编译器决定的: #define ARRAYSIZE 1000 不过,macro不是变量, 只是个文本替换而已, 不能纳入name - variable - value的范畴 Name name name: 这里的name是抽象的概念, 它指程序中的一段有意义的文本. $$ \\mathrm{name} = { \\mathrm{variable \\ name}, \\mathrm{macro \\ name}, \\mathrm{expression \\ name}, \\mathrm{keyword} } $$ identifer identifer: name的子集, 它的文本是符号形式的, 标识了一个程序中的实体 $$ \\mathrm{identifer} = { \\mathrm{variable \\ name}, \\mathrm{macro \\ name} } $$ identifer不包括expression name, 因为expression name是一段文本而不是一个符号; 也不包括keyword, 因为keyword并不标识什么实体, 在编程模型中也不发挥什么作用. 例如: class Foo&#123;&#125;class Bar&#123;Foo foo;&#125;Bar bar = new Bar();bar.foo; //expression name 这里的bar和foo都是variable name( identifer ), 但bar.foo是一个expression name macro name macro name: macro的文本名字, 由于macro是预处理器面对的东西, 是一个文本上的替换, 对编译器是透明的, 对于程序而言不算“真正的”实体. 因此macro不属于变量, macro name也不是variable name. variable name variable name: 变量的文本名字, 它和identifier的唯一区别就是不含macro name. 严格地说, 由于name只对编译器有用, 变量名作为一个文本, 其用处就是帮助编译器找到对应的变量. 从这个意义上讲, 变量名其实要包括变量的类型, 对于普通变量: int x = 1; double y = 0.43 具体的变量名应该是int x, double y 对于函数, 它的变量名就是Function Signature. 当然这样讲太啰嗦了, 因此通常我们就说变量名时不说类型. Variable variable: 这里的“变量”是比较抽象的说法, 它是一块具有值的内存的location( 也就是一个地址/指针 ), 或者说左值lvalue. 因此任何在内存中实际存在, 或者说对程序可见的“实体”都可以称为“变量”. ( Macro在编译前就被替换了, 也无所谓什么内存, 肯定不是变量 ). 注意, “变量是指针”这个说法比较拗口. 所以我们一般就用变量指向的值来代表该变量, 也就是说一般省略掉“变量是指针”这一事实 比如对于int x = 3, 我们就说x是个int类型的变量. 不会说“x是个lvalue, 它指向的rvalue是int类型的3” 同理, 对于int *p = new Obj(), 我们就说p是个指针变量, 不会说&quot;p是个lvalue, 它指向的rvalue是一个指针, 该指针又指向了对象&quot; 对于Java这样的“所有变量都是引用(指针)”的语言, 我们就说某某变量是个引用, 不会说“该变量是个lvalue, 它指向的rvalue是一个指针, 该指针又指向了对象” 常规的“变量”: int x = 3; x是个变量名, 它被environment映射到一个具体的变量( OR location), 为了叙述方便, 我们通常直接说&quot;变量x&quot;. 我们使用变量名x, 也就是通过它找到对应的那个变量或者说左值, 为了方便我们就将其称为变量x, 它就是个地址, 我们需要使用它的内容, 也就是它对应的右值value. 函数也可以看作变量: #define ARRAYSIZE 1000void f() &#123; ... int x = ARRAYSIZE;&#125; f是一个变量名, 当然严谨地说, 它的name是它的函数signature: void f(). 和常规的“变量”一样, 我们定义一个函数时, 会创建这个函数名对应的实体( 为了方便, 该实体就称为函数f ), 并为其分配内存. 在将类型作为first-class member的语言中, 类型也可以看做变量, 例如在Zig中: const std = @import(&quot;std&quot;);const assert = std.debug.assert;test &quot;types are values&quot; &#123; const T1 = u8; const T2 = bool; assert(T1 != T2); const x: T2 = true; assert(x);&#125; const std = @import(&quot;std&quot;);fn List(comptime T: type) type &#123; //类型可以作为参数和返回值传递 return struct &#123; items: []T, len: usize, &#125;;&#125;pub fn main() void &#123; var buffer: [10]i32 = undefined; var list = List(i32)&#123; .items = &amp;buffer, .len = 0, &#125;; std.debug.print(&quot;&#123;d&#125;\\n&quot;, .&#123;list.items.len&#125;);&#125; 可以看到, 所有的基本数据类型, 和用户自定义类型, 都是**type类型的变量**, 类型甚至可以作为参数和返回值传递. 这就把类型和变量的地位等同起来了. Assignment and getValue 变量其实是一个指针, 是一个lvalue. 变量的赋值, 也就是把一个rvalue copy到变量( 即lvalue )指向的空间, 即将原有的rvalue擦除，而以一个新值来替代. 变量的取值, 就是把变量( 即lvalue)所指向的空间的rvalue取出来. 注意, 只有lvalue可以取值, rvalue本身就是值. 我们所谓的“使用变量”, 其实是使用变量指向的值, 也就是lvalue对应的rvalue. 所以“使用变量”要先对变量取值 对于Java这样的所有变量皆引用的语言, 其“使用变量”实际上是先得到rvalue( 一个指针/引用 ), 然后使用rvalue所指向的值(对象). 例子: int x = 3; //变量赋值sum(x, 5); //使用变量 把rvalue 3 copy到了变量x指向的空间. 以后访问lvalue x, 得到的值就是3 使用了变量x. 也就是对x先进行取值, 得到rvalue 3, 作为sum的第一个argument Declaration, Definition and Initialization 声明和定义都是对变量而言. 表达式是个rvalue, 不存在“声明”, 都是“定义”. 对于C/CPP而言, Declaration, Definition and Initialization有明确区分, 对于其他语言就不一定了. Declaration: 使一个variable name为程序所知, 并且规定其type extern int x; // 在c/cpp中, 需要加extern来做到只声明不定义 Definition: 真的创建该variable, 即为该变量申请一块内存. 定义时, 变量会被赋值, 此时的赋值称为“初始化”. 可以手动赋值, 也就是“显示初始化”: int x = 1; // 声明并定义, 并显式初始化extern int k = 1//声明并定义k. 任何包含了显式初始化的声明即成为定义，这样写语法上没错，但会抵消extern的作用 也可以不手动赋值,则会“隐式初始化”, 其赋的值要取决于具体情况: int x; // 声明并定义, 不显式初始化 很多时候我们把声明和定义写在一起. 为了方便, 声明+定义统称为定义: int x = 1; Initialization: 定义时, 变量会被赋值, 此时的赋值称为“初始化” 可以手动赋值, 也就是“显示初始化”: int x = 1; // 声明并定义, 并显式初始化 也可以不手动赋值,则会“隐式初始化”, 其赋的值要取决于具体情况: int x; // 声明并定义, 不显式初始化 函数也是变量, 因此对函数的声明就是写下函数签名( 别忘了Function Signature就是函数的variable name ), 使该函数名为程序所知. 对函数的定义也就是创建该函数实体, 为函数分配内存, 并赋值, 赋的“值”( rvalue )就是其函数实现.在C/CPP中, 我们经常把变量/函数的声明和定义分开 Parameter and Argument parameter: 形式参数, 函数本身的参数 argument: 实际参数, 函数调用方传入的参数 a = 1def f(b): return b*2print( f(a) ) 这里的a是argument, b是parameter 函数的参数是可以是lvalue(变量), 也可以是rvalue( 表达式, 常量... ). Parameter Passing Call by Value int a = 3;void f( int b )&#123; b = b * 2;&#125; 变量名a对应变量a ( lvalue a), 它其实是个形如0X432...的地址, 在0X432...处存储了值3. lvalue b也同理, 是个形如0X872... 按值传递就是取出lvalue a 的rvalue, 然后赋值到lvalue b. 即取出0X432...的值, 然后赋值到0X872... Call by Value: 得到argument的rvalue, 然后赋值给parameter. 函数内部操作的是parameter, 其值的改变不会影响原来的argument. 如果argument是个variable, 即一个lvalue 就会对其取值,得到rvalue 如果argument是个expression, 则会对其求值, 得到结果(rvalue) Call by Reference int a = 3;void f( int&amp; b )&#123; b = b * 2;&#125; 按引用传递就是直接把a的lvalue copy给b , 现在b的lvalue就是0X432... ( b原来的的lvalue就没有了 ). 对b的操作其实就是对a的操作. Call by Reference: 直接将argument的lvalue copy给parameter, 不会取argument的rvalue 在Java中所有变量都是引用( 参见前文, 这话的实际意思是, 变量指向的rvalue是一个指针 ), 所有参数都是按值传递. 由于rvalue是一个指针, 按值传递又会赋值rvalue, 最终效果就是把指针赋值给了parameter. 对parameter的更改也就会导致对同一对象的更改. Static Scope and Block Structure 绝大部分语言的Scope是静态的, 语言被组织为一个个的block: C使用花括号: &#123; ...&#125;&#123; ...&#125; Awk使用begin, end: begin ...end Procedures, Functions and Methods 我们经常把Procedure, Function和Method混用, 严格来讲它们是不同的: Function: 返回某个类型的值的函数 Procedure: 没有返回值的函数 Method: Function, 但是是某个对象的成员 C中只有Function, 但是通过void f(), Function也就可以看做Procedure. Java作为纯OO语言只有Method. Explicit Access Control OO语言会有: public protected private Aliasing 对于按引用传递的语言, 或者Java这样的类似按引用传递( 按值传递 + 变量即引用 )的语言, 由于直接传递了指向对象的指针, 因此可能会有两个变量指向同一个对象: String[] cars = &#123;&quot;Volvo&quot;, &quot;BMW&quot;, &quot;Ford&quot;, &quot;Mazda&quot;&#125;;void f( String[] a, String[] b )&#123; a[0] = &quot;Tesla&quot; b[0] = a[0] + &quot;Haha&quot;&#125; 这种情况称为Aliasing, 分析Aliasing对于编译器的Optimization非常重要: x = 2; 对于如上语句, 只有当编译器确定x所引用的对象没有Aliasing时, x才能直接被替换为2","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"Compilers","slug":"Compilers","permalink":"http://lyk-love.cn/tags/Compilers/"}]},{"title":"Computer History","slug":"Computer-History","date":"2022-09-12T16:30:01.000Z","updated":"2022-09-26T06:39:34.928Z","comments":true,"path":"2022/09/13/Computer-History/","link":"","permalink":"http://lyk-love.cn/2022/09/13/Computer-History/","excerpt":"介绍了计算机, 操作系统和编程语言的历史 Outline: The first true digital computer The First Generation (1945–55): Vacuum Tubes The Second Generation (1955–65): Transistors and Batch Systems The Third Generation (1965–1980): ICs and Multiprogramming The Fourth Generation (1980–Present): Personal Computers","text":"介绍了计算机, 操作系统和编程语言的历史 Outline: The first true digital computer The First Generation (1945–55): Vacuum Tubes The Second Generation (1955–65): Transistors and Batch Systems The Third Generation (1965–1980): ICs and Multiprogramming The Fourth Generation (1980–Present): Personal Computers The first true digital computer was designed by the English mathematician Charles Babbage (1792–1871) Babbage realized that he would need software for his analytical engine, so he hired a young woman named Ada Lovelace 《信息简史》里讲了他们的故事，向他们致敬！ The First Generation (1945–55): Vacuum Tubes 编程语言就是二进制, 写在插板(后来是穿孔卡片)上, 也没有编译器 The Second Generation (1955–65): Transistors and Batch Systems Batch Systems： The idea behind it was to collect a tray full of jobs in the input room and then read them onto a magnetic tape using a small (relatively) inexpensive computer, such as the IBM 1401, which was quite good at reading cards, copying tapes, and printing output, but not at all good at numerical calculations. Other, much more expensive machines, such as the IBM 7094, were used for the real computing 用IBM 140把卡片翻译成磁带， 用7094处理磁带，把结果打印到磁带上， 再将结果磁带交给1401离线打印（不需要7094了） 典型：FMS（ the Fortran Monitor System ）和 IBSYS( IBM's system for the 7094 ). 主要用于科学计算 这一时期诞生了高级语言: Fortran for scienti􏰁c computation Cobol for business data pro cessing Lisp for symb olic computation 此时的编译器已经比较复杂了: the original fortran compiler was a multipass system that included a distinct scanner, parser, and register allocator, along with some optimizations [26, 27]. The Third Generation (1965–1980): ICs and Multiprogramming By the early 1960s, most computer manufacturers had two distinct, incompatible, product lines： 大型机（类似7094），用于科学、工业上的数值计算 商业机， 用于银行、保险公司等的磁带排序和印刷 System/360 IBM推出的一系列software-compatible machines,拥有同样的架构和指令集，彼此只有价格、性能不同。 IBM 360 was the first major computer line to use (small-scale) ICs, 带来了价格和性能上的巨大提升 360还有很多后继，如370, 4300, 3080, 3090， zSeries之类 OS/360: 360的操作系统 360的&quot;a family of compatible computers&quot;迅速流行。但是，这对软件（尤其是OS）的编写产生了巨大困难。 The original intention was that all software, including the operating system, OS/360, had to work on all models。 OS/360项目开发到最后,就如同“a herd of prehistoric beasts stuck in a tar pit” OS/360引入了许多新技术，包括 multiprogramming： 内存分区， 可以多任务, 避免单任务阻塞使得CPU idle spooling (from Simultaneous Peripheral Operation On Line)： 直接将任务从卡片读到磁盘上，输出反之。 再也不需要磁带了 ） CTSS(Compatible Time Sharing System) developed at M.I.T. on a specially modified 7094 timesharing, a variant of multiprogramming, in which each user has an online terminal。 即多用户。提高开发速度。 由于缺乏必要的protection hardware。 CTSS没有流行 MULTICS (MULTiplexed Information and Computing Service) M.I.T., Bell Labs, and General Electric研发（最后只有MIT坚持）， 目的是设计一款“computer utility”。 类似国家电力系统，支持同时间许多人的timesharing，MULTICS被设想为房子那样大的计算设施。 由于采用PL/I语言和太过有野心，MULTICS没有预期的那样成功 MULTICS的概念就是当今的云计算 minicomputers starting with the DEC PDP-1 in 1961 culminating in the PDP-11 UNIX Ken Thompson，one of the computer scientists at Bell Labs who had worked on the MULTICS project, subsequently found a small PDP-7 minicomputer that no one was using and set out to write a stripped-down, one-user version of MULTICS. This work later developed into the UNIX operating system Two major versions developed: System V, from AT&amp;T BSD (Berkeley Software Distribution) from the University of California at Berkeley POSIX 为了让程序能运行在所有UNIX系统，IEEE定义了一个UNIX标准，称为POSIX. POSIX defines a minimal system-call interface that conformant UNIX systems must support. 很多其他的OS也支持OS MINIX in 1987, the author released a small clone of UNIX, called MINIX, for educational purposes. 相当稳定 Linux Linus Torvalds为了得到一个完全免费的MINIX,在其基础上开发了LINUX The Fourth Generation (1980–Present): Personal Computers 由于LSI (Large Scale Integration) circuits的发展，个人计算机成为了可能，它和minicomputer的差距主要是价格上的 CP/M (Control Program for Microcomputers) a disk-based OS for 8080, the first general-purpose 8-bit CPU invented by Intel 由于Intel不认为CP/M有什么未来，就允许作者Kildall拿着专利自立门户，后者成立了Digital Research。 DOS (Disk Operating System) In the early 1980s, IBM designed the IBM PC and looked around for software to run on it. People from IBM contacted Bill Gates to license his BASIC interpreter. They also asked him if he knew of an operating system to run on the PC. Gates suggested that IBM contact Digital Research, then the world’s dominant operating systems company。 Kildall拒绝了， 盖茨就找了Seattle Computer Products， 购买了DOS，将其与BASIC捆绑卖给IBM, 后者想要一些修改，盖茨就雇了其DOS的作者Tim Paterson作为他的新公司MS的雇员。 The revised system was renamed MS-DOS (MicroSoft Disk Operating System) and quickly came to dominate the IBM PC market. By the time the successor to the IBM PC, the IBM PC/AT, came out in 1983 with the Intel 80286 CPU, MS-DOS was firmly entrenched and CP/M was on its last legs. MS-DOS was later widely used on the 80386 and 80486. GUI 早期的微电脑的OS，包括CP/M, MS-DOS等都基于用户从键盘输入命令。 Engelbart 发明了GUI ( Doug Engelbart at Stanford Research Institute in the 1960s ), complete with windows, icons, menus, and mouse. These ideas were adopted by researchers at Xerox PARC and incorporated into machines they built. MAC Jobs拜访PARC的时候看到了GUI，回去后开始开发带GUI的Apple，第一代&quot;Lisa&quot;失败了， 第二代“Mac”成功了 MAC OS X 1999,Apple采用了新内核，它CMU研发的微内核，而后者被最初设计用来替代BSD UNIX的内核 因此， MAC OS X是个UNIX操作系统 Windows 微软开发的MS-DOS的后继， 深受Mac成功的影响，因此带有GUI。事实上它更像是运行在MS-DOS上的shell Windows 95 有十年时间， 1985-1995，Windows 只是一个MS-DOS上的图形环境 Win95是95年发行的独立版本，拥有更多的操作系统的特性， 只将MS-DOS用于启动和运行旧的MS-DOS程序 However, starting in 1995 a freestanding version, Windows 95, was released that incorporated many operating system features into it, using the underlying MS-DOS system only for booting and running old MS-DOS programs. Windows 98 98年发行的Win95的轻微修改版。 这两个版本都还含有大量的Intel16位汇编代码 Windows NT(where the NT stands for New Technology) 和Windows 95兼容， 但是是a full 32-bit system 采用了很多VAX VMS的思想（ 因为主设计者也是后者的设计者之一 ） Version 5 of Windows NT was renamed Windows 2000 in early 1999. It was intended to be the successor to both Windows 98 and Windows NT 4.0 由于Win2000也没有足够成功， MS发行了Windows Me (Millennium Edition) Windows XP 发行于2001 a slightly upgraded version of Windows 2000 基本取代了之前所有版本 service packs Windows 2000之后，MS将Windos系列分成客户-服务器两台产品线。前者基于XP和其后继， 后者包含了Server 2003 and Windows 2008。 嵌入式的产品线后来也出现了。 All of these versions of Windows forked off their variations in the form of service packs. Vista Then in January 2007, Microsoft finally released the successor to Windows XP, called Vista Windows 7 相比Vista没有很多新特性，但是更稳定更不吃资源 Windows 8 Win7的后继，2012发行。","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"OS Basic","slug":"OS-Basic","permalink":"http://lyk-love.cn/tags/OS-Basic/"}]},{"title":"Compilers Basic","slug":"Compilers-Basic","date":"2022-09-11T01:18:57.000Z","updated":"2022-09-26T11:13:01.135Z","comments":true,"path":"2022/09/11/Compilers-Basic/","link":"","permalink":"http://lyk-love.cn/2022/09/11/Compilers-Basic/","excerpt":"Outline: Language Processors Example: GCC Phases Overview Front End Optimizer Back End Others","text":"Outline: Language Processors Example: GCC Phases Overview Front End Optimizer Back End Others Language Processors Compiler compiler maps a source program into a semantically equivalent target program 整个编译过程可以看成 a sequence of phases, 每个phase将源程序的一种表示转换成另一种表示, 程序的中间表示称为IR( Intermediate representation ). phrases大致分两个: Analysis: 解析源程序, 得到语法树, 根据语法树创建IR, 并收集源程序的信息存入symbol table, 与IR一起传入 synthesis part 可分为: Lexical Analysis, Syntax Analysis, Semantic Analysis Synthesis: 根据中间表示和符号表创建目标代码 我们定义phases的组合为“pass”, 也就是说, 编译器一共要经过3个pass: Frontend pass: 编译器的Analysis部分, 根据源代码生成IR Optimization: The optimizer is an ir-to-ir transformer that tries to improve the ir program in some way. Backend pass: 编译器的Synthesis部分, 根据IR生成目标代码. 如果target language是machine language, 那么用户可以在机器上直接执行target program. 从source program生成machine language的典型过程: Structure Interpreter 解释器不会将一种语言翻译为另一种, 而是直接根据 source language 执行 相比机器执行, 解释器执行起来效率更低, 但是更易于debug Hybrid Java的language processor结合了compiler和interpreter. Java program首先被编译为统一格式的bytecode作为target program, 后者再被放入JVM解释执行. 为了提高速度, 有的Java Compiler采用了JIT( just-in-time ), 在编译时不仅生成字节码, 还把字节码编译为机器码, 然后直接运行机器码的程序. 无论是将bytecode解释执行, 还是将其编译为机器吗执行, 都需要“dynamic cimpilation” Java is not the first language to employ such a mix. Lisp systems have long included both native-code compilers and virtual-machine implementation schemes [266, 324]. The Smalltalk-80 system used a bytecode distribution and a virtual machine [233]; several implementations added just-in-time compilers [126]. Example: GCC 动态链接和静态链接 以GCC处理C程序的过程为例: Preprocessor: source program $\\rightarrow$ modified source program gcc的预处理器cpp将.c, .h文件变成.i文件 Compiler: $\\rightarrow$ target assembly program GCC的编译器是cc1， 它把.i文件编译成汇编语言的.s文件。 Assembler: $\\rightarrow$ relocatable machine code GCC的汇编器是as， 它把汇编文件汇编成机器指令文件，并打包成&quot;可重定向文件&quot;， 这是个二进制文件，后缀为.o Linker/Loader: $\\rightarrow$ target machine code GCC的汇编器是ld， 它将上一阶段生成的可重定向文件和系统内已存在的可重定向文件，形成最终的可执行目标文件( executable object file )并被loader加载入内存 被链接的可重定向文件包括.o文件, 也包括静态库.a和动态库.so)链接起来( 在此期间会将相对地址解析为绝对地址 ) 在windows中, 可重定向文件后缀为obj, 可执行目标文件后缀为exe Phases Overview Symbol Table被所有phase使用 Lexical Analyzer： character stream $\\rightarrow$ token stream Syntax Analyzer： $\\rightarrow$ syntax tree Semantic Analyzer：$\\rightarrow$ syntax tree Intermediate Code Generator：$\\rightarrow$ intermediate representation Machine-Independent Code Optimizer： $\\rightarrow$ intermediate representation Code Generator： $\\rightarrow$ target-machine code Machine-Dependent Code Optimizer： $\\rightarrow$ target-machine code Example Front End The front end determines if the input code is well formed, in terms of lexicality, syntax and semantics. 我们规定: 语法 = 词法+句法, 即 grammer = lexicality + syntax. 由scanner和parser完成grammer的分析. Front ends rely on results from formal language theory and type theory 如果语法和语义都正确, 前端就会生成IR. 以句子“Compilers are engineered objects.” 为例: Lexical Analysis Lexical Analysis( aka scanning ): 找出词法单元, 并赋予其词性( a part of speech ). 最后生成的词法单元形式为(p,s), where p is the word’s part of speech and s is its spelling. Syntax Analysis Syntax Analysis( aka parsing ): 根据给出的语法规则( rule ), 得到derivation: rules: derivation: 可以看到, 原句的derivation满足了给定的rules, 因此原句在语法上正确( grammatically correct ) Semantic Analysis Semantic Analysis: 一个语法正确的句子未必是有意义的, 比如: “Rocks are green vegetables” 符合上述的语法, 但是没有意义. 程序语言的语义分析一般比较简单, 主要包括Type check, Object binding等等. Analyser使用语法树和符号表来检查源程序的语义, 使得源程序和目标程序的语义一致. 同时会收集类型信息，存入语法树或符号表，用于后续的中间代码生成. Optimizer Analysis The analysis determines where the compiler can safely and profitably apply the technique. Compilers use several kinds of analysis to support transformations: Data- flow analysis : reasons, at compile time, about the flow of values at runtime. Dependence analysis : uses number-theoretic tests to reason about the values that can beassumed by subscript expressions. Transformation Compiler不仅要分析IR, 还要利用分析的结果来将IR转换成更“好”的形式( 比如更快, 更节约空间, 更省电... ) Back End 根据IR来generate target-machine code Instruction Selection 首先要将IR映射到机器指令: This code assumes that a, b, c, and d are located at offsets @a, @b, @c, and @d from an address contained in the register rarp. 这里用的汇编语言是ILOC( Intermediate Language for an Optimizing Compiler ), 是一个简化版的汇编 Virtual register: 在Instruction Selection阶段, Compiler使用虚拟的寄存器, 而不关心机器实际的寄存器. 虚拟寄存器到物理寄存器的映射在Register Allocation完成. Register Allocation 然后将虚拟寄存器映射到目标机器的物理寄存器. 此时还要考虑一些优化问题. 比如, 下面的例子只使用了“最少的寄存器”: Instruction Scheduling 编译器可以对指令重拍序, 甚至删除一些指令, 来提高速度: Interactions Among Code-Generation Components code-genetaion时会遇到很多问题, 这些问题甚至可能是交错的. 比如, 指令重拍就会导致一些变量的依赖出现更改, 影响寄存器分配. Others 编译器的一些其他应用: Binary Translation: 把平台的机器码程序翻译到另一个平台. 比如把x86机器码翻译到VLIW平台. Hardware Synthesis: 硬件设计使用硬件描述语言: Verilog or VHDL( Very high speed integrated circuit Language). 它们工作在 register transfer level( RTL ) Compiled Simulation: 以前都是先有处理器再有编译器, 现在都是先有编译器, 通过模拟器来模拟一个处理器, 使用compiler来衡量该处理器的性能.( 也就是只要有架构设计就行了, 不需要硬件实现. )","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"Compilers","slug":"Compilers","permalink":"http://lyk-love.cn/tags/Compilers/"}]},{"title":"Finite Machine","slug":"Finite-Machine","date":"2022-09-11T00:15:27.000Z","updated":"2022-09-26T06:39:34.930Z","comments":true,"path":"2022/09/11/Finite-Machine/","link":"","permalink":"http://lyk-love.cn/2022/09/11/Finite-Machine/","excerpt":"Outline: Intro NFA DFA","text":"Outline: Intro NFA DFA Intro Finite-state machines can model a large number of problems, among which are electronic design automation,communication protocol design, parsing and other engineering applications. FA和状态转换图本质上相同 NFA, DFA, RE是等价的 NFA Def NFA( 不确定性有限状态机 ) 是一个五元组 A = (Σ, $S$, $s_0$ , δ, F ): 字母表 $\\sum$ ( $\\epsilon \\notin \\sum$ ) 有穷的状态集合 S 唯一的初始状态 $s_0 \\in S$ 状态转移函数 δ $δ : S × (Σ ∪ {ε}) → 2^S$ 这里$2^S$定义为$S$的幂集 接受状态集合 $F \\subseteq S$ Language 由一个NFA $A$定义(接受)的语言是从开始状态到某个接受状态的所有路径上的符号串集合，称为$L(A)$ 一个NFA接受输入字符串x，当且仅当对应的 转换图中存在一条从开始状态到某个接受状态 的路径，使得该路径中各条边上的标号组成符 号串x (路径中可能包含ε边) 只要存在从开始状态到接受状态的路径，符号串就认为被NFA接受 约定: 所有没有对应出边的字符默认指向一个不存在的 dead state DFA Def 一个NFA被称为DFA，如果: 没有$\\epsilon$之上的转换动作, 即标记为$\\epsilon$的边, 即$δ : S × (\\sum) → 2^S$ 对于每个状态s和每个输入符号a, 有且只有一条标号为a的边","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"Compilers","slug":"Compilers","permalink":"http://lyk-love.cn/tags/Compilers/"}]},{"title":"Regular Expression","slug":"Regular-Expression","date":"2022-09-10T23:58:50.000Z","updated":"2022-09-26T06:39:34.939Z","comments":true,"path":"2022/09/11/Regular-Expression/","link":"","permalink":"http://lyk-love.cn/2022/09/11/Regular-Expression/","excerpt":"Outline: Intro Def Extension","text":"Outline: Intro Def Extension Intro 每个正则表达式$r$可以描述一个语言$L(r)$, 也即其定义的正则集合( Regular Set) 例如, C语言标识符的语言, 可以用如下正则表达式来表示: $$ \\mathrm{letter} _ (\\mathrm{letter}|\\mathrm{digit})* $$ 正则表达式不仅是数学工具, 也被各种编程语言所支持. 绝大部分语言的正则语法都差不多 Def 给定字母表 $\\sum$, $\\sum$ 上的正则表达式由且仅由以下规则定义: $\\epsilon$是正则表达式,它描述了语言$L(\\epsilon) = { \\epsilon }$ $\\forall a \\in \\sum$ , $a$是正则表达式, 它描述了语言$L(a) = { a }$ 选择: $(r) | (s)$ 是正则表达式, $L((r) | (s))=L(r) \\cup L(s)$ 连接: $(r)(s)$ 是正则表达式, $L((r)(s))=L(r)L(s)$ 闭包: $(r)^$ 是正则表达式 , $L((r)^)=(L(r))^*$ 括号: $(r)$ 是正则表达式, $L((r))=L(r)$ 运算的优先级: $*$ &gt; 连接符 &gt; $|$ $(a)|((b)(c))$可以改写为 $a|bc$ Example C语言的标识符集合: $\\mathrm{letter}$: $A|B|\\dots|Z|a|b|\\dots|z|_ $ $\\mathrm{digit}$: $0|1|\\dots|9$ $\\mathrm{id}$: $\\mathrm{letter} _ (\\mathrm{letter} _ |\\mathrm{digit})^*$ Pascal无符号数集合, 例如:1946, 11.28, 63.6E8, 1.99E−6 $\\mathrm{digit}$: $0|1|\\dots|9$ $\\mathrm{digits}$: $\\mathrm{digit} \\ \\mathrm{digit}^*$ $\\mathrm{optional _ fraction}$: $. \\mathrm{digits} | \\epsilon$ $\\mathrm{optional _ exponent}$: $(\\mathrm{E} ( + | − | \\epsilon ) \\ \\mathrm{digits} ) \\ | \\ \\epsilon$ $\\mathrm{num}$: $\\mathrm{optional _ fraction}\\ \\mathrm{optional _ exponent}$ Extension 扩展正则 为了方便, 可以用现有的正则来匹配一些常见的语言: \\d: 匹配一个数字 '00\\d'可以匹配'007'，但无法匹配'00A'； '\\d\\d\\d'可以匹配'010'； \\w: 匹配一个字母或数字. '\\w\\w\\d'可以匹配'py3' .: 匹配任意字符. 'py.'可以匹配'pyc'、'pyo'、'py!'... \\s可以匹配一个空格(也包括Tab等空白符), 所以\\s+表示至少有一个空格, 例如匹配' '，' '等； \\ws = (blank | tab | newline)+ 扩展运算符 一个或多个: $r^+$ , 等价于$rr^*$ 零个或一个: $r?$ 等价于$\\epsilon | r$ 字符类: 字符c的字面值: \\c 只写c会被认为是一个正则 $[abc]$等价于$a|b|c$ , 即字符串$abc$中的任意一个字符 $[a - z]$等价于$a|b|\\dots|z$ [0-9a-zA-Z\\_]: 匹配一个数字, 字母或者下划线 [0-9a-zA-Z\\_]+: 匹配至少由一个数字, 字母或者下划线组成的字符串，比如'a100', '0_Z', 'Py3000'等等 [a-zA-Z\\_][0-9a-zA-Z\\_]*: 匹配由字母或下划线开头. 后接任意个由一个数字、字母或者下划线组成的字符串，也就是Python合法的变量 ^s: 不在串$s$中的任意一个字符 $r{n}$: n个$r$ \\d&#123;3&#125;表示匹配3个数字, 例如'010' $r{m,n}$: 最少m个, 最多n个$r$的连接 \\d&#123;3,8&#125;: 匹配3-8个数字 ^: 行的开头 ^\\d表示必须以数字开头. $表示行的结束 \\d$表示必须以数字结束. 你可能注意到了, py也可以匹配'python', 但是加上^py$就变成了整行匹配, 就只能匹配'py'了. Example 前面的例子的简化表示: $\\mathrm{letter}$: $[\\mathrm{A}-\\mathrm{Z} \\mathrm{a}-\\mathrm{z}] $ $\\mathrm{digit}$: $[0-9]$ $\\mathrm{id}$: $\\mathrm{letter} _ (\\mathrm{letter} _ |\\mathrm{digit})^*$ $\\mathrm{digit}$: $[0-9]$ $\\mathrm{digits}$: $\\mathrm{digit}?$ $\\mathrm{num}$: $\\mathrm{digits} \\ (. \\mathrm{digits})? \\ (\\mathrm{E}[+-]? \\ \\mathrm{digits})?$","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"Compilers","slug":"Compilers","permalink":"http://lyk-love.cn/tags/Compilers/"}]},{"title":"CPU","slug":"CPU","date":"2022-09-08T00:41:56.000Z","updated":"2022-09-26T06:39:34.926Z","comments":true,"path":"2022/09/08/CPU/","link":"","permalink":"http://lyk-love.cn/2022/09/08/CPU/","excerpt":"Outline: Basic Concepts ...","text":"Outline: Basic Concepts ... Basic Concepts CPU( Package ) CPU:( Central Processing Unit): 中央处理单元. CPU是一个很模糊的概念, 它一般指的是“CPU package” 一个CPU可以有多个物理核. 如果开启了超线程, 一个物理核可以分成n个逻辑核, n为超线程的数量 举例来说, AMD的桌面级CPURyzen™ 5 5600G是6核12线程, 这意味着它有6个物理核, 通过超线程最大可以达到12个逻辑核 CPU package: 就是我们通常说的CPU, 它包含了外壳( 里面是一个或多个Die )和外面的镀金针脚等. 主板上的每个CPU插槽( socket )只能接受一个CPU Package 这就是CPU Package: Linux上查看CPU信息: lscpu | grep &#x27;CPU(s)&#x27; CPU Core CPU Core: 就是CPU的物理核 ( physical core ) , 是一个完整且独立的执行单元. 有独立的电路元件以及L1,L2缓存. Hyper-threading( 超线程 ), aka HT: 让一个core并发的执行多个控制流( 线程 ), 也就是把一个core虚拟化为多个虚拟核( Or 逻辑核 logical core ) 超线程在一个逻辑核等待指令执行的间隔把时间片分配到另一个逻辑核 CPU Die CPU Die: 就是一个半导体( usually silicon )片, 它包含了任意数量的core CPU Die之间通过片外总线（Infinity Fabric）互联, 并且不同CPU Die上的CPU内核不能共享CPU Cache. 同一个Die内的Core也许能共享某些Cache, 这取决于具体设计 Die是处理器在生产过程中引入的概念. 总的来说，Die或者CPU Die指的是处理器在生产过程中，从晶圆( Silicon Wafer) 上切割下来的一个个小方块( 这也是消费者看到的CPU芯片都是方形的原因 ), 在切割下来之前，每个小方块（Die）都需要经过各种加工，将电路逻辑刻到该Die上 CPU Cache 一般L1、L2为每个物理核独占. 曾经有过L4 Cache, 是个eDRAM,(Haswell/Broadwell:在Iris系列中) 缓存的设计 exclusive：L1 cahce中的内容不能包含在L2中 strictly inclusive：L1cache的内容一定严格包含在L2中。 Third one（没有正式名字）:不要求L1的一定包含在L2中 Multi-core Multi-core: 多核处理器, 就是有多个core的CPU ( 严谨地说, 是CPU Package ), 这些核心可以在同一个Die, 也可以在不同的Die. 现代CPU一般都有多个Die. 如下是AMD的64核/128线程CPU EPYC , 它含有4个Die: Multi-CPU 有些主板支持Multi-CPU, 顾名思义也就是多个CPU. 这需要主板上有多个CPU Socket, 多个CPU通过 QPI 链路相连. 多CPU属于并行技术了 核显, APU... 参见我的 GPU","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"Hardware","slug":"Hardware","permalink":"http://lyk-love.cn/tags/Hardware/"}]},{"title":"减肥食物大测评","slug":"减肥食物大测评","date":"2022-09-07T18:34:51.000Z","updated":"2022-10-04T06:06:05.340Z","comments":true,"path":"2022/09/08/减肥食物大测评/","link":"","permalink":"http://lyk-love.cn/2022/09/08/%E5%87%8F%E8%82%A5%E9%A3%9F%E7%89%A9%E5%A4%A7%E6%B5%8B%E8%AF%84/","excerpt":"Outline: 鸡胸肉 卤蛋 水煮蛋 荞麦面 全麦面包 零食 学校食堂","text":"Outline: 鸡胸肉 卤蛋 水煮蛋 荞麦面 全麦面包 零食 学校食堂 蛋白质 减肥人士一天摄入的蛋白质是: 体重(kg) * 1.5 健身人士是: 体重(kg) * 1.5 ~ 2.0 一个人一天要摄入100g左右蛋白质, 对于大多数减肥的人来说( 尤其是节食减肥 ), 他们每天的蛋白质摄入其实是不够的. 如果正常饮食甚至大吃大喝, 虽然蛋白质够了, 脂肪和碳水却又会增加, 因此需要吃一些高蛋白, 零脂肪, 低碳水的食物. 鸡胸肉 推荐指数: 10 (再难吃还能不吃吗) 减肥必吃食物. 100g鸡胸肉含有20g+的蛋白质, 0脂肪和很低的热量. 其营养上的优越性甚至略微高于牛肉, 但是鸡胸肉真的太难吃了, 前几周还好, 一个月之后吃了就想吐. 所以吃鸡胸肉就不要追求味道了,反正是受罪的. 购买 100g价格在4元不到 不要买鲨鱼菲特, 肉非常难吃, 做的和塑料一样, 都没有肉的质感了, 而且包装漏油, 吃得想吐. 肌肉小王子的鸡胸肉比较推荐. 这里说一下肌肉小王子这家店, 淘宝上买其实有点贵, 它每次发货都会送纸质的优惠券, 扫一下去微信购买会更便宜, 而且微信店还可以买到便宜的临期食品 袋鼠先生也很推荐, 味道比肌肉小王子还好, 而且便宜一点 卤蛋 推荐指数: 8( 相当美味 ) 相当美味, 虽然当不了主食. 网上买不到水煮蛋, 只能买卤蛋. 肌肉小王子的卤蛋味道非常好, 强烈推荐. 水煮蛋 推荐指数: 9 食堂里卖水煮蛋, 一块多一个, 又便宜又好吃又健康. 我恨不得一天吃10个. 如果嫌没味道的话还可以带上各种蘸料 荞麦面 推荐指数: 0 以下评价仅限肌肉小王子的荞麦面, 不过我猜其他店的都差不多. 什么垃圾玩意儿! 淘宝上说的天花乱坠, 说是减肥期间主食, 加上酱汁就能吃了. 实际上这个面的味道巨难吃, 一股石灰味,而且相当容易烂掉, 吃起来像面粉不像面条. 千万不要买它, 就是智商税, 难吃得要命. 宁可饿肚子也不吃 酱汁 推荐指数: 3( 太难吃了 ) 以下评价仅限肌肉小王子的酱汁, 不过我猜其他店的都差不多. 肌肉小王子的蒜蓉酱和鸡胸肉酱, 都是脂肪接近0的“健康酱料”, 用于配合荞麦面吃, 不过它们都非常非常难吃, 有股诡异的甜味, 而没有任何鲜味, 别买. 不过嘛, 对于一些没味道的食物, 比如水煮蛋之类, 倒是可以尝试一下结合酱汁, 毕竟开封了无法退货, 正好废物利用. 全麦面包 推荐指数: 0 也是个智商税, 难吃, 还贵 零食 鸭肉条 推荐指数: 6 以下评价仅限肌肉小王子的鸭肉条 平均8元一袋, 一袋就那么一丢丢. 肉非常非常硬, 要嚼半天. 味道倒是还行. 不过从味道也可以判断出来, 它不健康, 就和正常零食一样. 建议别吃, 控制自己的欲望 蛋白棒 推荐指数: 7 以下评价仅限肌肉小王子的蛋白棒 平均4元/根, 很贵. 含大量脂肪和蛋白质, 属于健身餐, 而不是减肥餐. 而且从味道可以判断出来, 它也不怎么健康, 相当于普通零食. 对于减肥人士来说极其不推荐, 碳水太高了. 对于健身人士来说推荐程度一般, 它就是个普通的甜的零食, 健康不到哪里去. 这价格就是忽悠人的. etc 此外还有各种鸡肉肠,牛肉肠, 素肉, 魔芋爽, 肉丸.... 所有这些, 都不要买. 一点都不健康, 本质上都是普通食物, 吃了就是碳水. 学校食堂 推荐指数: 10 早餐有水煮蛋, 玉米和红薯等等, 实在想放松欲望还可以吃土豆丝卷甚至土豆肉丝卷. 极度推荐. 而且价格非常便宜, 4个鸡蛋一个玉米只要6元. 所以甚至可以早上买一大堆鸡蛋玉米带回去当作口粮.","categories":[{"name":"Life","slug":"Life","permalink":"http://lyk-love.cn/categories/Life/"}],"tags":[]},{"title":"一码多端方案比较","slug":"一码多端方案比较","date":"2022-08-28T15:45:04.000Z","updated":"2022-09-26T06:39:34.942Z","comments":true,"path":"2022/08/28/一码多端方案比较/","link":"","permalink":"http://lyk-love.cn/2022/08/28/%E4%B8%80%E7%A0%81%E5%A4%9A%E7%AB%AF%E6%96%B9%E6%A1%88%E6%AF%94%E8%BE%83/","excerpt":"Outline: Intro Flutter Electron Tauri","text":"Outline: Intro Flutter Electron Tauri Intro 本来, 使用跨平台技术就是为了节约开发成本, 不过事实证明, 一种框架用到底, 最好适配问题一大堆, 开发成本反而越来越高. 所以最好的应用开发模式是Hybrid, 即原生+框架. 对于需要调用大量底层API, 并且UI不复杂的部分, 建议使用原生. 不需要大量底层API的部分就用框架. Flutter, RN, Electron都可以进行移动端和桌面端开发 Flutter因为生态最差, 桌面端开发就不要想了( 毕竟桌面应用大部分都是生产级的, 逻辑最复杂, 对底层调用最多 ). RN在桌面端略逊于Electron Electron在桌面端强于另外二者, 事实上, Electron在桌面端的对手是Tauri Tauri只用于桌面端, 目前还没有移动端适配,但是Tauri的roadmap上明确提出将来会做移动端适配. 相比其他三者, Tauri没有大公司撑腰, 所以移动端适配估计很烂. 当然,对于中国互联网, 还存在小程序端. Flutter, RN, Electron的小程序支持都不错. 此外还有人听说过Uniapp, 它使用Vue来开发移动端和小程序端, 对标RN. 这玩意儿实在是太烂了, 我都不想骂它. 用它的人好好反省一下, 自己的眼光为什么这么差. Flutter Flutter CLI很优雅, 但是Flutter生态太烂了. React Native 生态很好. 总体而言中规中矩, 没啥缺点. Electron 内置了Chromium引擎, 所以不同平台看到的都是一样的结果. 无论是移动端, 桌面端, Web端, 都是适配性最好. 且Electron开发就是纯粹的前端开发, JS + HTML + CSS. 学习成本低. Electron和RN都使用了JS运行时( node ,deno之类... ) 缺点是速度很慢, APP体积臃肿. Tauri Rendering Tauri在桌面端对标Electron. Electron内置了Chromium引擎, 而Tauri使用操作系统内置的浏览器引擎: Edge Webview2 (Chromium) on Windows WebKitGTK on Linux WebKit on macOS 但是, 因为Safari (Based on WebKit)臭名昭著地烂, 所以Tauri在Safari上可能有很多Bug. 总结: Tauri在桌面端的适配性略低于Electron. Backend Tauri的后端使用RUST, 因此没有runtime. 程序直接被编译为二进制代码, 速度更快. 缺点是需要学RUST. RN和Electron的后端都是NodeJS. 不过Tauri的roadmap也在考虑TS/JS运行时的适配. Tauri的后端暴露给前端简洁的接口. 但是在必要时刻, 还是得用RUST来写后端API. Frontend Tauri的前端和Electron一样, 是纯粹的 JS + HTML + CSS开发. Bundle Tauri APP由于是二进制码, 安装包大小比Electron小几十倍 Performance Tauri软件是二进制码, 因此速度比Electron快一倍. Startup time也大概快一倍. Security Tauri生成二进制代码( 无runtime ) , RN和Electron都生成JS代码( runtime为node ). 前者逆向起来的难度远高于后者. Developer experience Electron和Tauri的开发体验都很好.","categories":[{"name":"Frontend","slug":"Frontend","permalink":"http://lyk-love.cn/categories/Frontend/"}],"tags":[]},{"title":"Network Security","slug":"Computer Networking Network-Security","date":"2022-08-28T01:20:12.000Z","updated":"2022-09-26T06:39:34.927Z","comments":true,"path":"2022/08/28/Computer Networking Network-Security/","link":"","permalink":"http://lyk-love.cn/2022/08/28/Computer%20Networking%20Network-Security/","excerpt":"Outline: Intro Confidentiality Digital Signature Message Digest End-point Authentication Key Distribution Securing Wireless LANs and 4G/5G Cellular Networks Operational security","text":"Outline: Intro Confidentiality Digital Signature Message Digest End-point Authentication Key Distribution Securing Wireless LANs and 4G/5G Cellular Networks Operational security Intro Target 网络安全有四大目标: Confidentiality：只有信息的发送方和接收方才知道信息的内容 Message integrity：信息接收方需要确认所收到的信息没有被人篡改过. Authentication: 鉴别分为两种: Message Authentication: 对报文的鉴别, 即报文是不是对方发的, 是不是别人伪造的 很多书( 谢希仁, 自顶而下... ) 都把报文鉴别也放到 Message integrity 的范畴, 我觉得这样会引起混淆. 因此我用自己的分类方式. 事实上, 实现报文鉴别的方法, 也都实现了报文完整性, 因此我不会单独介绍Message integrity和Message Authentication, 而是直接介绍实现了这两个特性的方法: 数字签名 报文摘要 End-point authentication or Entity authenticatio: 对所通信的对象的鉴别 报文鉴别是对每一个报文的, 而实体鉴别是对于通信实体的. 因此实体鉴别只需在通信开始时实行一次. 一般来说, 实体鉴别是双向的. Operational security: 几乎所有组织的网络最终都是连到互联网的，所以网络很容易受到攻击, 需要确保网络的运行时安全. Problems 计算机网络的攻击可以分为四种: 截获——从网络上窃听他人的通信内容 中断——有意中断他人在网络上的通信 篡改——故意篡改网络上传送的报文 伪造——伪造信息在网络上传送 此外, 攻击还可以分为被动攻击和主动攻击: 被动攻击: Attacker只是观察和分析PDU( 协议数据单元 )而不干扰信息流 主动攻击: Attacker对PDU进行各种处理 Assumptions 在本文中，我们假设Bob和Alice在偷情, 他们互相在网络上通信. 而Bob的妻子Trudy, 想要通过攻击网络来发现这段恋情.（我认为这个例子更加生动形象，也更能体现网络安全的重要作用） Confidentiality cryptography: 密码编码学，是密码体制的设计学 加密算法也称为cypher cryptanalysis: 密码分析学，是在未知密钥的情况下从密文推演出明文或密钥的技术 cryptology：密码学， 密码编码学+密码分析学 密码体制的安全性: 无条件安全: 不论截取者获得了多少密文，但在密文中都没有足够的信息来唯一地确定出对应的明文, 即该密码体制理论上是不可破的 在无任何限制的条件下，目前几乎所有实用的密码体制均是可破的。 因此我们转而追求“计算上安全” 计算上安全： 密码不能在一定时间内被可以使用的计算资源破译 Cryptographic Model Alice向Bob发送的信息是明文（ plaintext, or cleartext ）, 通过encryption algorithm加密后，就得到了密文( ciphertext ) Encryption Alice发送明文$m$, 并提供一个密钥$K_A$, 加密算法( encryption algorithm ) $E$接受$m$和$K_A$， 生成密文$c$： $$ c = E_{K_A}(m) $$ 密钥( key )是一串秘密的字符串 Decryption Bob收到密文$c$后，提供一个密钥$K_B$，解密算法( decryption algorithm ) $D$接受$c$和$K_B$，得到明文$m$: $$ m = D_{K_B}(c) = D_{K_B}( E_{K_A}(m) ) $$ 解密算法是加密算法的逆运算，在进行解密运算时，如果不使用事先约定好的密钥就无法解出明文 Key System 对称加密系统( symmetric key system ): $K_A = K_B$ ， 且两个密钥都是保密的. 非对称加密系统( 也称为公钥加密系统 , public key system): $K_A \\ne K_B$ ， 且密钥分为公钥( public key )和私钥( private key ), 公钥对全世界公开， 而私钥只有通信的一方知道. 已有的非对称加密算法( RSA等 )相比已有的对称加密算法( DES, AES )都慢得多. 因此很多低时延场景都使用对称加密. 比如后文介绍的WIFI网络, 无论是交换密钥还是数据传输都使用对称加密. Attack Types 对密码体制的攻击，分为以下三种： Ciphertext-only attack： 攻击者只知道密文 Known-plaintext attack： 攻击者知道密文，以及一部分密文-明文对照( (plaintext, ciphertext) pairings ) Chosen-plaintext attack: 攻击者可以选择明文，然后获取其对应的密文。 使用Chosen-plaintext attack， 攻击者很容易破解整个密码体制 当然，对于后面介绍的现代密码体制，即使是Chosen-plaintext attack也很难破解 Symmetric Key System Old-Time Cipher 我们先来看看古代的对称加密系统： 凯撒加密（ Caesar cipher ）： 选择数字k, 把英文文本的每个字母替换成其在字母表上后面第k位的字母 很容易破解 单字母加密( monoalphabetic cipher ): 使用一张明文字母 &lt;--&gt; 密文字母的映射表，将明文按表加密 多字母加密( polyalphabetic encryption ): 对明文的每个字母，先后使用多个映射表加密 这些加密算法都很简陋，即使攻击者不知道任何明文( 即Ciphertext-only attack )，也可以根据一些手段进行破解： 通过统计学分析。 英文中，统计意义上最常见的字母是e和f，分别占文本的13%和9% 通过猜词，比如i和t常一起出现，成为it。 甚至对于Alice和Bob的情况，Trudy很可能会猜测文本中存在“love”、“Alice”、“Bob”等词，获得更多信息 Block Cipher 与古代按字母加密不同，现代加密系统都采用分组加密。 如下所示： 在加密前，先对整个的明文进行分组。对于64位的报文， 我们将其分成8位的组( 每个组也就是一个8位的二进制数据 ) 正由于Block Cypher接受固定位数的Block, 因此许多协议中都要加入一个padding字段, 用于填充报文以便于加密. 对每个组进行加密处理加密，形成64位的密文( 64-bit scrambler ) 可以按表加密，即图上的T。 更现代的做法是使用加密函数F 重排这64位密文，形成输出密文( 64-bit output ) 将输出密文作为输入，回到步骤1。 循环n次，得到最终结果 循环n次的原因在于增大输入中的每一位对输出的影响。 如果只循环一次，则输入中的1位，至多影响输出中的一位。加密过程会更容易破解 暴力破解算法，就需要把n位长的密钥都是一遍，也就是$2^n$次的开销 CBC 对于两个完全相同的分组，其分组加密后的密文也会是相同的。比如，对于HTTP报文， 普遍存在内容如“HTTP/1.1”的块， 它们对应的密文也是相同的，这会让攻击者很容易猜出一些信息。 为此，分组加密还引入了密钥+ 随机数： 我们假设组为k位。 对于第$i$组( 记为$m(i)$ )，报文发送方生成一个随机的k位数$r(i)$, 然后用密钥$S$加密: $$ c(i) = K_S( \\ m(i) \\oplus r(i) \\ ) $$ $r(i)$是随机的，因此对于内容相同的$m(i) = m(j)$， 有$c(i) \\ne c(j)$ 发送方把每组的密文$c(i)$和随机数$r(i)$一起发送。 接收方收到密文和随机数后，使用对称密钥$S$解密: $$ m(i) = K_S( \\ c(i) \\ )\\oplus r(i) $$ 攻击者可以获得$c(i)$和$r(i)$，但不知道密钥$S$，因此无法解密得到$m$ 该算法的缺点是：对于每个k位的组，都要生成一个k位的随机数r。 二者都被传输，使得报文带宽翻倍。解决方法是采用Cipher Block Chaining (CBC)： 在加密前，发送方先生成一个随机的k位数$c(0)$, 称为Initialization Vector (IV)， 将其用明文发送 对于第一个块，采用$c(0)$作为上文的$r(1)$, 用密钥$S$加密: $$ c(1) = K_S( \\ m(1) \\oplus c(0) \\ ) $$ 对于第$i$组， 发送方计算c(i): $$ c(i) = K_S( \\ m(i) \\oplus c(i-1) \\ ) $$ 注意到，CBC就是将$c(i-1)$作为$r(i)$. 对于第$i$组，发送方只需发送其密文$c(i)$. 因此接收方总共接收到$c(0), c(1), c(2), ...$， 总共只增加了$c(0)$的带宽 Modern Ciphers 如前所述，现代的公钥加密算法采用分组加密，且都用函数F( 而非图上的表T )加密， 算法都接受一个密钥，来决定每个块的加密函数($F_1, F_2,F_3...$)以及重排时的规则等等。 当然，该密钥还会用于CBC DES DES( Data Encryption Standard )：名为“数据加密标准”，由美国研发 uses 64-bit blocks with a 56-bit key DES对56位密钥很容易被攻破， 为此又出现了DES3。 它使用两个密钥，把一个64位明文用一个密钥加密，再用另一个密钥解密，然后再使用第一个密钥加密，即： $$ c = \\mathrm{DES}{K_1}( \\mathrm{DES}{K_2}^{-1}( \\mathrm{DES}_{K_1} ) ) $$ AES AES( Advanced Encryption Standard ): 名为“高级加密标准”。由美国标准与技术协会（NIST）对该标准对实现进行遴选，最终选中了Joan Daemen和Vincent Rijmen提交的Rijndael算法 uses 128-bit blocks and can operate with keys that are 128, 192, and 256 bits long Public Key System 公钥密码体制: 使用不同的加密密钥与解密密钥 由斯坦福（Stanford）大学的研究人员Diffie与Hellman于1976年提出［DIFF76]. 最著名的是RSA, 详见拙著RSA 在公钥密码体制中，加密密钥 PK （public key ）是向公众公开的，而解密密钥 SK （secret key )则是需要保密的. 加密算法E 和解密算法D也都是公开的. 公钥密码体制的加密和解密过程： 接收者B先生成一堆密钥：公钥$PK_{B}$ ( public key, 向公众公开. 用于加密 )和私钥$SK_{B}$ ( secret key, 保密. 用于解密 ). 发送者A用B的公钥$PK_{B}$通过E运算对明文加密，得出密文，发送给B: $$ c = E_{PK_{B}}(m) $$ B用自己的私钥$SK_{B}$通过D运算进行解密，恢复出明文，即: $$ m = D_{SK_{B}}(c) = D_{SK_{B}}( \\ E_{PK_{B}}(m) \\ ) $$ 虽然在计算机上可以容易地产生成对的$PK_{B}$和$SK_{B}$ ，但从已知的$PK_{B}$实际上不可能推导出$SK_{B}$，即从$PK_{B}$到$SK_{B}$是“计算上不可能的 ”. 参见我的RSA, RSA算法生成公钥$(n,e)$ 和私钥$(n, d)$ , 要根据$(n,e)$ 推导出$d$, 需要对$n$ 进行质数分解. 目前质数分解是计算上不可能的 虽然公钥可用来加密，但却不能用来解密，即 $$ D_{ PK_{B} } (E_{PK_{B}}(m)) \\ne m $$ 对RSA来说,它的加密函数和解密函数都是一个函数, 只是加密和解密时接受的参数不同. 因此可以把加密函数用于解密, 只要给它输入私钥和密文即可; 对于解密函数也同理 $$ D_{SK_{B}} (E_{PK_{B}}(m)) = E_{SK_{B}}(D_{PK_{B}}(m)) = m. $$ Cryptographic Hash Functions a hash function takes an input, $m$, and computes a fixed-size string $H(m)$ known as a hash. cryptographic hash function: 哈希函数, 且满足 It is computationally infeasible to find any two different messages x and y such that $H(x) = H(y)$ , which means it's a one-way function. cryptographic hash function 就是比较难发生碰撞的Hash函数, 常见的Checksum和CRC校验都属于Hash函数, 但都很容易碰撞, 因此不适合作为“加密哈希函数”. 具体例子见我的checksum实现 对于$(m, H(m))$ , 入侵者不可能使用$m'$替换$m$并使得$H(m') == H(m)$ 最初的著名的加密哈希函数是MD5 ( Message Digest version 5 ), 由Rivest发明. 不过MD5在2004年被王小云给破解了. 后来, 美国标准与技术协会NIST又提出了SHA( Secure Hash Algorithm ), SHA-1也被破解了. 现在一般用SHA-2, 3 ... MD5 MD5代码为128位 Steps: append: 先把报文按模 $2^{64}$ 计算其余数（64位），追加在报文后面 padding: 在报文和余数之间填充1～512位，使得填充后的总长度是512的整数倍. 填充的首位是1，后面都是0 填充到固定位数, 用于Block Cypher 把追加和填充后的报文分割为一个个512位的数据块，每个512位的报文数据再分成4个128位的数据块依次送到不同的散列函数进行4轮计算。每一轮又都按32位的小数据块进行复杂的运算。一直到最后计算出MD5报文摘要代码（128位） SHA Digital Signature 数字签名实现了以下三点: 接收者能够核实发送者对报文的签名。也就是说，接收者能够确信该报文的确是发送者发送的。其他人无法伪造对报文的签名. 即报文鉴别 接收者确信所收到的数据和发送者发送的完全一样而没有被篡改过. 即报文完整性 数字签名的步骤: 发送方A用自己的撕咬对报文进行加密, 发给接收方B. B用A的公钥对数据解密, 得到报文 因为除A外没有别人持有A的私钥. 别人如果伪造A向B发报文, B将无法用A对公钥进行解密, 这就实现了报文鉴别. 同理, 别人篡改过这篇报文, 但因为不知道A的私钥, B将将无法用A对公钥进行解密, 这就实现了报文完整性. 注意, 任何人用A的公钥都可以得出A发送的明文. 因此数据签名并不能用于保密. 如果要加密的话, 可以再使用加密算法. Message Digest Message Digest实现了报文完整性和报文鉴别, Message Digest通过报文鉴别码MAC实现. NB: 局域网中使用的媒体接入控制MAC也是使用这三个字母, 因此在看到MAC时应注意上下文. 报文鉴别码MAC和数字签名的最大区别: 数字签名需要对整个报文进行加密, 而MAC只需要对报文的Hash $H$ 进行加密. 而 $H$ 一般远小于整个报文, 因此MAC消耗的资源更少. 当然MAC存在哈希碰撞的问题, 所以可以理解为 MAC 比数字签名“轻”一点, 但是安全性弱一点. Negative Example 先来看一个反例. 下面给出的简单步骤, 看起来似乎可以实现报文的完整性和报文鉴别: 用户A首先根据自己的明文 $X$计算出散列 $H ( X )$ ( 例如,使用MD5 ). 为方便起见, 我们把得出的散列$H ( X )$记为$H$ . 用户A把散列$H$拼接在明文 $X$ 的后面，生成了扩展的报文$(X,H)$, 然后发送给B. 用户B收到了这个扩展的报文$(X,H)$. 因为散列的长度$H$是早已知道的固定值, 因此可以把收到的散列$H$和明文$X$分离开. B通过散列函数的运算, 计算出收到的明文$X$ 的散列$H ( X )$. 若$H ( X ) = H$, 则B似乎可以相信所收到的明文是A发送过来的. 像上面列举的做法，实际上是不可行的 . 设想某个入侵者创建了一个伪造的报文$M$ ，然后也同样地计算出其散列$H ( M )$ , 并且冒充A把拼接有散列的扩展报文发送给B. B收到扩展的报文$(M,H(M))$后，按照上面步骤 (3) 的方法进行验证, 发现一切都是正常的, 就会误认为所收到的伪造报文就是A发送的. Message Authentication Code 解决上面问题的办法并不复杂，就是对散列进行一次加密( 对称和非对称都可以 ): 在A从报文$X$导出散列$H$后，就对散列$H$用密钥 $K$ 加密. 这样得出的结果叫做报文鉴别码 MAC（Message Authentication Code ) 现在已经有了好几个不同的MAC标准，而使用最广泛的就是HMAC，它可以和MD5或SHA一起使用［RFC 2104, 6151]. A把已加密的报文鉴别码MAC拼接在报文 $X$ 的后面，得到扩展的报文，发送给B. B收到后把MAC与报文 $X$ 分离出来, 然后用密钥对MAC解密, 得到加密前的散列$H$, 再计算出$X$的散列$H ( X )$, 将$H ( X )$ 与 $H$比较. 由于入侵者无法对MAC解密. NB: 该这里为了节约计算资源, 没有加密 $X + H$ , 只加密了 $H$ . $H$ 大小一般远小于 $X$ . Usage 在链路层, 大量交换机需要交换报文, 为了实现报文完整性和报文鉴别, 且保证通信速度( Digital Signature太慢了 ), 一般都用Message Digest. Message Digest 使用了加密算法, 也就需要密钥分配, 详见后文Key Distribution End-point Authentication or Entity Authentication 你可能认为只要通信时每一个报文都采用HMAC之类的方式保证报文鉴别和报文完整性, 就能实现整个通信过程的安全, 这是错误的. 注意, 实体鉴别只需要在通信建立时实施一次. 此外在通信过程中的每一个报文, 依然需要进行报文鉴别和报文完整性检查, 以保证报文的安全性. IP Spoofing IP Spoofing: C可以截获A的IP地址( HMAC对报文头不加密, 因此可以获取IP地址 )，然后把A的IP地址冒充为自己的IP地址(这叫做IP欺骗). 不过IP Sproofing只能单向, 没啥用. Message Reordering C可以把两个报文重新排序, 更改其TCP seq号( 因为报文头没有被HMAC加密 ), 再发给B. 或者C可以干脆删除该报文. B就无法收到正确的信息了. Playback Attack 我们假设报文采用了HMAC, , 但还是会遇到实体鉴别问题 Playback Attack( 重放攻击 ): 入侵者C可以从网络上截获A发给B的报文, C并不需要破译这个报文 ( 因为这可能得花很长时间 ）, 而是直接把这个由A加密的报文发送给B, 使B误认为C就是A. 完成“实体鉴别后”, C就和B开始通信, B会向伪装成A的C发送许多本来应当发给A的报文. 重放攻击的关键在于, B不知道他所受到的报文是来自A的, 还是来自攻击者C的playback. 导致B将这些报文视作属于同一个会话的. 这个问题和TCP连接时的三报文握手是一样的: 当server遇到一个 SYN segment 时, 它如何判断这个segment是否属于这次连接? 毕竟它也可能属于上一次连接. TCP的解决方案是server向client发送的SYNACK报文中添加一个seq, 并等待client发来的ACK, 查看其中是否有ack = seq + 1. 也就是说, TCP server使用一个seq来维护一条连接. 对于不同的连接, 其seq不一样, 因此server能把重放攻击者和正常client区分. 使用序列号之后, 对报文排了序. 也可以顺便避免Message Reordering攻击 nounce 在实体鉴别时采用nonce( 不重数 )来维护一个“鉴别会话”. nounce就是一种seq, 不同连接的nounce的取值范围不相同. 因此, B和A的鉴别会话, 与B和重放者C的鉴别会话, 拥有不同的nounce, 也就被视作两个会话, B不会把A和C混淆. 当然, 为了保证鉴别会话的报文不被篡改, 还需要报文加密. 下图给出了采用对称加密的鉴别会话过程: A首先用明文发送其身份A和一个不重数 $R_A$ 给B. B响应A的查问，用共享的密钥$K_{AB}$ 对 $R_A$ 加密后发回给A，同时也给出了自己的不重数$R_B$ 最后, A再响应B的查问, 用共享的密钥$K_{AB}$ 对 $R_B$ 加密后发回给B. 这里很重要的一点是A和B对不同的会话必须使用不同的不重数集。由于不重数不能重复使用, 所以C在进行重放攻击时无法重复使用所截获的不重数. B用$K_{AB}$对 $R_B$ 解密, 发现确实是自己发送的 $R_B$ , 就相信通信的对方是A. 对称加密需要通信双方共享密钥. 为了省去这个麻烦, 似乎可以用非对称加密: 在前面的例子中, B可以用其私钥对不重数 $R_A$ 进行签名后发回给A. A用B的公钥核实签名, 如能得出自己原来发送的不重数 $R_A$ , 就核实了和自己通信的对方的确是B. 同样, A也用自己的私钥对不重数R B 进行签名后发送给B. B用A的公钥核实签名, 鉴别了A的身份. 上面的做法是有漏洞的, 让我们看下面的例子: C冒充是A，发送报文给B，说: ”我是A“ B选择一个不重数 $R_B$ , 发送给A，但被C截获了。 C用自己的私钥$SK_C$冒充是A的私钥，对 $R_B$ 加密，并发送给B。 B向A发送报文，要求对方把解密用的公钥发送过来，但这报文也被C截获了. C把自己的公钥 $PK_C$ 冒充是A的公钥发送给B。 B用收到的公钥PK C 对收到的加密的 $R_B$ 进行解密，其结果当然正确. 于是B相信通信的对方是A，接着就向A发送许多敏感数据，但都被C截获了. 然而上述这种欺骗手段不够高明，因为B只要打电话询问一下A就能戳穿骗局，因为A根本没有和B进行通信。但下面的“中间人攻击 ”（man-in-the-middle attack）就更加具有欺骗性. man-in-the-middle attack A想和B通信，向B发送“我是A”的报文, 并给出了自己的身份。这个报文被“中间人”C截获, C把这个报文原封不动地转发给B. B选择一个不重数 $R_B$ 发送给A，但同样被C截获后也照样转发给A. 中间人C用自己的私钥 $SK_C$ 对 $R_B$ 加密后发回给B, 使B误以为是A发来的。A收到 $R_B$ 后也用自己的私钥 $SK_C$ 对 $R_B$ 加密后发回给B，但中途被C截获并丢弃. B向A索取其公钥，这个报文被C截获后转发给A. C把自己的公钥 $PK_C$ 冒充是A的公钥发送给B，而C也截获到A发送给B的公钥$PK_A$ . B用收到的公钥 $PK_C$ （以为是A的）对数据 DATA 加密，并发送给A. C截获后用自己的私钥 $SK_C$ 解密，复制一份留下, 然后再用A的公钥 $PK_A$ 对数据DATA 加密后发送给A. A收到数据后，用自己的私钥 $SK_A$ 解密, 以为和B进行了保密通信. 其实, B发送给A的加密数据已被中间人C截获并解密了一份, 但A和B却都不知道. Key Distribution 密钥分配也是网络安全的重要问题. 对于对称加密, 常见问题是对称密钥的分配很麻烦. 需要一个设施来进行自动化的密钥分配. 对于非对称加密, 常见问题是声称拥有某人密钥的人, 在现实中并不是某人 , 这也是中间人攻击成功的原因. 其实这是一个网络-现实中的实体鉴别问题. 由于公钥是公开的, 因此非对称加密的密钥的分配效率不是问题吗不需要“让通信双方共享密钥”. 实体鉴别只能对通信方在网络上的身份进行鉴别. , 但是,网络上的身份不一定就是现实中的身份, 比如, Trudy声称自己是Bob, 然后使用自己的私钥和Alice通信. 在Alice看来, 一切都是正常的, 实体鉴别也通过了. 问题在她鉴别的“Bob”身份只是网络上的, 在现实中, 这个人是Trudy. Alice使用了所谓的Bob的密钥, 其实她使用的一直都是Trudy的. 因此需要有一个机构来在现实中进行鉴别, 建立网络 - 现实身份的映射 Symmentric Key Distribution KDC( Key Distribution Center ): KDC是大家都信任的机构，其任务就是给需要进行秘密通信的用户临时分配一个会话密钥（仅使用一次）. Public Key Distribution 认证中心CA ( Certification Authority ): 一个权威机构, 用于接受用户认证, 向用户发放证书（certificate）, 证书内容是用户的公钥和个人信息( 比如IP, 人名... ). CA对证书进行数字签名, 这样用户就保证了该证书真的属于改CA. 当然, CA的安全性取决于其权威性. 我自己也可以作为CA生成一个证书, 但是这样的证书肯定没人信. 使用CA的步骤: 每个实体需要持有CA发的证书 任何人想要获取该实体的公钥, 只需从CA处获取该实体的证书( 或者让该实体自己提供证书也行 ). 这样就能确保对方真的是现实中的那个人. 例如, 如果我用了数字签名, 他人就可以拿我证书上的公钥和我的公钥比对, 确保我网络上的身份和我现实中的身份一致. Securing Wireless LANs and 4G/5G Cellular Networks 在无线网络( 包括802.11 wireless LANs 和4G/5G cellular network )中, 攻击者可以嗅探到任何报文, 因此无线网络的安全很重要. Authentication and Key Agreement in 802.11 Wireless LANs WIFI的网络安全有两个目标: Mutual authentication: 如果移动设备要接入网络, 移动设备和网络需要互相鉴别 Encryption: 移动设备和access point (AP)通信的报文 ( 属于链路层frame ) 需要加密. 因为移动网络中的加密一般要求高速,所以采用对称加密. Authentication Server (AS) : WIFI还需要一个AS来负责鉴别. AP在鉴别时仅仅作为Mobile device和AS通信的桥接. 以下是一个Mobile device接入WIFI的过程: We can identify four distinct phases to the process of mutual authentication and encryption-key derivation and use in Figure 8.30: Discovery. AP广播其存在, 然后mobile device与其通信. 此时还没有经过鉴别, 也没有生成链路层frame加密的密钥. Mutual authentication and shared symmetric key derivation. : 详细过程在下面的Mutual Authentication and Shared Symmetric Session Key Derivation介绍 为了方便, mobile device都会和AS共享一个密钥( e.g. WIFI密码 )用于鉴别时加密, 双方通过这个密钥, nounce( 避免playback attack )和cryptographic hashing( 报文鉴别, 报文完整性 )来进行双向的实体鉴别. 并且会生成一个symmentric session key( ( 如前所述, 我们一般用对称加密 ), 用于加密mobile device和AP间传输的链路层frame. 一般而言,对称加密算法是AES Shared symmetric session key distribution. 步骤二中, mobile device和AS都拥有了session key, 此时还需要把session key也传输给AP. ( 不然mobile device和AP通信怎么加密呢.... ) Encrypted communication between mobile device and a remote host via the AP. Mutual Authentication and Shared Symmetric Session Key Derivation Wired Equivalent Privacy( WEP ): 就是WIFI网络的安全通信协议. WEP很快被WPA取代. WiFi Protected Access( WPA ): 就是WEP的改进版.目前都用的WPA3( 2018年发布 ) WEP a four-way handshake protocol 作用: mutual authentication shared symmetric session-key derivation 如图, WPA工作流程为: Mobile device( M )和AS事先会共享一个密钥( 比如WIFI密码 ), 记为$K_{AS-M}$ AS生成一个NonceAS发给M, 这用于避免重放攻击 M收到NonceAS, 并生成它自己的NonceM. 然后M使用NonceAS, NonceM, $K_{AS-M}$, 其MAC地址和AS的MAC地址生成symmentric session key $K_{M-AP}$. 然后将NonceM 和 一个HMAC加密的值( 包括了 $K_{AS-M}$ 和 NounceAS )发给AS AS通过decode HMAC, 得到NonceAS, 确认了M不是重放攻击. 同时得到了$K_{AS-M}$, 完成了实体鉴别( 即对方确实是拥有这个 $K_{AS-M}$ 的设备). AS然后执行和M一样的操作, 用它收到的NonceAS, NonceM, $K_{AS-M}$, 其MAC地址和M的MAC地址生成$K_{M-AP}$. 因为和M的参数一样, 它们推导出的 $K_{M-AP}$ 也一样. 接下来 $K_{M-AP}$ 会被通知给AP 802.11 Security Messaging Protocols Extensible Authentication Protocol (EAP): M和AS通信时采用的协议. 注意到M和AS其实是端到端的request/response通信. 因此EAP是一个端到端协议. EAP是链路层协议. 数据封装成EAPoL (EAP over LAN) 在WIFI链路上传输. AP收到后, 使用RADIUS protoco( or DIAMETER protocol )来解包为通用的链路层协议( TCP/UDP ) ,然后发给AS. Authentication and Key Agreement in 4G/5G Cellular Networks 4G/5G蜂窝网络的安全性要求和WIFI差不多, 都需要: 双向鉴别: 基站当然有必要鉴别手机. 但因为基站也有可能被黑, 所以手机也有必要鉴别基站.... 加密: 手机和基站依然需要生成一个对称加密密钥, 用来加密M和BS之间通信的链路层frame AKA Protocol in 4G 4G/5G蜂窝网络的鉴别和加密协议为AKA( Authentication and Key Agreement (AKA) protocol ), 它规定了如下步骤: 和WIFI一样, 手机和HSS之间需要共享一个密钥$K_{HSS-M}$用于鉴别时的加密. 和WIFI的密钥一般就是WIFI密码不同, $K_{HSS-M}$一般存在手机的SIM卡和HSS database里. Authentication request to HSS. 当mobile device第一次通过基站请求加入网络时, 它将发送一个包含其 international mobile subscriber identity (IMSI) 的sttach message, 基站会把该信息转发给被访网络( visited network, 图中称为VN )的MME. 然后MME把IMSI和VN的信息发送给家庭网络的HSS. IMSI在4G中被明文传输. Authentication response from HSS. HSS使用预先共享的$K_{HSS-M}$进行加密, 以获得一个认证令牌auth_token = $K_{HSS-M}(IMSI)$和 一个expected authentication response token $xres_{HSS}$. 前者最终被转发给mobile device, 后者发给MME. Authentication response from mobile device. mobile device收到auth_token后解密得到IMSI: $$ K_{HSS-M}(K_{HSS-M}(IMSI)) = IMSI $$ mobile device于是知道了该HSS拥有$K_{HSS-M}$, mobile device因此就鉴别了蜂窝网络 ( HSS和MME就是蜂窝网络的AS ). 并且, mobile device会使用和HSS相同的算法计算出$xres_{M}$, 只是使用了自己的key ( $K_{HSS-M}$) , 并将其$K_{HSS-M}$发送给MME Mobile device authentication. MME比对$xres_{M}$ 和 $xres_{HSS}$ , 由于HSS和mobile device拥有相同的$K_{HSS-M}$, 因此: $$ xres_{M} == xres_{HSS} $$ MME于是知道了mobile device拥有和HSS相同的key ,因此蜂窝网络鉴别了mobile device . 这里MME的作用类似WIFI中的AP, 仅仅是一个中介, 不需要知道$K_{HSS-M}$. 和WIFI中不同的是, MME在这里做了认证决定. Data plane and control plane key derivation. 步骤1~4完成了双向鉴别, 接下来mobile device就和基站共同生成了symmentric session key, 过程和WIFI中的一样. sesssion key用于加密蜂窝网络的链路层frame, 当然, data plane 和 control plane通信是不同的, 因此需要生成两个key. 加密算法是AES, 和WIFI一样 Changes in 5G 5G的鉴别和加密协议是AKA' , 它继承了4G的AKA. 此外, 5G还采用了WIFI中的EAP, 因此报文格式和4G的不同. 并且5G还新增了一个协议用于IoT场景, 该协议不需要$K_{HSS-M}$ . 4G的鉴别中, 是被访网络的MME做了mobile device的鉴别决定, 在5G中, 一般都由家庭网络来进行这个鉴别. 5G中的IMSI是被非对称加密的. 关于4G/5G安全的更多细节: [3GPP SAE 2019; Cable Labs 2019; Cichonski 2017] Operational Security 三大组件: Firewall Intrusion Detection System ( IDS ) Intrusion Prevention Systems (IPS) 防火墙, IDS等肯定会让网络变慢. 因此我们一般把网络划分为不同区域, 需要与外网连接的服务器被分到防御较宽松的demilita- rized zone (DMZ), 其他服务器被分到防御更严格的区域: 注意, IDS等设施同一时间要计算无数个报文, 工作量很大. 因此一般在网络中部署多个IDS, 达到分治的效果, 如上图所示. Firewalls A firewall is a combination of hardware and software that isolates an organization’s internal network from the Internet at large, allowing some packets to pass and blocking others Firewalls can be classified in 2 categories: packet filters: 工作在传输层, 是一种具有分组过滤功能的路由器，它根据过滤规则对进出内部网络的分组执行转发或者丢弃. application gateways: 也称为Proxy Server( 代理服务器 )工作在应用层, 就是一个应用层APP, 用来在应用层监控某个应用发出在报文. An application gateway is an application-specific server through which all application data (inbound and out- bound) must pass. 分组过滤器和应用网关可以结合使用, 如图: Packet Filters 分组过滤器分为两种: 无状态: 独立地处理每一个分组. 有状态: 会跟踪每个连接或会话的通信状态，并根据这些状态信息来决定是否转发分组. 以无状态过滤器为例, 假设要保护的网络是130.207/16, 它要和一个Web server at 130.207.244.203 通信: Policy Policy Firewall Setting No outside Web access. Drop all outgoing packets to any IP address, port 80. No incoming TCP connections, except those for organization’s public Web server only. Drop all incoming TCP SYN packets to any IP except 130.207.244.203, port 80. Prevent Web-radios from eating up the available bandwidth. Drop all incoming UDP packets—except DNS packets. Prevent your network from being used for a smurf DoS attack. Drop all ICMP ping packets going to a “broadcast” address (eg 130.207.255.25) Prevent your network from being tracerouted. Drop all outgoing ICMP TTL expired traffic. 上表中只是抽象的规则, 具体的规则存储在ACL. ACL ACL( access control list ): 就是 packet filters 里的过滤规则, ACL其实和路由表长得差不多 比如, 如果想要禁止所有的TCP连接的建立, 只要过滤掉所有ACK = 0的报文 ( 因为TCP中, 只有连接建立时第一个报文的ACK = 0, 此后所有的报文都有ACK =1 ) 如果想要禁止员工看视频, 可以把不必要的UDP报文都过滤掉. 不可能过滤掉全部的UDP报文, 因为DNS之类的服务也使用UDP ACL自顶而下顺序匹配. When a statement “matches”, no more statements are evaluated. The packet is either permitted or denied. There is an implicit “deny any” statement at the end of the ACL. If a packet does not match any of the statements in the ACL, it is dropped. 下面例子的表中显式地给出了这条规则, 下面是一个例子, 该防火墙要保护的网络是222.22/16 action source address dest address prototol source port dest port allow 222.22/16 outside of 222.22/16 TCP &gt; 1023 any allow outside of 222.22/16 222.22/16 TCP 80 &gt; 1023 allow 222.22/16 outside of 222.22/16 UDP &gt; 1023 53 allow outside of 222.22/16 222.22/16 UDP 53 &gt; 1023 deny all all all all all 前两条规则允许网络内部的主机单向地访问Web: 第一条规则允许TCP packet with destination port 80 离开网络, 即允许主机单方面发出TCP请求 第二条规则允许TCP packet with source port 80 and the ACK bit set进入网络, 即允许主机接受对方的TCP请求. 注意, 由于不允许外部主机的ACK = 0进入网络( 因此无法建立TCP连接 ), 所以外部主机是无法用TCP访问内部主机的. 因此是“单向” 接下来两条规则允许所有DNS packets进出网络. 最后一条规则就是deny any, 它是默认的, 禁止了除上述规则匹配的分组之外的所有其他分组. Application Gateways 相比工作在传输层的报文过滤器, 应用网关的控制能力更强更灵活. 比如, 组织可能希望某些内部人员必须先在内网进行登录认证后, 才能访问外网. 而“登录认证”的数据属于应用层, 不属于IP/TCP/UDP. 这一级别的访问控制只能由应用网关来处理. IDS IDS: 防火墙只能对分组进行简单的检查, 像上面所介绍的, 只能检查地址,协议类型等, 因此很容易被攻破. IDS能对分组进行深度检查, 并对可疑分组发出alert IDS分两种: Signature-based IDS( 基于特征的IDS ): 维护一个所有已知攻击标志性特征的数据库, 每个特征是一个与某种入侵活动相关联的规则集. 这些规则一般是手动输入的. 基于特征的IDS只能检测已知攻击，对于未知攻击则束手无策. 而且即使某个报文匹配了某条规则, 它也不一定就是个有害报文. Anomaly-based IDS( 基于异常的IDS ):通过观察正常运行的网络流量来动态判断. 不过区分正常和异常流量很困难, 所以基于异常的IDS能力有限 目前最常用的开源IDS是Snort, 它是Signature-based IDS. IPS IPS: 和IDS配套使用, 在IDS发出alert后, IPS负责把可疑分组清除 EXP: ANONYMITY AND PRIVACY 假如我要访问一个奇怪的网站( 比如, 某个含有不当言论的网站 ), 我想要: 不让网站知道我的IP 不让我的local ISP知道我在访问这个网站, 也就是不让ISP知道我的目的IP 不让local ISP知道我和网站通信交换的报文. 只使用SSL是不行的, SSL只能加密报文payload, 但是不能加密源和目的IP地址, 所以无法满足要求1, 2. 正确做法是使用Proxy Server( 代理服务器, 也就是应用网关 ) + SSL: 我使用SSL和代理服务器通信, 我向代理服务器发送的HTTP报文都是SSL加密的. 报文被代理服务器解密发给网站. 代理服务器和网站通信, 它们之间的报文不需要加密. 网站发给代理服务器的报文被SSL加密后传给我. 这种方式下, 代理服务器的IP作为ISP看到的的目的IP和网站看到的源IP. 数据也被SSL保护. 问题在于, 代理服务器会知道一切( 我的源IP, 目的IP, 我的数据 ), 因此这种方法的安全性取决于代理服务器的安全性. 更好的方法是采用TOR, 它使用一个 Proxy Server Pool, 其中每个Proxy Server都是不互通的( non-colluding ). 每次通信随机从中选一个Proxy Server. [[TOR 2020]","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"Computer Networking","slug":"Computer-Networking","permalink":"http://lyk-love.cn/tags/Computer-Networking/"},{"name":"Network Security","slug":"Network-Security","permalink":"http://lyk-love.cn/tags/Network-Security/"}]},{"title":"Security in Internet","slug":"Security-in-Internet","date":"2022-08-28T01:19:48.000Z","updated":"2022-09-26T06:39:34.939Z","comments":true,"path":"2022/08/28/Security-in-Internet/","link":"","permalink":"http://lyk-love.cn/2022/08/28/Security-in-Internet/","excerpt":"Outline: Intro Transport Layer: TLS Network-Layer: IPsec and VPN Application Layer: Securing E-Mail","text":"Outline: Intro Transport Layer: TLS Network-Layer: IPsec and VPN Application Layer: Securing E-Mail Intro 先指出一些误解: “只要在低层, 比如IP层, 提供安全协议, IP层及以上的层就都安全了.” 这是错误的, 因为IP报文的内容安全, 不意味着应用层数据安全. 其他层也是同理. &quot;在高层实现安全后, 底层可以不用实现安全&quot;: 这显然是错的. 最简单的例子, 应用层程序只能改变报文内容, 不能改变报文头, 而很多攻击就是针对报文头的. 因此低层协议也有必要实现安全. 因此, 需要在计算机网络的每一层都采取措施, 使得该层的通信安全. Assumptions 本文和Network Security 一样, 以Bob, Alice 和 Trudy 的三角恋为例 Transport Layer: TLS 运输层的安全协议是TLS( Transport Layer Security ), 它的更早期版本是SSL( Secure Socket Layer ) version3. 我们有时候也用SSL指代TLS. SSL( or TLS )位于网络层和运输层之间, 对TCP做了增强. 在开发者角度, SSL属于传输层. 在发送方，SSL从SSL套接字接收应用层的数据( 如HTTP或IMAP报文 ), 对数据进行加密，然后把加密的数据送往TCP套接字; 在接收方，SSL从TCP套接字读取数据, 解密后, 通过SSL套接字把数据交给应用层. 如果HTTP用了TLS, 则域名的协议名会变成https SSL并非仅用于HTTP, 而是可用于任何应用层的协议. 例如，SSL也可用于IMAP邮件存取的鉴别和数据加密. TCP的HTTPS端口号是443，而不是平时使用的端口号80 SSL提供的安全服务可归纳为以下三种： 加密 报文完整性 鉴别, 包括报文鉴别, 实体鉴别, 以及允许SSL client查看server的CA. 当然SSL server也可以查看客户的CA, 这是可选的. TLS Connection Setup TLS协议有三个阶段: handshake, key derivation, and data transfer. Handshake Bob和Alice建立TCP连接. TCP连接建立后, Bob和Alice就开始正式的TLS连接建立, 它们的数据传输单元就变成了TLS Record Bob向Alice发一个message( TLS hello ). Alice将以下内容发送给Bob: 可供选择的非对称加密算法( 例如, RSA with a specific key length ), 用于后续EMS的加解密 可供选择的HMAC算法( 例如SHA-3或MD6 ), 用于后续的HMAC计算 一个nounce, 相当于TCP里的seq. 使用nounce可以避免message reordering等问题, 还可以避免重放攻击. 后面通信中的报文都会带有nounce, 为了叙述方便我就省略了. Alice自己的证书, 其中包含了自己的公钥 由于证书由CA颁发, Alice于是就证明了自己确实是Alice 这一步完成了算法协商, 以及Alice的实体鉴别. Bob生成一个Master Secret (MS) (which will only be used for this TLS session), 用Alice的公钥加密成为Encrypted Master Secret (EMS), 然后发给Alice. Alice会用私钥解密EMS, 得到MS. Handshake结束后, Bob和Alice都知道MS了. Key Derivation 你可能会认为Bob和Alice将把MS作为对称加密的密钥. 然而TLS做得更加复杂. MS被用来生成以下四个密钥: $\\mathrm{E_B}$ = session encryption key for data sent from Bob to Alice $\\mathrm{M_B}$ = session HMAC key for data sent from Bob to Alice, where HMAC [RFC 2104] is a standardized hashed message authentication code (MAC) that we encountered in section 8.3.2 $\\mathrm{E_A}$= session encryption key for data sent from Alice to Bob $\\mathrm{M_A}$ = session HMAC key for data sent from Alice to Bob Bob和Alice各自用自己的MS生成这四个密钥, 由于两人的MS相同, 生成的密钥也是一样的. 这四个密钥中, 两个用于数据加密, 两个用于HMAC. 根据MS生成四个密钥的步骤比较复杂, 这里不详细介绍了. Data Transfer 数据传输过程依然采用TLS record作为数据传输单元 Bob在要传输的DATA后添加HMAC HMAC根据DATA, 使用密钥 $\\mathrm{M_B}$ 生成 Bob将DATA + HMAC一起用密钥 $\\mathrm{E_B}$ 加密, 发给Alice 注意, 我们之前学的采用MAC的实体鉴别只会对MAC加密. 但这里把DATA + MAC一起加密了. Alice收到后, 用 $\\mathrm{E_A}$ 解密DATA + HMAC, 再用 $\\mathrm{M_A}$ 检查HMAC, 进行报文鉴别和报文完整性检查. 后续Alice向Bob的通信也同理. TLS breaks the data stream into records, appends an HMAC to each record for integrity checking, and then encrypts the record + HMAC. To create the HMAC, Bob inputs the record data along with the key MB into a hash function, as discussed in Section 8.3. To encrypt the package record + HMAC, Bob uses his session encryption key EB. TLS Connection Closure truncation attack: 错误的想法是, Bob和Alice只断开TCP连接就可以断开TLS连接. 问题在于TCP是不安全的, Trudy可以伪造一个Bob发送的TCP FIN报文给Alice, 这样Bob和Alice就无法通信了. 这种攻击称为truncation attack. 因此, TLS也有自己的连接释放报文. 使用TLS时先释放TLS连接, 再释放TCP连接. TLS Record Type: 表明该Record是TLS Handshake报文, TLS Data Tranfer报文 还是 TLS连接释放报文 Network-Layer: IPsec and VPN 虚拟专用网VPN时中传送的信息都经过IPsec加密. IPsec并不是一个单一协议, 而是能够在IP层提供互联网通信安全的协议族 （不太严格的名词“IPsec协议”也常见到）.IPsec并没有限定用户必须使用何种特定的加密和鉴别算法。实际上，IPsec是个框架，它允许通信双方选择合适的算法和参数. 为保证互操作性，IPsec还包含了一套加密算法，所有IPsec的实现都必须使用。 IPsec协议族中的协议分为以下三部分： IP安全数据报格式的两个协议：鉴别首部AH （Authentication Header）协议 和 封装安全有效载荷ESP （Encapsulation Security Payload）协议. 使用ESP或AH协议的IP数据报称为IPsec datagram 有关加密算法的三个协议( 在此不讨论 ) 互联网密钥交换 IKE（Internet Key Exchange）协议 AH and ESP In the IPsec protocol suite, there are two principal protocols: the Authentication Header (AH) protocol and the Encapsulation Security Payload (ESP) protocol. AH协议提供源点鉴别和数据完整性，但不能保密。而ESP比AH协议复杂得多，它提供源点鉴别、数据完整性和保密。IPsec支持IPv4和IPv6。在IPv6中，AH和ESP都是扩展首部的一部分。AH协议的功能都已包含在ESP中，因此使用ESP就可以不使用AH协议。下面我们将不再讨论AH协议，而只介绍ESP. Security Associations 安全关联SA （Security Association ): 在源实体和目的实体之间创建的网络层的逻辑连接. 因为网络层不一定是有连接的( UDP就是无连接的 ), IPsec需要SA来确保连接建立. 为了节约资源, SA是单向连接 安全关联数据库 SAD（Security Association Database）: 一个 IPsec entity 把它的所有SA都存在SAD里. 当它要发送 IPsec数据报时, 就在SAD中查找相应的SA. SAD在OS kernel中 安全策略数据库 SPD ( Security Policy Database ) : 一个主机需要判断哪些数据包要进行IPsec处理处理. 为此它需要维护一个SPD, 指明过滤规则. 这取决于源地址、源端口、目的地址、目的端口，以及协议的类型等. 一个路由器可以同时转发很多分组, 它会通过查找SPD来决定哪些分组需要IPsec SA的内容包括: A 32-bit identifier for the SA, called the Security Parameter Index (SPI, 安全参数索引) 安全关联SA的源点和终点的IP地址（即路由器R 1 和R 2 的IP地址, 在这里就是200.168.1.100 和 193.68.2.23 The type of encryption to be used (for example, 3DES with CBC) The encryption key 报文完整性和鉴别的类型 ( for example, HMAC with SHA-3 ) The authentication key Example 假设公司总部和分公司各有路由器 R1 和 R2 (通常就是公司总部和分公司的防火墙中的路由器 ), R1和R2建立SA. 现假定公司总部的主机H1 要和分公司的主机H2 通过互联网进行安全通信: H1 发送给 H2 的IP datagram, 必须先经过R1, 然后经IPsec的加密处理后，成为IPsec datagram. 其首部目的地址是R2 IPIPsec datagram 经过互联网中很多路由器的转发, 最后到达R2 R2对IPsec datagram解密, 还原出原始的IP datagram, 原始IP datagram的首部目的地址是H2 R2把IP datagram发到H2 如果总部的主机H1 要和总部的另一台主机H3 通信, 由于都在公司内部, 不需要加密( 不使用IPsec), 因此不需要建立SA 从逻辑上看，IPsec datagram在SA上传送, 就好像通过一个安全的隧道. 这就是&quot;tunnel mode&quot;. IPsec Datagram IPsec has two different packet forms: tunnel mode : 在原始的IP数据报的前后分别添加若干控制信息，再加上新的IP首部，构成一个IP安全数据报 transport mode. 在整个运输层报文段的前后分别添加若干控制信息，再加上IP首部，构成IP安全数据报 我们只讨论 tunnel model, 因为VPN中广泛使用tunnel mode 无论使用哪种方式，最后得到的IPsec datagram的IP header都是不加密的, 否则互联网上的路由器就没法识别IP首部并进行转发了. 通常把IP datagram的数据部分称为datagram的payload. ESP IPsec datagramd的数据部分就是IP datagram. 因此ESP payload就是IP datagram. 我们假设公司总部的H1( 172.16.1.17 ) 向R1发送了IPv4 datagram, 目的地是公司分部的H2( 172.16.2.48 ). H1, H2不在同一个局域网, 二者通过VPN相连( 也就需要IPsec ). R1将 IPv4 datagram作为Payload, 转化成IPsec datagram: Appends to the back of the original IPv4 datagram (which includes the original header fields!) an “ESP trailer” field the ESP trailer consists of three fields: padding: 用全0填充. pad length: 指出padding的字节数. padding的目的是便于Block Cypher进行加密计算. 参见 Network Security -&gt; Confidentiality -&gt; Symmetric Key System -&gt; Block Cipher next header: 指ESP payload的协议类型. 这里是tunnel mode, 因此ESP payload就是original IPv4 datagram, 它是IP协议. 如果是transport mode, 那么ESP payload就是传输层报文段( TCP, UDP segment ), 它是传输层协议 The next header identifies the type (e.g., UDP) of data contained in the payload-data field. The payload data (typically the original IP datagram) and the ESP trailer are concatenated and then encrypted. Encrypts the result( ESP payload + ESP tailer ) using the algorithm and key specified by the SA Appends to the front of this encrypted quantity a field called “ESP header”; the resulting package is called the “enchilada” ESP header 不加密, 它包括两个field: 安全参数索引SPI: 用于标识一个SA. SPI可以作为SAD和SPD中的键. sequence number: 作用和TCP序号类似, 用于避免重放攻击 Creates an authentication MAC over the whole enchilada using the algorithm and key specified in the SA Appends the MAC to the back of the enchilada forming the payload 生成新的IP首部, 它和普通的IP数据报的首部的格式一样( normally 20 bytes long). 首部中的协议字段值是50, 表明这其实是个IPsec datagram using the ESP protocol. 最后生成的IPsec datagram在在格式上属于IP datagram, 但它的payload包括了: ESP header, 原始的IPsec datagram, ESP tailer, ESP MAC. R2收到该datagram后, 查看其首部的协议字段号, 发现是50, 就把它作为IPsec datagram using the ESP protocol 处理: 查看enchilada, 根据SPI来确定该报文属于哪条SA. 重新根据enchilada计算MAC, 然后和报文中的ESP MAC比对, 验证报文完整性且进行报文鉴别. 检查sequence-number field来确保这不是一个重放的报文. 根据SA中的信息( 该SA使用的加解密算法, 使用的Key )将加密部分( original IP datagram + ESPtailer )解密. 去掉padding, 得到original IP datagram. 对该original IP datagram进行转发. 注意到, 如果IPsec datagram被人截获, 则: 截获者无法对其解密, 也就无法知道original IP datagram的内容. 由于IPsec datagram使用了MAC, 截获者无法篡改IPsec datagram的内容. 由于IPsec datagram使用了sequence-number, 因此截获者无法进行重放攻击. IKE 互联网密钥交换 IKE（Internet Key Exchange）协议: an automated mechanism for creating the SAs IKE非常复杂 Application Layer: Securing E-Mail 我们以E-Mail为例介绍应用层安全的实现. E-Mail安全要实现以下几点: 内容加密 报文完整性检查 双向鉴别 此外, 对于电子邮件, 双方要共享对称密钥比较困难, 因此一般用非对称加密. Secure E-Mail的发送过程大概如下: 首先, Alice使用HMAC计算明文m的报文摘要 $H(m)$ , 然后用自己的私钥$K_A^-$ 加密 $H(M)$ , 得到 $K_A^-(H(M))$. 把m和 $K_A^-(H(M))$ 连接在一起, 作为一个 preliminary package 这一步实现了发送方鉴别和报文完整性. 如果Alice身份属实, 那么后面Bob可以用Alice的公钥 $K_A^+$ 来解密$K_A^-(H(M))$得到 $H(M)$ , 将其和自己根据m计算的 $H(M)$ 比对. 如果正确, 则既证明了 $K_A^+$ 确实属于Alice( 发送方鉴别 ), 也证明了报文完整性. 接着, Alice随机选择一个数作为session key $K_S$ , 用 $K_S$ 将 preliminary package 加密, 记为 $K_S(.)$ . 并且用Bob的公钥 $K_B^+$ 加密 $K_S$ . 记为 $K_B(K_s)$ . 将二者连接在一起, 发给Bob. 这一步实现了接收方鉴别和报文加密. 如果Bob身份属实, 那么Bob应该可以用自己的私钥 $K_B^-$ 解密得到 session key $K_S$ , 再用 $K_S$ 解密得到preliminary package . 再进行步骤一所述的验证, 如果正确, 则既证明了 $K_B^+$ 确实属于Bob( 接收方鉴别 ), 又使用 $K_S$ 实现了报文加密. 上述过程使用了非对称加密, 对于E-Mail使用者, 可以自己生成公钥, 把它放在个人主页上. 当然也可以去CA注册得到证书, 对方要获得自己的公钥可以去CA查. 目前常用的Secure E-Mail实现是GPG( GPG是GNU的开源软件, 参考自功能相同的商业软件PGP ), 它的逻辑和上面所述的差不多.","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"Computer Networking","slug":"Computer-Networking","permalink":"http://lyk-love.cn/tags/Computer-Networking/"},{"name":"Network Security","slug":"Network-Security","permalink":"http://lyk-love.cn/tags/Network-Security/"}]},{"title":"Python Debugging","slug":"Python-Debugging","date":"2022-08-26T00:49:57.000Z","updated":"2022-09-26T06:39:34.938Z","comments":true,"path":"2022/08/26/Python-Debugging/","link":"","permalink":"http://lyk-love.cn/2022/08/26/Python-Debugging/","excerpt":"Outline: logging pdb","text":"Outline: logging pdb logging Python内置的logging模块可以非常容易地记录错误信息： # err_logging.pyimport loggingdef foo(s): return 10 / int(s)def bar(s): return foo(s) * 2def main(): try: bar(&#x27;0&#x27;) except Exception as e: logging.exception(e)main()print(&#x27;END&#x27;) 同样是出错，但程序打印完错误信息后会继续执行，并正常退出： $ python3 err_logging.pyERROR:root:division by zeroTraceback (most recent call last): File &quot;err_logging.py&quot;, line 13, in main bar(&#x27;0&#x27;) File &quot;err_logging.py&quot;, line 9, in bar return foo(s) * 2 File &quot;err_logging.py&quot;, line 6, in foo return 10 / int(s)ZeroDivisionError: division by zeroEND logging不会抛出错误, 而且可以输出到文件： import loggings = &#x27;0&#x27;n = int(s)logging.info(&#x27;n = %d&#x27; % n)print(10 / n) logging.info()就可以输出一段文本。运行，发现除了ZeroDivisionError，没有任何信息。怎么回事？ 别急，在import logging之后添加一行配置再试试： import logginglogging.basicConfig(level=logging.INFO) 看到输出了： $ python err.pyINFO:root:n = 0Traceback (most recent call last): File &quot;err.py&quot;, line 8, in &lt;module&gt; print(10 / n)ZeroDivisionError: division by zero 这就是logging的好处，它允许你指定记录信息的级别，有debug，info，warning，error等几个级别，当我们指定level=INFO时，logging.debug就不起作用了。同理，指定level=WARNING后，debug和info就不起作用了。这样一来，你可以放心地输出不同级别的信息，也不用删除，最后统一控制输出哪个级别的信息。 logging的另一个好处是通过简单的配置，一条语句可以同时输出到不同的地方，比如console和文件。 pdb Python的调试器pdb让程序以单步方式运行，可以随时查看运行状态。我们先准备好程序： # err.pys = &#x27;0&#x27;n = int(s)print(10 / n) 然后启动： $ python -m pdb err.py&gt; /Users/michael/Github/learn-python3/samples/debug/err.py(2)&lt;module&gt;()-&gt; s = &#x27;0&#x27; 以参数-m pdb启动后，pdb定位到下一步要执行的代码-&gt; s = '0'。输入命令l来查看代码： (Pdb) l 1 # err.py 2 -&gt; s = &#x27;0&#x27; 3 n = int(s) 4 print(10 / n) 输入命令n可以单步执行代码： (Pdb) n&gt; /Users/michael/Github/learn-python3/samples/debug/err.py(3)&lt;module&gt;()-&gt; n = int(s)(Pdb) n&gt; /Users/michael/Github/learn-python3/samples/debug/err.py(4)&lt;module&gt;()-&gt; print(10 / n) 任何时候都可以输入命令p 变量名来查看变量： (Pdb) p s&#x27;0&#x27;(Pdb) p n0 输入命令q结束调试，退出程序： (Pdb) q 这种通过pdb在命令行调试的方法理论上是万能的，但实在是太麻烦了，如果有一千行代码，要运行到第999行得敲多少命令啊。还好，我们还有另一种调试方法。 pdb.set_trace() 这个方法也是用pdb，但是不需要单步执行，我们只需要import pdb，然后，在可能出错的地方放一个pdb.set_trace()，就可以设置一个断点： # err.pyimport pdbs = &#x27;0&#x27;n = int(s)pdb.set_trace() # 运行到这里会自动暂停print(10 / n) 运行代码，程序会自动在pdb.set_trace()暂停并进入pdb调试环境，可以用命令p查看变量，或者用命令c继续运行： $ python err.py &gt; /Users/michael/Github/learn-python3/samples/debug/err.py(7)&lt;module&gt;()-&gt; print(10 / n)(Pdb) p n0(Pdb) cTraceback (most recent call last): File &quot;err.py&quot;, line 7, in &lt;module&gt; print(10 / n)ZeroDivisionError: division by zero 这个方式比直接启动pdb单步调试效率要高很多，但也高不到哪去。","categories":[{"name":"Language","slug":"Language","permalink":"http://lyk-love.cn/categories/Language/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://lyk-love.cn/tags/Python/"}]},{"title":"Python Test","slug":"Python-Test","date":"2022-08-26T00:44:51.000Z","updated":"2022-09-26T06:39:34.938Z","comments":true,"path":"2022/08/26/Python-Test/","link":"","permalink":"http://lyk-love.cn/2022/08/26/Python-Test/","excerpt":"Outline: Unit Test Doc Test //TODO","text":"Outline: Unit Test Doc Test //TODO Unit Test 如果你听说过“测试驱动开发”（TDD：Test-Driven Development），单元测试就不陌生。 单元测试是用来对一个模块、一个函数或者一个类来进行正确性检验的测试工作。 比如对函数abs()，我们可以编写出以下几个测试用例： 输入正数，比如1、1.2、0.99，期待返回值与输入相同； 输入负数，比如-1、-1.2、-0.99，期待返回值与输入相反； 输入0，期待返回0； 输入非数值类型，比如None、[]、&#123;&#125;，期待抛出TypeError。 把上面的测试用例放到一个测试模块里，就是一个完整的单元测试。 如果单元测试通过，说明我们测试的这个函数能够正常工作。如果单元测试不通过，要么函数有bug，要么测试条件输入不正确，总之，需要修复使单元测试能够通过。 单元测试通过后有什么意义呢？如果我们对abs()函数代码做了修改，只需要再跑一遍单元测试，如果通过，说明我们的修改不会对abs()函数原有的行为造成影响，如果测试不通过，说明我们的修改与原有行为不一致，要么修改代码，要么修改测试。 这种以测试为驱动的开发模式最大的好处就是确保一个程序模块的行为符合我们设计的测试用例。在将来修改的时候，可以极大程度地保证该模块行为仍然是正确的。 我们来编写一个Dict类，这个类的行为和dict一致，但是可以通过属性来访问，用起来就像下面这样： &gt;&gt;&gt; d = Dict(a=1, b=2)&gt;&gt;&gt; d[&#x27;a&#x27;]1&gt;&gt;&gt; d.a1 mydict.py代码如下： class Dict(dict): def __init__(self, **kw): super().__init__(**kw) def __getattr__(self, key): try: return self[key] except KeyError: raise AttributeError(r&quot;&#x27;Dict&#x27; object has no attribute &#x27;%s&#x27;&quot; % key) def __setattr__(self, key, value): self[key] = value 为了编写单元测试，我们需要引入Python自带的unittest模块，编写mydict_test.py如下： import unittestfrom mydict import Dictclass TestDict(unittest.TestCase): def test_init(self): d = Dict(a=1, b=&#x27;test&#x27;) self.assertEqual(d.a, 1) self.assertEqual(d.b, &#x27;test&#x27;) self.assertTrue(isinstance(d, dict)) def test_key(self): d = Dict() d[&#x27;key&#x27;] = &#x27;value&#x27; self.assertEqual(d.key, &#x27;value&#x27;) def test_attr(self): d = Dict() d.key = &#x27;value&#x27; self.assertTrue(&#x27;key&#x27; in d) self.assertEqual(d[&#x27;key&#x27;], &#x27;value&#x27;) def test_keyerror(self): d = Dict() with self.assertRaises(KeyError): value = d[&#x27;empty&#x27;] def test_attrerror(self): d = Dict() with self.assertRaises(AttributeError): value = d.empty 编写单元测试时，我们需要编写一个测试类，从unittest.TestCase继承。 以test开头的方法就是测试方法，不以test开头的方法不被认为是测试方法，测试的时候不会被执行。 对每一类测试都需要编写一个test_xxx()方法。由于unittest.TestCase提供了很多内置的条件判断，我们只需要调用这些方法就可以断言输出是否是我们所期望的。最常用的断言就是assertEqual()： self.assertEqual(abs(-1), 1) # 断言函数返回的结果与1相等 另一种重要的断言就是期待抛出指定类型的Error，比如通过d['empty']访问不存在的key时，断言会抛出KeyError： with self.assertRaises(KeyError): value = d[&#x27;empty&#x27;] 而通过d.empty访问不存在的key时，我们期待抛出AttributeError： with self.assertRaises(AttributeError): value = d.empty Run 一旦编写好单元测试，我们就可以运行单元测试。最简单的运行方式是在mydict_test.py的最后加上两行代码： if __name__ == &#x27;__main__&#x27;: unittest.main() 这样就可以把mydict_test.py当做正常的python脚本运行： $ python mydict_test.py 另一种方法是在命令行通过参数-m unittest直接运行单元测试： $ python -m unittest mydict_test.....----------------------------------------------------------------------Ran 5 tests in 0.000sOK 这是推荐的做法，因为这样可以一次批量运行很多单元测试，并且，有很多工具可以自动来运行这些单元测试。 setUp与tearDown 可以在单元测试中编写两个特殊的setUp()和tearDown()方法。这两个方法会分别在每调用一个测试方法的前后分别被执行。 setUp()和tearDown()方法有什么用呢？设想你的测试需要启动一个数据库，这时，就可以在setUp()方法中连接数据库，在tearDown()方法中关闭数据库，这样，不必在每个测试方法中重复相同的代码： class TestDict(unittest.TestCase): def setUp(self): print(&#x27;setUp...&#x27;) def tearDown(self): print(&#x27;tearDown...&#x27;) 可以再次运行测试看看每个测试方法调用前后是否会打印出setUp...和tearDown...。 Doc Test 如果你经常阅读Python的官方文档，可以看到很多文档都有示例代码。比如re模块就带了很多示例代码： &gt;&gt;&gt; import re&gt;&gt;&gt; m = re.search(&#x27;(?&lt;=abc)def&#x27;, &#x27;abcdef&#x27;)&gt;&gt;&gt; m.group(0)&#x27;def&#x27; 可以把这些示例代码在Python的交互式环境下输入并执行，结果与文档中的示例代码显示的一致。 这些代码与其他说明可以写在注释中，然后，由一些工具来自动生成文档。既然这些代码本身就可以粘贴出来直接运行，那么，可不可以自动执行写在注释中的这些代码呢？ 答案是肯定的。 当我们编写注释时，如果写上这样的注释： def abs(n): &#x27;&#x27;&#x27; Function to get absolute value of number. Example: &gt;&gt;&gt; abs(1) 1 &gt;&gt;&gt; abs(-1) 1 &gt;&gt;&gt; abs(0) 0 &#x27;&#x27;&#x27; return n if n &gt;= 0 else (-n) 无疑更明确地告诉函数的调用者该函数的期望输入和输出。 并且，Python内置的“文档测试”（doctest）模块可以直接提取注释中的代码并执行测试。 doctest严格按照Python交互式命令行的输入和输出来判断测试结果是否正确。只有测试异常的时候，可以用...表示中间一大段烦人的输出。 让我们用doctest来测试上次编写的Dict类： # mydict2.pyclass Dict(dict): &#x27;&#x27;&#x27; Simple dict but also support access as x.y style. &gt;&gt;&gt; d1 = Dict() &gt;&gt;&gt; d1[&#x27;x&#x27;] = 100 &gt;&gt;&gt; d1.x 100 &gt;&gt;&gt; d1.y = 200 &gt;&gt;&gt; d1[&#x27;y&#x27;] 200 &gt;&gt;&gt; d2 = Dict(a=1, b=2, c=&#x27;3&#x27;) &gt;&gt;&gt; d2.c &#x27;3&#x27; &gt;&gt;&gt; d2[&#x27;empty&#x27;] Traceback (most recent call last): ... KeyError: &#x27;empty&#x27; &gt;&gt;&gt; d2.empty Traceback (most recent call last): ... AttributeError: &#x27;Dict&#x27; object has no attribute &#x27;empty&#x27; &#x27;&#x27;&#x27; def __init__(self, **kw): super(Dict, self).__init__(**kw) def __getattr__(self, key): try: return self[key] except KeyError: raise AttributeError(r&quot;&#x27;Dict&#x27; object has no attribute &#x27;%s&#x27;&quot; % key) def __setattr__(self, key, value): self[key] = valueif __name__==&#x27;__main__&#x27;: import doctest doctest.testmod() 运行python mydict2.py： $ python mydict2.py 什么输出也没有。这说明我们编写的doctest运行都是正确的。如果程序有问题，比如把__getattr__()方法注释掉，再运行就会报错： $ python mydict2.py**********************************************************************File &quot;/Users/michael/Github/learn-python3/samples/debug/mydict2.py&quot;, line 10, in __main__.DictFailed example: d1.xException raised: Traceback (most recent call last): ... AttributeError: &#x27;Dict&#x27; object has no attribute &#x27;x&#x27;**********************************************************************File &quot;/Users/michael/Github/learn-python3/samples/debug/mydict2.py&quot;, line 16, in __main__.DictFailed example: d2.cException raised: Traceback (most recent call last): ... AttributeError: &#x27;Dict&#x27; object has no attribute &#x27;c&#x27;**********************************************************************1 items had failures: 2 of 9 in __main__.Dict***Test Failed*** 2 failures. 注意到最后3行代码。当模块正常导入时，doctest不会被执行。只有在命令行直接运行时，才执行doctest。所以，不必担心doctest会在非测试环境下执行.","categories":[{"name":"Language","slug":"Language","permalink":"http://lyk-love.cn/categories/Language/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://lyk-love.cn/tags/Python/"}]},{"title":"Python REGEX","slug":"Python-REGEX","date":"2022-08-26T00:43:35.000Z","updated":"2022-09-26T06:39:34.938Z","comments":true,"path":"2022/08/26/Python-REGEX/","link":"","permalink":"http://lyk-love.cn/2022/08/26/Python-REGEX/","excerpt":"Outline: re模块 切分字符串 分组 贪婪匹配 编译","text":"Outline: re模块 切分字符串 分组 贪婪匹配 编译 re模块 Python提供re模块，包含所有正则表达式的功能。由于Python的字符串本身也用\\转义，所以要特别注意： s = &#x27;ABC\\\\-001&#x27; # Python的字符串# 对应的正则表达式字符串变成：# &#x27;ABC\\-001&#x27; 因此我们强烈建议使用Python的r前缀，就不用考虑转义的问题了： s = r&#x27;ABC\\-001&#x27; # Python的字符串# 对应的正则表达式字符串不变：# &#x27;ABC\\-001&#x27; 先看看如何判断正则表达式是否匹配： &gt;&gt;&gt; import re&gt;&gt;&gt; re.match(r&#x27;^\\d&#123;3&#125;\\-\\d&#123;3,8&#125;$&#x27;, &#x27;010-12345&#x27;)&lt;_sre.SRE_Match object; span=(0, 9), match=&#x27;010-12345&#x27;&gt;&gt;&gt;&gt; re.match(r&#x27;^\\d&#123;3&#125;\\-\\d&#123;3,8&#125;$&#x27;, &#x27;010 12345&#x27;)&gt;&gt;&gt; match()方法判断是否匹配，如果匹配成功，返回一个Match对象，否则返回None。常见的判断方法就是： test = &#x27;用户输入的字符串&#x27;if re.match(r&#x27;正则表达式&#x27;, test): print(&#x27;ok&#x27;)else: print(&#x27;failed&#x27;) 切分字符串 用正则表达式切分字符串比用固定的字符更灵活，请看正常的切分代码： &gt;&gt;&gt; &#x27;a b c&#x27;.split(&#x27; &#x27;)[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;&#x27;, &#x27;&#x27;, &#x27;c&#x27;] 嗯，无法识别连续的空格，用正则表达式试试： &gt;&gt;&gt; re.split(r&#x27;\\s+&#x27;, &#x27;a b c&#x27;)[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;] 无论多少个空格都可以正常分割。加入,试试： &gt;&gt;&gt; re.split(r&#x27;[\\s\\,]+&#x27;, &#x27;a,b, c d&#x27;)[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;] 再加入;试试： &gt;&gt;&gt; re.split(r&#x27;[\\s\\,\\;]+&#x27;, &#x27;a,b;; c d&#x27;)[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;] 如果用户输入了一组标签，下次记得用正则表达式来把不规范的输入转化成正确的数组。 分组 除了简单地判断是否匹配之外，正则表达式还有提取子串的强大功能。用()表示的就是要提取的分组（Group）。比如： ^(\\d&#123;3&#125;)-(\\d&#123;3,8&#125;)$分别定义了两个组，可以直接从匹配的字符串中提取出区号和本地号码： &gt;&gt;&gt; m = re.match(r&#x27;^(\\d&#123;3&#125;)-(\\d&#123;3,8&#125;)$&#x27;, &#x27;010-12345&#x27;)&gt;&gt;&gt; m&lt;_sre.SRE_Match object; span=(0, 9), match=&#x27;010-12345&#x27;&gt;&gt;&gt;&gt; m.group(0)&#x27;010-12345&#x27;&gt;&gt;&gt; m.group(1)&#x27;010&#x27;&gt;&gt;&gt; m.group(2)&#x27;12345&#x27; 如果正则表达式中定义了组，就可以在Match对象上用group()方法提取出子串来。 注意到group(0)永远是原始字符串，group(1)、group(2)……表示第1、2、……个子串。 提取子串非常有用。来看一个更凶残的例子： &gt;&gt;&gt; t = &#x27;19:05:30&#x27;&gt;&gt;&gt; m = re.match(r&#x27;^(0[0-9]|1[0-9]|2[0-3]|[0-9])\\:(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]|[0-9])\\:(0[0-9]|1[0-9]|2[0-9]|3[0-9]|4[0-9]|5[0-9]|[0-9])$&#x27;, t)&gt;&gt;&gt; m.groups()(&#x27;19&#x27;, &#x27;05&#x27;, &#x27;30&#x27;) 这个正则表达式可以直接识别合法的时间。但是有些时候，用正则表达式也无法做到完全验证，比如识别日期： &#x27;^(0[1-9]|1[0-2]|[0-9])-(0[1-9]|1[0-9]|2[0-9]|3[0-1]|[0-9])$&#x27; 对于'2-30'，'4-31'这样的非法日期，用正则还是识别不了，或者说写出来非常困难，这时就需要程序配合识别了。 贪婪匹配 最后需要特别指出的是，Py的正则匹配默认是贪婪匹配，也就是匹配尽可能多的字符。举例如下，匹配出数字后面的0： &gt;&gt;&gt; re.match(r&#x27;^(\\d+)(0*)$&#x27;, &#x27;102300&#x27;).groups()(&#x27;102300&#x27;, &#x27;&#x27;) 由于\\d+采用贪婪匹配，直接把后面的0全部匹配了，结果0*只能匹配空字符串了。 必须让\\d+采用非贪婪匹配（也就是尽可能少匹配），才能把后面的0匹配出来，加个?就可以让\\d+采用非贪婪匹配： &gt;&gt;&gt; re.match(r&#x27;^(\\d+?)(0*)$&#x27;, &#x27;102300&#x27;).groups()(&#x27;1023&#x27;, &#x27;00&#x27;) 编译 当我们在Python中使用正则表达式时，re模块内部会干两件事情： 编译正则表达式，如果正则表达式的字符串本身不合法，会报错； 用编译后的正则表达式去匹配字符串。 如果一个正则表达式要重复使用几千次，出于效率的考虑，我们可以预编译该正则表达式，接下来重复使用时就不需要编译这个步骤了，直接匹配： &gt;&gt;&gt; import re# 编译:&gt;&gt;&gt; re_telephone = re.compile(r&#x27;^(\\d&#123;3&#125;)-(\\d&#123;3,8&#125;)$&#x27;)# 使用：&gt;&gt;&gt; re_telephone.match(&#x27;010-12345&#x27;).groups()(&#x27;010&#x27;, &#x27;12345&#x27;)&gt;&gt;&gt; re_telephone.match(&#x27;010-8086&#x27;).groups()(&#x27;010&#x27;, &#x27;8086&#x27;) 编译后生成Regular Expression对象，由于该对象自己包含了正则表达式，所以调用对应的方法时不用给出正则字符串。","categories":[{"name":"Language","slug":"Language","permalink":"http://lyk-love.cn/categories/Language/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://lyk-love.cn/tags/Python/"}]},{"title":"CAP Theory","slug":"CAP-Theory","date":"2022-08-25T22:09:03.000Z","updated":"2022-09-26T06:39:34.926Z","comments":true,"path":"2022/08/26/CAP-Theory/","link":"","permalink":"http://lyk-love.cn/2022/08/26/CAP-Theory/","excerpt":"Outline: Intro Consistency Availability Partition Tolerance CAP: Tradeoff Yield &amp;&amp; Harvest BASE Theory","text":"Outline: Intro Consistency Availability Partition Tolerance CAP: Tradeoff Yield &amp;&amp; Harvest BASE Theory Intro In 2000, Dr. Eric Brewer 在 Proceedings of the Annual ACM Symposium on Principles of Distributed Computing[^1]上提出了CAP理论的猜想: a shared-data system can have at most two of the three following properties: Consistency: Availability tolerance to network Partitions. In 2002, Gilbert and Lynch[^2] 形式化地证明了该猜想. CAP理论揭示出分布式系统不可能同时满足CAP这三者. Consistency From Gilbert and Lynch^2: Atomic, or linearizable, consistency is the condition expected by most web services today. Under this consistency guarantee, there must exist a total order on all operations such that each operation looks as if it were completed at a single instant. This is equivalent to requiring requests of the distributed shared memory to act as if they were executing on a single node, responding to operations one at a time. 一致性( Consistency ): a system is consistent if an update is applied to all relevant nodes at the same logical time. 也就是说系统各节点的状态是一致的, 对多个节点的操作从结果上看就好像是对一个节点的操作, 不存在状态不一致的节点. 这也要求操作的原子性. 比如, 所有节点访问同一份最新的数据副本. database replication不是强一致性的, 因为副本同步需要时间. 即使和全局的一致性是不可能的, 我们要求的只是相对而言的一致性( 即不一致状态的时间粒度很小, 小到没有影响 ) 关于一致性的实现, 有强一致性算法和最终一致性算法 Strong Consistency //TODO Paxos：我最喜欢的图灵奖获得者Leslie Lamport于90年提出，几乎所有强一致性算法鼻祖. Raft(Paxos变种)：etcd使用 ZAB(Paxos变种)：ZooKeeper使用 Eventual Consistency Algorithm DNS系统：域名解析，超过TTL逐步向上询问 Gossip算法：逆熵算法，Cassandra中使用 Eventual Consistency Availability Again from Gilbert and Lynch[^2]: For a distributed system to be continuously available, every request received by a non-failing node in the system must result in a response. That is, any algorithm used by the service must eventually terminate … [When] qualified by the need for partition tolerance, this can be seen as a strong definition of availability: even when severe network failures occur, every request must terminate. 可用性(Availability): 每个对non-failing node的请求一定能被响应. 这里指的是有效的响应, 500服务器错误不是一次有效的响应. 绝对的可用性也是不可能的, 因为存在节点全部fail的情况 Partition Tolerance Once more, Gilbert and Lynch2: In order to model partition tolerance, the network will be allowed to lose arbitrarily many messages sent from one node to another. When a network is partitioned, all messages sent from nodes in one component of the partition to nodes in another component are lost. (And any pattern of message loss can be modeled as a temporary partition separating the communicating nodes at the exact instant the message is lost.) 分区容错性(Partition tolerance): 系统能够容忍任意的网络分区故障. 这里的网络分区故障是指, 网络分成许多区域, 不同区域间不能互通 网络丢包可以被认为是暂时的分区故障; node failure也可以被认为是分区故障, 因为所有发送给它的数据包都会丢失 当然也存在不需要分区容错性的系统, 比如单机系统. 不过对于分布式系统, 都存在分区问题. 只要是分布是 P is Mandatory P is mandatory in distributed systems. You cannot not choose it. 我们可以假设一下, 如果有一个CA的系统, 它有三个节点$A, B, V$, 管理同一块数据. 由于网络分区, 节点被分裂为${A, B}$和${C}$, 此时有一个更新数据的写请求到达节点$C$. 此时$C$只有两种选择: 接受这次写请求, 但是$A,B$ 在分区恢复前永远不知道数据已经被更新 拒绝这次写请求, 直到分区恢复后再开始接受请求 选择1只能保证可用性, 选择2只能保证一致性. 但没法同时保持CA. 因此, 只要是分布式系统, 都必须保持P. 因此系统要么是CP, 要么是AP, 绝不可能是CA. 除非这个分布式系统能够在网络分区的情况下更新其他节点的状态, 但这是不可能的. CAP: Tradeoff Consistency &gt; Availability 如果一个系统是CP的, 也就是提供一致性( C )而不是可用性( A ), 它将通过拒绝响应一些请求来保证其一致性( i.e. 原子读写, 事务) 常见例子是大部分分布式数据库以及银行系统 Availability &gt; Consistency 如果一个系统是AP的, 也就是提供可用性( A ), 而不是一致性( C ), 它将对所有的请求做出响应，可能会返回旧的读和接受冲突的写. 有很多数据模型可以接受旧的读, 比如金融行业, 淘宝, 12306 ... Yield &amp;&amp; Harvest 有许多种对分布式系统的度量, 其中很有名的是 Fox 和 Brewer在“Harvest, Yield, and Scalable Tolerant Systems”[^3]中提出的yield and harvest. yield: he probability of completing a request yield与&quot;正常运行时间&quot;很接近，但在实践中更有用，因为并非所有的时间都有同等价值. yield直接映射到用户体验. 在没有请求的情况下宕机一秒钟, 会减少正常运行时间, 但对用户或yield没有影响 harvest: the fraction of the data reflected in the response, i.e. the completeness of the answer to the query. $$ \\mathrm{harvest}= \\frac{\\mathrm{total \\ data}}{\\mathrm{data \\ available}} $$ 例如, 假设我们在一个搜索引擎上工作，我们可以想象每个词都有单独的索引。使用 &quot;cute &quot;一词的网页的索引在$A$节点上，使用 &quot;婴儿 &quot;一词的网页的索引在$B$节点上，而 &quot;动物 &quot;一词的索引在$C$机器上。那么，搜索 &quot;cute baby animals&quot;，结合来自节点$A,B,C$的结果，${A,B,C}$将有100%的$\\mathrm{harvest}$. 然而, 如果节点$BB$不可用，我们可能只返回 &quot;cute animals&quot;的结果，这将是一个66%的$\\mathrm{harvest}$. 例如, 一个存储文件版本的系统在一些节点瘫痪的情况下, 可以选择呈现它能找到的最新版本的文件，即使它知道这有可能不是它所存储的最新版本. 关键在于, yield和harvest之间存在trade off. 在设计系统时要根据业务来侧重其中一方. Replicated systems 倾向于把fault等价为 reduced yield, 因为请求被拒绝了(为了同步); partitioned systems 倾向于把fault等价为 reduced harvest, 因为能够接受相同数量的请求, 但是某些分区中的数据不见了. BASE Theory CAP理论描述了分布式系统的理想状态, BASE理论是一个更妥协的理论, 在实践中更有用: Basically Available( 基本可用 ): 在出现故障时，允许部分可用性（基础可用） Soft State( 软状态 ): 允许出现中间状态，不同节点之间数据不一致 Eventual Consistency( 最终一致性 ): 软状态不可持续, 在有限期限后应能保证数据的最终一致 Refs [^1]: Brewer. Towards robust distributed systems. Proceedings of the Annual ACM Symposium on Principles of Distributed Computing (2000) vol. 19 pp. 7—10 [^2]: Gilbert and Lynch. Brewer’s conjecture and the feasibility of consistent, available, partition-tolerant web services. ACM SIGACT News (2002) vol. 33 (2) pp. 59 [^3]: Fox and Brewer. Harvest, yield, and scalable tolerant systems. Hot Topics in Operating Systems, 1999. Proceedings of the Seventh Workshop on (1999) pp. 174—178","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"http://lyk-love.cn/tags/Distributed-System/"}]},{"title":"Python Module","slug":"Python-Module","date":"2022-08-24T21:50:32.000Z","updated":"2022-09-26T06:39:34.938Z","comments":true,"path":"2022/08/25/Python-Module/","link":"","permalink":"http://lyk-love.cn/2022/08/25/Python-Module/","excerpt":"Outline: Basic Module Scope Searching Path Common Modules","text":"Outline: Basic Module Scope Searching Path Common Modules Basic 一个.py文件就是一个模块（Module） 模块可以避免代码的名冲突，因此命名模块时，应尽量不要与内置函数名字冲突 最好先查看系统是否已存在该模块，检查方法是在Python交互环境执行import abc，若成功则说明系统存在此模块。 一个文件夹下有__init__.py，该文件夹就成为了一个包，而__init__.py本身就是一个模块，它的模块名就是包名 可以通过包来组织模块 Usage Python本身就内置了很多非常有用的模块，只要安装完毕，这些模块就可以立刻使用。 我们以内建的sys模块为例，编写一个hello的模块： #!/usr/bin/env python3# -*- coding: utf-8 -*-&#x27; a test module &#x27;__author__ = &#x27;Michael Liao&#x27;import sysdef test(): args = sys.argv if len(args)==1: print(&#x27;Hello, world!&#x27;) elif len(args)==2: print(&#x27;Hello, %s!&#x27; % args[1]) else: print(&#x27;Too many arguments!&#x27;)if __name__==&#x27;__main__&#x27;: test() 第1行和第2行是标准注释，第1行注释可以让这个hello.py文件直接在Unix/Linux/Mac上运行，第2行注释表示.py文件本身使用标准UTF-8编码； 第4行是一个字符串，表示模块的文档注释，任何模块代码的第一个字符串都被视为模块的文档注释； 第6行使用__author__变量把作者写进去，这样当你公开源代码后别人就可以瞻仰你的大名； 以上就是Python模块的标准文件模板，当然也可以全部删掉不写，但是，按标准办事肯定没错。 后面开始就是真正的代码部分。 你可能注意到了，使用sys模块的第一步，就是导入该模块： import sys 导入sys模块后，我们就有了变量sys指向该模块，利用sys这个变量，就可以访问sys模块的所有功能。 sys模块有一个argv变量，用list存储了命令行的所有参数。argv至少有一个元素，因为第一个参数永远是该.py文件的名称，例如： 运行python3 hello.py获得的sys.argv就是['hello.py']； 运行python3 hello.py Michael获得的sys.argv就是['hello.py', 'Michael']。 最后，注意到这两行代码： if __name__==&#x27;__main__&#x27;: test() 当我们在命令行运行hello模块文件时，Python解释器把一个特殊变量__name__置为__main__，而如果在其他地方导入该hello模块时，if判断将失败，因此，这种if测试可以让一个模块通过命令行运行时执行一些额外的代码，最常见的就是运行测试。 我们可以用命令行运行hello.py看看效果： $ python3 hello.pyHello, world!$ python hello.py MichaelHello, Michael! 如果启动Python交互环境，再导入hello模块： $ python3Python 3.4.3 (v3.4.3:9b73f1c3e601, Feb 23 2015, 02:52:03) [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt; import hello&gt;&gt;&gt; 导入时，没有打印Hello, word!，因为没有执行test()函数。 调用hello.test()时，才能打印出Hello, word!： &gt;&gt;&gt; hello.test()Hello, world! Module Scope 在一个模块中，我们可能会定义很多函数和变量，但有的函数和变量我们希望给别人使用，有的函数和变量我们希望仅仅在模块内部使用。在Python中，是通过_前缀来实现的。 正常的函数和变量名是公开的（public），可以被直接引用，比如：abc，x123，PI等； 类似__xxx__这样的变量是特殊变量，可以被直接引用，但是有特殊用途，比如上面的__author__，__name__就是特殊变量，hello模块定义的文档注释也可以用特殊变量__doc__访问，我们自己的变量一般不要用这种变量名； 类似_xxx和__xxx这样的函数或变量就是非公开的（private），不应该被直接引用，比如_abc，__abc等； 之所以我们说，private函数和变量“不应该”被直接引用，而不是“不能”被直接引用，是因为Python并没有一种方法可以完全限制访问private函数或变量，但是，从编程习惯上不应该引用private函数或变量。 private函数或变量不应该被别人引用，那它们有什么用呢？请看例子： def _private_1(name): return &#x27;Hello, %s&#x27; % namedef _private_2(name): return &#x27;Hi, %s&#x27; % namedef greeting(name): if len(name) &gt; 3: return _private_1(name) else: return _private_2(name) 我们在模块里公开greeting()函数，而把内部逻辑用private函数隐藏起来了，这样，调用greeting()函数不用关心内部的private函数细节，这也是一种非常有用的代码封装和抽象的方法，即： 外部不需要引用的函数全部定义成private，只有外部需要引用的函数才定义为public。 Searching Path 当我们试图加载一个模块时，Python会在指定的路径下搜索对应的.py文件，如果找不到，就会报错： &gt;&gt;&gt; import mymoduleTraceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;ImportError: No module named mymodule 默认情况下，Python解释器会搜索当前目录、所有已安装的内置模块和第三方模块，搜索路径存放在sys模块的path变量中： &gt;&gt;&gt; import sys&gt;&gt;&gt; sys.path[&#x27;&#x27;, &#x27;/Library/Frameworks/Python.framework/Versions/3.6/lib/python36.zip&#x27;, &#x27;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6&#x27;, ..., &#x27;/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages&#x27;] 如果我们要添加自己的搜索目录，有两种方法： 一是直接修改sys.path，添加要搜索的目录： &gt;&gt;&gt; import sys&gt;&gt;&gt; sys.path.append(&#x27;/Users/michael/my_py_scripts&#x27;) 这种方法是在运行时修改，运行结束后失效。 第二种方法是设置环境变量PYTHONPATH，该环境变量的内容会被自动添加到模块搜索路径中。设置方式与设置Path环境变量类似。注意只需要添加你自己的搜索路径，Python自己本身的搜索路径不受影响。 Common Modules pandas read_csv()/read_table()文本文件的读取 Dataframe Pandas 数据结构 - DataFrame Pandas.DataFrame 的 iterrows()方法详解 argparse argparse模块使用教程","categories":[{"name":"Language","slug":"Language","permalink":"http://lyk-love.cn/categories/Language/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://lyk-love.cn/tags/Python/"}]},{"title":"Python OOP Advanced","slug":"Python-OOP-Advanced","date":"2022-08-24T21:39:06.000Z","updated":"2022-09-26T06:39:34.938Z","comments":true,"path":"2022/08/25/Python-OOP-Advanced/","link":"","permalink":"http://lyk-love.cn/2022/08/25/Python-OOP-Advanced/","excerpt":"Outline: Abstract Class Static Member Inner Class _slots_ @propetry multi-inheritance Specialize Class Enum metaclass 绝大部分抄自廖雪峰的教程","text":"Outline: Abstract Class Static Member Inner Class _slots_ @propetry multi-inheritance Specialize Class Enum metaclass 绝大部分抄自廖雪峰的教程 抽象类 静态成员 静态变量和静态方法都可以通过类名和对象进行访问 静态方法：加装饰器@staticmethod 静态变量： class ClassName: &quot;&quot;&quot;docstring for ClassName&quot;&quot;&quot; arg=0 def __init__(self, arg): self.arg = argobj = ClassName(2)print(obj.arg)print(ClassName.arg) 输出结果： 20[Finished in 0.1s] 内部类 ## 外部类class Outer: ## 内部类 class Inner: pass ## 多级内部类 class InnerInner: pass ## 另一个内部类 class _Inner: pass ## ... pass 可以使用self关键词来访问内部类，这样我们就可以快速创建内部类的实例，并且根据需要在外部类中执行操作。但是，我们是不能在内部类中访问外部类的。让我们看一下下面的例子： class Outer: &quot;&quot;&quot;外部类&quot;&quot;&quot; def __init__(self): ## 实例化内部类 self.inner = self.Inner() def reveal(self): ## calling the &#x27;Inner&#x27; class function display self.inner.inner_display(&quot;Calling Inner class function from Outer class&quot;) class Inner: &quot;&quot;&quot;内部类&quot;&quot;&quot; def inner_display(self, msg): print(msg) 现在，我们来创建外部类的实例，并调用它的reveal()方法来执行内部类的方法inner_display()。 ## 创建外部类的实例对象outer = Outer()## 调用&#x27;reveal()&#x27;方法outer.reveal() 让我们看看访问内部类的另一种方法，不过这种方式的效率相对低一些。 Outer().Inner().inner_display(&quot;Calling the Inner class method directly&quot;) 如果我们想摆脱外部类的控制，在运行的时候独立地创建一个内部类的实例对象，也是可以做到的。如下面的代码所示： outer = Outer()## 实例化内部类inner = outer.Inner() ## inner = Outer().Inner() or inner = outer.innerinner.inner_display(&quot;Just Print It!&quot;) 操作符重载 _slots_ 正常情况下，当我们定义了一个class，创建了一个class的实例后，我们可以给该实例绑定任何属性和方法，这就是动态语言的灵活性。先定义class： class Student(object): pass 然后，尝试给实例绑定一个属性： &gt;&gt;&gt; s = Student()&gt;&gt;&gt; s.name = &#x27;Michael&#x27; # 动态给实例绑定一个属性&gt;&gt;&gt; print(s.name)Michael 还可以尝试给实例绑定一个方法： &gt;&gt;&gt; def set_age(self, age): # 定义一个函数作为实例方法... self.age = age...&gt;&gt;&gt; from types import MethodType&gt;&gt;&gt; s.set_age = MethodType(set_age, s) # 给实例绑定一个方法&gt;&gt;&gt; s.set_age(25) # 调用实例方法&gt;&gt;&gt; s.age # 测试结果25 但是，给一个实例绑定的方法，对另一个实例是不起作用的： &gt;&gt;&gt; s2 = Student() # 创建新的实例&gt;&gt;&gt; s2.set_age(25) # 尝试调用方法Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: &#x27;Student&#x27; object has no attribute &#x27;set_age&#x27; 为了给所有实例都绑定方法，可以给class绑定方法： &gt;&gt;&gt; def set_score(self, score):... self.score = score...&gt;&gt;&gt; Student.set_score = set_score 给class绑定方法后，所有实例均可调用： &gt;&gt;&gt; s.set_score(100)&gt;&gt;&gt; s.score100&gt;&gt;&gt; s2.set_score(99)&gt;&gt;&gt; s2.score99 通常情况下，上面的set_score方法可以直接定义在class中，但动态绑定允许我们在程序运行的过程中动态给class加上功能，这在静态语言中很难实现。 使用slots 但是，如果我们想要限制实例的属性怎么办？比如，只允许对Student实例添加name和age属性。 为了达到限制的目的，Python允许在定义class的时候，定义一个特殊的__slots__变量，来限制该class实例能添加的属性： class Student(object): __slots__ = (&#x27;name&#x27;, &#x27;age&#x27;) # 用tuple定义允许绑定的属性名称 然后，我们试试： &gt;&gt;&gt; s = Student() # 创建新的实例&gt;&gt;&gt; s.name = &#x27;Michael&#x27; # 绑定属性&#x27;name&#x27;&gt;&gt;&gt; s.age = 25 # 绑定属性&#x27;age&#x27;&gt;&gt;&gt; s.score = 99 # 绑定属性&#x27;score&#x27;Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: &#x27;Student&#x27; object has no attribute &#x27;score&#x27; 由于'score'没有被放到__slots__中，所以不能绑定score属性，试图绑定score将得到AttributeError的错误。 使用__slots__要注意，__slots__定义的属性仅对当前类实例起作用，对继承的子类是不起作用的： &gt;&gt;&gt; class GraduateStudent(Student):... pass...&gt;&gt;&gt; g = GraduateStudent()&gt;&gt;&gt; g.score = 9999 除非在子类中也定义__slots__，这样，子类实例允许定义的属性就是自身的__slots__加上父类的__slots__。 @property 在绑定属性时，如果我们直接把属性暴露出去，虽然写起来很简单，但是，没办法检查参数，导致可以把成绩随便改： s = Student()s.score = 9999 这显然不合逻辑。为了限制score的范围，可以通过一个set_score()方法来设置成绩，再通过一个get_score()来获取成绩，这样，在set_score()方法里，就可以检查参数： class Student(object): def get_score(self): return self._score def set_score(self, value): if not isinstance(value, int): raise ValueError(&#x27;score must be an integer!&#x27;) if value &lt; 0 or value &gt; 100: raise ValueError(&#x27;score must between 0 ~ 100!&#x27;) self._score = value 现在，对任意的Student实例进行操作，就不能随心所欲地设置score了： &gt;&gt;&gt; s = Student()&gt;&gt;&gt; s.set_score(60) # ok!&gt;&gt;&gt; s.get_score()60&gt;&gt;&gt; s.set_score(9999)Traceback (most recent call last): ...ValueError: score must between 0 ~ 100! 但是，上面的调用方法又略显复杂，没有直接用属性这么直接简单。 有没有既能检查参数，又可以用类似属性这样简单的方式来访问类的变量呢？对于追求完美的Python程序员来说，这是必须要做到的！ 还记得装饰器（decorator）可以给函数动态加上功能吗？对于类的方法，装饰器一样起作用。Python内置的@property装饰器就是负责把一个方法变成属性调用的： class Student(object): @property def score(self): return self._score @score.setter def score(self, value): if not isinstance(value, int): raise ValueError(&#x27;score must be an integer!&#x27;) if value &lt; 0 or value &gt; 100: raise ValueError(&#x27;score must between 0 ~ 100!&#x27;) self._score = value @property的实现比较复杂，我们先考察如何使用。把一个getter方法变成属性，只需要加上@property就可以了，此时，@property本身又创建了另一个装饰器@score.setter，负责把一个setter方法变成属性赋值，于是，我们就拥有一个可控的属性操作： &gt;&gt;&gt; s = Student()&gt;&gt;&gt; s.score = 60 # OK，实际转化为s.set_score(60)&gt;&gt;&gt; s.score # OK，实际转化为s.get_score()60&gt;&gt;&gt; s.score = 9999Traceback (most recent call last): ...ValueError: score must between 0 ~ 100! 注意到这个神奇的@property，我们在对实例属性操作的时候，就知道该属性很可能不是直接暴露的，而是通过getter和setter方法来实现的。 还可以定义只读属性，只定义getter方法，不定义setter方法就是一个只读属性： class Student(object): @property def birth(self): return self._birth @birth.setter def birth(self, value): self._birth = value @property def age(self): return 2015 - self._birth 上面的birth是可读写属性，而age就是一个只读属性，因为age可以根据birth和当前时间计算出来。 多重继承 继承是面向对象编程的一个重要的方式，因为通过继承，子类就可以扩展父类的功能。 回忆一下Animal类层次的设计，假设我们要实现以下4种动物： Dog - 狗狗； Bat - 蝙蝠； Parrot - 鹦鹉； Ostrich - 鸵鸟。 如果按照哺乳动物和鸟类归类，我们可以设计出这样的类的层次： ┌───────────────┐ │ Animal │ └───────────────┘ │ ┌────────────┴────────────┐ │ │ ▼ ▼ ┌─────────────┐ ┌─────────────┐ │ Mammal │ │ Bird │ └─────────────┘ └─────────────┘ │ │ ┌─────┴──────┐ ┌─────┴──────┐ │ │ │ │ ▼ ▼ ▼ ▼┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐│ Dog │ │ Bat │ │ Parrot │ │ Ostrich │└─────────┘ └─────────┘ └─────────┘ └─────────┘ 但是如果按照“能跑”和“能飞”来归类，我们就应该设计出这样的类的层次： ┌───────────────┐ │ Animal │ └───────────────┘ │ ┌────────────┴────────────┐ │ │ ▼ ▼ ┌─────────────┐ ┌─────────────┐ │ Runnable │ │ Flyable │ └─────────────┘ └─────────────┘ │ │ ┌─────┴──────┐ ┌─────┴──────┐ │ │ │ │ ▼ ▼ ▼ ▼┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐│ Dog │ │ Ostrich │ │ Parrot │ │ Bat │└─────────┘ └─────────┘ └─────────┘ └─────────┘ 如果要把上面的两种分类都包含进来，我们就得设计更多的层次： 哺乳类：能跑的哺乳类，能飞的哺乳类； 鸟类：能跑的鸟类，能飞的鸟类。 这么一来，类的层次就复杂了： ┌───────────────┐ │ Animal │ └───────────────┘ │ ┌────────────┴────────────┐ │ │ ▼ ▼ ┌─────────────┐ ┌─────────────┐ │ Mammal │ │ Bird │ └─────────────┘ └─────────────┘ │ │ ┌─────┴──────┐ ┌─────┴──────┐ │ │ │ │ ▼ ▼ ▼ ▼┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐│ MRun │ │ MFly │ │ BRun │ │ BFly │└─────────┘ └─────────┘ └─────────┘ └─────────┘ │ │ │ │ │ │ │ │ ▼ ▼ ▼ ▼┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐│ Dog │ │ Bat │ │ Ostrich │ │ Parrot │└─────────┘ └─────────┘ └─────────┘ └─────────┘ 如果要再增加“宠物类”和“非宠物类”，这么搞下去，类的数量会呈指数增长，很明显这样设计是不行的。 正确的做法是采用多重继承。首先，主要的类层次仍按照哺乳类和鸟类设计： class Animal(object): pass# 大类:class Mammal(Animal): passclass Bird(Animal): pass# 各种动物:class Dog(Mammal): passclass Bat(Mammal): passclass Parrot(Bird): passclass Ostrich(Bird): pass 现在，我们要给动物再加上Runnable和Flyable的功能，只需要先定义好Runnable和Flyable的类： class Runnable(object): def run(self): print(&#x27;Running...&#x27;)class Flyable(object): def fly(self): print(&#x27;Flying...&#x27;) 对于需要Runnable功能的动物，就多继承一个Runnable，例如Dog： class Dog(Mammal, Runnable): pass 对于需要Flyable功能的动物，就多继承一个Flyable，例如Bat： class Bat(Mammal, Flyable): pass 通过多重继承，一个子类就可以同时获得多个父类的所有功能。 MixIn 在设计类的继承关系时，通常，主线都是单一继承下来的，例如，Ostrich继承自Bird。但是，如果需要“混入”额外的功能，通过多重继承就可以实现，比如，让Ostrich除了继承自Bird外，再同时继承Runnable。这种设计通常称之为MixIn。 为了更好地看出继承关系，我们把Runnable和Flyable改为RunnableMixIn和FlyableMixIn。类似的，你还可以定义出肉食动物CarnivorousMixIn和植食动物HerbivoresMixIn，让某个动物同时拥有好几个MixIn： class Dog(Mammal, RunnableMixIn, CarnivorousMixIn): pass MixIn的目的就是给一个类增加多个功能，这样，在设计类的时候，我们优先考虑通过多重继承来组合多个MixIn的功能，而不是设计多层次的复杂的继承关系。 Python自带的很多库也使用了MixIn。举个例子，Python自带了TCPServer和UDPServer这两类网络服务，而要同时服务多个用户就必须使用多进程或多线程模型，这两种模型由ForkingMixIn和ThreadingMixIn提供。通过组合，我们就可以创造出合适的服务来。 比如，编写一个多进程模式的TCP服务，定义如下： class MyTCPServer(TCPServer, ForkingMixIn): pass 编写一个多线程模式的UDP服务，定义如下： class MyUDPServer(UDPServer, ThreadingMixIn): pass 如果你打算搞一个更先进的协程模型，可以编写一个CoroutineMixIn： class MyTCPServer(TCPServer, CoroutineMixIn): pass 这样一来，我们不需要复杂而庞大的继承链，只要选择组合不同的类的功能，就可以快速构造出所需的子类。 Specialize Class 看到类似__slots__这种形如__xxx__的变量或者函数名就要注意，这些在Python中是有特殊用途的。 __slots__我们已经知道怎么用了，__len__()方法我们也知道是为了能让class作用于len()函数。 除此之外，Python的class中还有许多这样有特殊用途的函数，可以帮助我们定制类。 打印对象 当我们使用 print() 函数在 Python 中打印对象时，将调用对象的 __str__() 方法. 如果__str__()未定义，则 __str__() 返回 __repr__() 方法的返回值. __repr__() -&gt; str : 以字符串形式返回对象的可打印表示形式。默认情况下，它返回对象的类的名称和对象的地址。 __str__() -&gt; str: 返回 Python 中对象的字符串版本. 如果对象不具有 __str__() 方法，则它返回与 __repr__() 方法相同的值. 我们一般重载__str__() _str_ 我们先定义一个Student类，打印一个实例： &gt;&gt;&gt; class Student(object):... def __init__(self, name):... self.name = name...&gt;&gt;&gt; print(Student(&#x27;Michael&#x27;))&lt;__main__.Student object at 0x109afb190&gt; 打印出一堆&lt;__main__.Student object at 0x109afb190&gt;，不好看。 怎么才能打印得好看呢？只需要定义好__str__()方法，返回一个好看的字符串就可以了： &gt;&gt;&gt; class Student(object):... def __init__(self, name):... self.name = name... def __str__(self):... return &#x27;Student object (name: %s)&#x27; % self.name...&gt;&gt;&gt; print(Student(&#x27;Michael&#x27;))Student object (name: Michael) 这样打印出来的实例，不但好看，而且容易看出实例内部重要的数据。 但是细心的朋友会发现直接敲变量不用print，打印出来的实例还是不好看： &gt;&gt;&gt; s = Student(&#x27;Michael&#x27;)&gt;&gt;&gt; s&lt;__main__.Student object at 0x109afb310&gt; 这是因为直接显示变量调用的不是__str__()，而是__repr__()，两者的区别是__str__()返回用户看到的字符串，而__repr__()返回程序开发者看到的字符串，也就是说，__repr__()是为调试服务的。 解决办法是再定义一个__repr__()。但是通常__str__()和__repr__()代码都是一样的，所以，有个偷懒的写法： class Student(object): def __init__(self, name): self.name = name def __str__(self): return &#x27;Student object (name=%s)&#x27; % self.name __repr__ = __str__ _iter_ 如果一个类想被用于for ... in循环，类似list或tuple那样，就必须实现一个__iter__()方法，该方法返回一个迭代对象，然后，Python的for循环就会不断调用该迭代对象的__next__()方法拿到循环的下一个值，直到遇到StopIteration错误时退出循环。 我们以斐波那契数列为例，写一个Fib类，可以作用于for循环： class Fib(object): def __init__(self): self.a, self.b = 0, 1 # 初始化两个计数器a，b def __iter__(self): return self # 实例本身就是迭代对象，故返回自己 def __next__(self): self.a, self.b = self.b, self.a + self.b # 计算下一个值 if self.a &gt; 100000: # 退出循环的条件 raise StopIteration() return self.a # 返回下一个值 现在，试试把Fib实例作用于for循环： &gt;&gt;&gt; for n in Fib():... print(n)...11235...4636875025 _getitem_ Fib实例虽然能作用于for循环，看起来和list有点像，但是，把它当成list来使用还是不行，比如，取第5个元素： &gt;&gt;&gt; Fib()[5]Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: &#x27;Fib&#x27; object does not support indexing 要表现得像list那样按照下标取出元素，需要实现__getitem__()方法： class Fib(object): def __getitem__(self, n): a, b = 1, 1 for x in range(n): a, b = b, a + b return a 现在，就可以按下标访问数列的任意一项了： &gt;&gt;&gt; f = Fib()&gt;&gt;&gt; f[0]1&gt;&gt;&gt; f[1]1&gt;&gt;&gt; f[2]2&gt;&gt;&gt; f[3]3&gt;&gt;&gt; f[10]89&gt;&gt;&gt; f[100]573147844013817084101 但是list有个神奇的切片方法： &gt;&gt;&gt; list(range(100))[5:10][5, 6, 7, 8, 9] 对于Fib却报错。原因是__getitem__()传入的参数可能是一个int，也可能是一个切片对象slice，所以要做判断： class Fib(object): def __getitem__(self, n): if isinstance(n, int): # n是索引 a, b = 1, 1 for x in range(n): a, b = b, a + b return a if isinstance(n, slice): # n是切片 start = n.start stop = n.stop if start is None: start = 0 a, b = 1, 1 L = [] for x in range(stop): if x &gt;= start: L.append(a) a, b = b, a + b return L 现在试试Fib的切片： &gt;&gt;&gt; f = Fib()&gt;&gt;&gt; f[0:5][1, 1, 2, 3, 5]&gt;&gt;&gt; f[:10][1, 1, 2, 3, 5, 8, 13, 21, 34, 55] 但是没有对step参数作处理： &gt;&gt;&gt; f[:10:2][1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89] 也没有对负数作处理，所以，要正确实现一个__getitem__()还是有很多工作要做的。 此外，如果把对象看成dict，__getitem__()的参数也可能是一个可以作key的object，例如str。 与之对应的是__setitem__()方法，把对象视作list或dict来对集合赋值。最后，还有一个__delitem__()方法，用于删除某个元素。 总之，通过上面的方法，我们自己定义的类表现得和Python自带的list、tuple、dict没什么区别，这完全归功于动态语言的“鸭子类型”，不需要强制继承某个接口。 _getattr_ 正常情况下，当我们调用类的方法或属性时，如果不存在，就会报错。比如定义Student类： class Student(object): def __init__(self): self.name = &#x27;Michael&#x27; 调用name属性，没问题，但是，调用不存在的score属性，就有问题了： &gt;&gt;&gt; s = Student()&gt;&gt;&gt; print(s.name)Michael&gt;&gt;&gt; print(s.score)Traceback (most recent call last): ...AttributeError: &#x27;Student&#x27; object has no attribute &#x27;score&#x27; 错误信息很清楚地告诉我们，没有找到score这个attribute。 要避免这个错误，除了可以加上一个score属性外，Python还有另一个机制，那就是写一个__getattr__()方法，动态返回一个属性。修改如下： class Student(object): def __init__(self): self.name = &#x27;Michael&#x27; def __getattr__(self, attr): if attr==&#x27;score&#x27;: return 99 当调用不存在的属性时，比如score，Python解释器会试图调用__getattr__(self, 'score')来尝试获得属性，这样，我们就有机会返回score的值： &gt;&gt;&gt; s = Student()&gt;&gt;&gt; s.name&#x27;Michael&#x27;&gt;&gt;&gt; s.score99 返回函数也是完全可以的： class Student(object): def __getattr__(self, attr): if attr==&#x27;age&#x27;: return lambda: 25 只是调用方式要变为： &gt;&gt;&gt; s.age()25 注意，只有在没有找到属性的情况下，才调用__getattr__，已有的属性，比如name，不会在__getattr__中查找。 此外，注意到任意调用如s.abc都会返回None，这是因为我们定义的__getattr__默认返回就是None。要让class只响应特定的几个属性，我们就要按照约定，抛出AttributeError的错误： class Student(object): def __getattr__(self, attr): if attr==&#x27;age&#x27;: return lambda: 25 raise AttributeError(&#x27;\\&#x27;Student\\&#x27; object has no attribute \\&#x27;%s\\&#x27;&#x27; % attr) 这实际上可以把一个类的所有属性和方法调用全部动态化处理了，不需要任何特殊手段。 这种完全动态调用的特性有什么实际作用呢？作用就是，可以针对完全动态的情况作调用。 举个例子： 现在很多网站都搞REST API，比如新浪微博、豆瓣啥的，调用API的URL类似： http://api.server/user/friends http://api.server/user/timeline/list 如果要写SDK，给每个URL对应的API都写一个方法，那得累死，而且，API一旦改动，SDK也要改。 利用完全动态的__getattr__，我们可以写出一个链式调用： class Chain(object): def __init__(self, path=&#x27;&#x27;): self._path = path def __getattr__(self, path): return Chain(&#x27;%s/%s&#x27; % (self._path, path)) def __str__(self): return self._path __repr__ = __str__ 试试： &gt;&gt;&gt; Chain().status.user.timeline.list&#x27;/status/user/timeline/list&#x27; 这样，无论API怎么变，SDK都可以根据URL实现完全动态的调用，而且，不随API的增加而改变！ 还有些REST API会把参数放到URL中，比如GitHub的API： GET /users/:user/repos 调用时，需要把:user替换为实际用户名。如果我们能写出这样的链式调用： Chain().users(&#x27;michael&#x27;).repos 就可以非常方便地调用API了。有兴趣的童鞋可以试试写出来。 _call_ 一个对象实例可以有自己的属性和方法，当我们调用实例方法时，我们用instance.method()来调用。能不能直接在实例本身上调用呢？在Python中，答案是肯定的。 任何类，只需要定义一个__call__()方法，就可以直接对实例进行调用。请看示例： class Student(object): def __init__(self, name): self.name = name def __call__(self): print(&#x27;My name is %s.&#x27; % self.name) 调用方式如下： &gt;&gt;&gt; s = Student(&#x27;Michael&#x27;)&gt;&gt;&gt; s() # self参数不要传入My name is Michael. __call__()还可以定义参数。对实例进行直接调用就好比对一个函数进行调用一样，所以你完全可以把对象看成函数，把函数看成对象，因为这两者之间本来就没啥根本的区别。 如果你把对象看成函数，那么函数本身其实也可以在运行期动态创建出来，因为类的实例都是运行期创建出来的，这么一来，我们就模糊了对象和函数的界限。 那么，怎么判断一个变量是对象还是函数呢？其实，更多的时候，我们需要判断一个对象是否能被调用，能被调用的对象就是一个Callable对象，比如函数和我们上面定义的带有__call__()的类实例： &gt;&gt;&gt; callable(Student())True&gt;&gt;&gt; callable(max)True&gt;&gt;&gt; callable([1, 2, 3])False&gt;&gt;&gt; callable(None)False&gt;&gt;&gt; callable(&#x27;str&#x27;)False 通过callable()函数，我们就可以判断一个对象是否是“可调用”对象。 Enum 当我们需要定义常量时，一个办法是用大写变量通过整数来定义，例如月份： JAN = 1FEB = 2MAR = 3...NOV = 11DEC = 12 好处是简单，缺点是类型是int，并且仍然是变量。 更好的方法是为这样的枚举类型定义一个class类型，然后，每个常量都是class的一个唯一实例。Python提供了Enum类来实现这个功能： from enum import EnumMonth = Enum(&#x27;Month&#x27;, (&#x27;Jan&#x27;, &#x27;Feb&#x27;, &#x27;Mar&#x27;, &#x27;Apr&#x27;, &#x27;May&#x27;, &#x27;Jun&#x27;, &#x27;Jul&#x27;, &#x27;Aug&#x27;, &#x27;Sep&#x27;, &#x27;Oct&#x27;, &#x27;Nov&#x27;, &#x27;Dec&#x27;)) 这样我们就获得了Month类型的枚举类，可以直接使用Month.Jan来引用一个常量，或者枚举它的所有成员： for name, member in Month.__members__.items(): print(name, &#x27;=&gt;&#x27;, member, &#x27;,&#x27;, member.value) value属性则是自动赋给成员的int常量，默认从1开始计数。 如果需要更精确地控制枚举类型，可以从Enum派生出自定义类： from enum import Enum, unique@uniqueclass Weekday(Enum): Sun = 0 # Sun的value被设定为0 Mon = 1 Tue = 2 Wed = 3 Thu = 4 Fri = 5 Sat = 6 @unique装饰器可以帮助我们检查保证没有重复值。 访问这些枚举类型可以有若干种方法： &gt;&gt;&gt; day1 = Weekday.Mon&gt;&gt;&gt; print(day1)Weekday.Mon&gt;&gt;&gt; print(Weekday.Tue)Weekday.Tue&gt;&gt;&gt; print(Weekday[&#x27;Tue&#x27;])Weekday.Tue&gt;&gt;&gt; print(Weekday.Tue.value)2&gt;&gt;&gt; print(day1 == Weekday.Mon)True&gt;&gt;&gt; print(day1 == Weekday.Tue)False&gt;&gt;&gt; print(Weekday(1))Weekday.Mon&gt;&gt;&gt; print(day1 == Weekday(1))True&gt;&gt;&gt; Weekday(7)Traceback (most recent call last): ...ValueError: 7 is not a valid Weekday&gt;&gt;&gt; for name, member in Weekday.__members__.items():... print(name, &#x27;=&gt;&#x27;, member)...Sun =&gt; Weekday.SunMon =&gt; Weekday.MonTue =&gt; Weekday.TueWed =&gt; Weekday.WedThu =&gt; Weekday.ThuFri =&gt; Weekday.FriSat =&gt; Weekday.Sat 可见，既可以用成员名称引用枚举常量，又可以直接根据value的值获得枚举常量。 metaclass type() 动态语言和静态语言最大的不同，就是函数和类的定义，不是编译时定义的，而是运行时动态创建的。 比方说我们要定义一个Hello的class，就写一个hello.py模块： class Hello(object): def hello(self, name=&#x27;world&#x27;): print(&#x27;Hello, %s.&#x27; % name) 当Python解释器载入hello模块时，就会依次执行该模块的所有语句，执行结果就是动态创建出一个Hello的class对象，测试如下： &gt;&gt;&gt; from hello import Hello&gt;&gt;&gt; h = Hello()&gt;&gt;&gt; h.hello()Hello, world.&gt;&gt;&gt; print(type(Hello))&lt;class &#x27;type&#x27;&gt;&gt;&gt;&gt; print(type(h))&lt;class &#x27;hello.Hello&#x27;&gt; type()函数可以查看一个类型或变量的类型，Hello是一个class，它的类型就是type，而h是一个实例，它的类型就是class Hello。 我们说class的定义是运行时动态创建的，而创建class的方法就是使用type()函数。 type()函数既可以返回一个对象的类型，又可以创建出新的类型，比如，我们可以通过type()函数创建出Hello类，而无需通过class Hello(object)...的定义： &gt;&gt;&gt; def fn(self, name=&#x27;world&#x27;): # 先定义函数... print(&#x27;Hello, %s.&#x27; % name)...&gt;&gt;&gt; Hello = type(&#x27;Hello&#x27;, (object,), dict(hello=fn)) # 创建Hello class&gt;&gt;&gt; h = Hello()&gt;&gt;&gt; h.hello()Hello, world.&gt;&gt;&gt; print(type(Hello))&lt;class &#x27;type&#x27;&gt;&gt;&gt;&gt; print(type(h))&lt;class &#x27;__main__.Hello&#x27;&gt; 要创建一个class对象，type()函数依次传入3个参数： class的名称； 继承的父类集合，注意Python支持多重继承，如果只有一个父类，别忘了tuple的单元素写法； class的方法名称与函数绑定，这里我们把函数fn绑定到方法名hello上。 通过type()函数创建的类和直接写class是完全一样的，因为Python解释器遇到class定义时，仅仅是扫描一下class定义的语法，然后调用type()函数创建出class。 正常情况下，我们都用class Xxx...来定义类，但是，type()函数也允许我们动态创建出类来，也就是说，动态语言本身支持运行期动态创建类，这和静态语言有非常大的不同，要在静态语言运行期创建类，必须构造源代码字符串再调用编译器，或者借助一些工具生成字节码实现，本质上都是动态编译，会非常复杂。 metaclass 除了使用type()动态创建类以外，要控制类的创建行为，还可以使用metaclass。 metaclass，直译为元类，简单的解释就是： 当我们定义了类以后，就可以根据这个类创建出实例，所以：先定义类，然后创建实例。 但是如果我们想创建出类呢？那就必须根据metaclass创建出类，所以：先定义metaclass，然后创建类。 连接起来就是：先定义metaclass，就可以创建类，最后创建实例。 所以，metaclass允许你创建类或者修改类。换句话说，你可以把类看成是metaclass创建出来的“实例”。 metaclass是Python面向对象里最难理解，也是最难使用的魔术代码。正常情况下，你不会碰到需要使用metaclass的情况，所以，以下内容看不懂也没关系，因为基本上你不会用到。 我们先看一个简单的例子，这个metaclass可以给我们自定义的MyList增加一个add方法： 定义ListMetaclass，按照默认习惯，metaclass的类名总是以Metaclass结尾，以便清楚地表示这是一个metaclass： # metaclass是类的模板，所以必须从`type`类型派生：class ListMetaclass(type): def __new__(cls, name, bases, attrs): attrs[&#x27;add&#x27;] = lambda self, value: self.append(value) return type.__new__(cls, name, bases, attrs) 有了ListMetaclass，我们在定义类的时候还要指示使用ListMetaclass来定制类，传入关键字参数metaclass： class MyList(list, metaclass=ListMetaclass): pass 当我们传入关键字参数metaclass时，魔术就生效了，它指示Python解释器在创建MyList时，要通过ListMetaclass.__new__()来创建，在此，我们可以修改类的定义，比如，加上新的方法，然后，返回修改后的定义。 __new__()方法接收到的参数依次是： 当前准备创建的类的对象； 类的名字； 类继承的父类集合； 类的方法集合。 测试一下MyList是否可以调用add()方法： &gt;&gt;&gt; L = MyList()&gt;&gt;&gt; L.add(1)&gt;&gt; L[1] 而普通的list没有add()方法： &gt;&gt;&gt; L2 = list()&gt;&gt;&gt; L2.add(1)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: &#x27;list&#x27; object has no attribute &#x27;add&#x27; 动态修改有什么意义？直接在MyList定义中写上add()方法不是更简单吗？正常情况下，确实应该直接写，通过metaclass修改纯属变态。 但是，总会遇到需要通过metaclass修改类定义的。ORM就是一个典型的例子。 ORM全称“Object Relational Mapping”，即对象-关系映射，就是把关系数据库的一行映射为一个对象，也就是一个类对应一个表，这样，写代码更简单，不用直接操作SQL语句。 要编写一个ORM框架，所有的类都只能动态定义，因为只有使用者才能根据表的结构定义出对应的类来。 让我们来尝试编写一个ORM框架。 编写底层模块的第一步，就是先把调用接口写出来。比如，使用者如果使用这个ORM框架，想定义一个User类来操作对应的数据库表User，我们期待他写出这样的代码： class User(Model): # 定义类的属性到列的映射： id = IntegerField(&#x27;id&#x27;) name = StringField(&#x27;username&#x27;) email = StringField(&#x27;email&#x27;) password = StringField(&#x27;password&#x27;)# 创建一个实例：u = User(id=12345, name=&#x27;Michael&#x27;, email=&#x27;test@orm.org&#x27;, password=&#x27;my-pwd&#x27;)# 保存到数据库：u.save() 其中，父类Model和属性类型StringField、IntegerField是由ORM框架提供的，剩下的魔术方法比如save()全部由metaclass自动完成。虽然metaclass的编写会比较复杂，但ORM的使用者用起来却异常简单。 现在，我们就按上面的接口来实现该ORM。 首先来定义Field类，它负责保存数据库表的字段名和字段类型： class Field(object): def __init__(self, name, column_type): self.name = name self.column_type = column_type def __str__(self): return &#x27;&lt;%s:%s&gt;&#x27; % (self.__class__.__name__, self.name) 在Field的基础上，进一步定义各种类型的Field，比如StringField，IntegerField等等： class StringField(Field): def __init__(self, name): super(StringField, self).__init__(name, &#x27;varchar(100)&#x27;)class IntegerField(Field): def __init__(self, name): super(IntegerField, self).__init__(name, &#x27;bigint&#x27;) 下一步，就是编写最复杂的ModelMetaclass了： class ModelMetaclass(type): def __new__(cls, name, bases, attrs): if name==&#x27;Model&#x27;: return type.__new__(cls, name, bases, attrs) print(&#x27;Found model: %s&#x27; % name) mappings = dict() for k, v in attrs.items(): if isinstance(v, Field): print(&#x27;Found mapping: %s ==&gt; %s&#x27; % (k, v)) mappings[k] = v for k in mappings.keys(): attrs.pop(k) attrs[&#x27;__mappings__&#x27;] = mappings # 保存属性和列的映射关系 attrs[&#x27;__table__&#x27;] = name # 假设表名和类名一致 return type.__new__(cls, name, bases, attrs) 以及基类Model： class Model(dict, metaclass=ModelMetaclass): def __init__(self, **kw): super(Model, self).__init__(**kw) def __getattr__(self, key): try: return self[key] except KeyError: raise AttributeError(r&quot;&#x27;Model&#x27; object has no attribute &#x27;%s&#x27;&quot; % key) def __setattr__(self, key, value): self[key] = value def save(self): fields = [] params = [] args = [] for k, v in self.__mappings__.items(): fields.append(v.name) params.append(&#x27;?&#x27;) args.append(getattr(self, k, None)) sql = &#x27;insert into %s (%s) values (%s)&#x27; % (self.__table__, &#x27;,&#x27;.join(fields), &#x27;,&#x27;.join(params)) print(&#x27;SQL: %s&#x27; % sql) print(&#x27;ARGS: %s&#x27; % str(args)) 当用户定义一个class User(Model)时，Python解释器首先在当前类User的定义中查找metaclass，如果没有找到，就继续在父类Model中查找metaclass，找到了，就使用Model中定义的metaclass的ModelMetaclass来创建User类，也就是说，metaclass可以隐式地继承到子类，但子类自己却感觉不到。 在ModelMetaclass中，一共做了几件事情： 排除掉对Model类的修改； 在当前类（比如User）中查找定义的类的所有属性，如果找到一个Field属性，就把它保存到一个__mappings__的dict中，同时从类属性中删除该Field属性，否则，容易造成运行时错误（实例的属性会遮盖类的同名属性）； 把表名保存到__table__中，这里简化为表名默认为类名。 在Model类中，就可以定义各种操作数据库的方法，比如save()，delete()，find()，update等等。 我们实现了save()方法，把一个实例保存到数据库中。因为有表名，属性到字段的映射和属性值的集合，就可以构造出INSERT语句。 编写代码试试： u = User(id=12345, name=&#x27;Michael&#x27;, email=&#x27;test@orm.org&#x27;, password=&#x27;my-pwd&#x27;)u.save() 输出如下： Found model: UserFound mapping: email ==&gt; &lt;StringField:email&gt;Found mapping: password ==&gt; &lt;StringField:password&gt;Found mapping: id ==&gt; &lt;IntegerField:uid&gt;Found mapping: name ==&gt; &lt;StringField:username&gt;SQL: insert into User (password,email,username,id) values (?,?,?,?)ARGS: [&#x27;my-pwd&#x27;, &#x27;test@orm.org&#x27;, &#x27;Michael&#x27;, 12345] 可以看到，save()方法已经打印出了可执行的SQL语句，以及参数列表，只需要真正连接到数据库，执行该SQL语句，就可以完成真正的功能。 不到100行代码，我们就通过metaclass实现了一个精简的ORM框架，是不是非常简单？","categories":[{"name":"Language","slug":"Language","permalink":"http://lyk-love.cn/categories/Language/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://lyk-love.cn/tags/Python/"}]},{"title":"RSA","slug":"RSA","date":"2022-08-23T23:31:16.000Z","updated":"2022-09-26T06:39:34.938Z","comments":true,"path":"2022/08/24/RSA/","link":"","permalink":"http://lyk-love.cn/2022/08/24/RSA/","excerpt":"Outline: Intro Prerequisites Key Generation Reliability Encription &amp;&amp; Decription Key Format Use RSA","text":"Outline: Intro Prerequisites Key Generation Reliability Encription &amp;&amp; Decription Key Format Use RSA Intro RSA是最著名的公钥密码体制, 由三位美国科学家Rivest，Shamir和Adleman于1976年提出, 并在1978年正式发表. 它基于数论的大数分解问题. 我的RSA Python实现 PrerequisiFtes 互质 互质( coprime ): 如果两个正整数，除了 1 以外没有其他的公因数，则他们互质. 互质有如下结论: 任意两个质数构成互质关系，比如13和61 若一个数是质数, 则两个数互质 == 另一个数不是前者的倍数 如果两个数之中，较大的那个数是质数，则两者构成互质关系，比如97和57。 1和任意一个自然数是都是互质关系，比如1和99。 p是大于1的整数，则p和p-1构成互质关系，比如57和56。 p是大于1的奇数，则p和p-2构成互质关系，比如17和15。 欧拉函数 欧拉函数: 对于任意正整数$n$, 在小于等于n的正整数之中，与n构成互质关系的正整数个数为 $$ \\phi(n) = n (1 - \\frac{1}{p_1}) (1 - \\frac{1}{p_2})…(1 - \\frac{1}{p_n}) $$ 其中, $p_i$表示 n 的不重复的质因子. 例如, $10 = 2 \\times 5$，所以在 &lt;=10 的正整数中，与 10 互质的正整数个数为 4 个。我们可以验证一下，它们分别是 1, 3, 7, 9. 注意到, 如果$n$刚好是两个质数的积, 记为$n = pq$ , 则有$\\phi(n) = pq(1 - \\frac{1}{p})(1 - \\frac{1}{q}) = (p-1)(q-1)$ 欧拉定理 欧拉定理：如果两个正整数 $a$ 和 $n$ 互质, 则如下等式成立: $$ a^{\\phi(n)} \\equiv 1 \\pmod n $$ 注意到, 如果$p$为质数, 则有: $$ \\phi(p) = p - 1 $$ 欧拉定理的证明比较复杂，这里就省略了. 例如, 7 和 10 互质, $7^{\\phi(10)} = 7^4 = 2401$, 减去 1 等于 2400, 可以整除 10. 同理, $10^{\\phi(7)} = 10^6 = 1000000$, 减去 1 等于 999999, 可以整除 7. 费马小定理 欧拉定理存在一个特殊情况：如果 $p$ 是质数，而 $a $ 不是 $p$ 的倍数，此时 $a$ 和 $p$ 必然互质. 因为$\\phi(p) = p - 1$, 所以 $$ a^{\\phi(p)} = a^{p - 1} \\equiv 1 \\pmod p $$ 这就是 费马小定理,它是欧拉定理的特例. 模反元素 如果两个正整数 $a$ 和 $n$ 互质，那么一定可以找到一个正整数 $b$，使得 $ab - 1$ 被 $n$ 整除: $$ ab \\equiv 1 \\mod n $$ 此时, $b$ 就叫做 $a$ 的 模反元素 . 我们可以用欧拉定理来证明模反元素一定存在: $$ a^{\\phi(n)} = a \\times a^{\\phi(n) - 1} \\equiv 1 \\pmod n $$ $a^{\\phi(n) - 1}$就是 $a$ 相对 $n$ 的模反元素, 且 $a^{\\phi(n) - 1} + kn, k \\in N$ 也是 $a$ 相对 $n$ 的模反元素. 因此模反元素不唯一 Key Generation Choose two large prime numbers, $p$ and $q$ , then compute $n = pq$ . $p$ 和 $q$ 越大越好, 一般要求 $n$ 大于1024bit. Compute $z = \\phi(n) = (p - 1)(q - 1)$ 选择一个与 $z$ 互质的数 $e, e &gt; 1$ The letter $e$ is used since this value will be used in encryption. $e &gt; 1$: 因为后面要计算 $e$ 相对 $z$ 的模反元素, 如果$e = 1$, 则任何正整数都是$e$关于$n$d的模反元素, 也就导致 $d$ 可以取任何正整数, 算法很容易生成一个很简单的$d$ , $d$ 作为私钥就很容易被破解. 计算 $e$ 相对 $z$ 的模反元素 $d$, 即使得 $ed \\mod z = 1$, 即 $ed \\equiv 1 \\pmod {\\phi(n)}$ The letter $d$ is used because this value will be used in decryption. 公钥就是$(n,e)$, 私钥就是$(n,d)$ 计算模反元素 $d$ 前面已经知道, 如果两个正整数 $a$ 和 $n$ 互质, 则 $a$ 相对 $n$ 的模反元素包括 $a^{\\phi(n) - 1}$. 那么这里 $e$ 和 $z = {\\phi(n)}$ 互质, 则模反元素$ d = e^{\\phi(z) - 1} = e^{\\phi({\\phi(n)}) - 1}$ . 我们已经计算出了$z = \\phi(n)$ , 它是个很大的数, 这意味着计算 $\\phi[\\phi(n)]$ 也很大, 导致$d = e^{\\phi[\\phi(n)]- 1}$是一个很高很高的幂次, 运算复杂度无法接受. 因此要换个方法求$d$ 因为: $$ ed \\equiv 1 \\pmod {\\phi(n)} $$ 得到: $$ ed = k\\phi(n) + 1 $$ 移项得到: $$ ed - k\\phi(n) = 1 $$ 所以，这个问题转化为：已知两个数 a 和 b，求解 x 和 y 满足如下方程： $$ ax + by = 1 $$ ( $a = e, b = \\phi(n)=z, x = d, y = -k$ ) 根据 扩展欧几里得算法，这个方程有解的充分必要条件是 a 和 b 互质. 在我们的情况中，$e$ 和 $\\phi(n)$ 是互质的，所以这个方程式有解. 同时，通过扩展欧几里得算法，可以非常容易的通过迭代求解出$d$ . Encription &amp;&amp; Decription 被加密的消息 $m$ 是一个整数( 将字符编码为整数 ), 满足 $ m &lt; n $ . 如果消息太大，解读为整数以后比 n 要大，那么分段加密即可. 加密过程使用 $e$ , 计算密文 $c$ : $$ c \\equiv m^e \\pmod n $$ 解密过程使用$d$, 计算明文 $m$ : $$ m \\equiv c^d \\pmod n $$ 注意到加密函数$E_{(n,e)}(m) \\equiv m^e \\pmod n $, 解密函数$D_{(n,d)}(c) \\equiv m^d \\pmod n $, 它们其实是一个函数在形式上都是: $$ f(m, n, e) \\equiv m^e \\pmod n $$ 只是加密和解密时接受的参数不同. Example 因为加密的消息 m 必须要小于 n, 并且计算密文$c \\equiv m^e \\pmod n$ 需要求幂, 这是个昂贵的操作. 因此在实践中, 我们不会用RSA来直接加密消息, 而是用它来加密一个对称密钥( 称为session key ), 用session key来加密消息. 举例来说, 假如Alice要和Bob加密通讯, 步骤为: Alice选择一个session key, 记为$K_S$ Alice把session key用Bob的公钥加密, 即计算$(K_S)^e$, 并发送给Bob Bob收到后, 计算明文$K_S \\equiv c^d \\pmod n$ , 得到了$K_S$ , 现在Bog知道了共享密钥$K_S$ Alice和Bob然后通过$K_S$进行对称加密通信, 当然他们实现肯定约定好了对称加密算法AES/DES 当然直接用RSA加密消息也是可以的, 下面演示Alice与Bob采用RSA明文加密的通信步骤 Ciphertext $c$ $c^d$ $m = c^d \\pmod n$ Plaintext Letter 17 4819685721067509 150915091411825223071697 248832 l 15 127834039403948858939111232757568359375 15 o 22 851643319086537701956194499721106030592 22 v 10 1000000000000000000000000000000 5 e Alice’s RSA encryption, $e$ = 5, $n = 35$ Plaintext Letter $m$: numeric representation $m^e$ Ciphertext $c \\equiv m^e \\pmod n$ l 12 248832 17 o 15 759375 15 v 22 5153632 22 e 5 3125 10 Bob’s RSA decryption, $d$= 29, $n$ = 35 Proof 下面证明该式: $$ \\begin{equation}\\label{eq1} c^d \\equiv m \\pmod n \\end{equation} $$ Proof: $$ \\because m^e \\equiv c \\pmod n \\ \\therefore c = m^e - kn $$ 将 $c$ 代入$\\eqref{eq1}$: $$ (m^e - kn)^d \\equiv m \\pmod n $$ 根据 二项式定理，左边展开后的每一项，除$m^{ed}$以外, 都含有$kn$, 因此, 证明上面的式子等同于证明: $$ \\begin{equation}\\label{eq2} m^{ed} \\equiv m \\pmod n \\end{equation} $$ 注意到: $$ \\because ed \\equiv 1 \\pmod {\\phi(n)} $$ $$ \\therefore ed = 1 + h\\phi(n), h \\in N $$ 代入$\\eqref{eq2}$得到: $$ \\begin{equation}\\label{eq3} m^{h\\phi(n) + 1} \\equiv m \\pmod n \\end{equation} $$ 我们要证明$\\eqref{eq3}$. 接下来分类讨论: 如果 m 和 n 互质. 根据欧拉定理: $$ m^{\\phi(n)} \\equiv 1 \\pmod n $$ 所以: $$ \\begin{align} m^{\\phi(n)} = kn + 1, n \\in N \\nonumber \\ (m^{\\phi(n)})^h = (kn + 1)^h, h \\in N \\nonumber \\end{align} $$ 根据二项式定理得到: $$ (kn + 1)^h = (kn)^h + \\complement_h^1 (kn)^{h-1} + \\dots + \\complement_h^{h-1} (kn) + 1 $$ let $k' = k^hn^{h-1} + \\complement_h^1 k^{h-1}n^{h-2} + \\dots$ , 得到: $$ (m^{\\phi(n)})^h = k’n + 1, k' \\in N $$ 也就是: $$ (m^{\\phi(n)})^h \\equiv 1 \\pmod n $$ 从而: $$ (m^{\\phi(n)})^h \\times m \\equiv m \\pmod n $$ 也就是: $$ m^{h \\phi(n) + 1 } \\equiv m \\pmod n $$ 前面已经得到: $$ ed = 1 + h\\phi(n) $$ 所以有: $$ m^{ed} \\equiv m \\pmod n $$ Q.E.D. 如果 $m$ 和 $n$ 不互质. 因为 $n$ 是质数 $p$ 和 $q$ 的乘积，此时 $m$ 必然为 $kp$ 或 $hq$ ( $k,h \\in N^+$ )[^1] 以 $m = kp$ 为例, 因为 $m &lt; n$ , 所以 $m = kp &lt; n = pq$，所以 $k &lt; q$ , 而 $q$ 是一个质数, 小于 $q$ 的所有数都与 $q$ 互质, 所以 $k$ 与 $q$ 互质. 同时 $kp$ 必然也与 $q$ 互质. 我们用反证法: 如果 $kp$ 和 $q$ 不互质, 则 $kp$ 必然是 $q$ 的倍数[^2]. 现在 $m = kp $ 既是 $p$ 的倍数又是 $q$ 的倍数, $p, q$ 又是质数, 则 $ m = kp $ 是 $n = pq$ 的倍数, 与前提 $m &lt; n$ 矛盾. 因此 $kp$ 与$q$互质. 由于 $kp$ 和 $q$ 互质，根据欧拉定理: $$ (kp)^{q - 1} \\equiv 1 \\pmod q $$ 所以: $$ (kp)^{q - 1} = tq + 1 $$ 两边同时进行 $h(p-1)$ 次方: $$ [(kp)^{q - 1}]^{h(p-1)} = (tq + 1)^{h(p-1)} $$ 同理根据二项式定理，右边展开除了 1 , 每一项都含有$ q$，所以得到: $$ [(kp)^{q - 1}]^{h(p-1)} \\equiv 1 \\pmod q $$ 从而得到: $$ [(kp)^{q - 1}]^{h(p-1)} \\times kp \\equiv kp \\pmod q $$ 注意到: $$ \\begin{align} n = pq \\nonumber \\ \\phi(n) = n(1 - \\frac{1}{p})(1 - \\frac{1}{q}) == (p-1)(q-1) \\nonumber \\end{align} $$ 所以: $$ [(kp)^{q - 1}]^{h(p-1)} \\times kp = (kp)^{h\\phi(n) + 1} $$ 前面已经得到: $$ ed = 1 + h\\phi(n) $$ 所以有: $$ (kp)^{ed} \\equiv kp \\pmod q $$ 再改写为如下形式: $$ (kp)^{ed} = kp + tq $$ 左边 $(kp)^{ed}$ 是 $p$ 的倍数，右边 $kp$ 是 $p$ 的倍数，所以 $tq$ 必然是 $p$ 的倍数。而 $q$ 是 $p$ 互质的，因此 $t$ 必然是 $p$ 的倍数，我们记为 $t = t’p$ , 代入得到: $$ (kp)^{ed} = kp + t’pq $$ 也就是: $$ m^{ed} \\equiv m \\pmod n $$ Q.E.D Reliability 接下来我们来看为什么 RSA 是可靠的，也就是说，在得知公钥 $[n, e]$ 的情况下，怎样保证私钥 $[n,d]$ 的安全。 因为 $n$ 和 $e$ 是公开的，所以私钥的安全就是 $d$ 的安全，那么有没有可能在得知 $n$ 和 $e$ 的情况下，推导得出 $d$ ？ 因为 $ed \\equiv 1 \\pmod {\\phi(n)}$ , 想知道 $d$ 需要知道 $e$ 和 $\\phi(n)$ 因为 $e$ 是公开的，所以想知道 $d$ 需要知道 $\\phi(n) $ 而 计算 $\\phi(n) $ 需要对正整数 $n$ 进行质数分解 所以， $d$ 的安全性依赖于对 $n$ 进行质数分解的难度. 大整数的质数分解目前没有很快的算法. 但是,随着量子计算的发展, RSA也不一定安全[MIT TR 2019]. Key Format 以下内容摘抄自https://cjting.me/2020/03/13/rsa/ 我们现在来生成一对RSA秘钥看看他们的格式是怎样的，上面的几个关键数字 n, e, d 又是怎样保存的。 ssh-keygen -f rsa 我们得到了 rsa 和 rsa.pub 文件，其中 rsa 是私钥，rsa.pub 是公钥。 先来看公钥，rsa.pub 的内容如下。 ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDMIcdbPh0r8kftRomVX4+6HkCoZYYoWPvI7AQvcEvanZl+j2AqWEFoC8zHMXqXLlMPeE5Nt0tzLFixw9sKOhA3llc2CL4B3cJaYQ1GHI6bpSX1U1PkBtm1YaIMR+d/r22o5On/U0B4Zkmo5Ua+XI3yeYqkyCLgRWz1832IIl9dVvNSln9R89Ox1XOvuMxNnEeACcSBmnAGvY5Jykhf4TBDwwNRmqZpusqkpkfhA6Y9PvjbRNMfcDEz82VV1VeLxIg3ayC6MX5I4vXFORIzx+VbBnxwing8vQZAHj0lFNmWeOZzoh3o9k4uFCSzWezVQD9JV9xQorjsZ5AB1Zdqb1J5 cj@CJs-MacBook-Pro.local 这个格式是 OpenSSH 公钥格式，RFC4253 中有详细的说明。简单来说，公钥分为三个部分 秘钥类型：ssh-rsa PEM 编码的一段数据：AAAA..b1J5 备注：cj@CJs-Macbook-Pro.local PEM 的全称是 Privacy Enhanced Mail，是一种 Base64 编码，使用 ASCII 来编码二进制数据。 PEM 编码的数据是三个 (length, data) 数据块，length 为四个字节，BigEndian. 第一个 data 表示秘钥类型，和公钥第一部分相同 第二个 data 为 RSA exponent，也就是 $e$ 第三个 data 为 RSA modulus，也就是 $n$ 根据上面的知识，我们可以很容易地解析 rsa.pub 文件，下文中提到的 rsademo 程序实现了公钥解析的逻辑。 $ rsademo -parse rsa.pubOpenSSH Public Key algorithm: ssh-rsa e: 0x010001 n: 0xCC21C75B3E1D2BF247ED4689955F8FBA1E40A865862858FBC8EC042F704BDA9D997E8F602A5841680BCCC7317A972E530F784E4DB74B732C58B1C3DB0A3A103796573608BE01DDC25A610D461C8E9BA525F55353E406D9B561A20C47E77FAF6DA8E4E9FF5340786649A8E546BE5C8DF2798AA4C822E0456CF5F37D88225F5D56F352967F51F3D3B1D573AFB8CC4D9C478009C4819A7006BD8E49CA485FE13043C303519AA669BACAA4A647E103A63D3EF8DB44D31F703133F36555D5578BC488376B20BA317E48E2F5C5391233C7E55B067C708A783CBD06401E3D2514D99678E673A21DE8F64E2E1424B359ECD5403F4957DC50A2B8EC679001D5976A6F5279 我们也可以使用 openssl 来解析。因为公钥是 OpenSSH 的格式，需要先转换到标准的 PEM 格式。 $ ssh-keygen -e -m PEM -f rsa.pub | openssl asn1parse -inform PEM 0:d=0 hl=4 l= 266 cons: SEQUENCE 4:d=1 hl=4 l= 257 prim: INTEGER :CC21C75B3E1D2BF247ED4689955F8FBA1E40A865862858FBC8EC042F704BDA9D997E8F602A5841680BCCC7317A972E530F784E4DB74B732C58B1C3DB0A3A103796573608BE01DDC25A610D461C8E9BA525F55353E406D9B561A20C47E77FAF6DA8E4E9FF5340786649A8E546BE5C8DF2798AA4C822E0456CF5F37D88225F5D56F352967F51F3D3B1D573AFB8CC4D9C478009C4819A7006BD8E49CA485FE13043C303519AA669BACAA4A647E103A63D3EF8DB44D31F703133F36555D5578BC488376B20BA317E48E2F5C5391233C7E55B067C708A783CBD06401E3D2514D99678E673A21DE8F64E2E1424B359ECD5403F4957DC50A2B8EC679001D5976A6F5279 265:d=1 hl=2 l= 3 prim: INTEGER :010001 可以很容易地看出第一个数字是 n，第二个数字是 e，都是十六进制的表达方式。 私钥的内容如下。 -----BEGIN OPENSSH PRIVATE KEY-----b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAABFwAAAAdzc2gtcnNhAAAAAwEAAQAAAQEAzCHHWz4dK/JH7UaJlV+Puh5AqGWGKFj7yOwEL3BL2p2Zfo9gKlhBaAvMxzF6ly5TD3hOTbdLcyxYscPbCjoQN5ZXNgi+Ad3CWmENRhyOm6Ul9VNT5AbZtWGiDEfnf69tqOTp/1NAeGZJqOVGvlyN8nmKpMgi4EVs9fN9iCJfXVbzUpZ/UfPTsdVzr7jMTZxHgAnEgZpwBr2OScpIX+EwQ8MDUZqmabrKpKZH4QOmPT7420TTH3AxM/NlVdVXi8SIN2sgujF+SOL1xTkSM8flWwZ8cIp4PL0GQB49JRTZlnjmc6Id6PZOLhQks1ns1UA/SVfcUKK47GeQAdWXam9SeQAAA9B/9/tPf/f7TwAAAAdzc2gtcnNhAAABAQDMIcdbPh0r8kftRomVX4+6HkCoZYYoWPvI7AQvcEvanZl+j2AqWEFoC8zHMXqXLlMPeE5Nt0tzLFixw9sKOhA3llc2CL4B3cJaYQ1GHI6bpSX1U1PkBtm1YaIMR+d/r22o5On/U0B4Zkmo5Ua+XI3yeYqkyCLgRWz1832IIl9dVvNSln9R89Ox1XOvuMxNnEeACcSBmnAGvY5Jykhf4TBDwwNRmqZpusqkpkfhA6Y9PvjbRNMfcDEz82VV1VeLxIg3ayC6MX5I4vXFORIzx+VbBnxwing8vQZAHj0lFNmWeOZzoh3o9k4uFCSzWezVQD9JV9xQorjsZ5AB1Zdqb1J5AAAAAwEAAQAAAQEAiC1gmPXu8ApJAXk0/3kooLjd2Xkg7nmuPnN0t1DqyYSpiUyMkrMdrxNwINJZPdGhh4hydFX693J2GODXlxL1DqA0vc9HMmeF6FUmTcdvO1YI5IgaRtxrEB15xUeSoBOfzDQqBjK7p5ZVPV72urdz2nZKj3MUERk/fzRYYiDMDa9o4frPay3vc2NLSjqbrpFXTHGBYGpVoIY1R7awczBILIz+TqVZ0Awlpp89aU3K9K4Sbgnb6p0dcGD8FLoRI5geviLOwYbAnuELxzMrJSVC4xH6UMiLGGqm07qpB3cxDd2M6jW1179bNko5qHnbsi87SYO5ms+3mRnin6I08kBogQAAAIEAlAGXHrG3+L73gXfK748qoC//E97EdtjPZImAr4Ess62TTfOi3SBungRvmXtWY9s/gkimZa6BL2elyEWlwlLlllX2jZLbLDjRbGdEmjEwIlzF6Dlkv5EiuGzzJ06MirVuOVpWSgtI3GL+Ir8ovibHq+zz7MGPMQdsqqASDZXvPn8AAACBAP/frz0gP1YC6w1ZPIcNCFgcWqfKofSwQviZGthpk04ZwwCkNu2XsG61MqhnsrUt2vJhMtB0khboXa1SxHO6og5duCH2Tn8uWlZsTiFAjhqOxuZwaCd2f+1tgc4SUpIdavJrkeLLUM+7JprdUeqGGrv9ae5vtfhEBozbwDGm3CJFAAAAgQDMO487OesMXGh2p2WES/pw+LxJuFqtZZY8Oy2uBNJKXNeFWXioiL4EglMLBgPz5zFkg73qMF2cTP/XFSiO8zq6LUJOy6FnKDPF8eo5jkaIjyLK3ue9BjF79AB2vkB5APSwNBS6Q5sryKqlaT1u3mx+45FZHLB/Zl4iDn404UoMpQAAABhjakBDSnMtTWFjQm9vay1Qcm8ubG9jYWwB-----END OPENSSH PRIVATE KEY----- 我并没有找到标准的格式说明文档，不过这篇博客 The OpenSSH Private Key Format 写的很清楚，我验证了一下，是对的。 简单来说，除去开头和结尾的 Marker，中间部分是 Base64 编码的一段数据，数据格式如下： &quot;openssh-key-v1&quot;0x00 # NULL-terminated &quot;Auth Magic&quot; string32-bit length, &quot;none&quot; # ciphername length and string32-bit length, &quot;none&quot; # kdfname length and string32-bit length, nil # kdf (0 length, no kdf)32-bit 0x01 # number of keys, hard-coded to 1 (no length)32-bit length, sshpub # public key in ssh format 32-bit length, keytype 32-bit length, pub0 32-bit length, pub132-bit length for rnd+prv+comment+pad 64-bit dummy checksum? # a random 32-bit int, repeated 32-bit length, keytype # the private key (including public) 32-bit length, pub0 # Public Key parts 32-bit length, pub1 32-bit length, prv0 # Private Key parts ... # (number varies by type) 32-bit length, comment # comment string padding bytes 0x010203 # pad to blocksize (see notes below) 根据上面的描述，我们会发现，其实私钥文件中完整编码了公钥的信息，所以通过私钥我们可以很容易地“恢复”出公钥文件。 $ ssh-keygen -y -f rsassh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDMIcdbPh0r8kftRomVX4+6HkCoZYYoWPvI7AQvcEvanZl+j2AqWEFoC8zHMXqXLlMPeE5Nt0tzLFixw9sKOhA3llc2CL4B3cJaYQ1GHI6bpSX1U1PkBtm1YaIMR+d/r22o5On/U0B4Zkmo5Ua+XI3yeYqkyCLgRWz1832IIl9dVvNSln9R89Ox1XOvuMxNnEeACcSBmnAGvY5Jykhf4TBDwwNRmqZpusqkpkfhA6Y9PvjbRNMfcDEz82VV1VeLxIg3ayC6MX5I4vXFORIzx+VbBnxwing8vQZAHj0lFNmWeOZzoh3o9k4uFCSzWezVQD9JV9xQorjsZ5AB1Zdqb1J5 有了结构说明，就不难自己实现解析器了。同样，下文的 rsademo 程序实现了私钥的解析逻辑。 $ rsademo -parse rsaOpenSSH Private Key keyType: ssh-rsa n: 0xCC21C75B3E1D2BF247ED4689955F8FBA1E40A865862858FBC8EC042F704BDA9D997E8F602A5841680BCCC7317A972E530F784E4DB74B732C58B1C3DB0A3A103796573608BE01DDC25A610D461C8E9BA525F55353E406D9B561A20C47E77FAF6DA8E4E9FF5340786649A8E546BE5C8DF2798AA4C822E0456CF5F37D88225F5D56F352967F51F3D3B1D573AFB8CC4D9C478009C4819A7006BD8E49CA485FE13043C303519AA669BACAA4A647E103A63D3EF8DB44D31F703133F36555D5578BC488376B20BA317E48E2F5C5391233C7E55B067C708A783CBD06401E3D2514D99678E673A21DE8F64E2E1424B359ECD5403F4957DC50A2B8EC679001D5976A6F5279 e: 0x010001 d: 0x882D6098F5EEF00A49017934FF7928A0B8DDD97920EE79AE3E7374B750EAC984A9894C8C92B31DAF137020D2593DD1A18788727455FAF7727618E0D79712F50EA034BDCF47326785E855264DC76F3B5608E4881A46DC6B101D79C54792A0139FCC342A0632BBA796553D5EF6BAB773DA764A8F731411193F7F34586220CC0DAF68E1FACF6B2DEF73634B4A3A9BAE91574C7181606A55A0863547B6B07330482C8CFE4EA559D00C25A69F3D694DCAF4AE126E09DBEA9D1D7060FC14BA1123981EBE22CEC186C09EE10BC7332B252542E311FA50C88B186AA6D3BAA90777310DDD8CEA35B5D7BF5B364A39A879DBB22F3B4983B99ACFB79919E29FA234F2406881 p: 0xFFDFAF3D203F5602EB0D593C870D08581C5AA7CAA1F4B042F8991AD869934E19C300A436ED97B06EB532A867B2B52DDAF26132D0749216E85DAD52C473BAA20E5DB821F64E7F2E5A566C4E21408E1A8EC6E6706827767FED6D81CE1252921D6AF26B91E2CB50CFBB269ADD51EA861ABBFD69EE6FB5F844068CDBC031A6DC2245 q: 0xCC3B8F3B39EB0C5C6876A765844BFA70F8BC49B85AAD65963C3B2DAE04D24A5CD7855978A888BE0482530B0603F3E7316483BDEA305D9C4CFFD715288EF33ABA2D424ECBA1672833C5F1EA398E46888F22CADEE7BD06317BF40076BE407900F4B03414BA439B2BC8AAA5693D6EDE6C7EE391591CB07F665E220E7E34E14A0CA5 如果使用 openssl 的话，可以通过如下指令解析私钥。ssh-keygen 无法直接更改私钥的格式，需要曲线救国，使用它“修改密码”的功能，参考 这个提问。 $ cp rsa rsa.pem$ ssh-keygen -p -m PEM -f rsa.pem$ cat rsa.pem-----BEGIN RSA PRIVATE KEY-----MIIEpQIBAAKCAQEAzCHHWz4dK/JH7UaJlV+Puh5AqGWGKFj7yOwEL3BL2p2Zfo9gKlhBaAvMxzF6ly5TD3hOTbdLcyxYscPbCjoQN5ZXNgi+Ad3CWmENRhyOm6Ul9VNT5AbZtWGiDEfnf69tqOTp/1NAeGZJqOVGvlyN8nmKpMgi4EVs9fN9iCJfXVbzUpZ/UfPTsdVzr7jMTZxHgAnEgZpwBr2OScpIX+EwQ8MDUZqmabrKpKZH4QOmPT7420TTH3AxM/NlVdVXi8SIN2sgujF+SOL1xTkSM8flWwZ8cIp4PL0GQB49JRTZlnjmc6Id6PZOLhQks1ns1UA/SVfcUKK47GeQAdWXam9SeQIDAQABAoIBAQCILWCY9e7wCkkBeTT/eSiguN3ZeSDuea4+c3S3UOrJhKmJTIySsx2vE3Ag0lk90aGHiHJ0Vfr3cnYY4NeXEvUOoDS9z0cyZ4XoVSZNx287VgjkiBpG3GsQHXnFR5KgE5/MNCoGMrunllU9Xva6t3PadkqPcxQRGT9/NFhiIMwNr2jh+s9rLe9zY0tKOpuukVdMcYFgalWghjVHtrBzMEgsjP5OpVnQDCWmnz1pTcr0rhJuCdvqnR1wYPwUuhEjmB6+Is7BhsCe4QvHMyslJULjEfpQyIsYaqbTuqkHdzEN3YzqNbXXv1s2SjmoeduyLztJg7maz7eZGeKfojTyQGiBAoGBAP/frz0gP1YC6w1ZPIcNCFgcWqfKofSwQviZGthpk04ZwwCkNu2XsG61MqhnsrUt2vJhMtB0khboXa1SxHO6og5duCH2Tn8uWlZsTiFAjhqOxuZwaCd2f+1tgc4SUpIdavJrkeLLUM+7JprdUeqGGrv9ae5vtfhEBozbwDGm3CJFAoGBAMw7jzs56wxcaHanZYRL+nD4vEm4Wq1lljw7La4E0kpc14VZeKiIvgSCUwsGA/PnMWSDveowXZxM/9cVKI7zOrotQk7LoWcoM8Xx6jmORoiPIsre570GMXv0AHa+QHkA9LA0FLpDmyvIqqVpPW7ebH7jkVkcsH9mXiIOfjThSgylAoGBANSTUnQXCWd8zyjs3TNZ6XfCPrKtzvWJRmpgUIRA2eeF0ZMD2rpzTln7YdW1KSwKp568j8nNPt2XONRZMervv9jtlZ9pkPdqXBT2r8ZCaoy315j1BCLc+RUY6EF6yWyo0gQKyE3CGiYq1rzMaFTOCwHpXAuCdYyHf2Wg38CgXrx9AoGAHvN3xXYFlR38Bt9flykcjzpi7pktxNF8byxYw+KfK/3d+6uPiZsPkQdfJnCG8NO8vIrqoS8rQKC6tRHTz7Y01Do/rklV8Jg7IGiFIqvZLKDkmPInFJJ3tV1JJLW4d54ZdwqtiXztazlCA0drs/2pW6GJSYP7i5Mr+OVRYxoxarECgYEAlAGXHrG3+L73gXfK748qoC//E97EdtjPZImAr4Ess62TTfOi3SBungRvmXtWY9s/gkimZa6BL2elyEWlwlLlllX2jZLbLDjRbGdEmjEwIlzF6Dlkv5EiuGzzJ06MirVuOVpWSgtI3GL+Ir8ovibHq+zz7MGPMQdsqqASDZXvPn8=-----END RSA PRIVATE KEY----- 得到 PEM 格式的私钥以后，剩下就好办了。 $ openssl asn1parse -inform PEM &lt; rsa.pem 0:d=0 hl=4 l=1189 cons: SEQUENCE 4:d=1 hl=2 l= 1 prim: INTEGER :00 7:d=1 hl=4 l= 257 prim: INTEGER :CC21C75B3E1D2BF247ED4689955F8FBA1E40A865862858FBC8EC042F704BDA9D997E8F602A5841680BCCC7317A972E530F784E4DB74B732C58B1C3DB0A3A103796573608BE01DDC25A610D461C8E9BA525F55353E406D9B561A20C47E77FAF6DA8E4E9FF5340786649A8E546BE5C8DF2798AA4C822E0456CF5F37D88225F5D56F352967F51F3D3B1D573AFB8CC4D9C478009C4819A7006BD8E49CA485FE13043C303519AA669BACAA4A647E103A63D3EF8DB44D31F703133F36555D5578BC488376B20BA317E48E2F5C5391233C7E55B067C708A783CBD06401E3D2514D99678E673A21DE8F64E2E1424B359ECD5403F4957DC50A2B8EC679001D5976A6F5279 268:d=1 hl=2 l= 3 prim: INTEGER :010001 273:d=1 hl=4 l= 257 prim: INTEGER :882D6098F5EEF00A49017934FF7928A0B8DDD97920EE79AE3E7374B750EAC984A9894C8C92B31DAF137020D2593DD1A18788727455FAF7727618E0D79712F50EA034BDCF47326785E855264DC76F3B5608E4881A46DC6B101D79C54792A0139FCC342A0632BBA796553D5EF6BAB773DA764A8F731411193F7F34586220CC0DAF68E1FACF6B2DEF73634B4A3A9BAE91574C7181606A55A0863547B6B07330482C8CFE4EA559D00C25A69F3D694DCAF4AE126E09DBEA9D1D7060FC14BA1123981EBE22CEC186C09EE10BC7332B252542E311FA50C88B186AA6D3BAA90777310DDD8CEA35B5D7BF5B364A39A879DBB22F3B4983B99ACFB79919E29FA234F2406881 534:d=1 hl=3 l= 129 prim: INTEGER :FFDFAF3D203F5602EB0D593C870D08581C5AA7CAA1F4B042F8991AD869934E19C300A436ED97B06EB532A867B2B52DDAF26132D0749216E85DAD52C473BAA20E5DB821F64E7F2E5A566C4E21408E1A8EC6E6706827767FED6D81CE1252921D6AF26B91E2CB50CFBB269ADD51EA861ABBFD69EE6FB5F844068CDBC031A6DC2245 666:d=1 hl=3 l= 129 prim: INTEGER :CC3B8F3B39EB0C5C6876A765844BFA70F8BC49B85AAD65963C3B2DAE04D24A5CD7855978A888BE0482530B0603F3E7316483BDEA305D9C4CFFD715288EF33ABA2D424ECBA1672833C5F1EA398E46888F22CADEE7BD06317BF40076BE407900F4B03414BA439B2BC8AAA5693D6EDE6C7EE391591CB07F665E220E7E34E14A0CA5 798:d=1 hl=3 l= 129 prim: INTEGER :D49352741709677CCF28ECDD3359E977C23EB2ADCEF589466A60508440D9E785D19303DABA734E59FB61D5B5292C0AA79EBC8FC9CD3EDD9738D45931EAEFBFD8ED959F6990F76A5C14F6AFC6426A8CB7D798F50422DCF91518E8417AC96CA8D2040AC84DC21A262AD6BCCC6854CE0B01E95C0B82758C877F65A0DFC0A05EBC7D 930:d=1 hl=3 l= 128 prim: INTEGER :1EF377C57605951DFC06DF5F97291C8F3A62EE992DC4D17C6F2C58C3E29F2BFDDDFBAB8F899B0F91075F267086F0D3BCBC8AEAA12F2B40A0BAB511D3CFB634D43A3FAE4955F0983B20688522ABD92CA0E498F227149277B55D4924B5B8779E19770AAD897CED6B394203476BB3FDA95BA1894983FB8B932BF8E551631A316AB1 1061:d=1 hl=3 l= 129 prim: INTEGER :9401971EB1B7F8BEF78177CAEF8F2AA02FFF13DEC476D8CF648980AF812CB3AD934DF3A2DD206E9E046F997B5663DB3F8248A665AE812F67A5C845A5C252E59655F68D92DB2C38D16C67449A3130225CC5E83964BF9122B86CF3274E8C8AB56E395A564A0B48DC62FE22BF28BE26C7ABECF3ECC18F31076CAAA0120D95EF3E7F 我们得到了一堆数字，对照如下的说明，就可以知道每个数字的含义。 RSAPrivateKey ::= SEQUENCE &#123; version Version, modulus INTEGER, -- n publicExponent INTEGER, -- e privateExponent INTEGER, -- d prime1 INTEGER, -- p prime2 INTEGER, -- q exponent1 INTEGER, -- d mod (p-1) exponent2 INTEGER, -- d mod (q-1) coefficient INTEGER, -- (inverse of q) mod p otherPrimeInfos OtherPrimeInfos OPTIONAL&#125; Use RSA 这里我们来演示一下直接加密一个消息 # 生成我们的私密消息$ echo &quot;This is our secret message.&quot; &gt; secret.txt# 使用 RSA 加密，注意要转换公钥格式到 PKCS8$ openssl rsautl -encrypt -oaep -pubin -inkey &lt;(ssh-keygen -e -m PKCS8 -f rsa.pub) -in secret.txt -out secret.txt.enc# 加密以后的文件是 secret.txt.enc# 接下来使用 RSA 解密，同样要转换私钥格式# 我们使用上文中得到的 PEM 格式私钥，rsa.pem$ openssl rsautl -decrypt -oaep -inkey rsa.pem -in secret.txt.enc -out result.txt# 验证一下是否得到了原始消息$ cat result.txtThis is our secret message. 上面我们提到的 RSA 加密过程，也就是 ，也被称为教科书式 RSA。工程应用中，不会直接这样处理，而是会存在一个 Padding 的过程，具体不再展开，感兴趣可以去看 RSA - theory and implementation。 注意，密码学中有很多微妙的问题要考虑。我们这里所做的一切都是为了学习和理解他们的工作原理，而不是为了自己去实现他们。千万不要自己去实现任何加密解密算法，专业的事情交给专业的人员处理就好. [^1]: $m$​, $n$​ 不互质, 则$m$​与$n$​有1以外的公因数. $m = pq$​ , $m$​ 的因数有: 1, $p$​, $q$​, $pq$​ . 因此 $n$​ 必定至少有因数 $p$​, $q$​, $pq$​ 之一, 因此 $m$​ 为 $p$​ 或 $q$​ 的倍数 [^2]: &quot;若一个数是质数, 则两个数互质 == 另一个数不是前者的倍数&quot;","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"Computer Networking","slug":"Computer-Networking","permalink":"http://lyk-love.cn/tags/Computer-Networking/"},{"name":"Cryptology","slug":"Cryptology","permalink":"http://lyk-love.cn/tags/Cryptology/"}]},{"title":"HTML Basic","slug":"HTML-Basic","date":"2022-08-22T19:02:39.000Z","updated":"2022-09-26T06:39:34.930Z","comments":true,"path":"2022/08/23/HTML-Basic/","link":"","permalink":"http://lyk-love.cn/2022/08/23/HTML-Basic/","excerpt":"Outline: Basic HTML Attributes Head Elements Block and Inline Elements Other Elements CSS","text":"Outline: Basic HTML Attributes Head Elements Block and Inline Elements Other Elements CSS Basic &lt;!DOCTYPE&gt; The &lt;!DOCTYPE&gt; declaration represents the document type, and helps browsers to display web pages correctly. It must only appear once, at the top of the page (before any HTML tags). The &lt;!DOCTYPE&gt; declaration is not case sensitive. The &lt;!DOCTYPE&gt; declaration for HTML5 is: HTML Elements An HTML element is defined by a start tag, some content, and an end tag: &lt;tagname&gt; Content goes here... &lt;/tagname&gt; tag及其包括的content构成了HTML元素 HTML tag不是大小写敏感的 &lt;P&gt; means the same as &lt;p&gt; HTML元素可以嵌套 某些HTML元素没有内容部分(比如 &lt;br&gt; 元素)，称为空元素。 空元素不能有end tag 基本标签 The &lt;!DOCTYPE html&gt; declaration defines that this document is an HTML5 document The &lt;html&gt; element is the root element of an HTML page The &lt;head&gt; element contains meta information about the HTML page The &lt;title&gt; element specifies a title for the HTML page (which is shown in the browser's title bar or in the page's tab) The &lt;body&gt; element defines the document's body, and is a container for all the visible contents, such as headings, paragraphs, images, hyperlinks, tables, lists, etc. The &lt;h1&gt; element defines a large heading The &lt;p&gt; element defines a paragraph Usage All HTML documents must start with a document type declaration: &lt;!DOCTYPE html&gt;. The HTML document itself begins with &lt;html&gt; and ends with &lt;/html&gt;. The visible part of the HTML document is between &lt;body&gt; and &lt;/body&gt;. &lt;nav&gt; &lt;nav&gt; &lt;a href=&quot;/html/&quot;&gt;HTML&lt;/a&gt; | &lt;a href=&quot;/css/&quot;&gt;CSS&lt;/a&gt; | &lt;a href=&quot;/js/&quot;&gt;JavaScript&lt;/a&gt; | &lt;a href=&quot;/jquery/&quot;&gt;jQuery&lt;/a&gt;&lt;/nav&gt; &lt;aside&gt; The &lt;aside&gt; element defines some content aside from the content it is placed in (like a sidebar). The &lt;aside&gt; content should be indirectly related to the surrounding content. HTML Attributes HTML attributes provide additional information about HTML elements. All HTML elements can have attributes Attributes are always specified in the start tag Attributes usually come in name/value pairs like: name=&quot;value&quot; 基本属性 The &lt;a&gt; tag defines a hyperlink. The href attribute specifies the URL of the page the link goes to: &lt;a href=&quot;https://www.w3schools.com&quot;&gt;Visit W3Schools&lt;/a&gt; Class The class attribute can be used on any HTML element. Note: The class name is case sensitive! HTML elements can belong to more than one class. HTML Head Elements The &lt;head&gt; element is a container for metadata (data about data) and is placed between the &lt;html&gt; tag and the &lt;body&gt; tag. HTML metadata is data about the HTML document. Metadata is not displayed. Metadata typically define the document title, character set, styles, scripts, and other meta information. You can have several &lt;header&gt; elements in one HTML document. However, &lt;header&gt; cannot be placed within a &lt;footer&gt;, &lt;address&gt; or another &lt;header&gt; element. &lt;title&gt; The title must be text-only defines a title in the browser toolbar displays a title for the page in search engine-results &lt;style&gt; &lt;style&gt; body &#123;background-color: powderblue;&#125; h1 &#123;color: red;&#125; p &#123;color: blue;&#125;&lt;/style&gt; &lt;meta&gt; Define the character set used: &lt;meta charset=&quot;UTF-8&quot;&gt; Define keywords for search engines: &lt;meta name=&quot;keywords&quot; content=&quot;HTML, CSS, JavaScript&quot;&gt; Define a description of your web page: &lt;meta name=&quot;description&quot; content=&quot;Free Web tutorials&quot;&gt; Define the author of a page: &lt;meta name=&quot;author&quot; content=&quot;John Doe&quot;&gt; Refresh document every 30 seconds: &lt;meta http-equiv=&quot;refresh&quot; content=&quot;30&quot;&gt; Setting the viewport to make your website look good on all devices: &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;link&gt; The &lt;link&gt; tag is most often used to link to external style sheets: &lt;link rel=&quot;stylesheet&quot; href=&quot;mystyle.css&quot;&gt; &lt;script&gt; &lt;script&gt;function myFunction() &#123; document.getElementById(&quot;demo&quot;).innerHTML = &quot;Hello JavaScript!&quot;;&#125;&lt;/script&gt; &lt;base&gt; The &lt;base&gt; element specifies the base URL and/or target for all relative URLs in a page. The &lt;base&gt; tag must have either an href or a target attribute present, or both. There can only be one single &lt;base&gt; element in a document! &lt;head&gt;&lt;base href=&quot;https://www.w3schools.com/&quot; target=&quot;_blank&quot;&gt;&lt;/head&gt;&lt;body&gt;&lt;img src=&quot;images/stickman.gif&quot; width=&quot;24&quot; height=&quot;39&quot; alt=&quot;Stickman&quot;&gt;&lt;a href=&quot;tags/tag_base.asp&quot;&gt;HTML base Tag&lt;/a&gt;&lt;/body&gt; Block and Inline Elements The &lt;div&gt; element is a block-level and is often used as a container for other HTML elements The &lt;span&gt; element is an inline container used to mark up a part of a text, or a part of a document Block A block-level element always starts on a new line, and the browsers automatically add some space (a margin) before and after the element. A block-level element always takes up the full width available (stretches out to the left and right as far as it can). Two commonly used block elements are: &lt;p&gt; and &lt;div&gt;. The &lt;p&gt; element defines a paragraph in an HTML document. The &lt;div&gt; element defines a division or a section in an HTML document. Inline An inline element does not start on a new line. An inline element only takes up as much width as necessary. Note: An inline element cannot contain a block-level element! &lt;span&gt;Hello World&lt;/span&gt; Other Elements &lt;label&gt; The for attribute of &lt;label&gt; must be equal to the id attribute of the related element to bind them together. A label can also be bound to an element by placing the element inside the &lt;label&gt; element. &lt;form action=&quot;/action_page.php&quot;&gt; &lt;input type=&quot;radio&quot; id=&quot;html&quot; name=&quot;fav_language&quot; value=&quot;HTML&quot;&gt; # 对应for = &quot;html&quot; &lt;label for=&quot;html&quot;&gt;HTML&lt;/label&gt;&lt;br&gt; &lt;input type=&quot;radio&quot; id=&quot;css&quot; name=&quot;fav_language&quot; value=&quot;CSS&quot;&gt;# 对应for = &quot;css&quot; &lt;label for=&quot;css&quot;&gt;CSS&lt;/label&gt;&lt;br&gt; &lt;input type=&quot;radio&quot; id=&quot;javascript&quot; name=&quot;fav_language&quot; value=&quot;JavaScript&quot;&gt; &lt;label for=&quot;javascript&quot;&gt;JavaScript&lt;/label&gt;&lt;br&gt;&lt;br&gt; &lt;input type=&quot;submit&quot; value=&quot;Submit&quot;&gt;&lt;/form&gt; CSS Using CSS CSS can be added to HTML documents in 3 ways: Inline - by using the style attribute inside HTML elements Internal - by using a &lt;style&gt; element in the &lt;head&gt; section External - by using a &lt;link&gt; element to link to an external CSS file Inline &lt;h1 style=&quot;color:blue;&quot;&gt;A Blue Heading&lt;/h1&gt; Internal &lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;style&gt;body &#123;background-color: powderblue;&#125;h1 &#123;color: blue;&#125;p &#123;color: red;&#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;This is a heading&lt;/h1&gt;&lt;p&gt;This is a paragraph.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; External &lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;styles.css&quot;&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;This is a heading&lt;/h1&gt;&lt;p&gt;This is a paragraph.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; Class To create a class; write a period (.) character, followed by a class name. Then, define the CSS properties within curly braces {}: &lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;style&gt;.city &#123; background-color: tomato; color: white; padding: 10px;&#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h2 class=&quot;city&quot;&gt;London&lt;/h2&gt;&lt;p&gt;London is the capital of England.&lt;/p&gt;&lt;h2 class=&quot;city&quot;&gt;Paris&lt;/h2&gt;&lt;p&gt;Paris is the capital of France.&lt;/p&gt;&lt;h2 class=&quot;city&quot;&gt;Tokyo&lt;/h2&gt;&lt;p&gt;Tokyo is the capital of Japan.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;","categories":[{"name":"Frontend","slug":"Frontend","permalink":"http://lyk-love.cn/categories/Frontend/"}],"tags":[{"name":"HTML","slug":"HTML","permalink":"http://lyk-love.cn/tags/HTML/"}]},{"title":"Javascript Basic","slug":"Javascript-Basic","date":"2022-08-22T18:51:59.000Z","updated":"2022-09-26T06:39:34.931Z","comments":true,"path":"2022/08/23/Javascript-Basic/","link":"","permalink":"http://lyk-love.cn/2022/08/23/Javascript-Basic/","excerpt":"Outline: Intro Basic Function ...","text":"Outline: Intro Basic Function ... Intro MDN -&gt; JS 廖雪峰 -&gt;JS Basic Object JavaScript的对象是一种无序的集合数据类型，它由若干键值对组成. JavaScript用一个&#123;...&#125;表示一个对象，键值对以xxx: xxx形式申明，用,隔开。注意，最后一个键值对不需要在末尾加,，如果加了，有的浏览器（如低版本的IE）将报错. JS对象的属性的key是一个字符串, value可以是任意类型. JS中访问不存在的属性会返回undefined： 成员访问 如果属性名包含特殊字符，就必须用''括起来： let xiaohong = &#123; name: &#x27;小红&#x27;, &#x27;middle-school&#x27;: &#x27;No.1 Middle School&#x27;&#125;; xiaohong的属性名middle-school不是一个有效的变量，就需要用''括起来。访问这个属性也无法使用.操作符，必须用['xxx']来访问： xiaohong[&#x27;middle-school&#x27;]; // &#x27;No.1 Middle School&#x27;xiaohong[&#x27;name&#x27;]; // &#x27;小红&#x27;xiaohong.name; // &#x27;小红&#x27; 也可以用xiaohong['name']来访问xiaohong的name属性，不过xiaohong.name的写法更简洁。我们在编写JavaScript代码的时候，属性名尽量使用标准的变量名，这样就可以直接通过object.prop的形式访问一个属性了。 Function Destructuring ES6支持解构赋值. 这是对容器、对象等对象赋值的语法糖. 解构赋值可以支持嵌套结构, 只需保持赋值时的嵌套的层次一致 数组的解构赋值 对数组: let [x, y, z] = [&#x27;hello&#x27;, &#x27;JavaScript&#x27;, &#x27;ES6&#x27;]; 对嵌套的数组元素的解构赋值: let [x, [y, z]] = [&#x27;hello&#x27;, [&#x27;JavaScript&#x27;, &#x27;ES6&#x27;]]; 解构赋值还可以忽略某些元素： let [, , z] = [&#x27;hello&#x27;, &#x27;JavaScript&#x27;, &#x27;ES6&#x27;]; // 忽略前两个元素，只对z赋值第三个元素z; // &#x27;ES6&#x27; 对象的解构赋值 对象的解构赋值是按成员名字匹配的: var person = &#123; name: &#x27;小明&#x27;, age: 20, gender: &#x27;male&#x27;, passport: &#x27;G-12345678&#x27;, school: &#x27;No.4 middle school&#x27;&#125;;var &#123;name, age, passport&#125; = person; var person = &#123; name: &#x27;小明&#x27;, age: 20, gender: &#x27;male&#x27;, passport: &#x27;G-12345678&#x27;, school: &#x27;No.4 middle school&#x27;, address: &#123; city: &#x27;Beijing&#x27;, street: &#x27;No.1 Road&#x27;, zipcode: &#x27;100001&#x27; &#125;&#125;;var &#123;name, address: &#123;city, zip&#125;&#125; = person;name; // &#x27;小明&#x27;city; // &#x27;Beijing&#x27;zip; // undefined, 因为属性名是zipcode而不是zip// 注意: address不是变量，而是为了让city和zip获得嵌套的address对象的属性:address; // Uncaught ReferenceError: address is not defined 变量名zip不匹配对象的address属性的zipcode, 所以是undefined address没有被赋值为变量 如果要使用的变量名和属性名不一致，可以用下面的语法获取： var person = &#123; name: &#x27;小明&#x27;, age: 20, gender: &#x27;male&#x27;, passport: &#x27;G-12345678&#x27;, school: &#x27;No.4 middle school&#x27;&#125;;// 把passport属性赋值给变量id:let &#123;name, passport:id&#125; = person;name; // &#x27;小明&#x27;id; // &#x27;G-12345678&#x27;// 注意: passport不是变量，而是为了让变量id获得passport属性:passport; // Uncaught ReferenceError: passport is not defined 解构赋值还可以使用默认值，这样就避免了不存在的属性返回undefined的问题： var person = &#123; name: &#x27;小明&#x27;, age: 20, gender: &#x27;male&#x27;, passport: &#x27;G-12345678&#x27;&#125;;// 如果person对象没有single属性，默认赋值为true:var &#123;name, single=true&#125; = person;name; // &#x27;小明&#x27;single; // true 有些时候，如果变量已经被声明了，再次赋值的时候，正确的写法也会报语法错误： // 声明变量:let x, y;// 解构赋值:&#123;x, y&#125; = &#123; name: &#x27;小明&#x27;, x: 100, y: 200&#125;;// 语法错误: Uncaught SyntaxError: Unexpected token = 这是因为JavaScript引擎把&#123;开头的语句当作了块处理，于是=不再合法。解决方法是用小括号括起来： (&#123;x, y&#125; = &#123; name: &#x27;小明&#x27;, x: 100, y: 200&#125;); 应用 假设props为: props = &#123; name: &#x27;Arto Hellas&#x27;, age: 35,&#125; React中: const Hello = (props) =&gt; &#123; const &#123; name, age &#125; = props const bornYear = () =&gt; new Date().getFullYear() - age return ( &lt;div&gt; &lt;p&gt;Hello &#123;name&#125;, you are &#123;age&#125; years old&lt;/p&gt; &lt;p&gt;So you were probably born in &#123;bornYear()&#125;&lt;/p&gt; &lt;/div&gt; )&#125; 可以更加简化, 在形参中写解构赋值: const Hello = (&#123; name, age &#125;) =&gt; &#123; //...&#125; object spread syntax ref Spread syntax (...) allows an iterable, such as an array or string, to be expanded in places where zero or more arguments (for function calls) or elements (for array literals) are expected. In an object literal, the spread syntax enumerates the properties of an object and adds the key-value pairs to the object being created. function sum(x, y, z) &#123; return x + y + z;&#125;const numbers = [1, 2, 3];console.log(sum(...numbers));// expected output: 6console.log(sum.apply(null, numbers));// expected output: 6 应用 在React中, useState()可以和解构赋值结合: const App = () =&gt; &#123; const [clicks, setClicks] = useState(&#123; left: 0, right: 0 &#125;) const handleLeftClick = () =&gt; &#123; const newClicks = &#123; left: clicks.left + 1, right: clicks.right &#125; setClicks(newClicks) &#125; const handleRightClick = () =&gt; &#123; const newClicks = &#123; left: clicks.left, right: clicks.right + 1 &#125; setClicks(newClicks) &#125; return ( &lt;div&gt; &#123;clicks.left&#125; &lt;button onClick=&#123;handleLeftClick&#125;&gt;left&lt;/button&gt; &lt;button onClick=&#123;handleRightClick&#125;&gt;right&lt;/button&gt; &#123;clicks.right&#125; &lt;/div&gt; )&#125; 注意到, 状态clicks有两部分: clicks.left, clicks.right. 每次更新其中一部分时, 还要写另一部分: const handleLeftClick = () =&gt; &#123; const newClicks = &#123; left: clicks.left + 1, right: clicks.right // clicks.right没有改变, 但setter里还是要写 &#125; setClicks(newClicks)&#125;//handleRightClick同理 这样很冗余,可以用object spread syntax来简化: const handleLeftClick = () =&gt; &#123; const newClicks = &#123; ...clicks, left: clicks.left + 1 &#125; setClicks(newClicks)&#125;const handleRightClick = () =&gt; &#123; const newClicks = &#123; ...clicks, right: clicks.right + 1 &#125; setClicks(newClicks)&#125; 这里的...clicks实际上被展开为: //以handleLeftClick内为例const newClicks = &#123; //...clicks 被展开为: left: xx right: xx // 这里重新声明了right属性,也就覆盖了前面的right left: clicks.left + 1 &#125; 由于JS对象支持重复声明属性, 并且后面的属性会覆盖前面同名的属性, 这样做实际上就保留了left属性, 只更改right属性","categories":[{"name":"Language","slug":"Language","permalink":"http://lyk-love.cn/categories/Language/"}],"tags":[{"name":"JS","slug":"JS","permalink":"http://lyk-love.cn/tags/JS/"}]},{"title":"React Basic","slug":"React-Basic","date":"2022-08-22T18:50:01.000Z","updated":"2022-09-26T06:39:34.938Z","comments":true,"path":"2022/08/23/React-Basic/","link":"","permalink":"http://lyk-love.cn/2022/08/23/React-Basic/","excerpt":"Outline: Intro Components JSX ...","text":"Outline: Intro Components JSX ... Intro React Official Docs Full Stack Tutorial -&gt; Web Development How do React hooks really work? React is a declarative, efficient, and flexible JavaScript library for building user interfaces. It lets you compose complex UIs from small and isolated pieces of code called “components”. Components Component是React中的一等公民. 有两种方式创建Component, 它们是等价的: Class Components Function Components Class Components React.Component是一个抽象基类. 这意味着直接引用React.Component是毫无意义的. 你可以实现一个它的子类，并且至少定义一个render()方法. class ShoppingList extends React.Component &#123; render() &#123; return ( &lt;div className=&quot;shopping-list&quot;&gt; &lt;h1&gt;Shopping List for &#123;this.props.name&#125;&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Instagram&lt;/li&gt; &lt;li&gt;WhatsApp&lt;/li&gt; &lt;li&gt;Oculus&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; ); &#125;&#125;// Example usage: &lt;ShoppingList name=&quot;Mark&quot; /&gt; Function Components Function Components比Class Components更简洁, 它默认没有State, 可以用hook函数给Function Components添加State. function Square(props) &#123; return ( &lt;button className=&quot;square&quot; onClick=&#123;props.onClick&#125;&gt; &#123;props.value&#125; &lt;/button&gt; );&#125; props 组件通过props来接收参数. props是Read-Only的 All React components must act like pure functions with respect to their props. 对于Class Component, props是其自带的成员 class Welcome extends React.Component &#123; render() &#123; return &lt;h1&gt;Hello, &#123;this.props.name&#125;&lt;/h1&gt;; &#125;&#125; 对于Function Component, 函数必须接受props为参数 function Welcome(props) &#123; return &lt;h1&gt;Hello, &#123;props.name&#125;&lt;/h1&gt;;&#125; 外部传参数给组件时, 参数都会被绑定到props的属性: const element = &lt;Welcome name=&quot;Sara&quot; /&gt;; 现在Welcome组件实例的props拥有了属性props.name, 其值为&quot;Sara&quot; Fragments the content of a React component (usually) needs to contain one root element: 可以用array包裹: const App = () =&gt; &#123; return [ &lt;h1&gt;Greetings&lt;/h1&gt;, &lt;Hello name=&quot;Maya&quot; age=&#123;26 + 10&#125; /&gt;, &lt;Footer /&gt; ]&#125; 也可以用&lt;div&gt;&lt;div/&gt;等标签包裹, 不过这样很冗余. React为此推出了 fragments, 其语法是一个空标签: const App = () =&gt; &#123; const name = &#x27;Peter&#x27; const age = 10 return ( &lt;&gt; &lt;h1&gt;Greetings&lt;/h1&gt; &lt;Hello name=&quot;Maya&quot; age=&#123;26 + 10&#125; /&gt; &lt;Hello name=&#123;name&#125; age=&#123;age&#125; /&gt; &lt;Footer /&gt; &lt;/&gt; )&#125; JSX JSX形式上类似HTML, 它会被编译为JS代码. 因此JSX实际上是JS表达式. 在JSX内可以通过curly braces来使用任何JS表达式: const name = &#x27;Josh Perez&#x27;;const element = &lt;h1&gt;Hello, &#123;name&#125;&lt;/h1&gt;; 可以直接写pure JS来替代JSX: const element = ( &lt;h1 className=&quot;greeting&quot;&gt; Hello, world! &lt;/h1&gt;);const element = React.createElement( &#x27;h1&#x27;, &#123;className: &#x27;greeting&#x27;&#125;, &#x27;Hello, world!&#x27;); 可以看到这样写很繁琐. JSX可以简化开发 Rendering Elements React Element React element就是一段JSX( or JS表达式 ), element通过render()来被渲染, 并在页面展示. const element = &lt;h1&gt;Hello, world&lt;/h1&gt;; DOM Element React假定HTML页面有一个root element, 它是个DOM element: &lt;div id=&quot;root&quot;&gt;&lt;/div&gt; React把该DOM element转化成React element: const root = ReactDOM.createRoot( document.getElementById(&#x27;root&#x27;)); Conditional Rendering render() render()方法返回React element. 最简单的做法是让root element( 此时是React element )来render: root.render(element); 事实上, 大部分React APP只会调用root.render()一次, 用于首次加载 对于组件的重新渲染, 我们使用Stateful Components. 因为需要重新渲染的组件, 都是有状态的组件( Stateful Components ) 事实上, 当且仅当组件的状态发生变更时, 才会触发Rerender 改变props不会触发rerender Stateful Components 对于Class Components, 通过加入state成员等操作, 使其变为Stateful Components 对于Function Components, React引入了hook函数来使其变为Stateful Components Notes Do Not Modify State Directly since it can result in unexpected side effects. For example, this will not re-render a component: // Wrongthis.state.comment = &#x27;Hello&#x27;; Instead, use setState(): // Correctthis.setState(&#123;comment: &#x27;Hello&#x27;&#125;); The only place where you can assign this.state is the constructor. For Class Component 对于Class Component, 可以使用state成员来保存要变化(因此要被重新rend)的成员. 此外, 将return ...改为render() &#123; ... &#125; 每次state的属性有更改,就会触发render() class Clock extends React.Component &#123; constructor(props) &#123; super(props); this.state = &#123;date: new Date()&#125;; &#125; render() &#123; return ( &lt;div&gt; &lt;h1&gt;Hello, world!&lt;/h1&gt; &lt;h2&gt;It is &#123;this.state.date.toLocaleTimeString()&#125;.&lt;/h2&gt; &lt;/div&gt; ); &#125;&#125;const root = ReactDOM.createRoot(document.getElementById(&#x27;root&#x27;));root.render(&lt;Clock /&gt;); State Updates May Be Asynchronous React may batch multiple setState() calls into a single update for performance. Because this.props and this.state may be updated asynchronously, you should not rely on their values for calculating the next state. For example, this code may fail to update the counter: // Wrongthis.setState(&#123; counter: this.state.counter + this.props.increment,&#125;); To fix it, use a second form of setState() that accepts a function rather than an object. That function will receive the previous state as the first argument, and the props at the time the update is applied as the second argument: // Correctthis.setState((state, props) =&gt; (&#123; counter: state.counter + props.increment&#125;)); Lifting State Up Often, several components need to reflect the same changing data. We recommend lifting the shared state up to their closest common ancestor. Example 子组件的状态都被封装到父组件: const App = () =&gt; &#123; const [ counter, setCounter ] = useState(0) const increaseByOne = () =&gt; setCounter(counter + 1) const decreaseByOne = () =&gt; setCounter(counter - 1) const setToZero = () =&gt; setCounter(0) return ( &lt;div&gt; &lt;Display counter=&#123;counter&#125;/&gt; &lt;Button onClick=&#123;increaseByOne&#125; text=&#x27;plus&#x27; /&gt; &lt;Button onClick=&#123;setToZero&#125; text=&#x27;zero&#x27; /&gt; &lt;Button onClick=&#123;decreaseByOne&#125; text=&#x27;minus&#x27; /&gt; &lt;/div&gt; )&#125; 子组件不需要保存State: const Display = (props) =&gt; &#123; return ( &lt;div&gt;&#123;props.counter&#125;&lt;/div&gt; )&#125; const Button = (props) =&gt; &#123; return ( &lt;button onClick=&#123;props.onClick&#125;&gt; &#123;props.text&#125; &lt;/button&gt; )&#125; Hooks Hooks are a new addition in React 16.8. They let you use state and other React features without writing a class. Hooks allow you to reuse stateful logic without changing your component hierarchy. Hooks are backwards-compatible. useState() React provides a few built-in Hooks like useState. You can also create your own Hooks to reuse stateful behavior between different components. We’ll look at the built-in Hooks first. import React, &#123; useState &#125; from &#x27;react&#x27;; //imports the useState functionfunction Example() &#123; // Declare a new state variable, which we&#x27;ll call &quot;count&quot; const [count, setCount] = useState(0); return ( &lt;div&gt; &lt;p&gt;You clicked &#123;count&#125; times&lt;/p&gt; &lt;button onClick=&#123;() =&gt; setCount(count + 1)&#125;&gt; Click me &lt;/button&gt; &lt;/div&gt; );&#125; useState returns a pair: the current state value and a function that lets you update it. You can call this function from an event handler or somewhere else. It’s similar to this.setState in a class, except it doesn’t merge the old and new state together. (We’ll show an example comparing useState to this.state in Using the State Hook.) The only argument to useState is the initial state. In the example above, it is 0 because our counter starts from zero. Note that unlike this.state, the state here doesn’t have to be an object — although it can be if you want. The initial state argument is only used during the first render. Complex state The component's state or a piece of its state can be of any type. function ExampleWithManyStates() &#123; // Declare multiple state variables! const [age, setAge] = useState(42); const [fruit, setFruit] = useState(&#x27;banana&#x27;); const [todos, setTodos] = useState([&#123; text: &#x27;Learn Hooks&#x27; &#125;]); // ...&#125; 可以和解构赋值以及object spread syntax结合来简化代码: const App = () =&gt; &#123; const [clicks, setClicks] = useState(&#123; //解构赋值 left: 0, right: 0 &#125;) const handleLeftClick = () =&gt; &#123; const newClicks = &#123; ...clicks, //object spread syntax left: clicks.left + 1 &#125; setClicks(newClicks) &#125; const handleRightClick = () =&gt; &#123; const newClicks = &#123; ...clicks, right: clicks.right + 1 &#125; setClicks(newClicks) &#125; return ( &lt;div&gt; &#123;clicks.left&#125; &lt;button onClick=&#123;handleLeftClick&#125;&gt;left&lt;/button&gt; &lt;button onClick=&#123;handleRightClick&#125;&gt;right&lt;/button&gt; &#123;clicks.right&#125; &lt;/div&gt; )&#125; useEffect() The Effect Hook, useEffect, adds the ability to perform side effects from a function component. It serves the same purpose as componentDidMount, componentDidUpdate, and componentWillUnmount in React classes, but unified into a single API. (We’ll show examples comparing useEffect to these methods in Using the Effect Hook.) For example, this component sets the document title after React updates the DOM: import React, &#123; useState, useEffect &#125; from &#x27;react&#x27;;function Example() &#123; const [count, setCount] = useState(0); // Similar to componentDidMount and componentDidUpdate: useEffect(() =&gt; &#123; // Update the document title using the browser API document.title = `You clicked $&#123;count&#125; times`; &#125;); return ( &lt;div&gt; &lt;p&gt;You clicked &#123;count&#125; times&lt;/p&gt; &lt;button onClick=&#123;() =&gt; setCount(count + 1)&#125;&gt; Click me &lt;/button&gt; &lt;/div&gt; );&#125; When you call useEffect, you’re telling React to run your “effect” function after flushing changes to the DOM. Effects are declared inside the component so they have access to its props and state. By default, React runs the effects after every render — including the first render. (We’ll talk more about how this compares to class lifecycles in Using the Effect Hook.) Handling Events Add Event Handler Handling events with React elements is very similar to handling events on DOM elements. There are some syntax differences: React events are named using camelCase, rather than lowercase. With JSX you pass a function as the event handler, rather than a string. For example, the HTML: &lt;button onclick=&quot;activateLasers()&quot;&gt; Activate Lasers&lt;/button&gt; is slightly different in React: &lt;button onClick=&#123;activateLasers&#125;&gt; Activate Lasers&lt;/button&gt; Another difference is that Event Hander When using React, you generally don’t need to call addEventListener to add listeners to a DOM element after it is created. Instead, just provide a listener when the element is initially rendered. Event handler is either a function or a function reference, not a function call 将函数作为event handler: &lt;button onClick=&#123;() =&gt; setCounter(counter + 1)&#125;&gt; plus&lt;/button&gt; 将函数引用作为event handler : &lt;button onClick=&#123;increaseByOne&#125;&gt; plus&lt;/button&gt; 错误示范, 将函数调用作为event handler: &lt;button onClick=&#123;setCounter(counter + 1)&#125;&gt; 这样做的问题在于, 当React渲染该组件时, 它会直接调用setCounter(counter + 1), 如果后者是一个改变该组件内部状态的函数, 就会导致该组件重新渲染, 然后继续调用该函数, 继续重渲染.... Prevent Default Behavior Unlike HTML, you cannot return false to prevent default behavior in React. You must call preventDefault explicitly. For example, with plain HTML, to prevent the default form behavior of submitting, you can write: &lt;form onsubmit=&quot;console.log(&#x27;You clicked submit.&#x27;); return false&quot;&gt; &lt;button type=&quot;submit&quot;&gt;Submit&lt;/button&gt;&lt;/form&gt; In React, this could instead be: function Form() &#123; function handleSubmit(e) &#123; e.preventDefault(); console.log(&#x27;You clicked submit.&#x27;); &#125; return ( &lt;form onSubmit=&#123;handleSubmit&#125;&gt; &lt;button type=&quot;submit&quot;&gt;Submit&lt;/button&gt; &lt;/form&gt; );&#125; Here, e is a synthetic event. React defines these synthetic events according to the W3C spec, so you don’t need to worry about cross-browser compatibility. React events do not work exactly the same as native events. See the SyntheticEvent reference guide to learn more. Event Handler as A Method 可以将Event Handler作为Component的成员 bind(this) bind(this): class Toggle extends React.Component &#123; constructor(props) &#123; super(props); this.state = &#123;isToggleOn: true&#125;; // This binding is necessary to make `this` work in the callback this.handleClick = this.handleClick.bind(this); &#125; handleClick() &#123; this.setState(prevState =&gt; (&#123; isToggleOn: !prevState.isToggleOn &#125;)); &#125; render() &#123; return ( &lt;button onClick=&#123;this.handleClick&#125;&gt; &#123;this.state.isToggleOn ? &#x27;ON&#x27; : &#x27;OFF&#x27;&#125; &lt;/button&gt; ); &#125;&#125; You have to be careful about the meaning of this in JSX callbacks. In JavaScript, class methods are not bound by default. If you forget to bind this.handleClick and pass it to onClick, this will be undefined when the function is actually called. This is not React-specific behavior; it is a part of how functions work in JavaScript. Generally, if you refer to a method without () after it, such as onClick=&#123;this.handleClick&#125;, you should bind that method. class fields syntax If calling bind annoys you, there are two ways you can get around this. You can use public class fields syntax to correctly bind callbacks: class LoggingButton extends React.Component &#123; // This syntax ensures `this` is bound within handleClick. handleClick = () =&gt; &#123; console.log(&#x27;this is:&#x27;, this); &#125;; render() &#123; return ( &lt;button onClick=&#123;this.handleClick&#125;&gt; Click me &lt;/button&gt; ); &#125;&#125; This syntax is enabled by default in Create React App. arrow function If you aren’t using class fields syntax, you can use an arrow function in the callback: class LoggingButton extends React.Component &#123; handleClick() &#123; console.log(&#x27;this is:&#x27;, this); &#125; render() &#123; // This syntax ensures `this` is bound within handleClick return ( &lt;button onClick=&#123;() =&gt; this.handleClick()&#125;&gt; Click me &lt;/button&gt; ); &#125;&#125; The problem with this syntax is that a different callback is created each time the LoggingButton renders. In most cases, this is fine. However, if this callback is passed as a prop to lower components, those components might do an extra re-rendering. We generally recommend binding in the constructor or using the class fields syntax, to avoid this sort of performance problem. Passing Arguments to Event Handlers Inside a loop, it is common to want to pass an extra parameter to an event handler. For example, if id is the row ID, either of the following would work: &lt;button onClick=&#123;(e) =&gt; this.deleteRow(id, e)&#125;&gt;Delete Row&lt;/button&gt;&lt;button onClick=&#123;this.deleteRow.bind(this, id)&#125;&gt;Delete Row&lt;/button&gt; The above two lines are equivalent, and use arrow functions and Function.prototype.bind respectively. In both cases, the e argument representing the React event will be passed as a second argument after the ID. With an arrow function, we have to pass it explicitly, but with bind any further arguments are automatically forwarded. Lists and Keys React的列表组件( e.g. &lt;li&gt;&lt;/li&gt; )需要指定列表元素对key. 如果不指定, 则会默认使用元素的array下标作为key, 并且报一个warning. 使用元素的数组下标作为key在排序时会遇到问题. 可以使用key=&#123;i&#125;来规避掉warning, 但依然无法解决排序时的问题. 因此必须为列表组件手动指定正确的key: &lt;li key=&#123;user.id&#125;&gt;&#123;user.name&#125;: &#123;user.taskCount&#125; tasks left&lt;/li&gt; Keys Must Only Be Unique Among Siblings key is a special and reserved property in React (along with ref, a more advanced feature). When an element is created, React extracts the key property and stores the key directly on the returned element. Even though key may look like it belongs in props, key cannot be referenced using this.props.key Examples 使用map(): function NumberList(props) &#123; const numbers = props.numbers; const listItems = numbers.map((number) =&gt; &lt;ListItem key=&#123;number.toString()&#125; value=&#123;number&#125; /&gt; ); return ( &lt;ul&gt; &#123;listItems&#125; &lt;/ul&gt; );&#125; OR: function NumberList(props) &#123; const numbers = props.numbers; return ( &lt;ul&gt; &#123;numbers.map((number) =&gt; &lt;ListItem key=&#123;number.toString()&#125; value=&#123;number&#125; /&gt; )&#125; &lt;/ul&gt; );&#125; Forms HTML form elements work a bit differently from other DOM elements in React, because form elements naturally keep some internal state. For example, this form in plain HTML accepts a single name: &lt;form&gt; &lt;label&gt; Name: &lt;input type=&quot;text&quot; name=&quot;name&quot; /&gt; &lt;/label&gt; &lt;input type=&quot;submit&quot; value=&quot;Submit&quot; /&gt;&lt;/form&gt; This form has the default HTML form behavior of browsing to a new page when the user submits the form. If you want this behavior in React, it just works. But in most cases, it’s convenient to have a JavaScript function that handles the submission of the form and has access to the data that the user entered into the form. The standard way to achieve this is with a technique called “controlled components”. Controlled Components In HTML, form elements such as &lt;input&gt;, &lt;textarea&gt;, and &lt;select&gt; typically maintain their own state and update it based on user input. In React, mutable state is typically kept in the state property of components, and only updated with setState(). We can combine the two by making the React state be the “single source of truth”. Then the React component that renders a form also controls what happens in that form on subsequent user input. An input form element whose value is controlled by React in this way is called a “controlled component”. For example, if we want to make the previous example log the name when it is submitted, we can write the form as a controlled component: class NameForm extends React.Component &#123; constructor(props) &#123; super(props); this.state = &#123;value: &#x27;&#x27;&#125;; this.handleChange = this.handleChange.bind(this); this.handleSubmit = this.handleSubmit.bind(this); &#125; handleChange(event) &#123; this.setState(&#123;value: event.target.value&#125;); &#125; handleSubmit(event) &#123; alert(&#x27;A name was submitted: &#x27; + this.state.value); event.preventDefault(); &#125; render() &#123; return ( &lt;form onSubmit=&#123;this.handleSubmit&#125;&gt; &lt;label&gt; Name: &lt;input type=&quot;text&quot; value=&#123;this.state.value&#125; onChange=&#123;this.handleChange&#125; /&gt; &lt;/label&gt; &lt;input type=&quot;submit&quot; value=&quot;Submit&quot; /&gt; &lt;/form&gt; ); &#125;&#125; Controlled Input Null Value Specifying the value prop on a controlled component prevents the user from changing the input unless you desire so. If you’ve specified a value but the input is still editable, you may have accidentally set value to undefined or null. The following code demonstrates this. (The input is locked at first but becomes editable after a short delay.) ReactDOM.createRoot(mountNode).render(&lt;input value=&quot;hi&quot; /&gt;);setTimeout(function() &#123; ReactDOM.createRoot(mountNode).render(&lt;input value=&#123;null&#125; /&gt;);&#125;, 1000); The textarea Tag In HTML, a &lt;textarea&gt; element defines its text by its children: &lt;textarea&gt; Hello there, this is some text in a text area&lt;/textarea&gt; In React, a &lt;textarea&gt; uses a value attribute instead. This way, a form using a &lt;textarea&gt; can be written very similarly to a form that uses a single-line input: class EssayForm extends React.Component &#123; constructor(props) &#123; super(props); this.state = &#123; value: &#x27;Please write an essay about your favorite DOM element.&#x27; &#125;; this.handleChange = this.handleChange.bind(this); this.handleSubmit = this.handleSubmit.bind(this); &#125; handleChange(event) &#123; this.setState(&#123;value: event.target.value&#125;); &#125; handleSubmit(event) &#123; alert(&#x27;An essay was submitted: &#x27; + this.state.value); event.preventDefault(); &#125; render() &#123; return ( &lt;form onSubmit=&#123;this.handleSubmit&#125;&gt; &lt;label&gt; Essay: &lt;textarea value=&#123;this.state.value&#125; onChange=&#123;this.handleChange&#125; /&gt; &lt;/label&gt; &lt;input type=&quot;submit&quot; value=&quot;Submit&quot; /&gt; &lt;/form&gt; ); &#125;&#125; Notice that this.state.value is initialized in the constructor, so that the text area starts off with some text in it. The select Tag In HTML, &lt;select&gt; creates a drop-down list. For example, this HTML creates a drop-down list of flavors: &lt;select&gt; &lt;option value=&quot;grapefruit&quot;&gt;Grapefruit&lt;/option&gt; &lt;option value=&quot;lime&quot;&gt;Lime&lt;/option&gt; &lt;option selected value=&quot;coconut&quot;&gt;Coconut&lt;/option&gt; &lt;option value=&quot;mango&quot;&gt;Mango&lt;/option&gt;&lt;/select&gt; Note that the Coconut option is initially selected, because of the selected attribute. React, instead of using this selected attribute, uses a value attribute on the root select tag. This is more convenient in a controlled component because you only need to update it in one place. For example: class FlavorForm extends React.Component &#123; constructor(props) &#123; super(props); this.state = &#123;value: &#x27;coconut&#x27;&#125;; this.handleChange = this.handleChange.bind(this); this.handleSubmit = this.handleSubmit.bind(this); &#125; handleChange(event) &#123; this.setState(&#123;value: event.target.value&#125;); &#125; handleSubmit(event) &#123; alert(&#x27;Your favorite flavor is: &#x27; + this.state.value); event.preventDefault(); &#125; render() &#123; return ( &lt;form onSubmit=&#123;this.handleSubmit&#125;&gt; &lt;label&gt; Pick your favorite flavor: &lt;select value=&#123;this.state.value&#125; onChange=&#123;this.handleChange&#125;&gt; &lt;option value=&quot;grapefruit&quot;&gt;Grapefruit&lt;/option&gt; &lt;option value=&quot;lime&quot;&gt;Lime&lt;/option&gt; &lt;option value=&quot;coconut&quot;&gt;Coconut&lt;/option&gt; &lt;option value=&quot;mango&quot;&gt;Mango&lt;/option&gt; &lt;/select&gt; &lt;/label&gt; &lt;input type=&quot;submit&quot; value=&quot;Submit&quot; /&gt; &lt;/form&gt; ); &#125;&#125; Overall, this makes it so that &lt;input type=&quot;text&quot;&gt;, &lt;textarea&gt;, and &lt;select&gt; all work very similarly - they all accept a value attribute that you can use to implement a controlled component. Note You can pass an array into the value attribute, allowing you to select multiple options in a select tag: &lt;select multiple=&#123;true&#125; value=&#123;[&#x27;B&#x27;, &#x27;C&#x27;]&#125;&gt; The file input Tag In HTML, an &lt;input type=&quot;file&quot;&gt; lets the user choose one or more files from their device storage to be uploaded to a server or manipulated by JavaScript via the File API. &lt;input type=&quot;file&quot; /&gt; Because its value is read-only, it is an uncontrolled component in React. It is discussed together with other uncontrolled components later in the documentation. When you need to handle multiple controlled input elements, you can add a name attribute to each element and let the handler function choose what to do based on the value of event.target.name. Refactoring Components 原代码: const Button = (props) =&gt; &#123; return ( &lt;button onClick=&#123;props.onClick&#125;&gt; &#123;props.text&#125; &lt;/button&gt; )&#125; 重构后: const Button = (&#123; onClick, text &#125;) =&gt; &lt;button onClick=&#123;onClick&#125;&gt;&#123;text&#125;&lt;/button&gt;","categories":[{"name":"Frontend","slug":"Frontend","permalink":"http://lyk-love.cn/categories/Frontend/"}],"tags":[{"name":"React","slug":"React","permalink":"http://lyk-love.cn/tags/React/"}]},{"title":"How Digital Cameras Work","slug":"How-Digital-Cameras-Work","date":"2022-08-21T18:48:46.000Z","updated":"2022-09-27T08:20:36.366Z","comments":true,"path":"2022/08/22/How-Digital-Cameras-Work/","link":"","permalink":"http://lyk-love.cn/2022/08/22/How-Digital-Cameras-Work/","excerpt":"Outline: Pixel's Color Bayer Array Interpolation/Approximation Other applications of interpolation","text":"Outline: Pixel's Color Bayer Array Interpolation/Approximation Other applications of interpolation Pixel's Color 在Resolution on Screen中, 我介绍了Pixel的概念. 我们也知道颜色是由红, 绿, 蓝 三原色组成的: 每个像素都有颜色和亮度. 但是感光元件只能衡量亮度, 无法衡量光的波长(颜色), 要想感知色彩，需要在每个感光元件前加滤色片( filter ), 滤色片是单色的. 感光元件的材料有CCD (charge coupled device), and CMOS (complementary metal oxide semiconductor ). 对应人眼球的中央凹(当然中央凹不仅可以感知光的强度, 还能感知色彩) CMOS作为一种SDRAM,也被用于计算机领域,参见Computer Storage 对于显示屏来说, 每个像素(感光元件)都是由三个红, 绿, 蓝的单色子像素(子感光元件)组成的. 数码图像和显示器一样, 每个像素也由三个单色的子像素构成 对于相机来说, 每个像素只是单色的, 缺失的另两种颜色从邻居“借来” 照理来说可以像显示器一样, 将三个不同颜色的像素作为子像素合并为一个像素. 但是这样做效率很低, 对于相机而言不可接受. 上面说法有个前提, 那就是感光元件使用Bayer Array. Bayer Array Uneven Distribution of Filters 相机的每个像素都有一个单色对Filter. 令人惊讶的是, Filter的数量是不均匀的. 其实, 人眼对于色彩的感知是不均匀的. 人眼的锥体( 大概6 million )有50%对绿色敏感, 25%对蓝色和红色敏感: Bryce Bayer根据这点发明了Bayer Array. 其中绿色Filter占50%, 红色和蓝色Filter各占25%: 由于简单好用, Bayer Array被绝大多数相机镜头采纳 Mind the Gap 下面介绍一下Bayer Array的成像步骤: 这是一张花的图片: 框起来的区域: 我们要对框起来的区域使用bayer filter, 也就是单色的filter: 这是输出的红色像素: 可以看到, 拥有红色Filter的像素大概占50% 这是绿色像素: 大概25%. 这是蓝色像素: 大概25%. 将这三张像素图拼起来, 再结合它们对应的颜色: 就得到了: 可以看到, 结果图片还是很模糊的, 这是因为Bayer Array的像素都是单色的, 还缺失另外两原色. 对此, 我们需要采用数学手段, “猜出” 另外两原色. Interpolation/Approximation 猜色问题可以抽象为: 已知两个点, 如何“猜出”它们的中间点. 这就是一个近似/插入问题. 对此, 我们有很多方法. Nearest-neighbor 最简单的方法就是向邻居“借”颜色. 也就是中间点的取值 == 离中间点最近的点的取值. 在1维图像上, 就是这样: 二维图像是这样: 对于相机的场景, 假设左图为原图, 右图采用了最近邻: 最近邻的优点是没有引入任何的虚拟值. 缺点是形成的图像成块状, 不够连续. Linear 线性建模就是把已知点用直线连起来. 1维: 2维: 对于之前的相机的例子: 线性近似相比最近邻, 没有那么“blocky”. 但是如果图像有sharp edge, 用线性近似会把边界模糊掉 Quadratic 既然一阶的线性多项式效果不好, 不妨试试二阶. 二阶多项式需要三个点: 图中的黄色点就是二阶近似生成的 很可惜, 二阶近似没法使用, 因为: 图中可以看到, 二阶近似产生的中间值很可能过高或过低. 二阶方程没有拐点, 曲线不够平滑 Cubic 再试试四阶近似, 这需要四个点: 1维: 3阶近似的效果不错. 但由于每次只考虑4个点, 前四个点的3阶方程曲线, 和后四个点的曲线, 可能会有比较大的差异. 所以在某些情况下, 3阶近似效果一般. Catmull-Rom Splines 这是对3阶近似的一个改进. 依然需要4个点, 设为$x_{-1},x_0, x_1, x_2$, 假设要求$x_0, x_1$的中间点, $x_0, x_1$中间曲线的三阶近似方程为: $$ \\begin{align} f(x) = ax^3 + bx^2 + cx + d \\nonumber \\ f'(x) = 3ax^2 + 2bx + c \\nonumber \\ f'(x) = 3ax^2 + 2bx + c \\nonumber \\end{align} $$ 我们已经知道了$f(x_0), f(x_1)$, Catmull-Rom Splines要求设3阶方程在$x_0, x_1$处的斜率分别是直线$x_{-1}x_1, x_0x_2$的斜率. 也就是： $$ \\begin{align} f'(x_0) = \\frac {y_1 - y_{-1}}{2} \\nonumber \\newline f'(x_1) = \\frac {y_2 - y_0}{2} \\nonumber \\end{align} $$ 1维: 2维: 相机: 可以和普通的3阶近似做对比. 实践证明, Catmull-Rom Splines方法效果更好, 因此大部分相机都使用该算法. General Case 对于像素填充问题, 我们只需要在两个点之间近似一个中间点就好了. 对于更一般的情况, 即在两个点之间近似一条曲线, 我们还有更多的方法. Half-Cosine 取余弦函数曲线的一半, 它具有如下的“优秀”特性: 平滑 两端点处导数为0. 因此不会生成比已有值更高/低的近似值 1维: 不过, 由于$\\cos(\\pi/4) = 1/2$, 对于求离散点的中点的情况, 其结果和线性近似一样. 所以对于相机像素的近似, 没必要用cos近似 Smoothstep 此外还可以将高阶多项式近似继续推广. Smoothstep只用于奇数阶多项式, 为了不产生过高/过低的估计值, 它假设段点处的高阶导数为0. 假设$x \\in [0,1], y \\in [0,1]$, 已知$f(0), f(1)$ 3阶Smoothstep就是3阶多项式, 并且其$f'(0) = f'(1) = 1$ 5阶Smoothstep就是5阶多项式, 并且其$f''(0) = f''(1) = f'(0) = f'(1) = 1$ 以此类推 Other applications of interpolation Damping motion 前面介绍的近似方法可以应用在很多场景. 考虑一个点的直线运动的近似, 只有起点和终点两个已知点. 我们可以用: 最近邻 如果信息足够多的话, 已知点足够多, 比如有7个, 最近邻可以“好看”一点 Half Cosine 3阶Smoothstep 5阶Smoothstep 7阶Smoothstep 可以看出不同的近似方法的差别 Ref https://datagenetics.com/blog/may12018/index.html","categories":[{"name":"Potpourri","slug":"Potpourri","permalink":"http://lyk-love.cn/categories/Potpourri/"}],"tags":[]},{"title":"Nvim Developing Environment","slug":"Nvim-Developing- Environment","date":"2022-08-17T22:52:46.000Z","updated":"2022-09-26T06:39:34.934Z","comments":true,"path":"2022/08/18/Nvim-Developing- Environment/","link":"","permalink":"http://lyk-love.cn/2022/08/18/Nvim-Developing-%20Environment/","excerpt":"Outline: Nvim Nvim Config LspInstall Dap Keybinds","text":"Outline: Nvim Nvim Config LspInstall Dap Keybinds Nvim Nvim vs VSCode Nvim和VSCode是两个主流的编辑器/IDE. VS Code’s use of Electron makes it available on all major platforms. This is how GitHub Codespaces or Gitpod can access to the entire VS Code ecosystem. See our interview with GitPod’s Chief Architect / Head of Engineering about how important this has been. 虽然微软团队对VSCode的Electron做了很多魔改, 极大地提升了性能, 但依然使得我个人感觉很不舒服 Nvim更快 Features Performance: 🚀 Very fast. Syntax highlighting: Neovim 0.5 now includes Treesitter and supports LSP, but still requires some config. Customizable: ✅ Large ecosystem of plugins and themes. Cross-platform: Linux ✅ Windows ✅ macOS ✅ Config Neovim设置全局配置文件,需要在/etc/profile添加： export VIM=/usr/share/nvimexport PATH=&quot;$PATH:$VIM/sysinit.vim&quot; 有可能会出现这种情况： 直接使用 nvim ... 打开文件时一切正常，配置也生效; 但当你使用 sudo nvim ... 打开文件时，配置文件并没有生效 出现这种情况的原因是：当你使用 sudo命令的时候，用户的身份切换了（默认是root）,此时你的环境变量也被重置了，系统当然就找不到你的配置文件。 解决的方案大致有2种： 使用 sudo -E nvim ... 打开文件 （最快速的方法，不过每次都需要加上 -E, 有点麻烦） 修改 sudo 的配置文件: /etc/sudoers(如果用nvim打开是空文件的话，可以试一下用vim 或者 visudo打开，后面就不细说了，超纲了) Nvim Config 使用ayamir的Nvim配置, 它集成了大量插件, 包括Nvim + LSP + Dap. 其配置文件位于~/.config/nvim/lua Config Install https://github.com/ayamir/nvimdots/wiki/Prerequisites # for neovim python modulepip install neovim --user# clonegit clone git@github.com:ayamir/nvimdots.git ~/.config/nvim# sync plugins(maybe need multiple times)nvim +PackerSync Structure init.lua is the kernel config file. It requires configuration in lua directory. lua directory contains 3 parts. core directory contains base configuration of neovim. keymap directory contains keybindings of plugins. modules directory contains 5 parts. completion directory contains code completion's configuration. editor directory contains plugins' configuration about editing. lang directory contains plugins' configuration about specific language. tools directory contains telescope and its extensions' configuration. ui directory contains plugins' configuration about ui. Usage 已经集成了nvim-lsp-installer, 只需手动安装对应语言的language server. lsp-installer默认安装的html LSP有问题, 需要手动安装新的: npm i -g vscode-html-languageserver-bin For nvim-treesitter, ensure installed parsers are configured here, you can add or remove parsers on your own demand. For efm-langserver, you need to install itself and format/lint tools manually. brew install clangbrew install eslintgo install golang.org/x/tools/...@latest You can use FormatToggle command to enable/disable format-on-save which is enabled by default. Also, you can disable format-on-save for specific workspace by add its absolute path in format_disabled_dirs.txt in new line. You can use :checkhealth command to check whether all modules works or not. Please ensure all of tools installed above can be found in PATH. You can configure these tools in your own habit like .eslintrc.js and .prettierrc.json. Recommanded way to set PATH variable is export it in your ~/.profile like this: export LOCAL_BIN=&quot;$HOME/.local/bin&quot;export GOROOT=&quot;/usr/lib/go&quot;export GOPATH=&quot;$HOME/go&quot;export GOBIN=&quot;$HOME/go/bin&quot;export HS_BIN=&quot;$HOME/.cabal/bin&quot;export NPM_BIN=&quot;$HOME/.npm-global/bin&quot;export KT_BIN=&quot;$HOME/.kotlin/bin&quot;export RS_BIN=&quot;$HOME/.cargo/bin&quot;export JAVA_HOME=&quot;/usr/lib/jvm/default&quot;export BENTO_BIN=&quot;$HOME/.local/share/bento4/bin&quot;export ROFI_BIN=&quot;$HOME/.config/rofi/bin&quot;export PATH=$LOCAL_BIN:$GOBIN:$HS_BIN:$NPM_BIN:$RS_BIN:$JAVA_HOME:$BENTO_BIN:$ROFI_BIN:$PATH Customize https://github.com/ayamir/nvimdots/wiki/Usage nvim添加插件, 需要用packaer.nvim LspInstall https://github.com/williamboman/nvim-lsp-installer#available-lsps Commands :LspInstallInfo - opens a graphical overview of your language servers :LspInstall [--sync] [server] ... - installs/reinstalls language servers. Runs in a blocking fashion if the --sync argument is passed (only recommended for scripting purposes). :LspUninstall [--sync] &lt;server&gt; ... - uninstalls language servers. Runs in a blocking fashion if the --sync argument is passed (only recommended for scripting purposes). :LspUninstallAll [--no-confirm] - uninstalls all language servers :LspInstallLog - opens the log file in a new tab window :LspPrintInstalled - prints all installed language servers Dap Official Repositoty nvim-dap is a Debug Adapter Protocol client, or &quot;debugger&quot;, or &quot;debug-frontend&quot;. With the help of a debug adapter it can: Launch an application to debug Attach to running applications to debug them Set breakpoints and step through code Inspect the state of the application Dap插件的配置代码位于: ~/.config/nvim/lua/modules/editor/plugins.lua, 我们不需要改动. ...editor[&quot;rcarriga/nvim-dap-ui&quot;] = &#123; opt = false, config = conf.dapui, requires = &#123; &#123; &quot;mfussenegger/nvim-dap&quot;, config = conf.dap &#125;, &#123; &quot;Pocco81/dap-buddy.nvim&quot;, opt = true, cmd = &#123; &quot;DIInstall&quot;, &quot;DIUninstall&quot;, &quot;DIList&quot; &#125;, commit = &quot;24923c3819a450a772bb8f675926d530e829665f&quot;, config = conf.dapinstall, &#125;, &#125;,&#125;... Structure You'll need to install and configure a debug adapter per language. See :help dap.txt the Debug-Adapter Installation wiki :help dap-adapter :help dap-configuration A debug adapter is a facilitator between nvim-dap (the client), and a language-specific debugger: DAP-Client ----- Debug Adapter ------- Debugger ------ Debugee(nvim-dap) | (per language) | (per language) (your app) | | | Implementation specific communication | Debug adapter and debugger could be the same process | Communication via the Debug Adapter Protocol adapter 示例: dap.adapters.python = &#123; type = &quot;executable&quot;, command = os.getenv(&quot;HOME&quot;) .. &quot;/.local/share/nvim/dapinstall/python/bin/python&quot;, args = &#123; &quot;-m&quot;, &quot;debugpy.adapter&quot; &#125;,&#125; configurations 示例: dap.configurations.python = &#123; &#123; -- The first three options are required by nvim-dap type = &quot;python&quot;, -- the type here established the link to the adapter definition: `dap.adapters.python` request = &quot;launch&quot;, name = &quot;Launch file&quot;, -- Options below are for debugpy, see https://github.com/microsoft/debugpy/wiki/Debug-configuration-settings for supported options program = &quot;$&#123;file&#125;&quot;, -- This configuration will launch the current file if used. pythonPath = function() -- debugpy supports launching an application with a different interpreter then the one used to launch debugpy itself. -- The code below looks for a `venv` or `.venv` folder in the current directly and uses the python within. -- You could adapt this - to for example use the `VIRTUAL_ENV` environment variable. local cwd = vim.fn.getcwd() if vim.fn.executable(cwd .. &quot;/venv/bin/python&quot;) == 1 then return cwd .. &quot;/venv/bin/python&quot; elseif vim.fn.executable(cwd .. &quot;/.venv/bin/python&quot;) == 1 then return cwd .. &quot;/.venv/bin/python&quot; else return &quot;/usr/bin/python&quot; end end, &#125;,&#125; Usage A typical debug flow consists of: Setting breakpoints via :lua require'dap'.toggle_breakpoint(). Launching debug sessions and resuming execution via :lua require'dap'.continue(). Stepping through code via :lua require'dap'.step_over() and :lua require'dap'.step_into(). Inspecting the state via the built-in REPL: :lua require'dap'.repl.open() or using the widget UI (:help dap-widgets) See :help dap.txt, :help dap-mapping and :help dap-api. Specific Daps 具体语言的Dap配置代码位于: ~/.config/nvim/lua/modules/editor/config.lua. 具体语言的Dap的安装文档: Doc ayamir默认配置好了许多Dap, 大部分情况下我们只需安装对应的依赖, 让ayamir的代码生效即可. C/C++/Rust (via lldb-vscode) ayamir用的就是lldb-vscode, 只需要自己自己下载lldb-vscode, 确保其能够在命令行调用, 再在ayamir的配置代码里把lldb-vscode的路径改为自己的路径即可: install: lldb-vscode是LLVM的一个包, 只需安装LLVM, 并配置到环境变量即可 找到LLVM的路径,将其bin目录配置到PATH, 这样该目录下的子命令( 包括lldb-vscode )就可以在命令行调用了. 这一步也是在安装LLVM的时候做的 lldb-vscode 在LLVM目录下, 太曲折了. 我们弄一个符号链接. 注意, 注意一般OS的软件都放在/usr/bin,但Mac不允许在/usr/bin随意操作( ln会报错&quot;Operation not permitted&quot; ), 因此我的软件链接放在/usr/local/bin ln -s /opt/homebrew/opt/llvm/bin/lldb-vscode /usr/bin/lldb-vscode 修改代码中的路径: # ~/.config/nvim/lua/modules/editor dap.adapters.lldb = &#123; type = &quot;executable&quot;, command = &quot;/usr/local/bin/lldb-vscode&quot;, # 改到自己的路径 name = &quot;lldb&quot;, &#125; Go 默认配置代码不需要改动, 只需安装依赖: Install delve: go install github.com/go-delve/delve/cmd/dlv@latest Install vscode-go: git clone https://github.com/golang/vscode-gocd vscode-gonpm installnpm run compile Keybinds https://github.com/ayamir/nvimdots/wiki/Keybindings","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://lyk-love.cn/tags/Linux/"},{"name":"Editor","slug":"Editor","permalink":"http://lyk-love.cn/tags/Editor/"}]},{"title":"Lossless and Lossy Comprehension","slug":"Lossless-and-Lossy-Comprehension","date":"2022-08-14T21:21:32.000Z","updated":"2022-09-26T06:39:34.933Z","comments":true,"path":"2022/08/15/Lossless-and-Lossy-Comprehension/","link":"","permalink":"http://lyk-love.cn/2022/08/15/Lossless-and-Lossy-Comprehension/","excerpt":"Outline: Lossless and Lossy Compression Comparison Between the two PNG AVIF WebP JPEG XL","text":"Outline: Lossless and Lossy Compression Comparison Between the two PNG AVIF WebP JPEG XL Lossless and Lossy Compression Lossless Comporession: 就是可逆的压缩, 压缩文件可以被解压为原文件 流行的无损压缩方法: 普通文件: ZIP, RAR Web中的JS和CSS文件: GZIP 图像: PNG 无损压缩适合:logos, screenshots, charts, and graphics. 因为这些图片都有着连续的色块, 可以用类似Run-Length Encoding (RLE)的算法来压缩. Lossy Comporession: 在压缩时会损失一些信息, 但是压缩效果比无损压缩好很多 流行的有损压缩方法: JPEG, lossy WebP 有损压缩适合处理照片. 因为损失一些细节并不能被用户看出. 但是它不适合图表( graphics )之类的有连续色块的图像, 在这些图像上损失一点细节都会被用户察觉 Comparison Between the two 如上“可回收图案”有大量连续色块. 用无损压缩PNG压缩到3KB, 其效果远好于无损压缩的JPEG的3KB, 甚至略好于JPEG的24KB. 这证明了对于Graphics等图片, 无损压缩的优势 PNG PNG ( Portable Network Graphics ): 最古老的图像格式之一. 基于DEFLATE compression algorithm, 并且有一个颜色筛选器. 可以把PNG的DEFLATE算法替换成Google的 Zopfli算法, 后者压缩效果更好, 但消耗时间更长 PNG format is supported by all browsers. 背景: 1996年发布, 旨在取代GIF. PNG拥有24-bit color (8 bits per channel) and an alpha channel. 而 GIF 只有24种颜色和一个透明度值 优化步骤 有许多PNG优化工具: PNGOUT, OptiPNG, and Rust-based OxiPNG I compressed PNG files using OxiPNG v5.0.0, using the maximum possible optimization level with the following command: oxipng /path/to/input/image.png --out /path/to/output/image.png --opt max --strip safe I also optimized the same set of images using the Zopfli compression which takes a really long time but results in even smaller files. oxipng /path/to/input/image.png --out /path/to/output/image.png --opt max --strip safe --zopfli PNG优化效果很一般, OXIPNG能优化12%左右. 开启--zopfli后优化时间大幅提升, 但最终效果也只有18%. 总的来说, PNG除了支持度比较高之外没什么优势. 要降低图片大小, 与其使用PMG + 优化, 不如直接使用其他图片格式( 无损的WebP和无损的JPEG XL ) AVIF AVIF( AV1 Image File Format ): 基于AV1 video codec( codec = encoder + decoder, 编解码器 )的新图像格式. 同时支持无损和有损压缩 AVIF is supported in recent versions of Google Chrome and can be enabled in Firefox by using a configuration flag. AVIF有许多advanced features: high bit depth, HDR ... 压缩步骤 压缩工具是avifenc from libavif With avifencv0.9.1I, used the lowest possible speed to get the best compression. avifenc --lossless --speed 0 /path/to/input/image.png /path/to/output/image.avif 无损和有损的AVIF压缩效果都很一般,不推荐 WebP WebP: Google在2008年发布了基于VP8 video codec的有损WebP; 又在2012年发布了不基于VP8 video codec的无损WebP0.3 While lossy WebP is limited to 4:2:0 Chroma subsampling which discards some color information, lossless WebP will retain all original image data. WebP is now supported by all major browsers except Safari 压缩步骤 For WebP, I used the official cwebp tool, version 1.2.0. For best compression, I defined quality of 100 and method 6. cwebp /path/to/input/image.png -o /path/to/output/image.webp -q 100 -m 6 -lossless Lossless WebP压缩效果暴打PMG, 其大小能降低大约41%, 压缩速度也很快. WebP的支持度也很高. 强烈推荐使用WebP JPEG XL JPEG XL: JPEG XL是JPEG( aka JPG )格式的后继, 它相当于两种图像格式的结合: Pik developed by Google + FUIF (Free Universal Image Format) developed by Cloudinary. 同时支持无损和有损压缩 JPEG XL is supported in Chrome and Firefox, but it’s not enabled by default. Support for the format must be enabled by using a feature flag. 压缩步骤 Pik使用的CLI tool是cjxl, 再开启--modular选项就可以添加FUIF格式 To compress JPEG XL files, I used the official cjxl tool, version 0.3.7_1. cjxl /path/to/input/image.png /path/to/output/image.jxl --modular --quality 100 --speed 9 -E 3 -E flag: means &quot;extra arguments&quot;, I used the value 3, which is recommended for best compression. Using this option shaved off a few kilobytes from the file size while the image took a bit longer to compress. JEPG XL的压缩效果略好于WebP, 二者详细对比见这篇文章","categories":[{"name":"Potpourri","slug":"Potpourri","permalink":"http://lyk-love.cn/categories/Potpourri/"}],"tags":[]},{"title":"My Programming Principles","slug":"My-Programming-Principles","date":"2022-08-14T15:47:42.000Z","updated":"2022-09-26T06:39:34.934Z","comments":true,"path":"2022/08/14/My-Programming-Principles/","link":"","permalink":"http://lyk-love.cn/2022/08/14/My-Programming-Principles/","excerpt":"Outline: 一些吐槽 Programming Language Testing Infrasturcture Open Source Dev","text":"Outline: 一些吐槽 Programming Language Testing Infrasturcture Open Source Dev 一些吐槽 硬件 我讨厌面向机器编程 在上世纪90年代, 1G RAM要1w多美金, 前人为了省钱而使用各种奇技淫巧情有可原. 问题在于这些“奇技淫巧”传承下来形成了一个庞大的代码库, 甚至被纳入了C/CPP开发的最佳实践. 现代硬件已经十分发达了. 再去搞这些东西不是搞笑么. 不过只要从事该领域总会遇到这些东西, 惹不起我还躲不起么. 所以我死也不干硬件/OS/高性能领域. 也不会在这方面写任何项目 语言 C/CPP C/CPP代码是我见过的最丑陋最离谱的代码: Macro编程: Macro编程在我看来根本不能算编程, Macro编程就是一坨大便. 结构体操作( in C): C的结构体可以有很多离奇的内存操作. 我指的不是内存对奇, 而是大量的指针运算, 这玩意儿结合Macro之后, 在语法上就是乱七八糟的一堆. 有些人觉得很秀, 我只觉得不安全而且恶心 元编程: 模板根本不能调试, 而且我也不知道元编程有什么用. 诡异的语法: C/CPP的Technical Debt大到难以想象. 许多上古代码库都在语法边缘疯狂试探, 鬼知道后代编译器能不能兼容. Linux内核代码至今无法用clang编译, 就是因为里面有很离谱的语法. 当然gcc和linux有着阴险的交易, 居然兼容了这种语法, 这更让我恶心了 上述问题在几乎所有大型C/CPP库都普遍存在, 让我十分厌恶. 当然C/CPP本身还有很多丑陋之处: C/CPP生态惨不忍睹,连个包管理工具都没有,开发体验很迷. 一大堆Technical Debt; CPP backward compatibility又保留了大量上古语法, 而且越积越多. 这些技术债和上古语法混杂在一起, 让人在开发时没有最佳实践. 不知道用哪些库, 也不知道用哪些特性, 甚至异常都不知道该不该用 其实我很好奇CPP真的能向后兼容C么, 据我所知G++对于C的语法比GCC要严格, 某些GCC能过的C代码, G++根本编译不过. 那我用G++编译一个C++混着上述“G++编译不过的C代码”的文件, 应该也不能成功啊 CPP发展很慢,委员会操作成谜. Google锐评为&quot;The language’s evolution is also stymied by a bureaucratic committee process&quot; memory safety: 其实我不反感CPP的指针操作, 相反我很喜欢它, 因为十分符合直觉. 指针只是一种机制, 用不用得好取决于个人. 在我看来,只要编程有最佳实践, 照着实践原则来, 就不会发生这种问题. 然而CPP刚好又没有最佳实践, 大家想怎么写怎么写, 水平参差不齐. 这才是memory safety problem的根源 JVM语言 JAVA: 其实我觉得JAVA很简洁. 因为语法很简单. 当然它肯定不够现代, 不过对我来讲, JAVA == SpringBoot == CURD, 就是一个干活的工具罢了. 能用并且可靠就行 Scala: 用过, 感觉很厉害, 但是又感觉没啥用. 语法新一点又不能当饭吃. 毕竟它的对手是语法上没有什么大问题的JAVA, Scala的新语法不是什么杀手级的特性 Kotlin: 我的评价是呵呵. Kotlin这辈子也就这样了. 不过哪天Pivoto大发善心把SpringBoot移植到Kotlin,我还是很愿意试一下的 Go/Carbon/Rust Go: 语法让人感觉很丑. 这可能是我们看C like的代码太多, 养成了习惯的缘故. 此外Go倒是没什么缺点. Go最大的优势是许多云原生基础设施都是Go写的, 这刚好是我感兴趣的领域之一 Carbon: 没发布,但我希望能做好 Rust: 这个东西.... 就我个人来说, 我希望它被Carbon取代. Rust很伟大, 很新. 但我认为它和Carbon只能活一个. 毕竟二者定位差的不是很多. 脚本语言 Python: 写脚本没有缺点. 当然一些涉及底层的脚本还得用Go写. Python最大的问题是开发者, 许多论文的代码都没有把依赖写清楚. Python进行重量级开发表现惨不忍睹, 但由于没人用Python进行重量级开发, 所以这个问题不存在. Bash: 垃圾. 连 [[]]和[[[]]]都有区别 Ts/JS: 我会在后文详细吐槽这二者. 有人说Ts/Js不只是脚本语言,而是个前后端通用语言. 我觉得和笑话没什么分别 OS 我讨厌OS开发,原因有: 如前所述, OS和硬件关系很密切, 而且C/CPP的丑陋代码在Linux中广泛存在. 我忍受不了这样的开发 Linux已经是上世纪的古董了. 本质上我们还处于计算机科学的石器时代, 我们使用的是古老的Linux, 命令行工具使用的是古老的Unix CommandLine, shell是目前看来特别垃圾的bash, bash script的语法现在看来简直是惨不忍睹, 基础设施的编程语言是古老且臃肿且丑陋且缺乏统一的现代设计的C/CPP. 总而言之, 现代CS的大部分问题都不是人的问题, 是CS世界发展缓慢的问题. 写Shell很痛苦, 不是因为程序员不行, 是Shell本身就是个该被淘汰的垃圾语言. C/CPP, OS 等等同理. 是世界的原始, 造成了人的痛苦. 对此我的对策是慢慢等. 等新的编程语言接替C/CPP, 等到有人用新的编程语言来开发包括OS在内的一切基础设施, 让基础设施开发变得体验更好. 在这个美好的未来来临之前, 我不想参与任何的硬件和OS开发 测试 提高代码质量唯一可行的手段不是写测试，而是反复的提炼自己的思维，写简单清晰的代码 “测试能帮助后来人”的误区, 测试对于后来人的作用，并不是你有些人想象的那么大。创造清晰的代码才是解决这个问题的关键。 最优雅的程序往往也是最高效的 Programming Language 综上所述, 我自己的项目开发都需要选择新的语言,保证代码的简洁优雅和现代. 基础设施: Go. 首先,我不干硬件和OS开发, 但有可能做一些基础设施(编译器, 服务器, JVM, Docker, 网络协议)的开发, 不可避免地会接触一点底层和Linux. 因此我需要的是一个和硬件没有关系的,又有一定底层能力的, 语法和工具链足够现代的语言, 我选择Go. OS/硬件: Carbon. 虽然嘴上说不干, 但困难并不会随着我的逃避而消失. 首先C是不可能的, 我需要的是一个在功能上完美替代CPP的现代语言. 我选择Carbon. 当然Carbon目前还没发布, 但我目前也没干OS/硬件, 等我干这个领域的时候, 希望Carbon能成长起来吧(有人说Carbon最终只会是另一个Kotlin, 但我巴不得Carbon能彻底取代CPP) 算法: CPP. 算法不需要复杂的语法特性, 有指针和容器库就够了. 虽然CPP很垃圾,但那是建立在庞大混乱的生态和稀奇古怪的语法上. 自己写算法不需要面对这些问题. 就把CPP当作C + OOP + STL就够了 小脚本: Python. 随时随地写点小脚本. 自己写脚本没有性能和平台要求, 一切以简介优雅为主. 没事儿干谁会写Shell AI算法: Python 区块链: 这个都用DSL的, solidity之类的. 目前我没啥时间做区块链,先搁置吧 APP: 后端Java: 自己做APP,核心关注点肯定是功能, 其次就是前端. 其余部分(尤其是后端)怎么简单怎么来就好, 反正全是CURD, 也学不到东西. SpringBoot方便省事, 甚至有低代码工具可以让我不写后端的话我也会欣然接受. 我也对Ruby做过调研,据说这玩意儿又老又笨重, 所以不考虑了. Python Flask和FastAPI个人体验就是个笑话. Python根本没法写业务代码. 动态弱类型的代码安全性惨不忍睹. 写来写去最后Python后端还是写成了Java的样子 前端: 最好的用户体验需要Hybrid开发, 即安卓原生 + 框架了. 我没有iOS设备, 所以暂时不考虑iOS原生 框架React/React Native: 我又不是职业前端,我不想在前端浩如烟海的生态中浪费时间, 选择够自己用的工具集就行了. 对于魔术代码, 只有我理解了其底层实现我才用得放心, 但again, 我又不是职业前端, 没那么多时间看底层代码, 所以Vue我是不考虑的. React轮子比较多,写法也比较符合后端直觉. BTW, Vue的一码多端我好像没听说有很好的方案. 只知道一个uniapp, 它被我的同学评价为一坨垃圾 对于一码多端还有一个选择是Flutter, 不过我都会React了,暂时没必要学它 语言Ts: Ts.... 我不想评价, 在我看来它就是一坨冒着热气的大便, 但是Js连大便都不如, 我没得选择. 虽然Ts宣传JS + 类型系统, 不过一门新语言实际上肯定会带来许多新特性, 当然了, 其中唯一值得我说的就是类型系统. 我到现在也没搞明白TS类型系统的设计思想是啥. 感觉非常非常复杂, Interface,Type和Class区别是什么. 类型窄化又是干嘛的, 为啥要使用结构等价来判断类型, 为啥会有any这种奇怪的类型. 总之,要么是Ts设计太现代, 我经验太浅不足以充分认知; 要么就是Ts设计者脑子出问题了, 纯炫技. 而且Ts开发非常非常难受, 类型后置+大量箭头表达式+范型使得代码丑陋至极. 维护起来特别恶心 CSS有很多选择, LESS和Tailwind. 这个取决于我要的样式效果复不复杂. Testing 对于企业来讲, 测试是必须的, 甚至TDD也情有可原. 但测试本质上是用来防止人多和人蠢的 , 我自己开发还写个锤子测试. Infrasturcture 目前我掌握的Docker, K8S/Docker Swarm, Jenkins配合工作的挺好的. 不需要更改 Open Source Dev 开源项目最重要的是兴趣/需求, 其次是规划. 需求这个东西, 如果有的话,早就去创业了. 大部分情况下都是没有什么重大需求的, 所以就日常写一些有趣味的小项目就好了. 对于规划来讲,很多开源库文档里面第一件事就是Define the vision. 毕竟开源项目成功靠的是坚持, 是积少成多. 没有清晰可靠的规划是写不完一个项目的. Some Experience 对于一些小项目, 在出现了很奇怪复杂的错误时, 与其花时间Debug, 不如直接重开, 关键文件复制进来就完事儿了. Some Quotes 研究物理的人如果遇到不理解的事情，总是可以责怪上帝，世界这么复杂不是你的错。但是如果你的程序有问题，那就找不到替罪羊了。0就是0，1就是1，就是你把它搞砸了。 当没有计算机的时候，编程不是问题。当有了比较弱的计算机时，编程成了中等程度的问题。现在我们有了巨大的计算机，编程就成了巨大的问题。 为什么这么少的人追求优雅？这就是现实。如果说优雅也有缺点的话，那就是你需要艰巨的工作才能得到它，需要良好的教育才能欣赏它。 工程师和设计师都有巨人主义情节，喜欢构建庞然大物，对很多人来说这是一种永无止境的诱惑. 副作用这种东西，其实是根本的，有用的。对于这一点，我喜欢跟人这样讲：在计算机和电子线路最开头发明的时候，所有的线路都是“纯”的，因为逻辑门和导线没有任何记忆数据的能力。后来有人发明了触发器（flip-flop），才有了所谓“副作用”。是副作用让我们可以存储中间数据，从而不需要把所有数据都通过不同的导线传输到需要的地方。没有副作用的语言，就像一个没有无线电，没有光的世界，所有的数据都必须通过实在的导线传递，这许多纷繁的电缆，必须被正确的连接和组织，才能达到需要的效果。我们为什么喜欢 WiFi，4G 网，Bluetooth，这也就是为什么一个语言不应该是“纯”的。 开源项目可以塑造一个人. 很多人没有意识到这一点，一般都是作者塑造作品，但是有些作品可以塑造作者. 你做着做着，变成了跟原来不一样的人 真的想做一个项目，那就开始得越快越好。早点做出有价值的成果，你会吸引到客户；如果做不出来，你就可以早点放弃，节省自己的时间，并了解哪里行不通 JS笑话 JavaScript的优点是可以写任何东西, 缺点是你真的会用它去写这些东西. JS非常糟糕，以至于实际上所有JavaScript程序员都必须使用linter (例如 JSLint或ESLint)。尽管近年来ECMAScrint经历了许多许多改进，但委员会无法完全消除JavaScript最严重的错误。所以问问自已这个问题:还有什么其他现代编程语言如此糟糕 以至于为了安全起见最推荐使用linter? A: I hate Go, its ugly! B: You want to talk about ugly? Look at JavaScript!","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://lyk-love.cn/categories/Software-Engineering/"}],"tags":[]},{"title":"Resolution on Screen","slug":"Resolution-on-Screen","date":"2022-08-13T20:35:23.000Z","updated":"2022-09-26T06:39:34.939Z","comments":true,"path":"2022/08/14/Resolution-on-Screen/","link":"","permalink":"http://lyk-love.cn/2022/08/14/Resolution-on-Screen/","excerpt":"Outline: Pixel Resolution Pixel in Other Domains","text":"Outline: Pixel Resolution Pixel in Other Domains Pixel Pixel( &quot;px&quot; for short ): 像素，简称px. pixel是构成digtial picture的基本单位 Mega Pixel( &quot;MP&quot; for short ): 百万像素. 比如IPhone13的单相机像素就是12MP. 在计算机科学中, Mega一般表示$2^{20}$; 不过这里的Mega是$10^{6}$ 一般用 [width] x [height] px来衡量图片和屏幕的清晰度, 比如下面的50 x 50 px图片一小格代表一个pixel; 比如常见的1920 x 1080 px屏幕. 也可用总像素数表示, 50 x 50px也就是2500 px. “总像素数”的表示方式一般用在相机/屏幕上,图片描述都用[width] x [height] 形式 Resolution 图像的Resolution( 解析度 or 清晰度 )是比较模糊的概念, 对于图像本身而言, 就是像素的密度, 单位为PPI; 而对于打印出的图像而言, 是打印时的墨点密度, 单位为DPI. DPI和PPI很容易被混淆, 其实二者是不同的 对于设备而言,有时候直接把总像素数称为清晰度, 所以说到”清晰度“时,一定要注意指的是px, PPI还是DPI PPI Pixels Per Inch( &quot;PPI&quot; for short ): 一英寸内的像素数量, PPI用来表示像素密度 相同尺寸下, 不同像素数的图片, 清晰度差距是很大的: 上面两张图的图片尺寸一样, 左图PPI=16, 右图PPI=32 只要屏幕大于300 PPI, 人眼就不能分辨. 问题在于不仅是屏幕大于300PPI, 屏幕内显示的图片也要跟上这个解析度 DPI Dots Per Inch( &quot;DPI&quot; for short ): 每英寸的墨点数, DPI用来表示打印机的墨点密度. 对于喷墨式打印机, DPI取决于其X, Y轴的马达步进速度. 所以打印机的DPI参数一般是[ X ], [Y] dpi. 你可以见到600dpi的打印机, 这意味着X和Y轴的DPI都是600 对于非喷墨的打印机,当然就不存在“墨点”,但它们的成像密度也都用DPI表示 不同媒体对DPI的“Dot”的定义不一样, Dot本来是成像设备的墨点, 和图片没有关系. 但是图片的参数也一般会有DPI, 图片的&quot;Dot&quot;就是Pixel, 因此DPI就是PPI. 而当图片被传到打印机, &quot;Dot&quot;又被解读为墨点. 可以看到,DPI在这里就是PPI( pixels/inch ) 手机屏幕作为成像设备, 其“Dot”就是屏幕的物理像素点, 所以手机屏幕的PPI和DPI也是一回事 在打印时,DPI一般首先取决于纸张和打印机,. 假设有400 * 400 dpi的打印机, 要求输出图像宽度为A4纸宽度( = 8.268 inch = 21 cm ), 则图像在X方向的像素数为: $$ X \\ \\mathrm{px} / (400 \\ \\mathrm{px/inch}) = 8.268 \\ \\mathrm{inch} \\ X = 3307 \\ \\mathrm{px} $$ 假设图片是正方形的, 也就是Y方向像素数也是3307 px, 这张图片的像素数就是3,307 * 3,307 ≈ 10.9 MP, 需要千万像素的相机才能拍出( 比如iPhone13的12MP摄像头). 以此类推, 一张正方形图片,以400DPI按宽度满版打印, 需要的总像素数如下: Size Millimeters Inches Pixels A0 size 841 x 1189 mm 33.1 x 46.8 in 175MP A1 size 594 x 841 mm 23.4 x 33.1 in 87.6MP A2 size 420 x 594 mm 16.5 x 23.4 in 43.6MP A3 size 297 x 420 mm 11.7 x 16.5 in 21.9MP A4 size 210 x 297 mm 8.268 x 11.7 in 10.9MP 现在最新的专业相机,其像素数也就是60MP, 入门级的一般就20MP, 在400DPI下连A3纸大小的图片都打印不了. 事实上,大部分图片的人眼观看距离都在30cm以上, 对于贴在墙上的海报, 那距离就更远了. 在这么远的距离下, 没有必要维持很高的DPI, 所以一般DPI都小于400 DPI &amp;&amp; PPI Pixel in Other Domains device 屏幕像素 屏幕对角线尺寸 PPI CSS Pixel DPR iPhone SE2 750 * 1134 px 4.7 inch 326 375 * 667 px @2x iPhone 12 Pro Max 1284 * 2778 px 6.68 inch 458 428 * 926 px @3x iPhone 13 Pro Max 2778 * 1284 px 6.7 inch 458 926 * 428 px @3x 移动设备的屏幕分辨率普遍比桌面屏幕好, iPhone13的分辨率1170 x 2532已经大于桌面屏幕的1920 x 1080了. 而iPhone13的宽度只有8cm! 移动设备的屏幕较小, 但分辨率很高, CSS media query很容易把1170 x 2532 px认为是桌面屏幕. 为此, 对于移动设备, 我们使用的是CSS Pixel, CSS pixel in Web CSS Pixel CSS Pixel: Web领域的Pixel, 可以认为是虚拟像素 or 标准像素. 它与真正的屏幕Pixel成比例关系( DPR ) 经过DPR的转化, iPhone13的CSS Pixel只有926 x 428 px, CSS media query会将其判定为移动设备 事实上DPR和CSS Pixel的转换都是隐式发生的. CSS media query和JS都只能查询到CSS Pixel. 因此我们开发时不需要考虑二者的转化, 只需要面对CSS Pixel &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot;&gt; DPR Device Pixel Ratio( DPR ): 就是CSS Pixel和屏幕Pixel的比值. 别名有: Dots Per PiXel( DPPX ) : 这里的“Dot”就是屏幕像素, 而&quot;Pixel&quot;是CSS Pixel or pixel density 不同设备的屏幕大小不同, 因此DPR也不同, 比如iPhone SE2的DPR是2, 那么每2个屏幕Pixel对应一个CSS Pixel; iPhone13的DPR是3,每三个屏幕Pixel对应一个CSS Pixel; 因此, 对于100 x 100 px( 这里是CSS Pixel ) 的图片, 在iPhone SE2被放大为200 x 200 屏幕px, 在iPhone13被放大为300 x 300 屏幕px, 清晰度会受影响. 因此有必要为不同DPR的设备提供不同清晰度的图像 pt &amp;&amp; dp in APP 对于APP( 或者说客户端 ), 不同平台使用的Pixel分别是: ios: point( “iOS pt” for short) Android: device-independent pixel( “dp” for short ) 二者其实都是 屏幕Pixel / DPR DPR and PPI/DPI 我们知道手机的PPI( or DPI, 二者对于手机是一个东西 )是屏幕的像素密度, 而DPR是屏幕像素与 虚拟/标准像素( CSS Pixel or iOS pt or dp ) 的比. 对于手机, Web( 使用CSS Pixel )的标准PPI是160; APP的标准依平台而定. Android( 使用dp )的标准PPI是160, iOS( 使用iOS pt )的标准PPI是163( 这是iPhone13的PPI ). 那么: 1 CSS pixel ≈ 1/160 inch 1 dp ≈ 1/160 inch 1 iOS pt ≈ 1/163 inch 以CSS pixel为例, 其标准PPI是160, 那么对于PPI = 320的屏幕, 其DPR就是 320 / 160 = 2. 即每两个屏幕像素被转化成一个虚拟/标准像素 以此类推,对于iOS, 以iPhone13为例, 其PPI = 458, 则其DPR就是 458 / 163 = 3","categories":[{"name":"Potpourri","slug":"Potpourri","permalink":"http://lyk-love.cn/categories/Potpourri/"}],"tags":[]},{"title":"OLAP","slug":"OLAP","date":"2022-08-12T22:25:50.000Z","updated":"2022-09-26T06:39:34.935Z","comments":true,"path":"2022/08/13/OLAP/","link":"","permalink":"http://lyk-love.cn/2022/08/13/OLAP/","excerpt":"Outline: OLAP Database Columnar Database ETL vs ELT","text":"Outline: OLAP Database Columnar Database ETL vs ELT OLAP Database OLAP数据库是数据分析领域过去几十年内常用的数据仓库，近年来已经被列数据库逐步取代。 OLTP VS OLAP OLTP: using a database to run your business 对OLTP 数据库的查询形如: &quot;one Honda Civic by Jane Doe in the London branch on the 1st of January, 2020&quot;; OLAP: using a database to understand your business 对OLAP数据库的查询形如: &quot;give me the total sales of green Honda Civics in the UK for the past 6 months&quot; 可以看到，OLAP的查询比OLTP复杂的多。 关系型数据库可以胜任OLTP， 但是对于每个OLTP查询，都可能要进行大量连表，效率很低。 而在BI( Bussiness Intelligence )中, 有着大量的OLAP查询 可以看到，每一次子查询就是查询某个&quot;维度(dimention)&quot;。 比如，如果要查“找出某天某地生产的所有产品”， 那么对于“时间”、“地点”、“产品”这三个维度，就要进行三次连表，开销是不可接受的 What is OLAP DataBase OLAP 数据库把数据按维度组织成高维数组。 一组数据就形成了一个&quot;cube&quot;， 进行OLAP查询时复杂度只有O(1) 假如OLAP cube内存占用过大，很多OLAP数据库只会加载cube的一部分进内存，其余的放在磁盘 Problems OLAP数据库的问题有： 每个cube只能适应一小部分的OLAP查询。 假如出现了新查询或者新的类型的数据，需要新建Cube. 比如说，对于上图的Cube，如果数据分析师想要进一步分析“省份”数据，而原有Cube没有“省份”这个维度，就得让数据工程师新建一个包含了这个维度的Cube 要把数据仓库中的数据转化为OLAP Cube, 需要对数据仓库进行一定的建模( 需要满足一些稀奇古怪的范式要求， 比如 Kimball dimensional modeling, Inmon-style entity-relationship modeling, or data vault modeling )； 并且采用大量的pipeline， 比如ETL (extract-transform-load) 数据分析师的工作会严重依赖数据工程师。 因为每次有新的分析，都需要数据工程师来创建新Cube。 工作效率极低。 同时数据工程师还得维护pipeline。 Fall of OLAP OLAP Cube的问题来源于它要牺牲劳动力来节省计算和存储资源。近年来，随着CPU性能提升，内存价格降低，以及列数据库的出现， OLAP已经逐步被列数据库所取代。 列数据库可以胜任OLAP workload，并且性能更高。 这样做的好处有: 直接使用原数据，不做任何转化。 也就不需要那么多的pipeline和配套的工具，也不用雇那么多数据工程师了 既然不需要转化成OLAP Cube， 那么数据仓库设计时也不必遵循奇奇怪怪的范式 数据分析师不需要依赖数据工程师来创建cube了， 可以自己直接用SQL查询 Columnar Database What is Columnar Database 如图，列数据库把数据按列存储，特点有： 对于&quot;Read only&quot;的OLAP workload(也就是有好多维度的只读查询)，这种存储方式是完美契合的，每个维度查一列就行了 同时也可以看到列数据库的缺点 --- 修改既有数据的效率非常低，因为要把该行的每一列进行一次删除。 好在OLAP workload大部分是Read Only的， 事实上很多OLAP查询根本就不允许修改原有数据 由于数据按列存储，而同一列的数据一般而言格式相似，因此列数据库比行数据库更容易压缩，减少内存占用 因为列数据库可以更大程度地压缩，也就能腾出更多的内存来给其他进程使用。 列数据库的排序等操作因此更快 ETL vs ELT ETL：Extract, Transform, and Load. 即将源数据库( OLTP database )的数据“提取( extract )”到专门的数据处理服务器， 后者将数据“转换( transform )”为符合要求的形式，最终结果被加载到最终的数据仓库( data warehouse ) ETL和OLAP是紧密联系的。 ETL的目的数据仓库的数据已经是“提取、转换”之后的数据了。 因此ETL具有和OLAP相同的缺点： 每次新查询都要重新ETL 需要专人来进行ETL 最糟的是，海量数据的ETL是很耗时的。 那应该在什么时候进行ETL呢？ 你可能认为可以在晚上ETL，白天工人上班的时候直接拿结果来用。 但是如果是跨国公司，某个地方的晚上是另一个地方的白天，就没法做到ETL的同步，数据也就没法同步 ELT： Extract, Load, and Transform. 随着硬件的发展，现在普遍将数据直接传输到目标数据仓库，然后数据分析师手动进行数据处理( Transform )","categories":[{"name":"Potpourri","slug":"Potpourri","permalink":"http://lyk-love.cn/categories/Potpourri/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://lyk-love.cn/tags/Database/"}]},{"title":"Computer I/O Device","slug":"Computer-I:O-Device","date":"2022-08-06T13:11:23.000Z","updated":"2022-09-26T06:39:34.928Z","comments":true,"path":"2022/08/06/Computer-I:O-Device/","link":"","permalink":"http://lyk-love.cn/2022/08/06/Computer-I:O-Device/","excerpt":"Outline: Device Controller Device Driver Driver Devices Transport Buses Transfer Protocals Physical Interfaces","text":"Outline: Device Controller Device Driver Driver Devices Transport Buses Transfer Protocals Physical Interfaces Device Controller OS不会直接处理外部设备, OS处理的是device controller controller是一个或一组芯片，位于南桥，向OS提供一个更“简单”（相对直接控制设备而言）的接口 比如，OS可能命令“read sector 11,206 from disk 2”。controller需要将将线性的扇区号映射为实际的cylinder, sector, and head，考虑到外侧柱面比内侧多、坏的删区要被重映射为其它山区等等，这个映射会很复杂。 然后controller要决定磁臂停在哪个扇区。 It has to wait until the proper sector has rotated under the head and then start reading and storing the bits as they come off the drive, removing the preamble and computing the checksum. Finally, it has to assemble the incoming bits into words and store them in memory. To do all this work, controllers often contain small embedded computers that are programmed to do their work. Device Register controller不和device直接交互, 而是和controller的寄存器们( 称为device register )交互. driver从OS得到指令，将其翻译后写入device registers。 For example, a minimal disk controller might have registers for specifying the disk address, memory address, sector count, and direction (read or write). To activate the controller, the driver gets a command from the operating system, then translates it into the appropriate values to write into the device registers. The collection of all the device registers forms the I/O port space, a subject we will come back to in Chap. 5. device registers会被映射到OS的地址空间或者一个特殊的I/O port space 前者不需要特殊的I/O instructions，可以被像普通内存数据一样读写，但是消耗了地址空间（ device registers的地址无法被其他程序使用，因此是安全的 ） 后者不消耗地址空间（每个寄存器被映射为一个port address）但需要额外的instructions Driver 在OS kernel层面, 使用device driver( 设备驱动 )来控制各种controller driver talks to a controller, giving it commands and accepting responses 每个controller厂商需要为每个操作系统提供一个driver driver一般在kernel mode工作 由于各种driver互不相同，每个计算机需要的都不一样. 因此driver一般都以内核模块的形式安装到OS, 很少直接编译到内核中 事实上也有运行在user mode的driver： only very few current systems, such as MINIX 3, run all drivers in user space. Drivers in user space must be allowed to access the device in a controlled way, which is not straightforward Devices 计算机物理设备及其接口, 都被对应的device controller处理, OS通过driver来处理controller, 不需要关心真正的物理硬件 既然各种设备都通过物理接口, 由总线连接到计算机. 我们只需要关心总线以及数据传输协议就行了. 并且我还简要介绍了连接总线的物理接口规范. 注意到, 传输协议在软件层对应着不同的driver Transport Buses ref: 讲总线的文章 存储领域的总线ATA/SATA/PCIe/SCSI/SAS/FC， 对应同名的物理接口, 比如SATA接口, PCIe接口, SCSI接口.... 此外，还有我们很熟悉的USB和雷电总线 注意, 有些人用&quot;通道&quot;来指总线 ATA AT A（AT Attachment）：总的来说，ATA是一个总线技术，它并行传输数据, 总线位宽为 16bits. 因此ATA也称为PATA(Parallel ATA, 并行ATA ) In case you are curious what AT stands for, this was IBM’s second generation &quot;Personal Computer Advanced Technology&quot; built around the then-extremely-potent 6-MHz 80286 processor that the company introduced in 1984. ATA对应的物理接口就是ATA( or PATA )， 现在都没人用了 SATA SATA ( Serial ATA, 串行ATA )对PATA进行了改进， 故名思义就是使用了串行, 规避了并行总线在高速下的串扰和同步问题 在2000 年发明 SATA 只有 4 根线，分别为：发送数据线，接收数据线，电源线，地线。 SATA 有两个标准，分别为 SATA 和 SATA II. SATA 的有效带宽为 150MB/s ，数据速率为 1.5Gbps( 传输的数据经过了8B/10B 变换， 150MB/s*10=1.5Gbps) ， SATA II 的有效带宽为 300MB/s ，数据速率为 3Gbps SATA配套的传输协议是AHCI, SATA和AHCI是HDD时代的产物, ATA总线的传输速率上限也就是几百MB, 所以SATA + AHCI在HDD时代是够用的, 虽然目前主流SSD依然支持它们, 但其实已经不够用了 PCIe PCIe( Peripheral Component Interconnect Express ): 是一个比SATA快好几倍的总线, 2004年发明 PCIe是PCI的后继, 而PCI总线取代了更古老的ISA (Industry Standard Architecture)总线 2004年PCIe刚发明时，流行shared bus architecture，许多设备用一条线传输数据，需要一个arbiter来调度 PCIe可以连接各种设备, 包括显卡、SSD（需要M.2接口或者PCIe接口）、无线网卡、有线网卡、声卡、视频采集卡、PCIe转接M.2接口、PCIe转接USB接口、PCIe转接Tpye-C接口 PCI使用parallel bus architecture，即将每个数据字分多条线传输。 比如32-bit数据需要32根并行的线 PCIe使用serial bus architecture，把数据包装成一个message， 点对点传输（一次连接称为一个lane）。 PICe也支持并行传输，按lane( 通道 )的个数可分为 x1 x2 x4 x8 x16 x32（最大可支持32个通道） 如果PCI e x1接口的网卡插x16的插槽， 只占用一条lane 微星的B450主板支持的就是PCI-E 3.0, B550支持PCI-E 4.0 现在的PCIe 5.0 已经支持132GB/s 了 此外，PCIe还对应着同名的传输协议( PCIe ) PCIe性能： PCIe 与 PCI 的区别: category speed transport model hardware PCI PCI的工作频率分为33MHz和66MHz，最大吞吐率 266MB/s PCI 是并行数据传输，一次传输4字节/8字节，半双工 传输PCI信号的是普通电平 PCIe PCIe 1.0 x1 的吞吐率就达到了250MB/s PCIe是串行数据传输，全双工. 而且是点对点传输(当然也可以并行) 传输PCIe信号的是差分电平 SCSI SCSI( Small Computer System Interfae, 小型计算机系统专用接口 ) : 一种连接主机和外围设备的接口，支持包括硬盘、光驱及扫描仪在内的多种设备。SCSI 总线是一种并行总线，其优点是适应面广，性能高；缺点是价格昂贵，安装复杂 SAS SAS( Serial Attached SCSI, 串行连接SCSI) : 串行化的SCSI. 和SATA总线类似, 都采用串行技术以获得更高的传输速度. 一般用于企业级硬盘 SAS 的接口技术可以向下兼容SATA 。具体来说，二者的兼容性主要体现在物理层和协议层的兼容: 在物理层，SAS 接口和SATA 接口完全兼容，SATA 硬盘可以直接使用在SAS 的环境中， 从接口标准上而言，SATA 是SAS 的一个子标准，因此SAS 控制器可以直接操控SATA 硬盘，但是SAS 却不能直接使用在SATA 的环境中，因为SATA 控制器并不能对SAS 硬盘进行控制 FC FC: 用于光纤 USB USB (Universal Serial Bus, 通用串行总线) : 用于连接计算机的外部设备, 它们的I/O通常很慢 此外，USB还对应着同名的传输协议( USB ) Thunder bolt 大名鼎鼎的雷电总线，由Intel研发，用于取代电脑上奇奇怪怪的各种总线( SCSI, SATA, USB, PCIe ... )，如果成功的话，以后电脑里只有一种总线了 拿Macbook 2021来说, 它拥有3个Type-C雷电4接口, 这里的 和USB一样，对应着同名的传输协议( Thunder bolt ). 我们说雷电的时候，既指总线也指协议，因为雷电总线好像只支持雷电协议 Transfer Protocals 有了总线和对应的物理接口，还需要抽象的数据传输协议规范（ 就和计算机网络的协议一样 ）, 在软件层面，针对协议来编写驱动，来实现数据I/O AHCI AHCI( Serial ATA Advanced Host Controller Interface, 串行ATA高级主控接口/高级主机控制器接 ): 适配SATA和PCIe( 主要是适配SATA )， 是HDD时代的通信标准，速度比较慢 NVMe NVMe( Non-Volatile Memory express, 非易失性内存主机控制器接口规范 )， 适配SATA和PCIe接口. 简而言之NVMe比AHCI快好几倍，是SSD时代的传输协议 NVMe建立在M.2接口上, 且使用PCIe总线 NVMe1.X只支持SSD, 从2.X开始也支持HDD了. 不过话说回来，现在谁还用HDD呢... PCIe 这个协议没有自己的名字，因为用于PCIe总线，就用PCIe来命名了... USB 和PCIe协议一样, 没有自己的名字... USB协议目前发展到USB4 Thunder bolt 和USB协议一样, 没有自己的名字... 一般称为雷电协议, 目前发展到雷电4( Thunderbolt™4 ) 雷电4相比USB4, 带宽更高, 兼容的传输协议也更多 雷电4适配了PCIe协议和Displayport协议 Physical Interfaces 总线需要连接到主板的物理接口上, 这里的“接口”指的是物理尺寸和形状, 内部的电路逻辑不做讨论 接口, 总线和协议的关系 SATA Interface SATA接口只能接SATA总线. 这个接口比较悲催，没有自己的名字, 我们用它适配的总线( SATA )称呼它 SATA2和SATA3总线对应不同的SATA2接口和SATA3接口 PCIe Interface PCIe接口只能接PCIe总线. 这个接口和SATA接口一样, 也没有自己的名字.... M.2 Interface M.2( 以前叫 NGFF( Next Generation Form Factor )，现在统一叫 M.2 了) 接口可以接SATA和 PCIe总线 M.2接口有两种类型：Socket 2和Socket 3 Socket2只能接SATA总线, 可以等价于SATA接口 Socket3兼容Socket2, 并且能接PCIe总线 Type A/B/C 常见的Type A/B/C也是一种物理接口 USB总线和雷电3总线一般都用Type C接口 不要把雷电3和Type C混为一谈，前者是总线/传输协议, 后者是物理接口 HDMI 懂得都懂","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"Hardware","slug":"Hardware","permalink":"http://lyk-love.cn/tags/Hardware/"}]},{"title":"Computer Storage","slug":"Computer-Storage","date":"2022-07-31T22:02:16.000Z","updated":"2022-09-26T06:39:34.928Z","comments":true,"path":"2022/08/01/Computer-Storage/","link":"","permalink":"http://lyk-love.cn/2022/08/01/Computer-Storage/","excerpt":"Outline: Basic Ideas: Cache Main Memory VRAM Disk Storage Material RAM ROM","text":"Outline: Basic Ideas: Cache Main Memory VRAM Disk Storage Material RAM ROM Basic Ideas 计算机的存储架构是分层的： CPU registers: 也就是CPU的 register file， 对它的访问没有延迟 cache memory: 有L1，L2， L3 ...，差别是timing 。访问L1没有延迟，访问L2有1-2个时钟周期的延迟 main memory: 主存, the work house of memory system, 也被称为RAM.（当然更精确的说法是DRAM） disk: 硬盘，是一种外部存储设备，最慢 越上层速度越快，价格越贵，其容量也就越小 还有虚拟内存 , 这是进程视角下的内存， 包含了主存和一部分从硬盘中置换出来的存储空间, 由MMU( Memory Management Unit )完成。总之，虚拟内存是一个抽象的内存概念，和现实的存储结构没有关系。 Cache Cache， 位于CPU内部的存储设备，其材质一般是SRAM. 一般有多级cache, 如L1/L2/L3 Main Memory main memory（OR system memory ）：主存，也称为system memory. 在冯诺依曼架构中, 处理器( 包括ALU和Controller )与存储器进行数据交互, 这里我们把和CPU交互的RAM称为主存. 后面会提到， 主板上的内存条( memory module, 由DRAM组成 )的集合就是主存，提供给CPU一个统一的内存视图 在通常的语境下，我们说的RAM都是主存，也就是和CPU交互的DRAM 板载内存( on-board memory )：焊接在电路板上的内存就是板载内存，它无法更换。 与之对应的是可插拔的内存条。 对于系统RAM，除了板载内存，还可以用可插拔的内存条。 不过对于显存( VRAM )，目前已经不存在可插拔的显存了，所以所有显存都是板载的( 起码目前为止 ) 系统RAM的板载内存一般焊接在主板上，而显存一般焊接在显卡的PCB( 印刷电路板，也就是显卡的“主板”)上 VRAM 显存的文档 Video RAM (VRAM): 显存， 用作GPU的frame buffer. 广义地讲, 显存不是一种特定的RAM材质, 任何用作主存的RAM( 也就是DRAM )都可以用作显存。 事实上我们可以把和CPU交互的DRAM叫主存，和GPU交互的叫显存。 当然了，在带宽、延迟和速度方面，CPU和GPU的需求是不同的。因此VRAM的材质一般采用GDDR( 下文会介绍 ) VRAM主要存储纹理数据( texture data )，用于和GPU交互。 因此它也被称为 Texture Memory 当显存不够用时，GPU将会不得不从主存中分配内存来作为VRAM。 甚至如果电脑采用集显，也就是和CPU集成到一起的GPU， GPU就没有自己的RAM。 GPU会将系统RAM的一部分作为VRAM，显而易见这很慢 也有一些计算机使用“统一内存”， 即CPU和GPU都使用为GPU设计的RAM，在运行时为两个处理器动态分配。这样就避免了给GPU和CPU使用不同的内存。 比如PlayStation 5和M1，统一使用GDDR 作为一个额外的好处，如果CPU和GPU都需要相同的数据，就不需要在两套不同的内存中拥有两个副本。 有些厂商会提供同一型号但是不同VRAM容量的显卡，其实一般来说，为了单单高一点VRAM就加钱没啥必要。。。 显存的指标：显存主要包括capacity、frequency和bandwidth这三个参数 Disk Disk：称为硬盘，是非易失的外部存储设备。因为早期的硬盘都是机械硬盘( HDD )，成盘状，所以称为硬“盘” 机械硬盘( HDD ):拥有磁头，扇区和盘片, 材料和ROM没关系. HDD比较重，而且运行时会呼呼地转，发出噪音. HDD价格低容量大，速度也远远慢于SSD. 适合存储一些音频、视频数据 现在M.2 NVMe固态硬盘速度能达到3500MB/s，就算是普通的SATA SSD速度也能到550MB/s，而机械硬盘现在能到200MB/s就很不错了 固态硬盘（ SSD， Solid State Disk )： SSD没有物理上的磁头和盘片，也没有扇区和磁道等概念. SSD的存储原理和机械硬盘不同，它是闪存盘的matrix，用某种控制芯片将多个NAND FLASH颗粒整合, Flash属于广义上的ROM( 见下文ROM ). 严格地讲，SSD不算Disk， 不过它在计算机架构中确实承担着Disk的职能( 外部存储设备 )，所以把SSD放入Disk一类 SSD价格贵, 速度快. 因为没有磁头, 寻道时间几乎为0. 因为没有机械部件, 因此抗震荡, 低功耗, 无噪音 现在流行的使用MVMe协议的SSD 机械硬盘只能实现 50-120 MB/秒的写入速度，而固态硬盘能达到 550 MB/秒的 SATA 总线极限值 注意，虽然SSD材料是ROM，但磁盘和ROM是两个概念。 现代的磁盘包括了HDD和SSD， 已经不是一个材料概念，而是一个计算机架构中的抽象的功能的概念。 而ROM是一个材料的概念。不能把ROM和Disk混为一谈。 Storage Material RAM RAM（ random access memory ）： 是volatile（易失性）的存储材料， 也就是说断电就会丢失所有信息。 RAM常用来做计算机的内存，包括cache和主存等 RAM分为两类： SRAM( Static RAM ): “静态”是指只要不掉电, 存储在SRAM中的数据就不会丢失 更快更贵，一般用作Cache DRAM( Dynamic RAM ): 在通电时还需要进行周期性的刷新操作, 才能保证数据不丢失 比SRAM慢，也更便宜，一般用作主存和显存 下图总结了SRAM和DRAM的区别： SRAM的存取比DRAM快 SRAM对干扰不敏感 SRAM每单元使用更多晶体管，密集度低，比DRAM更贵，功耗更大 我们规定RAM芯片的基本存储单位是bit，对应的物理结构称为cell SRAM SRAM 将每个bit存储在一个 bistable ( 双稳态 )的cell里。每个cell用一个 six-transistor电路实现。 双稳态： 上图是一个双稳态电路，它永远保持在两个不同的电压状态，也就是在上图左（右）稳态。其他任何状态都是不稳定的——电路会从不稳定状态迅速地转移到其中一个稳定状态。 上图其实有个地方应该是有偏差的，也就是中间那个状态。原则上来说，当左右两边的作用力相同时，钟摆在垂直的时候也能无限期地保持平衡，但是当左右两边稍微发生一点扰动，这个状态就会变成左稳态或右稳态之一。而且一旦倒下，便不会有机会再站起来。我们称这个状态为亚稳态。 因为SRAM有双稳态的特性，只要有电，它就会永远的保持它的值, 即使有干扰来扰乱电压, 当干扰时, 电路就会恢复到稳定值. 也就是说SRAM在通电时不需要刷新就能保存数据 但没有电的话, 双稳态的状态就不能保持了, 意味着数据也就丢失了. 这就是我们常说, 电脑断电后内存数据就会丢失的原因. DRAM DRAM将每个位存储为对一个电容的充电。DRAM可以制造得非常密集——每个单元由一个电容和一个访问晶体管组成。但是与SRAM不同的是，DRAM存储器的单元对干扰非常敏感.当电容的电压被扰乱之后，它就永远不会恢复了. DRAM加电时需要不断刷新, 才能保存数据 DRAM Structure DRAM芯片的每$w$个cell组成一个supercell，每$d$个supercell组成一个DRAM芯片的存储部分。 即， 一个$d \\times w$的DRAM总共存储了$d \\times w $ bit supercell被组织成一个$r$行$c$列的长方形阵列，其中$r \\times c = d$. 每个DRAM内使用$(i,j)$二维地址对supercell进行寻址( 即$(行号， 列号)$ ) 也就是说，DRAM的寻址的基本单位是supercell， DRAM没有提供机制给supercell内部的cell寻址 example： 上图展示了一个$16 * 8 $的DRAM芯片，其中含有$d = 16$个supercell，每个supercell包含$w = 8 $个cell，$r=4$行， $c=4$列 每个supercell含有8个cell，相当于每个supercell存储了1 Byte， 即该DRAM是按字节寻址的 pin pin( 引脚 ): 对芯片的外部物理接口的抽象称呼，这里我们只讨论用于信息I/O的pin， 每个携带1 bit信号。 其实不是所有的pin都能用来信息I/O， 比如Vcc和GND，除了供电，就没有其他用处。 DRAM芯片通过pin来进行信息传输，上图给出了两组pin： 8个data pin，能在芯片中I/O一个字节 2个addr pin，能携带一个2 bit的地址，用于对supercell的寻址 注意到，这个DRAM是4行4列的，所以一次只能对一维进行寻址 DRAM Addressing 每个DRAM芯片被连接到某个称为memory controller的电路。这个电路可一次向每个DRAM芯片I/O $w$位。memory controller读取supercell $(i,j)$的内容的步骤为： memory controller会通过addr pin发送2 bit的行地址$i$ 这个操作称为a RAS (row access strobe) request DRAM此时会选中supercell mateix的第$i$行，复制到内部行缓冲区: 再发送2 bit的列地址$j$ 这个操作称为a CAS (col access strobe) request DRAM此时会选中行缓冲区的第$j$列. 该元素就是supercell$(i,j)$的1 Byte内容 DRAM然后会将这1 Byte内容通过8个data pin发送给memory controller 这个模型把DRAM的supercell组织成二维而不是一维数组，是为了降低芯片上addr pin的数量。 如果supercell被组织成一维数组，就需要$\\log_216 = 4$个addr pin. 但是二维数组就需要如上所述的两次寻址，要花更多的时间 这里考虑最简情况，把一个DRAM芯片当做一个memory module。稍后我们会看到，一个memory module由多个DRAM组成，由 memory module和memory controller交互 Memory Module memory module: DRAM被组织成memory module，插到主板的插槽上 memory module也就是我们常说的内存条 main memory：主存，也称为system memory. 主板上内存条的集合，提供给计算机一个统一的内存视图，这就是主存 此外还存在“显存”， 其实二者是一样的。 给CPU用的叫主存，给GPU用的叫显存 上图展示了一个容量为64 MB 的memory module, 由8个 8MB * 8 （ $d = 8* 1024 * 8, w = 8$ ）的DRAM芯片组成，按0～7编号。每个supercell存储$w = 8$ bit 信息。 我们假设一个word为8 Byte. 地址A $(i,j)$上的一个word 就由这8个DRAM的地址为$(i,j)$的supercell组成。 也就是说DRAM0的$(i,j)$存储A的第0个Byte, DRAM1的$(i,j)$存储A的第1个Byte, 以此类推 8个DRAM中读取的8个Byte被发送给memory controller， 组合成一个word，再发送给CPU Enhanced DRAMs 除了上述我们介绍的常规DRAM，还有很多变种的DRAM FPM DRAM Fast page mode DRAM (FPM DRAM): 传统的DRAM在读取$(i,j)$ 数据时，会先将第$i$行copy到行缓冲区，然后读取第$j$列将其返回， 丢弃该行的其他元素。 如果workload是要读取第i 行的所有supercell，那么传统DRAM会在每个RAS/CAS后丢弃剩余数据，重新加载行缓冲区。这无疑是十分低效的。 FPM DRAM在完成$(i,j)$的RAS/CAS后， 不会丢弃剩余的元素。 因此对于第$i$行其他元素的RAS/CAS，可以直接返回结果 EDO DRAM Extended data out DRAM (EDO DRAM): 比FPM DRAM更先进， 可以把多个CAS缓冲CAS为一组，一起发送 SDRAM Synchronous DRAM (SDRAM)： 传统DRAM, FPM DRAM, EDO DRAM都是以异步方式进行I/O的( 使用RAS和CAS两个独立的信号 ). 而SDRAM的I/O都在系统时钟的上升沿执行，这使得它可以同步地存取数据. 简言之，SDRAM比传统DRAM快很多 SDRAM的材质分为双极性与CMOS( Complementary Metal Oxide Semiconductor, 互补金属氧化物半导体 ) CMOS还被用于在数字影像领域. 市面上常见的数码产品, 其感光元件主要就是CCD或者CMOS, 尤其是低端摄像头产品, 而通常高端摄像头都是CCD感光元件. 主板上就用了一个CMOS芯片来记录时间和硬件配置参数，比如该从哪个盘启动. 该硬件很悲催，没有自己的名字，我们就称为CMOS. CMOS里自带一个小电池, 因此虽然是易失性的，断了电也能工作 DDR SDRAM Double Data-Rate Synchronous DRAM (DDR SDRAM)：双通道DDR SDRAM, 是SDRAM的增强版, 在上升/下降沿都执行I/O 随着 prefetch buffer ( 预取缓冲区，决定了有效带宽 ) 的增大，DDR已经分为DDR (2 bits), DDR2 (4 bits), DDR3 (8 bits), DDR4( ) and DDR5( 16 bits ) 注意， DDR4的prefetch和3一样，都是8 bit，但是DDR4提高了核心频率，所以总线速度得以提高 DDR的&quot;Double Data-Rate&quot;指的是会在系统时钟的上升和下降沿都执行I/O，并不意味着DDR的速度是SDRAM的两倍。 事实上，随着DDR技术的进步，速度差距是越来越大的。 SDRAM DDR DDR2 DDR3 DDR4 prefetch buffer size 1 bit 2 bit 4 bit 8 bit 8 bit 电压 3.3 2.5 - 2.6 1.8 1.35 - 1.5 1.2 GDDR GDDR( Graphics Double Data Rate DRAM )： 一般用作VRAM( 显存 )，带宽更高，速度更快， 支持并发读写（当然容量也更小）。 市面上常见的有GDDR6 GDDR和主存的普通DDR的最大区别就是前者有一个宽的“总线”， 总线越宽，数据传输速率越高。 由于图形涉及并行操作大量数据，因此内存总线的宽度非常重要。 ROM ROM( Read-Only Memory ): 顾名思义是不可写的一种存储材质，它是novolatile( 非易失性)的， 数据不会随着断电而丢失。 不过，“不可写”其实是生产工艺的限制，而不是什么设计上的特性。 后来ROM发展出了可擦写的存储材质EPROM和EEPROM，因为是可擦写的，所以严格来讲不算&quot;Read-Only&quot;， 但是它们确实是在ROM技术上发展出来的，因此也称为ROM。 后来在这两种技术的发展上又发展出了NAND FLASH闪存，这就是我们现在用的U盘中用到的技术，同样，因为其体积小，容量和速度均不错，现在手机存储中的emmc颗粒也是用的这种技术，所以有手机厂商就把手机的存储容量约定俗成为ROM，其继承ROM断电不丢失数据的特性，而且有着较快的速度。（但是是可擦写的哦） EEPROM和flash都有写入次数限制 refs: CSAPP","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"Hardware","slug":"Hardware","permalink":"http://lyk-love.cn/tags/Hardware/"}]},{"title":"GPU","slug":"GPU","date":"2022-07-31T22:02:07.000Z","updated":"2022-09-26T06:39:34.930Z","comments":true,"path":"2022/08/01/GPU/","link":"","permalink":"http://lyk-love.cn/2022/08/01/GPU/","excerpt":"Outline: Preface：Graphic Card Basic Concepts Components of GPU","text":"Outline: Preface：Graphic Card Basic Concepts Components of GPU Preface：Graphic Card 很详细的介绍显卡的文章 显卡( Video card、Display card、Graphics card ): 由 GPU, 电路板( PCB ), 显存, 金手指, 供电 &amp; 显示接口以及散热等构成 因为显卡最重要的就是GPU, 我们也可以把二者混为一谈. 比如后文提到的集显和核显, 其实就是两块纯的GPU, 为了方便我们还是说成显卡. Basic Concepts Graphic Processing Unit, aka GPU: 为了处理图形计算任务而专门开发的硬件, 是一种特殊的CPU. GPU就是特殊的CPU, 这意味着电脑可以没有GPU，用普通的CPU也可以处理图形任务. 不过性能嘛... 给个例子， 顶级CPU 3990X 性能撕裂者勉强带得动前几代的《孤岛危机》 因此, GPU对于现代计算机是必要的. 没有外置显卡就用核显, 没有核显就用主板的集显, 连集显都没有那电脑可以入土了 集显 集显( 集成显卡 )：集成在主板北桥的GPU. 能和CPU共享散热器和系统RAM等资源 集显下GPU没有独立的RAM，只能将系统RAM的一部分作为VRAM( 显而易见这很慢 ) 因为就集成在主板上把它叫做“板载显卡” 早期的主板很多都带集显, 集显性能都还很垃圾. 现在的显卡都不带集显了( 比如b550m ), 不过集显的概念没有消失, 现在很多人都用“核显” 核显 核显:被封装到CPU Package中的GPU, 甚至有核显是刻在CPU的Die里面的 其目的和集显一样, 都为了和CPU共享散热器和系统RAM等资源, 因此也可以把核显认为是集显的一种 因为CPU和GPU放在一块, 也可以把这种CPU叫做APU( 按AMD的说法 ) AMD的R5 5600G就自带了UHD750显卡( 刻在了Die里面.... )它的性能相对5600G这块CPU很垃圾, 但也能在1080P中高画质上60帧数上运行诸如LOL的游戏了: Graphics Model Graphics Core Count Graphics Frequency Radeon™ Graphics 7 1900 MHz 独显 独显( 独立显卡 )：顾名思义就是可插拔的独立的显卡. 通过插槽插入计算机的主板，并且通常需要比通过卡插槽提供的更多的功率。因此，它也可能有来自计算机电源的专用电源连接。 GPU和CPU的区别 CPU被设计用于通用计算. 而GPU被设计为专门用于图形计算, 这些专用的功能会被直接集成到硬件中(起码在目前依然是这样) CPU被设计为高效地执行单线程代码( SMT / Hyper-Threading等技术也改进了CPU的多线程能力 )； 而GPU天然被设计为高效地执行多线程代码。 也就是说，面对需要多线程的worload时，GPU的效率远高于CPU GPU通过堆叠核心来扩展多线程性能, 所以核心数肯定比COU多。 AMD的高端CPU Epyc CPU是64核/128线程； 而Nvidia的最低端Pascal GPU就有384个内核。 GPU的性能度量 GPU的性能和架构的关系很大. 因此不同架构的GPU之间，不能单凭核心数来比较性能。 只有同一厂商的同一系列的GPU，才可以只用核心数比较。比如GTX 3070 和 GTX 3080 和 GTX 3080 Ti； 以及 [RX 5700 XT 和 RX 6700 XT] Components of GPU GPU由核心( Core )、纹理映射单元( Texture Mapping Unit, TMU )、渲染输出( Render OutPut, ROP )和 显存( Video Random Access Memory, VRAM ) 组成。 粗略地说， GPU的最小功能单元是计算资源的块( block of computing resources ) VRAM在拙著Computer Storage介绍 GPU可以用如下标准来表示: [ 核心数 ]:[ 纹理映射单元数 ]:[ 渲染输出数 ] 比如4096:160:64 基本单元： SM/CU ( 这是一个AMD的Pascal Multiprocessor (SM) ) GPU被构造成blocks of computing resources，block“可以看作”GPU的最小功能单元 Nvidia calls these blocks an SM (Streaming Multiprocessor), while AMD refers to them as a CU( Compute Unit ) . 严格地讲，block并没有涵盖GPU所有的功能 ---- 比如 video decode engines, render outputs required for actually drawing an image on-screen, and the memory interfaces used to communicate with onboard VRAM。 Core GPU的核心( Core )是block的集合，比如AMD的APU( having 8 or 11 Vega Compute Units ), 这就拥有GPU的大多数功能了 Each block contains a group of cores, a scheduler, a register file, instruction cache, texture and L1 cache, and texture mapping units. TMU 纹理映射单元（Texture mapping unit，TMU）是GPU的一个部件，用于 纹理映射 TMU的数量决定了GPU对于纹理映射workload的吞吐量和处理速度 TMU曾就是一个独立的物理部件，现在已经集成到着色器中 ROP 渲染输出( Render OutPut, ROP )（有时也称为光栅操作管线）是GPU的一个部件。 它将GPU的输出组装成图像在显示器上显示。 ROP的数量乘以GPU的时钟速度可以控制像素填充率。ROPs的数量越多，意味着可以同时输出更多的像素。 ROPs还可以处理抗锯齿","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"Hardware","slug":"Hardware","permalink":"http://lyk-love.cn/tags/Hardware/"}]},{"title":"Tips for Building Frontend Projects","slug":"Tips-for-Building-Frontend-Projects","date":"2022-07-30T17:23:43.000Z","updated":"2022-09-29T16:28:17.321Z","comments":true,"path":"2022/07/31/Tips-for-Building-Frontend-Projects/","link":"","permalink":"http://lyk-love.cn/2022/07/31/Tips-for-Building-Frontend-Projects/","excerpt":"Outline: Intro node npm yarn ...","text":"Outline: Intro node npm yarn ... Intro 前端包管理工具有npm和yarn,后者是npm的进化版，推荐用yarn 前端项目clone下来， [npm/yarn] install报错是很正常的， 其实依赖报错最主要的原因就是npm/yarn或node版本不对。 node node生态的基本工具 nodejs是JS的一个运行时库，用于在native环境下运行JS nvm：nodejs 的一个版本管理工具，用于管理 nodejs 和 npm 的版本。 nvm是一个独立于node/npm的Shell脚本，需要手动安装 npm or yarn: : nodejs的包管理工具（就是安装第三方模块的工具）主要是npm和yarn npm已经被包含在Nodejs包中，不需要手动安装 yarn被包含在corepack中，后者是一个对包管理工具进行管理的工具，其二进制包中默认包含了yarn。 也就是说安装了corepack就同时安装了yarn。 当然也可以独立地安装yarn，但是不推荐 n: n命令是nodejs的一个模块，用法类似于nvm. 不过作为一个集成进nodejs的模块，它的功能比nvm局限很多 包所在文件node_modules node_modules：node安装的第三方模块的所在文件夹。 通过包管理工具( npm or yarn )安装的第三方模块都会被下载到该文件夹。 包默认会被安装在项目根目录下的node_modules。 如果指定了全局安装： npm install -g: node目录下的node_modules， 比如~/.nvm/versions/node/v18.4.0/lib/node_modules 用npm root -g查看 yarn global add: yarn的全局安装目录不是node目录，而是形如~/.config/yarn/global 用yarn global dir查看 项目描述文件package.json package.json： 使用npm/ yarn生成node项目后，会生成项目的描述文件package.json，记录了当前项目信息，例如项目名称、版本、作者、github地址、当前项目依赖了哪些第三方模块等 一个文件夹，如果其目录下有package.json, 它就是一个node项目 package.json只是初步指定了项目的依赖( 依赖名和依赖的版本 )， 但没有指定依赖的详细信息。 例如， package.json只指定了大版本，也就是版本号的第一位，并不能锁定后面的小版本，每次 npm install 都是拉取的该大版本下的最新的版本。 版本管理粒度太粗了，为了稳定性考虑我们几乎不敢随意升级依赖。 所以在npm &gt; 5.0之后，增加了package-lock.json文件来指定具体的依赖信息(比如依赖具体来源和具体版本号）)。 yarn的yarn.lock文件也类似 简言之，package.json 由于历史遗留原因，没能做到细粒度的依赖管理。 具体依赖信息都记录在package-lock.json或yarn.lock中 对于**npm &lt; 5.0**, npm install [package]只会下载依赖，不会把依赖信息添加到package.json， 需要手动配置进去。使用--save选项，把依赖信息自动添加到到package.json: npm install [package] --save 对于yarn，yarn add [package]会将依赖信息写入package.json 对于**npm &gt; 5.0**，npm install [package]已经可以自动将依赖信息写入package.json了 example &#123; //项目名称 &quot;name&quot;: &quot;description&quot;, //项目的版本 &quot;version&quot;: &quot;1.0.0&quot;, //项目的描述 &quot;description&quot;: &quot;在这里可以快速了解当前项目的功能及作用&quot;, //项目的主入口文件,在模块化项目中都会有一个主模块,main 里面填写的就是主模块的入口文件 &quot;main&quot;: &quot;index.js&quot;, //定义命令别名,当命令很长时可以使用别名替换 //使用方法:npm run 别名 &quot;scripts&quot;: &#123; &quot;test&quot;: &quot;echo \\&quot;Error: no test specified\\&quot; &amp;&amp; exit 1&quot;, &quot;build&quot;: &quot;nodemon app.js&quot; &#125;, //关键字,它允许我们使用关键字去描述当前的项目 &quot;keywords&quot;: [], //项目的作者 &quot;author&quot;: &quot;&quot;, //项目遵循的协议,默认是ISC也就是开放源代码的协议 &quot;license&quot;: &quot;ISC&quot;, //项目依赖所需要的第三方模块（包） &quot;dependencies&quot;: &#123; &quot;formidable&quot;: &quot;^1.2.1&quot;, &quot;mime&quot;: &quot;^2.3.1&quot; &#125;, //开发依赖所需要的第三方模块（包） &quot;devDependencies&quot;: &#123; &quot;gulp&quot;: &quot;^3.9.1&quot; &#125;&#125; dependency categories package.json中记录的依赖有三种 npm install默认会下载pro和dev的依赖. 参见dev: These things will be installed when doing npm link or npm install from the root of a package pro production dependencies: 是依赖的默认种类.在项目开发和运行阶段都需要的依赖。 在添加依赖时( 使用npm install [包名]/ yarn add [包名] )，如果不手动指定，则都添加为producrtion dependencies 被记录在package.json的dependencies属性 dev dev dependencies: 只在开发过程中需要，而运行时不需要的依赖 比如 Babel, Flow 和&quot;@vue/cli-service&quot;: &quot;xx&quot;） 被记录在package.json的devDependencies属性 除非是想要把包发布在npm上, 否则pro和dev没什么区别, 都会被npm install下载. peer peer dependencies: 在发布包的时候需要的依赖。有这种依赖意味着安装包的用户也需要和包同样的依赖。 这对于像 react 这样也被人安装的、需要单一 react-dom 副本的包很有用 用于安装可能与使用者冲突的包 我们组件的包需要react，使用者的项目也需要react，两个react的版本可能不一致，这个时候可以使用peer-dependencies来安装我们的react，避免与使用者冲突。 被记录在peerDependencies 属性 可以用export NODE_ENV=production来制定默认依赖环境 Env 可以通过环境变量来指定下载的依赖类型: export NODE_ENV = production https://juejin.cn/post/6844903681192804359 https://cli.vuejs.org/zh/guide/mode-and-env.html#环境变量 具体依赖管理文件 package-lock.json package-lock.json: 在npm &gt; 5.0 使用npm install 下载依赖会生成package-lock.json文件。它记录了实际依赖版本信息，下次可以通过该文件去下载依赖，保证项目每次下载的依赖版本完全一致。 作用如下： 锁定包的版本，确保再次下载时不会因为包版本不同而产生问题 加快下载速度，因为该文件中已经记录了项目所依赖第三方包的树状结构和包的下载地址，重新安装时只需下载即可，不需要做额外的工作 注意： npm &lt; 5.0 没有package-lock.json，也不能自动把依赖信息写入package.json yarn.lock yarn.lock: 使用yarn add下载依赖会生成yarn.lock文件。其作用基本上等于npm的package-lock.json。 yarn.lock 文件会锁定你安装的每个依赖项的确切版本，这可以确保你不会意外获得不良依赖。如：以前是1.2.1，执行 yarn 后，变为 1.2.2 注意，如果一个项目既用了npm install又用了yarn add， 那么会同时存在package-lock.json和yarn.lock文件，会发生什么我也不知道，所以一个项目最好只用一个依赖管理工具 node模块 在node.js中模块与文件是一一对应的，也就是说一个node.js文件就是一个模块。 核心模块：nodejs已经内置的一些模块，不需要用包管理工具下载 外部模块：需要用包管理工具下载，下载后均存放在node_modules文件夹中 node模块查找规则 通过路径查找 当指定模块路径时 require(&#x27;./find.js&#x27;);require(&#x27;./find&#x27;); //模块文件后缀可以省略 require方法根据该路径查找模块，如果是完整路径，直接引入模块。 如果模块后缀省略，先找同名JS文件再找同名JS文件夹 如果找到了同名文件夹，找文件夹中的index.js 如果文件夹中没有index.js就会去当前文件夹中的package.json文件中查找main选项中的入口文件 如果找指定的入口文件不存在或者没有指定入口文件就会报错，模块没有被找到 通过模块名查找 可以不指定路径，直接写模块名: require(&#x27;find&#x27;); Node.js会假设它是系统模块 Node.js会去node_modules文件夹中 首先看是否有该名字的JS文件 再看是否有该名字的文件夹 如果是文件夹看里面是否有index.js 如果没有index.js查看该文件夹中的package.json中的main选项确定模块入口文件 否则找不到报错 简言之，先查同名的系统模块，找不到再去node_modules中查外部模块 查看node项目版本 如果项目使用 yarn 和 typescript，可以查看yarn.lock 里的@types/node@ 的 version: &quot;@types/node@*&quot;: version &quot;14.0.20&quot; resolved &quot;https://registry.npmjs.org/@types/node/-/node-14.0.20.tgz#0da05cddbc761e1fa98af88a17244c8c1ff37231&quot; integrity sha512-MRn/NP3dee8yL5QhbSA6riuwkS+UOcsPUMOIOG3KMUQpuor/2TopdRBu8QaaB4fGU+gz/bzyDWt0FtUbeJ8H1A== packageJson.engines，第三方模块都会有，自己的项目中有可能有 FROM，如果采用 docker 部署，查看基础镜像 Dockerfile 中 node 的版本号 如果以上方式都不可以，那就只能问人或者自己蒙了. 可以查找nodejs的版本发布表, 根据项目的日期大概能猜出来版本 那只有问人了. 因此强烈建议项目使用yarn + Ts nvm Commands 查看当前node版本: nvm current 或者: node -v 卸载指定版本node: nvm uninstall &lt;version&gt; 列出所有可以安装的node版本号: nvm ls-remote： 安装指定版本号的node nvm v15.1.0 或者( e.g. 对于node版本15.X ): nvm install 15 安装最新版本的node： nvm install node 全局地切换node的版本: nvm use v15.1.0 或者( e.g. 对于node版本15.X): nvm use 15 列出所有已经安装的node版本: nvm ls Installation 以下只演示macOS上的NVM安装过程， 对于Linux，nvm只要一行命令就可以安装了 Remove existing Node Versions brew uninstall --ignore-dependencies node brew uninstall --force node Install NVM on macOS brew update brew install nvm Next, create a directory for NVM at home. mkdir ~/.nvm 编辑配置文件(这里我用的是~/.zshrc) nvim ~/.zshrc 使编辑生效: source ~/.zshrc node Installation 在安装了nvm的情况下: Install any version listed in above output. You can also use aliases names like node for latest version, lts for latest LTS version, etc. nvm install node ## Installing Latest version nvm install 14 ## Installing Node.js 14.X version After installing you can verify what is installed with: nvm ls node_modules Management 一般通过包管理工具来对node_modeules进行管理。 不过手动删除这个操作无论是yarn还是npm都特别慢， 可以使用额外的工具rimraf， 它是node的一个包，可以快速删除node_modules，再也不用等半天了 安装： npm install rimraf -g 使用：rimraf node_modules npm npm doc npm vs yarn npm yarn 说明 npm install yarn 安装项目依赖 npm install react --save yarn add react 添加一个依赖到项目中，如：添加 react 到项目。 npm uninstall react --save yarn remove react 从项目删除一个依赖，如：从项目删除 react 依赖 npm install react --save-dev yarn add react --dev 将项目依赖添加到开发依赖，只有在开发模式下才能使用 npm update --save yarn upgrade 更新项目依赖 npm Installation node自带了npm，所以安装node后就默认安装了对应版本的npm Problems This version of npm is compatible with lockfileVersion@1, but package-lock.json was generated for lockfileVersion@2. I’ll try to do my best with it! 这是因为npm的版本和依赖文件中某些依赖所需的npm版本不同. 从报错可以看出npm适用于lockfileVersion@1, 但是package-lock.json是源于lockfileVersion@2的. 这时就需要更改npm版本 还有一种可能, 注意到npm -g和npm使用的是不同的npm, 所以出现这个问题很有可能是错误使用了npm -g Commands 查看npm版本: npm -v 安装/更新到最新版本: npm -g install npm 安装/更新到指定版本: npm -g install npm@6.14.11 查看全局安装目录: npm root -g 下载依赖： npm i[nstall] &lt;package&gt; --save-dev --save，简写为-s或-S：将依赖信息保存至 package.json, 默认保存到dependencies属性 该option后可以跟选项，指定具体的依赖类型 --save-dev，简写为-d或-D: 指定依赖类型为dev dependency, 依赖信息会被保存在devDependencies属性。 对于npm &gt; 5.0， 默认会启用--save -g:全局安装 卸载依赖: 在项目根目录, 即node_modules所在目录运行: 注意, 卸载并不会把依赖从package.json中去掉( 但是安装会 ) npm un[install] &lt;package-name&gt; -S 或 --save 标志，则此操作还会移除 package.json 文件中的引用. 如果程序包是开发依赖项（列在 package.json 文件的 devDependencies 中），则必须使用 -D 或 --save-dev 标志从文件中移除 如果该软件包是全局安装的，则需要添加 -g 或 --global 标志： 可以在系统上的任何位置运行npm un -g, 因为当前所在的文件夹无关紧要 Config npm获取配置的方式，优先级由高到低： 命令行参数： 以设置代理为例：--proxy http://server:port即将proxy的值设为http://server:port。 环境变量：以npm_config_为前缀的环境变量将会被认为是npm的配置属性。以设置proxy为例可以加入这样的环境变量npm_config_proxy=http://server:port。 用户配置文件：可以通过npm config get userconfig查看文件路径。 全局配置文件：可以通过npm config get globalconfig查看文件路径。 内置配置文件：安装npm的目录下的npmrc文件。 默认配置： npm本身有默认配置参数，如果以上都没设置，则npm会使用默认配置参数。 npm config set &lt;key&gt; &lt;value&gt; [--global] npm config get &lt;key&gt; npm config delete &lt;key&gt; npm config list npm config edit npm get &lt;key&gt; npm set &lt;key&gt; &lt;value&gt; [--global] 命令行操作说明： 在设置配置属性时属性值默认是被存储于用户配置文件中，如果加上--global，则被存储在全局配置文件中。 如果要查看npm的所有配置属性（包括默认配置），可以使用npm config ls -l npm run https://www.cnblogs.com/goloving/p/16306638.html Problems Dockerizing 使用Docker构建vue镜像: # ENV NODE_ENV productionRUN node -v &amp;&amp; npm -v \\ &amp;&amp; npm config set registry http://r.cnpmjs.org/ \\ &amp;&amp; npm installCOPY . .RUN npm run build 其中的npm run build会调用package.json中的build字段的命令: &quot;scripts&quot;: &#123; &quot;serve&quot;: &quot;vue-cli-service serve&quot;, &quot;build&quot;: &quot;vue-cli-service build&quot;, &quot;lint&quot;: &quot;vue-cli-service lint&quot;&#125;, 可以看到, vue-cli-service **都使用了vue-cli-service这个命令, 他对应了一系列依赖: ... &quot;@vue/cli-plugin-babel&quot;: &quot;~5.0.0&quot;, &quot;@vue/cli-plugin-eslint&quot;: &quot;~5.0.0&quot;, ... 这些依赖默认全部被安装到了devDependencies. 因此, 如果在install时使用了pro模式, 即只下载pro依赖, 则不会下载上述依赖, 也就会在npm run build时报错: Error: Cannot find module &#x27;@vue/cli-plugin-babel&#x27;//OR:vue-cli-service: not found 该问题发生的常见情况是, 在Dockerfile里配置了ENV NODE_ENV production, 因此, 千万不要在install的时候用production, 当然你可以在build和run的时候开启: &quot;scripts&quot;: &#123; &quot;serve&quot;: &quot;vue-cli-service serve --open --mode development&quot;, &quot;build&quot;: &quot;vue-cli-service build --mode production&quot;, &quot;lint&quot;: &quot;vue-cli-service lint&quot;&#125;, npx npx: a package runner tool that comes with npm 5.2+. 教程 由于node自带npm，后者( 在 npm &gt;= 5.2 后)又自带npx，所以安装node后可以直接使用npx. 手动安装npx: npm install -g npx 简单来说，npx可以指定使用某个版本的node，让该版本的node来执行命令。 避免了项目构建中需要降低node版本的尴尬情况 npx node@0.12.8 [command] 上面命令会使用 0.12.8 版本的 Node 执行脚本。原理是从 npm 下载这个版本的 node，使用后再删掉。 -p： 在安装指定版本的node后，不删除该node npx可以替代很多npm版本管理工具，比如nvm n 安装n模块: npm install -g n 报错则改用另一条命令： npm install -g n --force 升级node.js到最新稳定版 n stable# 或者升级到最新版n latest 安装指定版本node: n v14.16.0 切换node版本： n 7.10.0 yarn 文档 Yarn是facebook发布的一款取代npm的包管理工具， Yarn并行地安装包，速度比npm快。 推荐使用yarn corepack Installation corepack是一个对包管理器进行管理的工具，其安装包中包含了yarn. 也就是说，安装了corepack后就不需要手动安装yarn了 也可以用npm安装yarn，但是不推荐这种方法: npm install -g yarn Node.js &lt;16.10: Node默认包括了npm. 但在Node v16.10之前， Node没有默认包括corepack， 需要手动安装: npm i -g corepack Node.js &gt;=16.10: v16.10之后，node默认包含了corepack， 但只是作为可选项，没有默认启用。 需要手动启用: corepack enable 安装完corepack就可以直接使用yarn了！ Commands 把Yarn更新到最新版本 yarn set version stable 查看全局安装目录： npm install -g yarn react-native-cli Accessing the list of commands yarn help Starting a new project yarn init 同npm init，执行输入信息后，会生成package.json文件 Installing all the dependencies yarn OR: yarn install 这会安装package.json里所有包，并将所有依赖项信息保存进yarn.lock options: 强制重新下载所有包: yarn install --force Adding a dependency: yarn add [package] 依赖信息会自动更新到package.json和yarn.lock文件中 yarn global add： 全局安装 安装指定版本, 这里指的是主要版本，如果需要精确到小版本，使用-E参数 yarn add [package]@[version] 安装某个tag（比如beta,next或者latest）: yarn add [package]@[tag] 指定添加的依赖类型： yarn add [package] --dev # dev dependenciesyarn add [package] --peer # peer dependencies Upgrading a dependency yarn up [package]yarn up [package]@[version]yarn up [package]@[tag] Removing a dependency yarn remove [package] Upgrading Yarn itself yarn set version latestyarn set version from sources 启动项目: yarn start yrm yrm 是一个yarn源管理器，允许你快速地在源间切换 安装: yarn global add yrm 查看可用源: yrm ls 选择源: yrm use [source] Problems mac 运行react 项目时报错: /bin/sh: craco: command not found ， 很明显是craco这个命令没安装。 yarn add @craco/craco# OR npm install @craco/craco --save React 生成一个全新的 ts + react 的模版: npx create-react-app my__app__name --template typescriptORyarn create react-app my__app__name --template typescript Before we get started, &quot;Tailwind CSS requires Node.js 12.13.0 or higher&quot; (tailwindcss). Make sure you have Node.js installed and the correct version by running node --version in your command line. 使用tailwind CSS: 进入项目目录， yarn add tailwindcss -D After installation process is done, we need to create Tailwind CSS configuration file. To do so, we need to run the following command: npx tailwind init This will initialise Tailwind with a new configuration file in the root of your project. Vue vue是构建用户界面的渐进式JavaScript 框架 vue-cli是vue的一个官方脚手架工具（快速工程化命令工具）, 用来帮助程序员们快速搭建基于vue框架的开发环境。 vue有很多脚手架工具，vue-cli只是其中一种，侧重于单页面应用 (SPA) 的快速搭建，网址：cli.vuejs.org/zh/guide/ Vue-cli = Vue + 一堆的js插件 vue-cli 4.5以下, 对应的是Vue2 vue-cli 4.5及以上, 对应的是Vue3 @vue/cli是新版vue-cli，提供了GUI维护界面，@vue/cli 安装的是vue3及以上版本, vue-cli 安装的是vue2 命令行的vue命令其实是对应vue-cli这个程序, 而不是vue vue-cli 安装 安装之前先卸载旧版本 npm uninstall -g @vue/cli 安装3.0及其以后版本 npm install -g @vue/cli@x.x.x 安装3.0以前的旧版本 npm install -g vue-cli@2.x 查看vue-cli 版本 vue -V vue3安装 npm install vue@next 查看vue版本： npm list vue Tailwind yarn add tailwindcss -D","categories":[{"name":"Frontend","slug":"Frontend","permalink":"http://lyk-love.cn/categories/Frontend/"},{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[]},{"title":"Transport Layer","slug":"Computer Networking Transport-Layer","date":"2022-07-05T16:47:03.000Z","updated":"2022-09-26T06:39:34.927Z","comments":true,"path":"2022/07/06/Computer Networking Transport-Layer/","link":"","permalink":"http://lyk-love.cn/2022/07/06/Computer%20Networking%20Transport-Layer/","excerpt":"Outline: Intro UDP TCP 可靠传输 TCP Flow Conteol TCP Congestion Conteol","text":"Outline: Intro UDP TCP 可靠传输 TCP Flow Conteol TCP Congestion Conteol Intro 网络层(IP)为主机之间提供了逻辑通信， 而运输层为进程之间提供了端到端的逻辑通信 逻辑通信就是好像两个对等实体(无论是网络层的实体(主机)还是运输层的实体(进程))之间有一条水平方向的信道 运输层的端到端通信， 是进程间的通信 运输层的作用 复用( multiplexing )：client的不同进程都可以使用同一个运输层协议发送数据到网络层 分用( demultiplexing ): server的运输层在从网络层收到发送给各进程的数据后，可以分别交付给目的进程， 即server的不同进程可以用同一个运输层协议接收数据 对报文进行差错检测 运输层的协议 运输层协议主要有面向连接的TCP和无连接的UDP。 两种协议在协议栈中的位置： TCP的数据传输单元称为TCP报文段( TCP Segment ) UDP的数据传输单元称为UDP用户数据报( UDP Diagram ) 以下给出应用层协议主要使用的运输层协议: 应用 应用层协议 运输层协议 域名 --- IP 映射 DNS UDP 文件传送 TFTP UDP 路由选择 RIP（Router Informatio Protocal） UDP IP分配 DHCP UDP 远程文件服务器 NFS( Network File System ) UDP 多播 IGMP UDP Email SMTP TCP 远程终端接入 TELNET TCP 万维网 HTTP TCP 文件传送 FTP TCP Socket &amp;&amp; port 套接字( Socket ): 网络通信中的进程的唯一标识， 格式为( IP : port ) 协议端口号( protocal port number ): 简称端口( port )，用来唯一标识一个本机的（应用层）进程 数据通过IP发送到对应的主机， 主机上的运输层协议再通过端口来找到对应的应用层进程, 即: 端口是本机的应用层进程和运输层协议交互的一种地址 在本机中，进程也可以通过PID标识，不同的OS拥有不同格式的PID，格式无法统一，因此无法用于运输层与应用层的交互 这里的“端口”是软件端口， 与用于硬件设备交互的硬件端口不同 端口有16位, 即 0 ~ 65535， 可分为: server port: 0 ~ 49151 well-known port: 0 ~ 1023, 被保留用于一些常见的程序，如HTTP(80), FTP(21) register port: 1024 ~ 49151，被用于没有well-known port的程序 client port: 49152 ~ 65535xs 端口号 服务进程 说明 7 echo 将收到的数据报送到服务器 9 Discard 丢弃任何收到的数据报 21 FTP 活跃的用户 25 SMTP 简单邮件传输协议 53 DNS 69 HTTP 161 SNMP 443 HTTPS 23 Telnet 虚拟终端网络 UDP Features: 无连接 不可靠传输 面向报文：即将报文剥去/添加UDP头后转交给上/下层， 不进行任何的合并/拆分。这意味着进程必须选择合适大小的报文，若报文过长，IP层会进行分片；若报文太短，则会使得IP数据报的首部的相对长度较大，两种情况都会降低IP层效率 没有拥塞控制 首部开销小，只有 8 Byte UDP通信过程 UDP Diagram UDP Diagram分为Header和Dara两部分 Header是固定的8 Byte Header各字段如下: 源端口：源端口号；在需要对方回信时选用；不需要时用全0 目的端口：目的端口号；在终点交付报文时必须使用 长度：UDP用户数据报的长度，最小值是8(仅有首部) checksum：检测UDP用户数据报在传输过程中是否有错。有就丢弃 在计算checksum时，需要在UDP报文之前增加12 Byte的&quot;伪首部&quot;( 其结构和内容见上图 ), 伪首部不是报文的一部分，只是在计算checksum时临时添加的 ​ 如果Receiver的UDP发现收到的报文中的目的端口号非法( 比如，没有进程监听该端口 ), 就会丢弃该报文并由ICMP发送一个&quot;port unreachable&quot;差错报文（ type=3, code=3）给Sender 常用的UDP程序 Example 流程图见上文&quot;UDP通信过程&quot; UDP Client from socket import *serverName = &#x27;localhost&#x27; # 服务器地址，可以是IP或者域名，如果是域名，则会进行DNS lookupserverPort = 12000 # 服务器指定的端口# 创建Client Socker# AF_INET：表明IP采用IPv4# SOCK_DGRAM:表明这是个UDP SocketclientSocket = socket(AF_INET, SOCK_DGRAM) # 创建UDP套接字，使用IPv4协议message = input(&#x27;Input lowercase sentence:&#x27;).encode() # 用户输入信息，并编码为bytes以便发送clientSocket.sendto(message, (serverName, serverPort)) # 将信息发送到服务器# 从服务器接收信息，同时也能得到服务器地址(当然，其实Client已经知道了Server IP，因此这个信息是多余的)# 缓存长度2048modifiedMessage, serverAddress = clientSocket.recvfrom(2048) print(modifiedMessage.decode()) # 显示信息clientSocket.close() # 关闭套接字 UDP Server from socket import *serverPort = 12000 # 服务器指定的端口serverSocket = socket(AF_INET, SOCK_DGRAM) # 创建UDP套接字，使用IPv4协议serverSocket.bind((&#x27;&#x27;,serverPort)) # 将套接字绑定到之前指定的端口print(&quot;The server is ready to receive&quot;)while True: # 服务器将一直接收UDP报文 message, clientAddress = serverSocket.recvfrom(2048) # 接收客户端信息，同时获得客户端地址 modifiedMessage = message.upper() # 将客户端发来的字符串变为大写 serverSocket.sendto(modifiedMessage, clientAddress) # 通过已经获得的client地址，将修改后的字符串发回client户端 TCP Featues: 面向连接，且连接是点对点 即以Socket为进程的地址，一对一通信)的 TCP无法提供多播 可靠传输 全双工通信 面向字节流：虽然进程与TCP的交互以数据块（大小不等）为单位， 但TCP将应用层的数据仅仅看成无结构的字节流。 发送方的进程把数据写入TCP的发送缓存，接收方的进程从TCP的接收缓存中读取字节流 发送缓存实际上是发送窗口的超集，二者的关系详见下文滑动窗口与缓存 TCP连接 TCO连接的端点就是Socket，每条TCP连接唯一地被两个端点确定： TCP连接::={socket1,socket2} = {(IP1:port1),(IP2:port2)} TCP数据传输 当TCP连接建立后，两个进程就可以发送数据了 发送缓存：应用程序将要发送的数据通过Socket传递给TCP，TCP将数据引导到该连接的发送缓存，发送缓存大小是在三阶段握手的过程中确定的；之后TCP将时不时地从该缓存中拿出数据，封装成报文段进行发送 TCP规范中没有规定TCP应该在何时发送缓存里的数据，描述为“TCP应该在它方便的时候以报文段的形式发送数据”。 关于TCP如何控制发送报文段的时机，见下文TCP的传输效率 接收缓存：当TCP Segment到达接收端时，便进入了接收端的缓存，等待被应用程序读取 TCP连接的每一端都有发送和接收缓存 MSS(Maximum Segment Size)： TCP每次可以从缓存中发送的最大数据长度 一般来说，MSS+TCP/IP首部的长度要小于等于链路的MTU（即链路层最大帧长度Maximum Transport Unit） MSS的名字很让人误解， 事实上MSS指的只是Segment的Data部分的最大长度，而不是整个Segment的长度 TCP Segment TCP Segment分为Header和Data两部分 Header前20Byte固定，后面跟$4n$个可选的Byte，因此Header占$20 + 4n$ Byte Header各字段如下： 源端口和目的端口：各占2字节，分别是源端口号和目的端口号 序号：占4字节, 序号范围$[0,2^{32}-1]$。TCP中传输的数据流中的每一字节都按顺序编号。序号字段的值是本报文段所发送的数据的第一个字节的序号 确认号：占4字节，是期望收到对方下一个报文段的第一个数据字节的序号。 例如B正确收到了A发送过来的一个Segment。序号为501，而数据长度是200Byte，这表明B正确收到了A发送的到序号700为止的数据。 因此B期望收到A的下一个数据序号是701， 于是B在发送给A的Segment中把确认号置为701 若确认号=$N$，则表明到序号$N-1$为止所有数据都正确收到 数据偏移：占4位，指出TCP报文段的数据起始处与TCP报文段的起始处的距离，也就是Header的长度 保留：占6位，保留为今后使用，目前应置为0 紧急URG：当URG=1时，表明紧急指针字段有效，告诉系统此报文中有紧急数据，应尽快传送，而不采用原来的按排队顺序来传送 确认ACK：仅当ACK=1时确认号字段有效，TCP规定，在连接建立后所有数据报文段都把ACK置为1 推送PSH：当收到PSH=1的报文时，就尽快交付接收应用进程，而不再等到整个缓存都填满后再向上交付 复位RST：当RST=1时，表明TCP连接中出现严重差错，必须释放连接，然后重新建立连接。 RST=1还用来咀嚼一个非法的Segment或拒绝打开一个连接。 RST也称为重置位 同步SYN：在连接建立时用来同步序号；当SYN=1而ACK=0时，表明这是一个连接请求报文，对方若同意建立连接，则应在响应报文中使SYN=1，ACK=1（称为SYNACK）。 因此，SYN=1就表明这是一个连接请求( SYN Segment )或连接接受报文( SYNACK Segment ) 终止FIN：用来释放一个连接，当FIN=1时，表示此报文段的发送方已经发送完毕，并要求释放连接 窗口：占2字节，指的是发送本报文段的一方的接收窗口（而不是自己的发送窗口 ）。 例如，A是Sender， B是Receiver， A的窗口值作为A让对方设置其发送窗口（ B虽然是Receiver， 但也会向A发送数据 ）的依据；窗口字段明确指出了从本Sement Header的确认号算起，A目前允许B发送的数据量（以字节为单位），窗口值经常动态变化 例如，A发送给B一个Segment，确认号是701，窗口字段是1000， 这就是告诉B：“从701算起，我的接收缓存空间还可接收1000Byte数据，你在给我发送数据时，必须考虑到这点” 校验和：占2字节，检验和字段检验的范围包括首部和数据两部分。 在计算校验和是，需要在Header前面加上12Byte的伪首部 紧急指针：占2字节，在URG=1时才有意义，指出本报文段中的紧急数据的字节数 选项：长度可变，最长40Byte。 当没有使用任何选项时， Header长度是20Byte。 最后的填充字段仅仅是为了使整个TCP首部长度是4Byte的整数倍 MSS就是选项之一 TCP连接管理 TCP连接建立 我们将主动发起连接建立的进程称为client， 被动等待连接建立的进程称为server。 记A为client， B为Server， 连接建立过程为三报文握手： 一般将其称作“三次握手”，但严谨地说，TCP连接建立只是“在一次握手中报文交换三次”，因此称为“三报文握手” 注意，只有SYN和SYNACK报文段会被超时重传， ACK报文段是不会被重传的 起初，A和B都为CLOSED状态。在通信开始前，双方都得创建各自的传输控制块（TCB）。B创建完TCB后便进入LISTEN状态，此时准备接收A发来的连接请求。 第一个报文 client向server发送连接请求报文段。该报文段没有Data部分, Header中: SYN=1，ACK=0，seq = client_isn 该Segment称为SYN Segment 请求发送后，client便进入SYN-SENT状态 SYN=1，ACK=0表示该报文段为连接请求报文。 client_isn为本次TCP通信的字节流的初始序号， TCP规定：SYN=1的报文段不能有数据部分，但要消耗掉一个序号 client_isn和后文的server_isn可以随机选择，也可以（为了避免网络攻击）使用算法生成 第二个报文 服务端收到连接请求报文段后，如果同意连接，则会发送一个应答： SYN=1，ACK=1，seq=server_isn，ack=client_isn + 1 该Segment称为SYNACK Segment 该应答发送完成后，server便进入SYN-RCVD（同步收到）状态 SYN=1，ACK=1表示该segment为连接同意的应答报文 seq=server_isn表示server作为sender时，发送字节流的初始序号. 这个序号也可以是随机选取的 ack=client_isn+1表示服务端希望下一个数据报发送序号从client_isn+1开始的字节 TCP规定: SYNACK Segment要消耗一个序号，也就是B向A发送的下一个Segment的起始序号是server_isn+1 第三个报文 当客户端收到连接同意的应答后，还要向服务端发送一个确认报文段，表示：server发来的SYNACK已经成功收到。 该报文段的头部为： ACK=1，seq=client_isn + 1，ack=server_isn+1 该Sement称为ACK Segment client发完这个segment后便进入ESTABLISHED状态，server收到这个应答后也进入ESTABLISHED状态，此时连接建立完成 SYN = 0， ACK = 1 标识该segment为连接建立确认报文。 由于client发送完这个报文后就处于连接建立状态，因此SYN=0 TCP规定：ACK Segment可以携带数据，但如果不携带数据则不消耗序号，此时，A向B发送的下一个Segment的起始序号仍然是client_isn + 1 为什么连接建立需要发送第三个报文？ 防止失效的连接请求报文段被服务端接收，从而产生错误 考虑如下情况：我们采用两报文握手， A给B发送SYN Segment， 但因该报文丢失而没有收到确认（ SYNACK ），于是A再重传一次SYN Segment。 后来A收到了确认，建立了连接。通信结束后释放连接。 A总共发送了两个报文段，第一个丢失了，第二个正确到达了B，没有“失效的”报文段。这是正常情况。 现在假设一种异常情况， 即A发出的第一个SYN Segment没有丢失，而是迟到了，以致在连接释放后才到达B， 这是个已经失效的Segment，B收到该SYN Segment后，误以为A又发出了一次新的连接请求，于是就向A发送SYNACK Segment，同意建立连接， 由于这是“两报文握手”， B发送SYNACK后就进入了ESTABLISHED状态。 但此时A早已进入CLOSED状态， A会直接丢弃这个SYNACK，也不会给B发数据，更不会通知B，B会一直等待下去，持续地浪费其资源。 如果第三个报文握手丢失怎么办？ problem：如果第三个报文握手丢失，A处于ESTABLISHED状态，向B发送数据，而B仍然处于SYN_RECV状态，无法响应连接。 solution：B在发送SYNACK后，会根据 TCP的超时重传机制，等待3秒、6秒、12秒后重新发送SYN+ACK包，以便Client重新发送ACK包。 TCP连接释放 起初，A和B都为ESTABLISHED状态 第一个报文 若A认为数据发送完成，则它需要向B发送连接释放请求。该Segment只有Header，其中： FIN=1，seq=u 此时，A将进入FIN-WAIT-1状态 FIN=1表示该报文段是一个连接释放请求 seq=u，u-1是A向B发送的最后一个字节的序号 TCP规定： FIN 报文段总是会消耗一个序号 第二个报文 B收到连接释放请求后，会通知高层进程：A向B这个方向的连接已经释放。此时B进入CLOSE-WAIT状态，并向A发送连接释放的应答，Header包含： ACK=1，seq=v，ack=u+1 A收到该应答，进入FIN-WAIT-2状态，等待B发送连接释放请求。 ACK=1：除TCP连接请求和链接释放请求报文段以外，TCP通信过程中所有数据报的ACK都为1，表示应答。 seq=v，v-1是B向A发送的最后一个字节的序号。 ack=u+1表示希望收到从第u+1个字节开始的报文段，并且已经成功接收了前u个字节。（因为第一个报文序号为u，且会消耗一个序号，因此ack = u+1）A收到该应答，进入FIN-WAIT-2状态，等待B发送连接释放请求 第二次挥手完成后，A到B方向的连接已经释放，B不会再接收数据，A也不会再发送数据。但B到A方向的连接仍然存在，B可以继续向A发送数据， 此时连接处于**“半关闭”状态** 第三个报文 当B向A发完所有数据后，向A发送连接释放请求，Header包含： FIN=1，ACK=1，seq=w，ack=u+1 B然后进入LAST-ACK状态。 seq=w： B在半关闭状态可能又发送了一些数据，现在起始序号为w 第四个报文 A收到释放请求后，向B发送确认应答，Header包含： FIN=1，ACK=1，seq=w，ack=u+1 此时A进入TIME-WAIT状态，此时连接还没有释放掉，必须经过时间等待计时器( TIME-WAIT timer )设置的时间 2MSL 后，且该时间段内没有B的重发请求的话，A才会进入CLOSED状态，撤销TCB。当B收到确认应答后，也会进入CLOSED状态，撤销TCB。 最长报文段寿命MSL( Maximum Segment Lifetime ): 这个值是从工程上考虑的，一般是2min。 因此从A进入TIME-WAIT状态后，最少需要4min才能进入CLOSED 为什么A要先进入TIME-WAIT状态，等待时间后才进入CLOSED状态？ 为了保证B能收到A的确认应答。 若A发完确认应答后直接进入CLOSED状态，那么如果该应答丢失，B等待超时后就会重新发送连接释放请求，但此时A已经关闭了，不会作出任何响应，因此B永远无法正常关闭。 Example TCP Client from socket import *serverName = &#x27;localhost&#x27; # 指定服务器地址serverPort = 12000clientSocket = socket(AF_INET, SOCK_STREAM) # 建立TCP套接字，使用IPv4协议clientSocket.connect((serverName,serverPort)) # 向服务器发起连接sentence = input(&#x27;Input lowercase sentence:&#x27;).encode() # 用户输入信息，并编码为bytes以便发送clientSocket.send(sentence) # 将信息发送到服务器modifiedSentence = clientSocket.recvfrom(1024) # 从服务器接收信息print(modifiedSentence[0].decode()) # 显示信息clientSocket.close() # 关闭套接字 TCP Server 和Servlet类似，TCP Server实际上会先打开一个&quot;欢迎套接字&quot;来等待client连接，当client请求请求到来后， server通过“欢迎套接字”创建一个新的连接套接字(serverSocket.accept())，由后者进行与该client的连接 Client Socket与Server的Connection Socket(而不是欢迎套接字)建立了TCP连接 from socket import *serverPort = 12000serverSocket = socket(AF_INET, SOCK_STREAM) # 创建TCP欢迎套接字，使用IPv4协议serverSocket.bind((&#x27;&#x27;,serverPort)) # 将TCP欢迎套接字绑定到指定端口serverSocket.listen(1) # 最大连接数为1print(&quot;The server in ready to receive&quot;)while True: connectionSocket, addr = serverSocket.accept() # 接收到客户连接请求后，建立新的TCP连接套接字 print(&#x27;Accept new connection from %s:%s...&#x27; % addr) sentence = connectionSocket.recv(1024) # 获取客户发送的字符串 capitalizedSentence = sentence.upper() # 将字符串改为大写 connectionSocket.send(capitalizedSentence) # 向用户发送修改后的字符串 connectionSocket.close() # 关闭TCP连接套接字 SYN Flood Attack 在TCP三报文握手中，server为了响应一个SYN，会分配一些资源（初始化一些变量和缓存），然后发送一个SYNACK， 并等待client的ACK Segment。 如果client不发送ACK， 最终server将终止该半开连接并回收资源 SYN洪泛攻击( SYN flood attack ): 是一种经典DoS攻击, 在这种攻击中，攻击者发送大量的TCP SYN，而不对返回的SYNACK进行ACK。 随着大量SYN的到来，server会不停地新建半开连接并分配资源，导致server的资源耗尽 流行的解决方法称为SYN cookie， 过程为： 当服务器接收到一个SYN报文段时，其并不知道该报文段来自一个合法用户还是要进行SYN洪泛攻击的攻击者，因此服务器不会为该报文段生成一个半开的连接（即不会分配资源） 。 服务器返回的SYNACK中的server_isn由hash生成： server_isn = hash(srcIP,srcPort,dstIP,dstPort, magic) magic是只有server知道的一个秘密数字 server不会记住该server_isn及其相关的任何信息 如果client是合法的，则它将返回一个ACK Segment， 其中ack = server_isn+1， server收到该报文后只要根据IP、端口和自己的magic重新计算new_server_isn，并比较: if new_server_isn == ack - 1 判定该client是否合法。 如果合法，server会生成一个全开连接 如果客户没有返回一个ACK报文段，则说明之前的SYN报文段属于要进行SYN洪泛攻击的攻击者，但其并没有对服务器造成任何危害，因为服务器没有为它分配任何资源 可靠传输 理想的传输条件： 传输信道不产生差错 不管发送方以多快的速度发送数据，接收方总是来得及处理收到的数据 在以上网络条件( 也可以说是网络层条件 )下，不需要任何措施就可以实现可靠传输。 然而实际的网络都不具备以上理想条件（IP层就是不可靠的），因此要采用一些可靠传输协议来确保可靠传输。 基本上，可靠传输要用到以下机制： 计时器（超时重传） 序号 可靠传输协议 我们这里讨论抽象的可靠传输协议，其实现不一定局限于应用层。 我们把传输的数据都称为分组，而不考虑数据实际上是在哪一层发送的。 停止等待协议 对于全双工通信，双方既是Sender又是Receiver。 为了简化问题，我们仅考虑A为Sender， B为Receiver的情况。 停止等待： 每发送完一个分组就停止发送，等待对方的确认，在收到确认后再发送下一个分组 无差错情况 出现差错 以下是传输过程中出现差错的情况： B在接收M1时检测出差错，就丢弃M1，什么也不做（不通知A收到有差错的分组） B当然可以在检测出差错后给A发送错误报文，但这种方法很复杂，实践中一般不采用 M1在传输过程中就丢失了，B什么也不知道 在这两种情况下，B都不会发送任何信息。 而可靠传输协议规定，如果A超过一段时间依然没有收到确认，就会重传前面发送过的分组，这就是超时重传。 超时重传需要在每发送一个分组后设置超时计时器，如果在计时器在到期之前收到了确认，就重置计时器 注意: A发送完一个分组后，必须暂时保留已发送分组的副本，只有在收到相应确认后才删除 分组和确认都需要编号，才能明确哪个分组收到确认，哪个没收到 超时计时器的重传时间应比数据在分组传输的平均往返时间更长一些 确认丢失和确认迟到 假设M1的传输正常，但B对M1的确认丢失了，A在没有收到确认后会超时重传M1，B会： 丢弃这么重复的分组 向A发送确认 ​ ​ 还有一种情况，B对M1对确认没有丢失，而是迟到了。 A超时重传后，B会收到重复的M1，并将其丢弃，并重传确认分组， A会受=收到该重复的确认分组。，并丢弃该确认: ​ 信道利用率 上述的停止等待协议使用了超时重传，因此也被称为ARQ( Automatic Repeat reQuest )， ARQ的缺点是大部分的时间都浪费在等待确认上面，信道利用率低 为了提高信道利用率，可以用流水线传输来取代停止等待传输，流水线传输协议包括了连续ARQ协议和滑动窗口协议， 其中滑动窗口协议比较复杂，被TCP所采用，放到TCP可靠传输的实现一节 “滑动窗口协议”一般指TCP所采用的滑动窗口协议， 尽管连续ARQ协议也采用了滑动窗口 连续ARQ协议 发送方维持发送窗口，位于发送窗口内的分组都可以连续发送出去，而不需要等待对方确认，这样信道利用率就提高了 工作原理： ARQ规定，发送方每收到一个确认，就把发送窗口滑动一个分组位置，接收方采用累积确认方式，在收到几个分组后，对按序到达的最后一个分组发送确认 优点：容易实现，确认丢失也不必重传 缺点：不能向发送方反映出接收方已经正确收到的所有分组信息 例如，若Sennder发送了前五个分组，而中间的第三个分组丢失了，这时接收方只能对前两个分组发出确认。 Sender无法知道后面三个分组的下落，只能把后面三个分组再重传一次，这就叫做 Go-back-N. 表示需要再退回来重传已发送过的N个分组。 可见当网络质量不好时，连续ARQ协议的效率很低。 TCP可靠传输的实现 我们假定数据传输只在一个方向进行，即A为Sender， B为Receiver 滑动窗口 TCP滑动窗口以字节为单位。 假设A收到了B发来的ACK Segment，其中窗口是20Byte，确认号是31（表明B期望收到的下一个序号是31），A根据B的报文构造自己的发送窗口： 之前提到了，除了连接请求和连接释放请求, TCP通信过程中的所有Segment均含有ACK=1,这里是传输数据的Segment，也不例外 发送窗口表示：在没有收到B的确认时，A可以连续把窗口内的数据都发送出去。 凡是已发送的数据，在未收到确认之前必须暂时保留，以便在超时重传时使用 发送窗口中的序号表示允许发送的序号，窗口越大，发送方就可以在收到对方确认前连续发送更多的数据，因此可能获得更高的传输效率。 发送方的发送窗口大小一定不能超过接收方的接受窗口(这里是20) 发送窗口的后沿的后面部分表示：已发送且已经收到了确认的序号（显然这部分不需要保留）。 前沿的前面部分部分表示：不允许发送的序号。 发送窗口的位置由前沿和后沿决定 后沿变化的情况有两种： 不动（没有收到新的确认） 前移（收到了新的确认） 注： 由于不能撤销已收到的确认，因此后沿不能向后移动 前沿变化的情况有两种： 前移， 这是一般情况 不变，这包含两种情况： 没有收到新的确认，且对方通知的窗口大小也不变 收到了新的确认，但对方通知的窗口缩小了，使得前沿正好不懂 注意，发送窗口前沿不能向后移动，虽然实践上可以，但TCP标准强烈不建议这样做 注意，A的发送窗口是根据B的接受窗口设置的，此外还要考虑到网络情况和拥塞控制等等，因此A的发送窗口不一定和B的接受窗口一样大 滑动窗口与缓存 TCP的缓存和窗口的关系： 发送缓存存放： 发送应用程序发送给发送方TCP准备发送的数据 TCP已发送出但尚未收到确认的数据 发送窗口只是发送缓存的一部分，已被确认的数据应当从发送缓存中删除，因此发送缓存和发送窗口的后沿是重合的。 发送方进程最后写入发送缓存的字节减去最后被确认的字节，就是还保留在发送缓存中的被写入的字节数。 这些（发送缓存中的）字节在发送窗口内的部分，可以被一次性发送 接收缓存存放： 按序到达的、但尚未被接受应用程序读取的数据 未按序到达的数据 超时重传时间的选择 TCP连接建立时， 发送方如果在规定时间内没有收到确认， 就会超时重传已发送的报文，。为了得到标准的超时重传时间， 我们定义: RTT: 报文段的往返时间， 注意，每个报文的RTT都不一样 RTTs：报文段的加权平均往返时间， 's'表示Smoothed， 因为使用了加权平均，所以得到的结果更平滑, 算法为: $$ \\mathrm{新的RTT_S} = (1 - \\alpha) \\times (\\mathrm{旧的RTT_S}) + \\alpha \\times(\\mathrm{RTT_S} - \\mathrm{新的RTT样本}) $$ $\\alpha$一般为0.125 $\\mathrm{RTT_D}$: RTT的偏差的加权平均值， 它与RTTs和新的RTT样本之差有关。 RFC规定，在第一次测量时，$\\mathrm{RTT_D}$为测量到的RTT样本值的一半，在以后的测量中，则是用下式计算$\\mathrm{RTT_D}$: $$ \\mathrm{新的RTT_D} = (1 - \\beta) \\times (\\mathrm{旧的RTT_D}) + \\beta \\times|\\mathrm{RTT_S} - \\mathrm{新的RTT样本}| $$ $\\beta$一般为0.25 RTO( Retransmission Time-Out ): 超时重传时间，算法为 $$ \\mathrm{RTO}=\\mathrm{RTT_S}+4*\\mathrm{RTT_D} $$ RTT的定义是“报文往返时间”， 那么如何判定收到的确认报文是对第一次发送的Segment的曲确认，还是对之后超时重传的Segment的确认？ 解决方案之一是Karn算法： 在计算加权平均RTTs时，只要报文段重传了，就不采用其往返时间样本，这样得出的加权平均RTTs和RTO就较准确 然而Karn算法也不太合理（自行百度），实践中一般采用改进的Karn算法。 总之，RTO的选择是很复杂的。 选择确认SACK 如果收到的报文段无差错，只是中间缺少了一些序号。 选择确认( Selective ACK, aka SACK ) 可以让发送方只传送缺少的数据，而不是重传所有数据 示例： 假设TCP的接收方收到了序号1 ~ 1000, 1501 ~ 3000, 3501 ~ 4500, 其余序号都没有收到，如果所有序号的范围都在接收窗口内，则接收方会收下这些数据，并把已收到序号的“边界”告诉对方，让对方不要再重传这些数据。 对于本例，“边界”就是$[1,1001)$,$[1501,3001)$, $[3501, 4501)$， 注意边界的右界是序号的最右端 + 1 显然，TCP 首部中没有哪个字段是用来存放边界的， 因此边界信息都存放在首部“选项”字段。 由于首部选项最多只有40Byte， 而每个字节块有两个边界， 每个边界4Byte（因为序号用4byte表示）， 所以一个字节块就要用8Byte。 选项中还需要指明“启用SACK”（1Byte）和“SACK选项占用的字节数”（1Byte）， 对于n个字节块，一共会在选项字段占用： $$ 8n + 2 $$ 字节， 很容易就超过了选项字段40Byte的上限， 再加上SACK文档没有对发送方怎样响应SACK做规定， 因此SACK没有被广泛采用。 一般的实现还是重传所有数据块。 ​ TCP Flow Control 流量控制( Flow Control ): 让发送方发送速率不要太快，使接收方来得及接收。 这是端到端通信量的控制( 接收端控制发送端 )， 利用滑动窗口实现流量控制 TCP利用滑动窗口实现流量控制，示例： 我们假设A向B发送数据，且： 开始时B告诉A： “我的接受窗口rwnd=400”( rwnd = received window ) 每个报文段长100字节 数据报文段的序号初始值为1 我们看到，上图中B一共进行了三次流量控制。 第一次把窗口减小到rwnd = 300； 第二次把窗口减小到rwnd =100；第三次把窗口减小到rwnd = 0， 即不允许发送方再发送数据了，该状态持续到B重新发出新的窗口值为止。 这三个报文段的ACK均为1， 仅当ACK = 1时， 确认号字段才有意义 显然，发送方的发送窗口不能大于接收方给出的接收窗口( rwnd ) 假设B在发送rwnd = 0， 其接收窗口又有了一些空间， 然后B向A发送了rwnd = 400的报文段，但是这个报文段丢失了， 则A会一直等待B发送的非零窗口的报文段， B也在等待A传来的数据，形成死锁。 注意，B如果不继续收到A的报文段，自己是不会再次主动通知A的 （ 因为“通知”是ACK ， 而ACK报文段不会被重传） 为此，需要对TCP连接设置持续计时器：只要TCP连接的一方收到对方的零窗口通知，就启动持续计时器，若计时器到期，就发送一个零窗口探测报文段，而对方就在确认这个报文段时给出了现在的窗口值，若窗口值仍是零，那么收到报文的一方就重新设置持续计时器，若不是零，那么死锁就被打破 TCP的传输效率 进程把数据传送到TCP发送缓存后，就由TCP来控制数据的发送。 TCP需要控制报文的发送时机，来得到最大的数据传输效率。 目前的TCP实现中广泛采用Nagle算法 Nagle算法：若进程要把数据逐个字节地送到TCP发送缓存，则发送方就把第一个数据字节先发送出去，把后面到达的数据字节都缓存起来。当发送方收到对第一个数据字符的确认后，再把发送缓存中的所有数据组装成一个报文发送出去，同时继续对后到达的数据进行缓存。只有在收到对前一个报文段的确认后才继续发送下一个报文段 TCP Congestion Control 拥塞( congestion ): 网络中对资源的需求 &gt; 可用资源 拥塞控制( congestion control ): 防止过多的数据注入到网络中，使网络中的路由器或链路不致过载。 拥塞控制是一个全局的过程，面向整个网络 拥塞控制的原理 拥塞控制可分为开环控制和闭环控制 开环控制：在设计网络时事先将有关发生拥塞的因素考虑周到，力求网络在工作时不发生拥塞 闭环控制：基于反馈回路概念；检测网络系统以便检测到拥塞在何时、何处发生；把拥塞发生的信息传送到可采取行动的地方；调整网络系统的运行以解决出现的问题 检测网络拥塞的指标： 由于缺少缓存空间而被丢弃的分组的百分数 平均队列长度 超时重传分组数 平均分组时延 TCP拥塞控制的方法 TCP的拥塞控制方法有四种： 慢开始 拥塞避免 快重传 快恢复","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"Computer Networking","slug":"Computer-Networking","permalink":"http://lyk-love.cn/tags/Computer-Networking/"}]},{"title":"Dependabot","slug":"Dependabot","date":"2022-06-26T16:56:12.000Z","updated":"2022-09-26T06:39:34.928Z","comments":true,"path":"2022/06/27/Dependabot/","link":"","permalink":"http://lyk-love.cn/2022/06/27/Dependabot/","excerpt":"Outline： Intro Dependabot alerts Dependabot security updates Dependabot version updates","text":"Outline： Intro Dependabot alerts Dependabot security updates Dependabot version updates Intro Dependabot是Github的依赖管理工具, 它具有如下功能: Dependabot alerts: dependabot会检测vulnerable dependencies，在以下情况下，会触发dependabot alert: 当GitHub Advisory Database中收录了一个新的vulnerable dependency repo的依赖图发生了改变，比如新添加了某个依赖，或者用户改变了某个依赖的版本 这里的版本指：“语义版本”(semver) Dependabot updates: Dependabot security updates: 在一个dependabot alert产生时，自动触发一次update Dependabot version updates: 当用户依赖的上游有更新时update 通过depenency graph来得到用户依赖和上游依赖 用户需要编写依赖配置文件dependabot.yml， 来说明如何更新依赖 这确保了用户依赖始终是updated的 Dependabot alerts 在repo页面 -&gt; Settings -&gt; Security栏的Code security and analysis ， 开启 Dependency graph和Dependabot alerts Dependency graph和Dependabot alerts对于public repo是默认开启的，只有private repo需要手动开启这两项 Dependency graph： 根据依赖配置文件，生成依赖图 Dependabot alerts: 自动检测依赖，如果依赖有更新，就会向用户的邮箱发alert，alert中会包含被影响的文件link，以及漏洞修复的版本和信息 Dependabot security updates 在开启了Dependency graph和Dependabot alerts的基础上，再开启Dependabot security updates security updates: Dependabot会自动将漏洞修复，并且给repo提PR Dependabot version updates 在开启了Dependency graph和Dependabot alerts的基础上，再开启Dependabot version updates, 然后配置依赖文件，以指定依赖的更新过程 version updates: 在发现依赖有更新后，会自动创建一个 PR 来更新依赖文件，并说明依赖更新内容，用户自己选择是否 merge该PR 依赖配置文件dependabot.yml可以在Github上创建，也可以手动创建： 手动创建： 在repo根目录下创建.github folder，并且在folder下添加dependabot.yml（dependabot的config） 用Github创建：在repo页面通过 Insights -&gt; Dependency graph -&gt; Dependabot -&gt; Enable Dependabot 路径即可开启，之后就可以点击 Create config file 来创建配置文件了 dependabot.yml 具体见官方文档 version、updates、package-ecosystem 、schedule 是必填的，还可以配置 registries 来指定私有仓库地址及认证信息 example: # Basic dependabot.yml file with# minimum configuration for two package managersversion: 2updates: # Enable version updates for npm - package-ecosystem: &quot;npm&quot; # Look for `package.json` and `lock` files in the `root` directory directory: &quot;/&quot; # Check the npm registry for updates every day (weekdays) schedule: interval: &quot;daily&quot; # Enable version updates for Docker - package-ecosystem: &quot;docker&quot; # Look for a `Dockerfile` in the `root` directory directory: &quot;/&quot; # Check for updates once a week schedule: interval: &quot;weekly&quot;","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[]},{"title":"Empirical Software Engineering","slug":"Empirical-Software-Engineering","date":"2022-06-24T10:46:24.000Z","updated":"2022-09-26T06:39:34.930Z","comments":true,"path":"2022/06/24/Empirical-Software-Engineering/","link":"","permalink":"http://lyk-love.cn/2022/06/24/Empirical-Software-Engineering/","excerpt":"Outline: Philosophical Stances Empirical methods Research Measurements","text":"Outline: Philosophical Stances Empirical methods Research Measurements Philosophical Stances Positivism(实证主义): all knowledge must be based on logical inference from a set ofbasic observable facts Constructivism(建构主义): scientific knowledge CANNOT be separated from its human/social context Transformative/Participatory(改造/参与性): research needs to be related with a political agenda 研究需要与政治议程有关 Pragmatism(实用主义): all knowledge is approximate and incomplete and its value depends on the methods by which it was obtained 不同哲学立场对应不同的实证研究方法： Positivism: Experiments, Survey,Confirmatory case studies Constructivism: Ethnographies, Exploratory case studies Advocacy/Participatory: Action research, Interview Pragmatism: Mixed methods: Experiments, Case studies, Survey, Action research Empirical methods Empirical Method Precondition Sampling Challenges Experiments A clear hypothesis that guides the entire experimental design Random Control of variables except the chosen independent ones Case Studies A clear RQ concerned with how or why certain phenomena occur 有目的取样 Data collection &amp; analysis is more open to interpretation and researcher bias Survey Research A clear RQ that asks about the nature of a particular target population 从目标群体中选择 Control for sampling biasLow response ratesQuestion design to yield valid data Ethnographies A clear RQ that focuses on the cultural practices of a community w/o any pre-existing theories Chain or representative Detailed observation , data collection &amp; analysis while avoiding preconceptions Action Research A problem owner willing to collaborate to both identify a problem and engage in an effort to solve it n/a Immaturity as an empirical method Research Measurements 不同的实证研究方法，如experiment, case study, survey, 都有自己的度量，这里列出最general的度量 Formalism Immersion approaches rely more on researchers’ interpretive skills • Editing approaches includefewaprioricodesbasedonfindingsofthe researcher during the analysis Template approaches more formal with more a priori based on research questions Quasi-statistical approaches much formalized, e.g., word frequency analysis Validity Construct Validity: to what extent the measures really represent what is investigated by the research questions Criterion validity: how well the responses for the particular tests match others used in the field, or responses for different tests that should be correlated Predictive validity if a response can be shown to be accurately predictive of the intended phenomenon Content validity establishing acceptance that a construct measures what it claims to Face validity a cursory review of items by untrained judges Internal Validity whether one factor affects an investigated factor External validity to what extent it is able to generalize the findings Reliability to what extent the data and the analysis are dependent on the specific researchers Test-retest (intra-observer) if the same person responds to a survey twice, how likely to get the same answers each time? Alternate form reliability rewording or re-ordering questions in different versions of the questionnaire Inter-observer (inter-rater) the reliability of surveys that involve a (group) of trained person completing a survey instrument based on their own observations","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://lyk-love.cn/categories/Software-Engineering/"}],"tags":[{"name":"Empirical SE","slug":"Empirical-SE","permalink":"http://lyk-love.cn/tags/Empirical-SE/"}]},{"title":"Survey","slug":"Survey","date":"2022-06-24T10:46:04.000Z","updated":"2022-09-26T06:39:34.940Z","comments":true,"path":"2022/06/24/Survey/","link":"","permalink":"http://lyk-love.cn/2022/06/24/Survey/","excerpt":"Outline: Definition of Survey Survey Types Survey Steps","text":"Outline: Definition of Survey Survey Types Survey Steps Def Survey是对一大批样本进行研究，以理解样本的某些特征或行为 和Case Study的区别： Case Study研究某个具体的对象，而Survey对一大批样本进行研究 Survey研究示例表格： characteristic type of objectives To identify how KMS helps in developing and maintaining trust in GSD team members. type of survey design Cross-section. 182 responses were received from all over the world in three weeks. survey administration Web-based. a questionnaire was developed and administered on the web. developing instrument A questionnaire was prepared based on tested questions used in the literature in order to enhance the validity of the questions. Where validated items were unavailable, new questions were developed based on literature study. Responses of each questionnaire item was given on a 5-point scaling ranging from 5= “strongly agree” to 1= “strongly disagree”. The questionnaire was divided into two parts; part one was related to the personal information of the respondent and the second part consist of actual 35-items questions. (close question, ordinal scales) population The population consists of those people who are working in GSD organizations, posses some experience of using KMS and are using KMS in their organizations. sampling method Not mentioned. The survey link was sent through the email to invite the GSD organizations employees who are using KMS in their organizations. The sampling method of this study is not mentioned in the article, and we speculate that the sampling method of this study may be non-probabilistic sampling. sample size Not mentioned. A total of 182 responses were received from all over the world. After analysis 6 questionnaires were discarded as they were incomplete, hence total 176 questionnaires were considered for analysis. response rate Not mentioned. Across the study, we can only know that the respondents number is 182. But we don't know how many employees received the email with link of questionaire. evaluate the instrument Not mentioned. data analysis quantitative analysis Types Questionnaire-based survey longitudinal survey cross-sectional survey Interview structured interview semi-structured interview – open-ended interview? Literature survey ad hoc literature review专门的文献检查 systematic (literature) review – mapping study meta-analysis Steps Setting objectives Survey design Developing instrument (questionnaire) Evaluating instrument Obtaining data Analysing data Reporting survey 1.Setting objectives to evaluate the rate or frequency of some characteristic that occurs in a population to assess the severity of some characteristic or condition that occurs in a population to identify factors that influence a characteristic or condition 2.Survey design 横断面式Cross-sectional: 参与者被要求提供一个固定的时间点上的信息 纵向式Longitudinal: 参与者提供关于特定人群随时间变化的信息，以确定影响某一特征或条件的因素 Other forms of survey design e.g., compare different populations Self-administrated questionnaires：自编问卷 web-based (Internet) email or newsletter Telephone surveys Interviews one-to-one interviews one-to-multiple interviews 3.Developing instrument (questionnaire) Construct an Instrument Open or closed questions Designing questions appropriate language and fully defined terms standard grammar, punctuation and spelling single concept per question no vague or ambiguous qualifiers negative or positive questions but double negative no sensitive or privacy questions answer types numeric values response categories exhaustive but not too long mutually exclusive multiple selections? an “other” category? yes/no answers 顺序量表 文字回答 motivation What is the purpose of the study? 4.Evaluating instrument Reliability Validity 5.Obtaining Data Probabilistic sampling Cluster-base sampling： surveying individuals that belong to defined groups Non-probabilistic sampling： 不随机 6.Analyzing Data “If you are not sure or don’t know an answer just leave the line blank; otherwise it is important to answer YES or NO to the first section of every Technique/Technology section.”","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://lyk-love.cn/categories/Software-Engineering/"}],"tags":[{"name":"Empirical SE","slug":"Empirical-SE","permalink":"http://lyk-love.cn/tags/Empirical-SE/"}]},{"title":"Case Study","slug":"Case-Study","date":"2022-06-24T09:22:32.000Z","updated":"2022-09-26T06:39:34.926Z","comments":true,"path":"2022/06/24/Case-Study/","link":"","permalink":"http://lyk-love.cn/2022/06/24/Case-Study/","excerpt":"Outline: Definition of Case Study Types of Case Study Case Study Steps","text":"Outline: Definition of Case Study Types of Case Study Case Study Steps Def Case Study围绕一些具体的对象进行研究 Types Single or multiple case studies multiple-case is often be stronger than single-case Holistic or embedded case studies holistic: the case is studied as a whole 对整个case进行一次研究 embedded: multiple observations (units of analysis) are studied within a case。将case拆分成一个个模块，分别研究 Literal or theoretical replications theoretically replicated: the case is selected to predict contrasting results for anticipatable reasons literally replicated: the case is selected to predict similar results Steps Main Steps of A Case Study: Designing case study Preparing data collection Collecting data Analysing data Reporting case study 1.Designing Case Study Triangulation 在Case Study设计时要遵循三角分离原则 Data (source) triangulation using more than one data source or collecting the same data at different occasions 使用多数据源 Observer triangulation using more than one observer in the study 多个观察者 Methodological triangulation combining different types of data collection methods, e.g. qualitative and quantitative methods 使用多种数据收集方法 Theory triangulation using alternative theories or viewpoints 使用多种理论 2.Preparing data collection 3.Collecting Data data collection techniques direct methods: researchers are in direct contact with the subjects and collect data in real time 研究者与被研究对象直接接触 indirect methods:researchers directly collect raw data without actually interacting with the subjects during the data collection 研究者与被研究对象（在数据收集期间）不直接接触 independent analysis:use work artifacts already available and sometimes compiled data 研究者不接触研究对象，而是接触研究对象的数据 evidence sources documentation archival records interviews direct (independent) observation participant-observation physical artifacts Interviews： Unstructured interview 谈话过程完全根据研究者的主观意愿 Fully structured interview 所有问题，包括问题的顺序都是设计好的 Semi-structured interview 问题是设计好的，但问题的顺序可以自由安排 Interview modes： 漏斗式（ funnel mode）：从开放式问题开始，逐步引导到具体问题 金字塔式（pyramid mode）： 从特定问题开始，逐步引导到开放问题 沙漏式（time-glass mode）： 从开放式问题开始，中间引导到特定问题，最后再引导到开放问题 4.Analysing data Quantitative Analysis Descriptive statistics 对数据给出解释，比如用直方图 Correlation analysis &amp; predictive modelling： 解释一个变量如何影响之后的另一个变量 Hypothesis testing：检验一/多个变量是否对另一/多个变量有影响 Qualitative Analysis Theory generation：find hypotheses or develop a theory from the data Theory confirmation：confirm a hypothesis or theory is really true Negative case analysis：try to find alternative explanations that reject the hypotheses or a theory Combination of the above：use them iteratively and in combination 5.Reporting case study Alternative structures for case study report Linear-analytic: standard reporting structure Comparative: compares alternative cases Chronological: suitable for longitudinal studies Theory-building: follows a chain of evidence in order to constitute a theory Suspense: reverts the linear-analytic structure Unsequenced: none of the above e.g., when reporting general characteristics of a set of cases","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://lyk-love.cn/categories/Software-Engineering/"}],"tags":[{"name":"Empirical SE","slug":"Empirical-SE","permalink":"http://lyk-love.cn/tags/Empirical-SE/"}]},{"title":"Live Abroad","slug":"Live-Abroad","date":"2022-06-16T14:37:55.000Z","updated":"2022-09-26T06:39:34.933Z","comments":true,"path":"2022/06/16/Live-Abroad/","link":"","permalink":"http://lyk-love.cn/2022/06/16/Live-Abroad/","excerpt":"Outline: 身份 生活","text":"Outline: 身份 生活 身份 工作签证 工作签证属于移民签证， 美国的工作签证是要抽签的 从另一种视角说，如果你在某个领域很杰出，anyway都有办法留下来，比如 公司会把员工relocate到加拿大，然后第二年接着抽签 “国家利益豁免”签证 “杰出人才工作签证”，要求你只能在这个领域工作，但是不需要抽签 绿卡 绿卡”其实是一种特殊的签证。 intuition是，你在这里工作足够久了，于是政府说“你之后不用抽签了，也不一定要找到工作才能住在这里了；现在你只要想，就可以住在这里” “但是，你需要每年在这里住满六个月才行；并且要交和公民一样的税” 移民 这里指的是换国籍。 但是，不换国籍，只拿绿卡，也只需要向美国交税，不用向中国交税，所以换国籍的优点暂时不清楚。 生活 整体来说比中国好 而且绝对没有加班 安全性 一般来说，就你日常呆的地方而言，和中国差不多安全 交友 随便哪个国家的都可以 工作 不一定在硅谷，不一定在大厂","categories":[],"tags":[]},{"title":"ML Start Guide","slug":"ML-Start-Guide","date":"2022-06-16T12:14:30.000Z","updated":"2022-09-26T06:39:34.933Z","comments":true,"path":"2022/06/16/ML-Start-Guide/","link":"","permalink":"http://lyk-love.cn/2022/06/16/ML-Start-Guide/","excerpt":"Outline: 学习路线 经验 行业前景 就业","text":"Outline: 学习路线 经验 行业前景 就业 学习路线 Book 西瓜书 《动手学深度学习》 + 李沐的深度学习课 Course 之前提到的李沐的DL课 斯坦福2021秋季·实用机器学习 Articles github搜awesome … learning基本都有论文整理，要看最近几年的 例如： https://github.com/yassouali/awesome-semi-supervised-learning https://github.com/jindongwang/transferlearning#5transfer-learning-scholars-著名学者 Conference 以下均属于CCF A ML大会： icml neurips iclr 稍微水一点： aaai IJCAI Journal JMLR： 注重理论+应用，强调算法必须有理论解释 TPAMI： 文章看上去繁杂，没什么理论，但注重实验效果好 COLT：不要看。全是理论，涉及优化，PAC学习理论等 Math 对于CV，NLP这些方向，不需要什么数学。 对于纯ML，也就是关注ML本身问题的领域，需要懂线代和概率论，以及一些一些ML理论。 总结： 前期不需要很强的数学基础，线代+概率论已经足够 为了保险，还是把数分、泛函、拓扑、统计、数理逻辑、凸优化全学了吧。 不过这是高阶需求，可以慢慢来 经验 个人的经验是深度和广度同时并存。深度是找个方向把前沿（16年后至今的）的方法看一遍，广度是从知乎或者其他综述看看其他的方向有哪些经典重要的方法 推荐的方向：弱监督学习和迁移学习、图学习、自监督学习 这些都是比较大的方向，还能再细化。比如弱监督有噪声学习，部分标记学习，主动学习等；迁移学习有领域自适应，领域泛化等 行业前景 应用基本是深度学习。 包括CV， NLP，推荐算法等 CV和NLP已经卷烂了。 ML和CV、NLP是纵向、横向的关系 整个CS都在衰败，ML也不例外。 尽管如此，ML依然是CS领域最有前景的学科，ML完蛋就代表CS也完蛋了 就业 工业界 算法工程师，要求顶会论文。 工作就是调整模型甚至调参，希望以后能改变吧。 学术界 extreamly hard 博士毕业7篇A的一作才能去东南当个讲师 留南大当讲师起码10篇","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://lyk-love.cn/categories/Machine-Learning/"}],"tags":[]},{"title":"Design Principle","slug":"Design-Principle","date":"2022-06-12T01:07:46.000Z","updated":"2022-09-26T06:39:34.929Z","comments":true,"path":"2022/06/12/Design-Principle/","link":"","permalink":"http://lyk-love.cn/2022/06/12/Design-Principle/","excerpt":"Outline： 单一职责原则 开闭原则 里氏代换原则 依赖倒转原则 接口隔离原则 合成复用原则 迪米特法则","text":"Outline： 单一职责原则 开闭原则 里氏代换原则 依赖倒转原则 接口隔离原则 合成复用原则 迪米特法则 单一职责原则 Single Responsibility Principle， aka SRP &quot;A class should have only one reason to change&quot; — Robert Martin 一个对象应该只包含单一的职责，并且该职责被完整地封装在一个类中 一个类职责越多，就越更改频繁 类的职责见Modularity 开闭原则 Open-Closed Principle, aka OCP 一个软件实体应该对扩展开放，对修改关闭。软件实体可以是一个软件模块、一个由多个类组成的局部结构或一个单独的类。 Be open for extension module's behavior can be extended Be closed for modification source code for the module must not be changes RTTI is Ugly and Dangerous! RTTI = Run-Time Type Information 执行时期的类型信息 If a module tries to dynamically cast a base class pointer to several derived classes, any time you extend the inheritance hierarchy, you need to change the module 将一个多态指针转换为其实际指向对象的类型。 破坏了面向对象的纯洁性 recognize them by type switch or if-else-if structures 开闭原则是对单一职责原则的加强 里氏代换原则 Liskov Substitution Principle, aka LSP *“All derived classes must be substitutable for their base class” — Barbara Liskov, 1988 *“Functions that use pointers or references to base classes must be able to use objects of derived classes without knowing it.” — R. Martin, 1996 所有引用基类(父类)的地方必须能透明地使用其子类的对象 通俗表达: 软件中如果能够使用基类对象，那么一定能够使用其子类对象 里氏代换原则是开闭原则的具体实现 LSP is about Semantics and Replacement Understand before you design The meaning and purpose of every method and class must be clearly documented Lack of user understanding will induce de facto violations of LSP Replaceability is crucial Whenever any class is referenced by any code in any system, any future or existing subclasses of that class must be 100% replaceable LSP Summary: Design by Contract Advertised Behavior of an object: advertised Requirements (Preconditions) advertised Promises (Postconditions) Derived class services should require no more and promise no less 依赖倒转原则 Dependency Inversion Principle, aka DIP 高层模块不应该依赖于低层模块，他们都应该依赖抽象。抽象不应该依赖于细节，细节应该依赖于抽象。 另一种表述:要针对接口编程，而不要针对实现编程 依赖倒转原则是开闭原则的具体实现 接口隔离原则 Interface Segregation Principle, aka ISP 客户不应该依赖那些它不需要的接口 在接口拆分时需要满足单一职责原则 合成复用原则 Composite Reuse Principle， aka CRP 在系统中应该尽量多实用组合聚合关联关系，尽量少甚至不使用继承关系 迪米特法则 Law of Demter 在一个软件实体对其他实体的引用越少越好，或者说如果两个类不必彼此直接通信，那么这两个类就不应当发生直接的相互作用，而是通过引入一个第三者发生间接交互 You can play with yourself. You can play with your own toys, but you can't take them apart You can play with toys that were given to you. You can play with toys you've made yourself. 组合大于继承 Favor Composition Over Inheritance Use inherit for polymorphism Use delegate not inherit to reuse code! Coad's Rules of Using Inheritance Use inheritance only when all of the following criteria are satisfied: A subclass expresses &quot;is a special kind of&quot; and not &quot;is a role played by a&quot; An instance of a subclass never needs to become an object of another class A subclass extends, rather than overrides or nullifies, the responsibilities of its superclass A subclass does not extend the capabilities of what is merely an utility class","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://lyk-love.cn/categories/Software-Engineering/"}],"tags":[]},{"title":"Jenkins Tutorial","slug":"Jenkins-Tutorial","date":"2022-05-26T22:48:57.000Z","updated":"2022-09-29T10:25:40.666Z","comments":true,"path":"2022/05/27/Jenkins-Tutorial/","link":"","permalink":"http://lyk-love.cn/2022/05/27/Jenkins-Tutorial/","excerpt":"Outline: Jenkins SCM Access Token Credentials Webhook slave Jenkinsfile 日志","text":"Outline: Jenkins SCM Access Token Credentials Webhook slave Jenkinsfile 日志 Jenkins Jenkins是一个开源的CICD工具 安装 建议以容器形式安装并运行jenkins 容器形式运行jenkins，需要注意jenkins的数据持久化， jenkins的数据目录为/var/jenkins_home ， 需要将其挂载到宿主机的某个目录 注意，以容器形式运行的jenkins，其主体是容器，因此后续配置SSH 公钥，SSH私钥等等操作，都是对于Jenkins容器（而非宿主机）的。 甚至Jenkins容器对宿主机的ssh登陆也需要手动配置 pull镜像： docker pull jenkins/jenkins 启动容器： docker run -d -uroot -p 8081:8080 -p 50000:50000 --name jenkins -v /home/jenkins_home:/var/jenkins_home -v /etc/localtime:/etc/localtime jenkins/jenkins参数说明：-d # -d:后台启动-u root # 指定容器的用户--privileged # Running Docker in Docker currently requires privileged access to function properly. --name jenkins # 容器实例名--restart=always # 容器随docker自启动，因为重启docker时，默认容器都会被关闭-p # -p host_port:container_port: 指定将主机的端口映射到容器的端口-v # 映射目录（给容器挂载存储卷）-e # 指定环境变量注意：1. 将宿主机端口映射到容器端口，映射端口不要和宿主机冲突2. 将容器对应的数据目录映射到宿主机上（实现数据持久化）3. 将宿主机node、maven等前后台构建工具（环境变量）映射到容器中4. Jenkins镜像创建的容器自带Git工具（/usr/bin/git）5. Jenkins镜像创建的容器自带java环境（/usr/local/openjdk-8/bin/java） 接下来就可以通过浏览器访问jenkins 登陆 密码保存在容器的/var/jenkins_home/secrets/initialAdminPassword中，如果忘记了登录密码，可以查看该文件 进入jenkins容器 docker container exec jenkins bash Steps Create a Jenkins Item 点击&quot;New Item&quot;, 选择&quot;Pipeline&quot;形式 因为我喜欢用pipeline script. 而Freestyle我还不会.. 进入Jenkins的 job configuration页面 Configure the Hook Trigger 在Build Triggers 勾选: Github: GitHub hook trigger for GITScm polling by selecting this trigger, this job will run whenever Jenkins receive git hub webhook request from git hub. Git hub webhook will send request according to your choices which you selected in “which events would you like to trigger this webhook” option in git hub webhooks. That’s all we have successfully configured git hub webhook for Jenkins. Gitlab: Build when a change is pushed to GitLab. GitLab webhook URL 这需要Jenkins安装gitlab插件 Configure Webhook on Registry Webhooks are events sent by a webhook provider to your app jenkins + github配置 webhook教程：https://www.decodingdevops.com/configure-github-web-hook-for-jenkins/ jenkins可以暴露给github和gitlab等代码托管平台一个webhook url, 每当用户在代码托管平台触发某些事件时（比如进行了push，merge等），后者就可以给jenkins发送一个hook，让jenkins进行构建 jenkins在收到SCM的hook后，会检查在jenkins上注册的SCM 仓库，如果发现某个仓库的信息和hook信息一致，jenkins的SCM插件（gitlab插件/github插件）就会去检查SCM上的仓库内容，如果该仓库内容有改动，就会发起一次build 这意味着如果本次push没有对内容做更改，jenjins插件就不会检查出变动，也就不会发起构建。 对于用户来说，这意味着jenkins正常收到了hook消息，却一直无法触发构建，jenkins log中也查不到信息（GitHub Hook Log到了“Changes not found”就结束了，没有下文）， 需要在job的Github Hook Log页面查看信息（该页面只有github插件有） Note: Jenkins默认暴露8080端口，与github通信，因此要确保8080端口开放，否则github发送的消息无法到达jenkins. 由于我设置了容器的端口映射, Jenkins容器的8080端口实际被映射到了宿主机的8081端口, 因此下文的Payload URL中的端口是8081 Github 默认是http://[jenkins-host-ip]:[port]/github-webhook/, 也可以在配置页面override该url 先进入对应的仓库. click on the Settings tab then click Webhooks from the left panel. Now, click on the Add webhook button at the right. 接下来你可以看到一个表单: Payload URL : http://[jenkins-host-ip]:[port]/github-webhook/, 最终的Payload URL形如: http://124.222.135.47:8081/github-webhook/ Jenkins域名必须是可被Github可访问到的, 所以一般都是公网IP 也可以在配置页面override该url Content type: pick application/json Leave the rest of the options as they are, with the Just the push event option selected. For simplicity, I said before that GitHub would call Jenkins only when there’s a push in the repo. If you want to configure other actions, you’ll have to select Let me select individual events, but for now, let’s keep it simple. Finally, click on the green Add webhook button. 现在已经Webhook配置完毕了. 每当仓库被push, Github就会向设置好的Payload URL发送webhook Gitlab jenkins + gitlab配置webhook很简单 得到Payload URL: 之前在Build Triggers ** 勾选Build when a change is pushed to GitLab. GitLab webhook URL: [xxx] **时就可以得到Payload URL 形如https://JENKINS_URL/project/YOUR_JOB 复制Payload URL 进入Gitlab对应仓库, 进入Settings -&gt; Webhooks, URL: 填写 Payload URL Secret Token: 可以不填. 配置步骤: In the configuration of your Jenkins job, in the GitLab configuration section, select Advanced. Under Secret Token, select Generate. 将生成的Secret Token粘贴到Gitlab的Secret Token Trigger: 选择Push events SSL verification: 可以勾上Enable SSL verification Finally, click on the green Add webhook button. To test the webhook, select Test. Use SCM SCM就是Source Code Management， 即github/gitlab上的repository. jenkins可以用git从SCM上clone代码，并进行构建. jenkins需要SCM上clone Jenkinsfile,甚至有可能调用其他的API. 这就需要在Jenkins上[配置Credential](# Credentials) 对于SSH Key而言, 需要先在对应的Registry上配置Jenkins endpoint的公钥 注意, 由于容器和宿主机的公私钥不一样. 所以如果是以容器形式运行的jenkins, 这个“公钥”也必须是jenkins容器而非宿主机的公钥 我选择的Jenkinsfile形式是Pipeline, 且Jenkinsfile放在SCM中. 因此我在Pipleline区域设置: Definition: Pipeline script from SCM 这样Jenkins就会拉取SCM中的Jenkinsfile. 如果选择Pipeline script, 则会在Jenkins中设置Jenkinsfile, 这样太僵硬了 Repositories: 指定要clone的jenkinsfile的位置（ SCM地址, 所在分支, 脚本名 ） Repository URL: 使用HTTPS url和ssh url都可以 Credentials: 指定Jenkins使用的私钥, 由于是容器形式的Jenkins ,就选择容器的私钥 Branches to build: 选择使用的分支, 我只用master分支 注意： 使用github作为SCM时， 如果Branches to build 设为空，则必须取消lightweight checkout，详见https://issues.jenkins.io/browse/JENKINS-46588 Script Path: 指定SCM中的Jenkinsfile的名字 注意要写全名. 比如我的脚本为: Jenkinsfile-fast.groovy, 则在该栏填这个名字 Build Manually Webhook会自动触发构建, 当然也可以手动构建: click on the Build Now link Access Token jenkins与SCM交互，需要配置对这些SCM的Access Token 先在SCM生成Access Token github: 在github生成Access Token gitlab: 登录GitLab -&gt; 在用户头像下拉框，选择“Setting” -&gt; 点击“Access Tokens”，输入“Name”和“Expires at”，勾选“api” -&gt; 点击“Create personal access token”，生成access token，记录下此token 在jenkins， 系统管理 -&gt; Manage Credentials -&gt; Stores scoped to Jenkins -&gt; 全局凭据 添加该Token作为凭据 类型选择Username and password 在用户名中输入一个不存在的用户名 密码填写Personal Access Token 用该凭据来连接SCM: github: “Manage Jenkins” -&gt; &quot;Configure System&quot; -&gt; GitHub， 使用刚才配置的Credential github网址：https://api.github.com gitlab: “Manage Jenkins” -&gt; &quot;Configure System&quot; -&gt; &quot;GitLab&quot;，使用刚才配置的Credential Gitlab host URL：https://git.nju.edu.cn/ 我用的学校的gitlab， 所以用的是学校的gitlab的网址 Credentials https://www.jenkins.io/zh/doc/book/using/using-credentials/ https://blog.csdn.net/elva1087penny/article/details/115387664 Jenkins要调用其他服务的API, 这需要Credential( 凭据, 事实上就是一种密钥 ). 根据目标平台的不同, Jenkins可以使用SSH Key, Username and password, API Token 等形式的凭据 Credential不仅可以用于免密登陆平台, 免密clone, 还可以调用平台的各种API Credential Type Jenkins可以存储以下类型的credentials: Secret text - API token之类的token (如GitHub个人访问token) Username and password - 可以为独立的字段，也可以为冒号分隔的字符串：username:password(更多信息请参照 处理 credentials), Secret file - 保存在文件中的加密内容 SSH Username with private key: 需要jenkins终端已经将ssh 公钥发送给了目标主机, 以后只需要私钥就可以登录目标主机 ) Github和Gitlab都可以使用SSH Username with private key Certificate - a PKCS#12 证书文件 和可选密码 此外, 通过安装第三方插件, 还可以使用别的类型的凭据: Docker Host Certificate Authentication credentials: 需要安装Docker插件 Gitlab API Token: 同理, 使用API Token 注意, Gitlab认证不仅需要配置Credential, 还需要额外的步骤, [参见下文](#Configure Access Token) Github App: 同理, 但我不知道有什么用, 也不知道怎么用. Github认证我使用的是SSH Key Add Credential 需要Credentials插件 进入Manage Jenkins --&gt; Manage Credentials --&gt; Stores scoped to Jenkins --&gt; Global Credentials(unrestricted), 点击Add Credentials 填写表单: Type: 根据不同的通信类型选择不同的凭据形式 Scope: 选择Global Description: 可用于标记别名方便区别或记录凭据作用, 写上为妙 ID: Credential的唯一标识, 会显示在项目构建时的Credentials插件选项中. 可自定义, 若为空则自动生成. 对于SSH Key, 需要填写( 前提是已经在Github上配置了Jenkins的公钥 ): Username: ssh连接采用的的用户名 Private Key：ssh连接所需的私钥 Use Credential API Token API Token是一种用于认证的密钥, 其 作用SSH Key, Username and password 一样. 可以用API Token来访问平台提供的各种服务. Github Github中的API Token称作“PAT”( Personal Access Token ). Github的Credential支持SSH Key, 也支持PAT. 虽然我使用前者, 但用后者也是可行的 Github API Token: 首先要[得到PAT](Get Get API Token), 然后将其[添加进Crendential](#Add Credential), 可以以两种Type添加 Username and password : Username is the GitHub user ID and Password is the Password or a personal API Token (recommended). However, use of a password with the GitHub API is now deprecated. Secret Text: Scope: Global Secret: 填写之前生成的PAT ID: 写一个名字 Description：可以填入一些描述，如 GitHub with token Get PAT Creating a personal access token On Github, click Settings In the left sidebar, click Developer settings. In the left sidebar, click Personal access tokens. Click Generate new token. Give your token a descriptive name. To give your token an expiration, select the Expiration drop-down menu, then click a default or use the calendar picker. Select the scopes, or permissions, you'd like to grant this token. To use your token to access repositories from the command line, select repo. Click Generate token. Warning: Treat your tokens like passwords and keep them secret. When working with the API, use tokens as environment variables instead of hardcoding them into your programs. Use PAT For more information, see Jenkins-to-GitLab authentication. 根据[Add Credential](# Add Credential)将之前配置的API Token配置为Credential Type: Gitlab API Token API Token: 填写之前生成的API Token Gitlab Gitlab和Github一样, 可以用SSH Username with private key (这里就不演示了) 和 Gitlab API Token. Plugins Gitlab API Token 需要安装插件: GitLab Plugin ; Gitlab Hook Build Authorization Token Root Get Access Token Create a personal access token to authorize Jenkins to access GitLab. Sign in to GitLab as the user to be used with Jenkins. On the top bar, in the top right corner, select your avatar. Select Edit profile. On the left sidebar, select Access Tokens. Select the cope. api: 选上 read_user: 选上 read_repository: 选上 write_repository: 选上 点击 Create a personal access token Copy the personal access token. You need it to configure the Jenkins server. Configure Access Token For more information, see Jenkins-to-GitLab authentication. 根据[Add Credential](# Add Credential)将之前配置的API Token配置为Credential Type: Gitlab API Token API Token: 填写之前生成的API Token 理论上讲, 这样做之后Credential配置就结束了, 但是Gitlab插件还提供了一些深层次的Jenkins集成功能, 所以需要在Jenkins中继续配置, 来开启这些功能: Select Manage Jenkins &gt; Configure System. In the GitLab section, select Enable authentication for ‘/project’ end-point. 然后填写表单: Connection name: 自定义 GitLab host URL: Enter the GitLab server’s URL, 比如校园网公网的gitlab: https://git.nju.edu.cn/ Credentials: 选择之前配置的Credential 点击 Test Connection, 出现 Success, 表示成功 Use API Token 进入Jenkins Item的GitLab Connection, 填写GitLab Connection 选择Use alternative credential Credential : 选择之前生成的API Token DockerHub 我们需要Credential来log in Dockerhub, 以从Dockerhub pull或者向其push镜像 Dockerhub支持username-password和API Token作为Credential Get API Token Get the token using the following link. https://hub.docker.com/settings/security Create an access token using the New Access Token button on the security page. Use API Token 假设我们把username-password 或者 API Token 配置为了Credential, 就可以在jenkinsfile中使用: stage(&quot;login to dockerhub&quot;)&#123; withCredentials([usernamePassword(credentialsId: &#x27;DOCKERHUB_KEY&#x27;, passwordVariable: &#x27;password&#x27;, usernameVariable: &#x27;username&#x27;)]) &#123; sh &#x27;docker login -u $username -p $password&#x27; &#125; &#125; Credential:DOCKERHUB_KEY Build Jenkins从SCM得到代码以及jenkins脚本后，就要根据脚本进行项目构建， 而项目构建不一定由jenkins主机来做。 jenkins采用master - slave模式， jenkins主机作为master，可以将代码和脚本交给slave，让脚本执行构建过程 slave jenkins是master-slave模式， jenkins自身只是指挥者， 它根据jenkinsfile的内容选择slave，指挥slave运行脚本 如果某台主机被设置为slave主机，jenkins会在其中安装一个agent程序。 如果jenkins选择该slave进行构建，则会ssh连接到该slave， 运行agent进程，并将代码和jenkinsfile发给它, 实现通过slave构建 agent程序用java编写，所以slave节点上必须有jdk, 否则无法安装agent程序，也就无法将其作为slave节点 # 在slave节点上sudo apt-get install openjdk-8-jdk BTW，由于jenkins一般配合docker，所以slave节点上还要安装docker 默认情况下，jenkins的slave只有一个，即jenkins自身（ 如果是容器形式的jenkins， 那就是jenkins容器作为slave ）。 可以在Manage Jenkins --&gt; Manage Nodes and Clouds中查看和配置slave节点 可以看到，容器和宿主机是两个独立的实体，jenkins容器的默认slave是自身容器。在Jenkins容器眼中，宿主机和其他机器一样，jenkins把宿主机和其他机器添加为slave的步骤都相同 添加slave 由于我采用ssh方式连接到slave，jenkins主机需要先配置对slave节点的ssh免密登陆， 即把ssh public key发给slave节点： ssh-copy-id lyk@[slave-host] ​ 然后在Credentials页面，设置一个Credential， 内容是 jenkins主机到该slave节点的ssh private key 进入Manage Jenkins --&gt; Manage Nodes and Clouds ， 进入添加节点页面，输入相应信息，最终进入节点的配置页面 在节点配置页面填写信息： Labels： 指定该slave的label, 在jenkinsfile中通过label来引用对应的slave Remote root directory: jenkins agent的根目录，agent会在此目录下创建workspace目录，作为jenkins的工作目录，也就是构建上下文 这里将Remote root directory设为用户目录： /home/lyk 这会创建/home/lyk/workspace目录，后续的构建都在该目录下进行 Launch method: jenkins master登录到slave节点的方法，只有先登录到主机，才能启动slave agent进程 一般通过ssh： Launch agents via SSH 接下来输入ssh登录到slave节点所需的信息： Host： slave节点的ip Credentials：之前设置好的Credential， 即jenkins主机到该slave的ssh公钥 Host Key Verification Strategy： Mannually trusted Key Verification Strategy 注意： 主机要使用docker，需要把用户添加进docker群组，而这一步需要用户重新登陆才能生效。 但是，jenkins client是通过用户身份登陆的，而且应该是永久登陆，即使令节点&quot;Disconnect&quot;(在节点控制界面)， 依然不会断开登陆。 也就是说，如果在配置好slave主机的docker之前（即将用户添加至docker群组并重新登录），就令主机成为slave节点，这就会导致jenkins agent使用不了docker，报错: Got permission denied while trying to connect to the Docker daemon socket 解决方法是删除该节点，重新配置节点 Environment Variable Jenkins内置了一系列环境变量, 它们都是global的, 即可以在所有Jenkinsfile使用, 且作用域是整个Jenkinsfile 用户也可以在Jenkinsfike中自定义环境变量, 它们都是local per stage的(即只在指定的stage生效). 用法: (前提是环境变量在作用域内) 可以在Jenkinsfile中通过 env 关键字使用: $&#123;env.BUILD_ID&#125;$ Global Env 内置Env 可以通过$&#123;YOUR_JENKINS_HOST&#125;/env-vars.html 查看所有内置环境变量 通过执行 printenv shell 命令获取： sh &quot;printenv&quot; 使用Jenkins的内置环境变量时: 可以不写env, 如:$&#123;BUILD_ID&#125; 如果使用shell 命令, 甚至可以不用写 &#123;&#125;, 如: $BUILD_ID stage(&quot;Read Env Variables&quot;) &#123; echo &quot;带 env 的读取方式：$&#123;env.BUILD_NUMBER&#125;&quot; echo &quot;不带 env 的读取方式：$&#123;BUILD_NUMBER&#125;&quot; sh &#x27;echo &quot;shell 中读取方式 $BUILD_NUMBER&quot;&#x27; &#125; 以上用法会让人困惑, 保险起见还是全部用$&#123;env.BUILD_ID&#125;$吧 自定义Env 在Jenkins→Manage Jenkins→Confiure System找到Global properties→勾选”Environment variables”选框，单击“Add”按钮，在输入框中输入变量名和变量值即可。 自定义全局环境变量也会被加入env的属性列表中 常用Env BUILD_NUMBER 构建号，累加的数字。在打包时，它可作为制品名称的一部分，比如server-${BUILD_NUMBER}.jar BRANCH_NAME 多分支pipeline项目支持。当需要根据不同的分支做不同的事情时就会用到，比如通过代码将release分支发布到生产环境中、master分支发布到测试环境中。 BUILD_URL 当前构建的页面URL。如果构建失败，则需要将失败的构建链接放到邮件通知中，这个链接就可以是BUILD_URL GIT_BRANCH 通过git拉取的源码构建的项目才会有此变量。在使用env变量时，需要注意不同类型的项目，env变量所包含的属性及其值是不一样的。比如普通pipeline任务中的GIT_BRANCH变量值为roigin/master，在多分支pipeline中GIT BRANCH变量的值为master 所以，在pipeline中根据分支进行不同行为的逻辑处理时，需要留意。 BUILD_ID 当前版本ID，与BUILD_NUMBER相同，用于在1.597+中创建的构建，但较旧版本的YYYY-MM-DD_hh-mm-ss时间戳记 BUILD_DISPLAY_NAME 当前版本的显示名称，默认为“＃153” JOB_NAME 此构建项目的名称，如“foo”或“foo / bar” JOB_BASE_NAME 此建立项目的名称将剥离文件夹路径，例如“bar / foo”的“foo” BUILD_TAG: jenkins- $ &#123;JOB_NAME&#125; - $ &#123;BUILD_NUMBER&#125; 的字符串。JOB_NAME中的所有正斜杠（/）都会用破折号（ - ）替换。方便放入资源文件，jar文件等，以方便识别 EXECUTOR_NUMBER 识别执行此构建的当前执行程序（在同一台计算机的执行程序中）的唯一编号。这是您在“构建执行者状态”中看到的数字，但数字从0开始，而不是1。 NODE_NAME 代理的名称 NODE_LABELS 空格分隔的节点分配的标签列表 WORKSPACE 分配给构建作为工作区的目录的绝对路径 JENKINS_HOME Jenkins主节点上分配的目录绝对路径存储数据 JENKINS_URL 完整的Jenkins网址，例如http://server：port/jenkins/ （注意：只有在系统配置中设置了Jenkins URL） JOB_URL 此作业的完整URL，如http://server：port/jenkins/job/foo/ （必须设置Jenkins URL） Local Env 在Jenkinsfile中可以通过内置函数withEnc(['key=value'])来自定义环境变量, 注意它们都是local per stage的: node(&#x27;!windows&#x27;) &#123; withEnv([&#x27;DISABLE_AUTH=true&#x27;, &#x27;DB_ENGINE=sqlite&#x27;]) &#123; stage(&#x27;Build&#x27;) &#123; echo &quot;Database engine is $&#123;DB_ENGINE&#125;&quot; echo &quot;DISABLE_AUTH is $&#123;DISABLE_AUTH&#125;&quot; sh &#x27;printenv&#x27; &#125; &#125;&#125; 这里的 = 号两侧不能有空格, 必须是 key=value 的形式 如果使用Declarerative Script, 也可以用: stage(&#x27;Build&#x27;) &#123; environment &#123; NAME = &quot;RGYB&quot; &#125; &#125; 环境变量生效顺序 安装插件： https://wiki.jenkins.io/display/JENKINS/Build+Environment+Plugin https://wiki.jenkins-ci.org/display/JENKINS/EnvInject+Plugin 全局环境变量 &lt; Slave 配置环境变量 &lt; Job 参数 &lt; Job injected 环境变量 一般不Override 全局环境变量, 否则可能出现不可预知的问题 Jenkinsfile 官网教程：https://www.jenkins.io/doc/book/pipeline/ jenkins根据jenkinsfile来进行构建，该文件有两种写法： Declarative和pipeline script, 后者使用groovy语言，表达能力比较强，推荐使用 pipeline script由一个个stage组成，在每个stage内执行一些指令 选择slave 后面会看到，jenkins由slave来负责实际的构建过程，而slave就是根据jenkinsfile选择的: # 选择label为volatile-ai-slave的节点来构建此脚本node(&quot;volatile-ai-slave&quot;) &#123;...&#125; workspace jenkins会在配置slave时指定的Remote root directory下创建workspace目录,作为工作目录 可以在jenkinsfile中使用： def workspace = pwd()...echo $&#123;workspace&#125; 输出为[Remote root directory]/workspace Jenkinsfile Example Integration with Shell Many Pipeline steps also use the named-parameter syntax as a shorthand for creating a Map in Groovy, which uses the syntax [key1: value1, key2: value2]. For convenience, when calling steps taking only one parameter (or only one mandatory parameter), the parameter name may be omitted, for example: sh &#x27;echo hello&#x27; /* short form */sh([script: &#x27;echo hello&#x27;]) /* long form */ Jenkins没有Terminal, 所以shell里面没法用sh 'sudo [...]' 可以利用Shell的返回值: sh(script: &#x27;cmd&#x27;, returnStdout:true) 例如: LS_RESULT = &quot;$&#123;sh(script:&#x27;ls -lah&#x27;, returnStdout: true).trim()&#125;&quot; Integration with Git Many Pipeline steps also use the named-parameter syntax as a shorthand for creating a Map in Groovy, which uses the syntax [key1: value1, key2: value2]. Making statements like the following functionally equivalent: git url: &#x27;git://example.com/amazing-project.git&#x27;, branch: &#x27;master&#x27;git([url: &#x27;git://example.com/amazing-project.git&#x27;, branch: &#x27;master&#x27;]) e.g. def git_branch = &#x27;master&#x27; def git_repository = &#x27;git@github.com:VolatileReborn/Frontend-VolatileReborn.git&#x27; //Github &lt;Snip&gt; stage(&#x27;clone from github into slave\\&#x27;s workspace. Using branch: &#x27; + &quot;master&quot;) &#123; echo &quot;workspace: $&#123;workspace&#125;&quot; git branch: &quot;$&#123;git_branch&#125;&quot;, url: &quot;$&#123;git_repository&#125;&quot; &#125; Integration with Docker Using Docker with Pipeline Starting with Pipeline versions 2.5 and higher, Pipeline has built-in support for interacting with Docker from within a Jenkinsfile. 注意, Jenkins所在主机(宿主机或者容器)必须要安装Docker Customizing the execution environment Pipeline is designed to easily use Docker images as the execution environment for a single Stage or the entire Pipeline. Meaning that a user can define the tools required for their Pipeline, without having to manually configure agents. node &#123; /* Requires the Docker Pipeline plugin to be installed */ docker.image(&#x27;node:16.13.1-alpine&#x27;).inside &#123; stage(&#x27;Test&#x27;) &#123; sh &#x27;node --version&#x27; &#125; &#125;&#125; Volumes node &#123; /* Requires the Docker Pipeline plugin to be installed */ docker.image(&#x27;maven:3.8.1-adoptopenjdk-11&#x27;).inside(&#x27;-v $HOME/.m2:/root/.m2&#x27;) &#123; stage(&#x27;Build&#x27;) &#123; sh &#x27;mvn -B&#x27; &#125; &#125;&#125; Multiple Containers node &#123; /* Requires the Docker Pipeline plugin to be installed */ stage(&#x27;Back-end&#x27;) &#123; docker.image(&#x27;maven:3.8.1-adoptopenjdk-11&#x27;).inside &#123; sh &#x27;mvn --version&#x27; &#125; &#125; stage(&#x27;Front-end&#x27;) &#123; docker.image(&#x27;node:16.13.1-alpine&#x27;).inside &#123; sh &#x27;node --version&#x27; &#125; &#125; Build Image node &#123; checkout scm def customImage = docker.build(&quot;my-image:$&#123;env.BUILD_ID&#125;&quot;) customImage.push()&#125; Build with Tag One common usage of image &quot;tags&quot; is to specify a latest tag for the most recently, validated, version of a Docker image. The push() method accepts an optional tag parameter, allowing the Pipeline to push the customImage with different tags, for example: node &#123; checkout scm def customImage = docker.build(&quot;my-image:$&#123;env.BUILD_ID&#125;&quot;) customImage.push() customImage.push(&#x27;latest&#x27;)&#125; Using Dockerfile The build() method builds the Dockerfile in the current directory by default. This can be overridden by providing a directory path containing a Dockerfile as the second argument of the build() method, for example: node &#123; checkout scm def testImage = docker.build(&quot;test-image&quot;, &quot;./dockerfiles/test&quot;) testImage.inside &#123; sh &#x27;make test&#x27; &#125;&#125; Builds test-image from the Dockerfile found at ./dockerfiles/test/Dockerfile. It is possible to pass other arguments to docker build by adding them to the second argument of the build() method. When passing arguments this way, the last value in the that string must be the path to the docker file and should end with the folder to use as the build context) This example overrides the default Dockerfile by passing the -f flag: node &#123; checkout scm def dockerfile = &#x27;Dockerfile.test&#x27; def customImage = docker.build(&quot;my-image:$&#123;env.BUILD_ID&#125;&quot;, &quot;-f $&#123;dockerfile&#125; ./dockerfiles&quot;) &#125; Builds my-image:$&#123;env.BUILD_ID&#125; from the Dockerfile found at ./dockerfiles/Dockerfile.test. Push to Registry In order to use a custom Docker Registry, users of Scripted Pipeline can wrap steps with the withRegistry() method, passing in the custom Registry URL, for example: node &#123; checkout scm docker.withRegistry(&#x27;https://registry.example.com&#x27;) &#123; docker.image(&#x27;my-custom-image&#x27;).inside &#123; sh &#x27;make test&#x27; &#125; &#125;&#125; For a Docker Registry which requires authentication, add a &quot;Username/Password&quot; Credentials item from the Jenkins home page and use the Credentials ID as a second argument to withRegistry(): node &#123; checkout scm docker.withRegistry(&#x27;https://registry.example.com&#x27;, &#x27;credentials-id&#x27;) &#123; def customImage = docker.build(&quot;my-image:$&#123;env.BUILD_ID&#125;&quot;) /* Push the container to the custom Registry */ customImage.push() &#125;&#125; Bugs docker run -it ...报错: the input device is not a TTY Jenkins没有TTY, 所以不要使用-it Frontend node(&quot;volatile-ai-slave&quot;) &#123; def workspace = pwd() def git_branch = &#x27;**&#x27; def git_repository = &#x27;**&#x27; def vm_ip = &#x27;**&#x27; def vm_port = &#x27;**&#x27; def vm_user = &#x27;**&#x27; def IMAGE_NAME = &#x27;volatile_ai&#x27; def IMAGE_NAME_WITH_TAG = &#x27;volatile_ai:latest&#x27; def IMAGE_TO_RUN = &#x27;lyklove/volatile_ai:latest&#x27; def CONTAINER_NAME = &#x27;volatile_ai&#x27; stage(&#x27;clone from gitlab into slave\\&#x27;s workspace&#x27;) &#123; echo &quot;workspace: $&#123;workspace&#125;&quot; git branch: &quot;$&#123;git_branch&#125;&quot;, url: &quot;$&#123;git_repository&#125;&quot; &#125; stage(&#x27;cd to build context&#x27;) &#123; echo &quot;the context now is:&quot; sh &quot;ls -al&quot; sh &quot;cd $&#123;workspace&#125;&quot; echo &quot;cd to build context, now the context is:&quot; sh &quot;ls -al&quot; &#125; stage(&quot;build docker image&quot;)&#123; sh &quot;docker build -t $&#123;IMAGE_NAME&#125; .&quot; &#125;// stage(&quot;login to dockerhub&quot;)&#123;// withCredentials([usernamePassword(credentialsId: &#x27;DOCKERHUB_KEY&#x27;, passwordVariable: &#x27;password&#x27;, usernameVariable: &#x27;username&#x27;)]) &#123;// sh &#x27;docker login -u $username -p $password&#x27;// &#125;// &#125;// stage(&quot;push to dockerhub&quot;)&#123;// echo &quot;begin push to dockerhub&quot; sh &quot;docker image tag $&#123;IMAGE_NAME_WITH_TAG&#125; lyklove/$&#123;IMAGE_NAME_WITH_TAG&#125;&quot;// sh &quot;docker image push lyklove/$&#123;IMAGE_NAME_WITH_TAG&#125;&quot; &#125; stage(&quot;clean previous image and container&quot;)&#123; sh &quot;docker container rm -f $&#123;CONTAINER_NAME&#125;&quot;// sh &quot;docker image rm $&#123;IMAGE_NAME_WITH_TAG&#125;&quot;// sh &quot;docker image rm $&#123;IMAGE_TO_RUN&#125;&quot; &#125;// stage( &quot;pull image&quot; )&#123;// sh &quot;docker pull lyklove/$&#123;IMAGE_NAME_WITH_TAG&#125;&quot;// &#125; stage(&quot;run container&quot;) &#123; sh &quot;docker image ls&quot; sh &quot;docker container run --name $&#123;CONTAINER_NAME&#125; --net=host -d $&#123;IMAGE_TO_RUN&#125;&quot; &#125; stage(&quot;signal gitlab: deployed&quot;)&#123; updateGitlabCommitStatus name: &#x27;deployed&#x27;, state: &#x27;success&#x27; &#125;&#125; Integration with Gitlab Jenkins integration , 我的过程和文档上的略有不同 //Gitlabstage(&quot;signal github: deployed&quot;)&#123; echo &#x27;Notify GitLab&#x27; updateGitlabCommitStatus name: &#x27;build&#x27;, state: &#x27;pending&#x27; updateGitlabCommitStatus name: &#x27;build&#x27;, state: &#x27;success&#x27;&#125; Plugins 可以在 Jenkins plugins 搜索 也可以在 Manage Jenkins --&gt; Manage Plugins --&gt;可选插件 中选择 在jenkins安装时, 勾选推荐安装的插件(包括了Git插件) 需要手动安装: Install the Jenkins GitLab Plugin. 安装GitHub Plugin 系统管理--&gt;插件管理--&gt;可选插件 直接安装Github Plugin, jenkins会自动帮你解决其他插件的依赖，直接安装该插件Jenkins会自动帮你安装plain-credentials 、Git 、 credentials 、 github-api 日志 日志是最好的debug工具， jenkins日志位于：Dashboard -&gt; System Log Ref Gitlab Jenkins integration","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[{"name":"CICD","slug":"CICD","permalink":"http://lyk-love.cn/tags/CICD/"}]},{"title":"EulerOS","slug":"EulerOS","date":"2022-05-05T12:49:15.000Z","updated":"2022-09-26T06:39:34.930Z","comments":true,"path":"2022/05/05/EulerOS/","link":"","permalink":"http://lyk-love.cn/2022/05/05/EulerOS/","excerpt":"Outline： Introduction for EulerOS， 用于Linux考试","text":"Outline： Introduction for EulerOS， 用于Linux考试 2019年EulerOS被推送到开源社区，名为openEuler OpenEuler是一个开源的Linux发行版，支持Arm, x86, RISC-V等多种处理器架构 OpenEuler在多核调度技术，软硬件协同、轻量级虚拟化、指令集优化和智能优化引擎等方面做了增强 所有开发者、企业、商业组织都可以使用openEuler社区版本，也可以基于社区版本 发布自己二次开发的操作系统版本 线程间通信ITC： 互斥机制主要通过自旋锁实现，Euler提供了NUMA感知队列自旋锁实现互斥机制，减少了NUMA体系结构中使用自旋锁的开销 同步机制主要使用信号量实现，Euler提供了up和down原语 Euler还增强了两种ITC机制： 共享内存 消息传递 openEuler内存页相关说明 页表一般存储在一个地址连续的内存中，且能随机访问，以快速查找页表中相应的记录。在open Euler中，各级页表的表项大小为8B 页表的查询通常由与用的硬件内存管理单元(Memory Management Unit，MMU)快速完成，然 后交给OS完成(建表、设置基址寄存器、访存管理) openEuler将标准大页封装为一个伪文件系统(hugetlbfs)提供给用户程序申请并访问。 操作系统需要依据一定的页置换策略决定将哪些页进行换出，openEuler采用Least Recently Used (LRU)最近最久未使用策略实现页选择换出。 页在未来被访问的概率只能预测，不能精准判断 毕昇JDK:一款针对ARM优化的高性能 OpenJDK 发行版，SpecJBB 提升20% ARM64优化 :dmb指令消除等提升新能 快速序列化技术:提升序列化，反序列化性能 GC优化 :让系统减少卡顿 鲲鹏处理器是基于ARMv8-64位RISC指令集开发的通用处理器，使用大量寄存器：通用X0 - X30( 31个， 64位 ) + 特殊寄存器 + 系统寄存器 Euler还通过插件，对鲲鹏处理器做了优化，有： 对称/非对称加密 数字签名 压缩解压缩等算法，用于加速SSL/TLS应用和数据压缩","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://lyk-love.cn/tags/Linux/"}]},{"title":"Linux Programming","slug":"Linux-Programming","date":"2022-05-05T03:43:30.000Z","updated":"2022-09-26T06:39:34.933Z","comments":true,"path":"2022/05/05/Linux-Programming/","link":"","permalink":"http://lyk-love.cn/2022/05/05/Linux-Programming/","excerpt":"Outline： Linux文件操作api","text":"Outline： Linux文件操作api Linux File File types 从细节来说，文件类型分为: regular file character special file： 与设备进行交互的文件，按字符IO，如终端文件tty block special file: 同上，但是按块IO 管道文件/fifo ： 用于进程间通信 socket：表示一个socket连接 symbolic link：符号链接 directory ( 目录也是一种文件，我们只是把目录和文件分开讨论 ) File Structure File Structure: Byte stream; no particular internal structure FIle Descripter Linux中的文件描述符和文件指针FILE *的区别什么？ 文件描述符：在Linux系统中打开文件就会获得文件描述符，它是很小的正整数。每个进程在PCB(Process Control Block)中保存着一份文件描述表，文件描述符就是这个文件描述符的索引，每个表项都有一个指向已打开文件的指针。 文件指针：C语言中使用文件指针作为I/O的句柄，文件指针指向进程用户区中的一个被称为FILE结构的数据结构。FILE结果包括一个缓冲区和一个文件描述符。而文件描述符是文件描述符表的一个索引，因此从某种意义上文件指针就是句柄的句柄 Basic I/O System Calls File descriptor Basic I/O: open/creat, close, read, write, lseek dup/dup2 fcntl ioctl File descriptor File descriptor: A small non-negative integer int fd; 是thread local的 在UNIX中用于访问文件, 也可以将它作为指向文件对象的指针 in &lt;unistd.h&gt; STDIN_FILENO: 0 STDOUT_FILENO : 1 STDERR_FILENO: 2 进程总是会打开0,1,2这三个文件描述符 General steps of file operation: open read/write [lseek] close File Name Suffix suffix 解释 .c C source code which must be preprocessed .i C source code which should not be preprocessed .cc.cp.cpp.CPP. c++ .C .cxx C++sourcecodewhichmustbepreprocessed .ii C++ source code which should not be preprocessed .h C or C++ header file to be turned into a precompiled header .H .hh C++ header file to be turned into a precompiled header .s Assembler code .S Assembler code which must be preprocessed .o Object file .a Static library file (archive file) .so Dynamic library file (shared object) File Permission Perm File Directory r User can read contents of file User can list the contents of the directory w User can change contents of file User can change the contents of the directory x User can execute file as a command User can cd to directory and can use it in PATH SUID Program runs with effective user ID of owner SGID Program runs with effective group ID of owner Files created in directory inherit the same group ID as the directory Sticky bit Only the owner of the file and the owner of the directory may delete files in this directory mode 含义 S_IRUSR (00400) Read by owner S_IWUSR (00200) Write by owner S_IXUSR (00100) Execute by owner S_IRWXU(00700) Read, write and execute by owner S_IRGRP (00040) Read by group S_IWGRP (00020) Write by group S_IXGRP (00010) Execute by group S_IRWXG (00070) Read, write and execute by group S_IROTH (00004) Read by others S_IWOTH (00002) Write by others S_IXOTH (00001) Execute by others S_IRWXO (00007) Read, write and execute by others S_ISUID(04000) Set user ID on execution S_ISGID(02000) Set group ID on execution S_ISVTX(01000) Saved-text bit (sticky bit) Example: testing file permission: if (buf.st_mode &amp; S_IRUSR) printf(“readable by owner”);else printf(“unreadable by owner”); umask umask： 文件权限屏蔽字, 是用户在建立文件或目录时需要减掉的权限 file persission = mode &amp; ~umask 命令行工具： umask umask -S: 以符号形式显示 ❯ umask //表明group和others没有2（写）权限022//或者❯ umask -S u=rwx,g=rx,o=rx #include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;mode_t umask(mode_t mask); //为进程设置文件存取权限屏蔽字，并返回以前的值 Regular files: permissions symbol number default permissions rw-rw-rw- 666 umask ----w--w- 022 resulting permissions rw-r--r-- 644 Directories: permissions symbol number default permissions rwxrwxrwx 777 umask ----w--w- 022 resulting permissions rwxr-xr-x 755 access function 按实际用户ID和实际组ID测试文件存取权限 #include &lt;unistd.h&gt;int access(const char *pathname, int mode); Return: 0 if success; -1 if failure Parameter‏ mode: R_OK W_OK X_OK F_OK chmod/fchmod functions Change permissions of a file: #include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;int chmod(const char *path, mode_t mode);int fchmod(int fildes, mode_t mode); Return: 0 if success; -1 if failure File Attribute struct stat&#123; dev_t st_dev; //device 文件的设备编号 ino_t st_ino; //inode 文件的i-node mode_t st_mode; //protection 文件的类型和存取的权限 nlink_t st_nlink; //number of hard links 连到该文件的硬链接数目, 刚建立的文件值为1. uid_t st_uid; //user ID of owner 文件所有者的用户识别码 gid_t st_gid; //group ID of owner 文件所有者的组识别码 dev_t st_rdev; //device type 若此文件为装置设备文件, 则为其设备编号 off_t st_size; //total size, in bytes 文件大小, 以字节计算 unsigned long st_blksize; //blocksize for filesystem I/O 文件系统的I/O 缓冲区大小. unsigned long st_blocks; //number of blocks allocated 占用文件区块的个数, 每一区块大小为512 个字节. time_t st_atime; //time of lastaccess 文件最近一次被存取或被执行的时间, 一般只有在用mknod、utime、read、write 与tructate 时改变. time_t st_mtime; //time of last modification 文件最后一次被修改的时间, 一般只有在用mknod、utime 和write 时才会改变 time_t st_ctime; //time of last change i-node 最近一次被更改的时间, 此参数会在文件所有者、组、权限被更改时更新&#125;; shell中可以用stat工具 stat far File: far Size: 6 Blocks: 8 IO Block: 4096 regular fileDevice: 259,8 Inode: 2536724 Links: 1Access: (0777/-rwxrwxrwx) Uid: ( 1000/ lyk) Gid: ( 1001/ lyk)Access: 2021-11-26 21:31:56.163748288 +0800Modify: 2021-11-26 21:41:07.306528022 +0800Change: 2021-11-26 21:59:32.375301046 +0800 Birth: 2021-11-26 21:31:56.163748288 +0800 Linux Signal 1. SIGHUP连接挂断 2. SIGINT终端中断 3. SIGKILL终止进程（此信号不能被捕获或忽略） 4. SIGQUIT终端退出 5. SIGTERM终止 6. SIGCHLD子进程已经停止或退出 7. SIGCONT继续执行暂停进程 8. SIGSTOP停止执行（此信号不能被捕获或忽略） 9. SIGTSTP终端挂起 I/O系统调用 open/creat Function Open and possibly create a file or device #include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;int open(const char *pathname, int flags);int open(const char *pathname, int flags, mode_t mode);int creat(const char *pathname, mode_t mode); // equivalent to open with flags: O_CREAT|O_WRONLY|O_TRUNC Return: a new file descriptor if success; -1 if failure Parameter‏”flags” “flags”:‏file‏ access‏ mode 定义在 &lt;fcntl.h&gt; 分为主标志和副标志， 主标志必须，且是互斥的，即只能选择一种。 副标志是可选的，可以选择多个 标志之间用|隔开 主标志 含义 O_RDONLY 以只读方式打开文件 O_WRONLY 以只写方式打开文件 O_RDWR 以可读写方式打开文件 副标志 含义 O_TRUNC **若文件存在并且以可写的方式打开时，**此标志会将文件长度清为0，而原来存于该文件的资料也会消失 O_CREAT 若路径中的文件不存在则自动建立该文件 O_EXCL 如果与O_CREAT同时设置，此指令会去检查文件是否存在，文件若不存在则建立该文件，否则将导致打开文件错误。此外，若O_CREAT与O_EXCL同时设置，并且将要打开的文件为符号链接，则将导致打开文件失败 O_APPEND 读写文件从文件尾部开始移动，所写入的数据追加到文件尾 Parameter‏: mode mode: 设定新建的文件的权限, 详见下文《File Permission》 close Funtion // Close a file descriptor#include &lt;unistd.h&gt;int close(int fd);(Return: 0 if success; -1 if failure) open系统调用： #include &lt;sys/types.h&gt; #include &lt;sys/stat.h&gt; #include &lt;fcntl.h&gt;int main()&#123; int fd = open(&quot;foo&quot;, O_CREAT, O_WRONLY | O_TRUNC);&#125; open返回file discriptor,这是一个thread local的整数，在UNIX中用于访问文件。 也可以将它作为指向文件对象的指针 打开文件过程： 检索目录,把它的外存索引节点复制到活动索引节点表。 根据参数mode核对权限,如果非法,则这次打开失败。 当“打开”合法时,为文件分配用户打开文件表项和系统打开文件表项,并为表项设置初值。通过指针建立这些表项与活动索引节点间的联系。把fd,即用户打开文件表中相应文件表项的序号返回给调用者。 关闭文件过程： 根据fd找到用户打开文件表项,再找到系统打开文件表项。释放用户打开文件表项。 把对应系统打开文件表项中的f_count--如果非“0”,说明还有进程共享这一表项,不用释放直接返回;否则释放表项 把活动索引节点中的i_count --,若不为“0”,表明还有用户进程正在使用该文件,不用释放而直接返回,否则在把该活动索引节点中的内容复制回文件卷上的相应索引节点中后,释放该活动索引节点。 f_count和i_count分别反映进程动态地共享一个文件的两种方式, f_count反映不同进程通过同一个系统打开文件表项共享 一个文件的情况; i_count反映不同进程通过不同系统打开文件表项共享一 个文件的情况。 通过两种方式,进程之间既可用相同的位移指针f_offset, 也可用不同位移指针f_offset共享同一个文件。 read/write Function &lt;unistd.h&gt; // Read from a file descriptor#include &lt;unistd.h&gt;ssize_t read(int fd, void *buf, size_t count);(返回值: )//Write to a file descriptor#include &lt;unistd.h&gt;ssize_t write(int fd, const void *buf, size_t count); 第一个参数为文件描述符 第二个参数指向放置结果的缓冲区 第三个参数是缓冲区大小 返回值： read(): 读到的字节数，若已到文件尾为0. 若出错为-1 write(): 若成功为已写的字节数. 若出错为-1 // myCat.cwhile ((n = read(STDIN_FILENO, buf, BUFSIZE)) &gt; 0)&#123; if (write(STDOUT_FILENO, buf, n) != n) err_sys(“write error”); &#125;if (n&lt;0) err_sys(“read error”); 例子 使用strace跟踪system call &gt; echo hello&gt;foo&gt; strace cat foo ...&gt; open(&quot;foo&quot;, O_RDonly|O_LARGEFILE) //使用64位偏移量（ O_LARGEFILE ）&gt; read(3, &quot;hello\\n&quot;, 6 )&gt; read(3,&quot;&quot;,4096)&gt; close(3)...&gt;prompt 这是书上的输出，我自己manjaro64的打印结果如下： ...openat(AT_FDCWD, &quot;foo&quot;, O_RDONLY) = 3...read(3, &quot;hello\\n&quot;, 131072) = 6write(1, &quot;hello\\n&quot;, 6hello) = 6read(3, &quot;&quot;, 131072) = 0munmap(0x7f90b3935000, 139264) = 0close(3) = 0... 省略了一些输出 cat先打开文件准备读取 每个进程已经打开了三个文件： std input, std output, std err,其文件描述符分别为0，1，2。 因此open返回3 打开后，cat使用read() system call, ssize_t read(int fd, void * buf, size_t count); 第一个参数为文件描述符 第二个参数指向放置结果的缓冲区, strace显示了此时的读取结果hello\\n 第三个参数是缓冲区大小, 在我的电脑上是139264B 返回值为其读取的字节数，这里是6 同样能看到，write（）针对文件描述符1，这是标准输出。 这是cat要做的事，它可能直接调用write（），也可能调用库例程printf（），当然最终还是会调用write（） 然后，cat试图从文件中读取更多内容，但是文件中没有剩余字节，read（）返回0 程序知道它已经读取完了文件，因此调用close（），传入相应的文件描述符（3），该文件因此会关闭 lseek Function 不按顺序读取和写入 Reposition read/write file offset: #include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;off_t lseek(int fildes, off_t offset, int whence); Return: the resulting offset location if success; -1 if failure 参数 offset 的含义取决于参数 whence： 如果 whence 是 SEEK_SET，文件偏移量将被设置为 offset。 如果 whence 是 SEEK_CUR，文件偏移量将被设置为 当前偏移量加上 offset，offset 可以为正也可以为负。 如果 whence 是 SEEK_END，文件偏移量将被设置为文件长度加上 offset， offset 可以为正也可以为负。 使用lseek（） off_t lseek(int fildes, off_t offset, int whence); 可以看到，对每个进程打开的文件，OS都会跟踪一个当前偏移量offset。 要么每次读写后隐式更新，要么通过lssek（）指定 dup/dup2 Function dup用来复制oldfd所指的文件描述符。但复制成功时返回最小的尚未被使用的文件描述符。若有错误则返回－1，错误代码存入errno中。返回的新文件描述符和参数oldfd指向同一个文件，共享所有的锁定，读写指针，和各项权限或标志位 dup2可以用参数newfd指定新文件描述符的数值。若newfd已经被程序使用，系统就会将其关闭以释放该文件描述符；若newfd与oldfd相等，dup2将返回newfd，而不关闭他。dup2调用成功返回新的文件描述符，出错则返回－1 Duplicate a file descriptor #include &lt;unistd.h&gt;int dup(int oldfd);int dup2(int oldfd, int newfd); Return: the new file descriptor if success; -1 if failure File sharing Example: redirection, 步骤详见Using Shell， dup/dup2所做的就是给原有的文件描述符再分配一个复制，由于此时标准输出/输入一半已经关闭，新分配的fd一般就是标准输出/输入，这就实现了重定向 fcntl Function Manipulate a file descriptor，该函数对fd的操作比较全面 #include &lt;unistd.h&gt;#include &lt;fcntl.h&gt;int fcntl(int fd, int cmd);int fcntl(int fd, int cmd, long arg);int fcntl(int fd, int cmd, struct flock *lock);//可以对文件加锁//(返回值: 若成功则依赖于cmd，若出错为-1) The‏ operation‏ is‏ determined ‏by‏&quot;cmd&quot;. The‏ value‏ of‏&quot;cmd&quot; F_DUPFD: Duplicate a file descriptor F_GETFD/F_SETFD:‏Get/set ‏the‏ file‏d escriptor's ‏close-on exec flag：执行时是否关闭，文件描述符能否从父进程传递到子进程。 F_GETFL/F_SETFL:‏Get/set ‏the‏ file ‏descriptor's ‏flags(并不是所有情况都可以setfl的) F_GETOWN/F_SETOWN: Manage I/O availability signals(告诉当前进程是否I/O传来的信号)(不要求理解深刻) F_GETLK/F_SETLK/F_SETLKW: Get/set the file lock(暂时不讲) Example：dup/dup2 and fcntl cmd The ‏value ‏of‏ cmd: F_DUPFD: Duplicate a file descriptor F_GETFD/F_SETFD:‏Get/set‏the‏file‏descriptor‟s‏close-onexec flag. F_GETFL/F_SETFL:‏Get/set‏the‏file‏descriptor‟s‏flags F_GETOWN/F_SETOWN: Manage I/O availability signals F_GETLK/F_SETLK/F_SETLKW: Get/set the file lock Example: dup/dup2 and fcntl ioctl Function Control devices #include int ioctl(int d, int request, ...); fsync Function #include &lt;unistd.h&gt;int fsync(int fildes); 一般来说， 程序执行write（）系统调用时，文件系统会将写入在内存中缓冲一段时间。 要立即写入，需要fsync（） 强制写回脏数据 文件改名 int rename(char * oldname, char * newname); rename（）是原子操作 获取文件属性 int stat(const char * file_name, struct stat *buf); stat()用来将参数file_name 所指的文件状态, 复制到参数buf 所指的结构中 stat结构体见上文File Attribute 删除文件 shell命令rm使用unlink（）系统调用删除文件： int unlink(const char * pathname); change ownership Change ownership of a file #include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;int chown(const char *path, uid_t owner, gid_t group);int fchown(int fd, uid_t owner, gid_t group);int lchown(const char *path, uid_t owner, gid_t group); Return: 0 if success; -1 if failure 目录操作 仅仅列举函数名: mkdir / rmdir chdir/fchdir, getcwd Read a directory： opendir/closedir readdir telldir seekdir Data structures DIR: The data type of directory stream objects &lt;dirent.h&gt;: typedef struct __dirstream DIR; struct dirent: Directory item Defined in &lt;dirent.h&gt; ino_t d_ino; /* inode number */char d_name[NAME_MAX + 1]; /* file name */ 目录的打开、关闭、读、定位 #include &lt;sys/types.h&gt;#include &lt;dirent.h&gt;DIR *opendir(const char *name);int closedir(DIR *dir);struct dirent *readdir(DIR *dir);off_t telldir(DIR *dir);void seekdir(DIR *dir, off_t offset); example A directory scanning program DIR *dp;struct dirent *entry;if ( (dp = opendir(dir)) == NULL )err_sys(…);while ( (entry = readdir(dp)) != NULL ) &#123;lstat(entry-&gt;d_name, &amp;statbuf);if ( S_ISDIR(statbuf.st_mode) )…else…&#125;closedir(dp); 创建并挂载文件系统 mkfs()mount() Standard I/O Library File stream Standard I/O functions File Stream Stream ‏and ‏”FILE” ‏structure: FILE* fp; Predefined pointer: stdin, stdout, stderr  Buffered I/O Stream Buffering Operations #include &lt;stdio.h&gt; // 如果引入的是标准库，就不是系统调用，系统调用的输入参数一般是文件描述符而不是流指针 三种缓冲 块缓冲（完全缓冲） 行缓冲 无缓冲 setbuf用于打开或关闭流缓冲机制，参数buf指向一个长度为BUFSIZ（该常量在&lt;stdio.h&gt;中定义）的缓冲区；如果要关闭缓冲，则将buf设置为NULL即可: void setbuf(FILE *stream, char *buf); setvbuf用于精确地设置所需的缓冲类型 int setvbuf(FILE *stream, char *buf, int mode, size_tsize); mode取值如下： _IOFBF: 满缓冲 _IOLBF: 行缓冲 _IONBF: 无缓冲 如果指定了mode为带缓冲类型，而buf却为NULL，则系统会自动分配BUFSIZ个字节的缓冲区: Standard I/O Functions Stream open/close Stream read/write 每次一个字符的I/O 每次一行的I/O 直接I/O(二进制I/O) 格式化I/O Stream reposition Stream flush Stream open/close Open a stream: #include &lt;stdio.h&gt;FILE *fopen(const char *filename, const char *mode);int fclose(FILE *stream); Parameter‏ mode: r: Open text file for reading. w: Truncate file to zero length or create text file for writing. a: Open for appending. r+: Open for reading and writing. w+: Open for reading and writing. The file is created if it does not exist, otherwise it is truncated. a+: Open for reading and appending. The file is created if does not exist. Close a stream #include &lt;stdio.h&gt;int fclose(FILE *fp); Return: 0 if success; -1 if failure character operations input of a character #include &lt;stdio.h&gt;int getc(FILE *fp);int fgetc(FILE *fp);int getchar(void); Result: Reads the next character from a stream and returns it as an unsigned char cast to an int, or EOF on end of file or error. Three functions: ferror feof clearerr ungetc function: push a character back to a stream. output of a character #include &lt;stdio.h&gt;int putc(int c, FILE *fp);int fputc(int c, FILE *fp);int putchar(int c); Return: the character if success; -1 if failure Line of String operations Input of a Line of String #include &lt;stdio.h&gt;char *fgets(char *s, int size, FILE *stream);char *gets(char *s); //not recommended. fgets: reads in at most size-1 characters from stream and stores them into the buffer pointed by s. Reading stops after an EOF or a new line. A \\0 character is stored at the end of the buffer. Output of a Line of String #include &lt;stdio.h&gt;int fputs(const char *s, FILE *stream);int puts(const char *s); Binary Stream Input/Output #include &lt;stdio.h&gt;size_t fread(void *ptr, size_t size, size_t nmemb, FILE *stream);size fwrite(const void *ptr, size_t size, size_t nmemb, FILE *stream); Return: the number of a items successfully read or written Application: Read/write a binary array: float data[10];if ( fwrite(&amp;data[2], sizeof(float), 4, fp) != 4 )err_sys(“fwrite‏error”); Read/write a structure: struct &#123;short count; long total; char name[NAMESIZE];&#125;item;if ( fwrite(&amp;item, sizeof(item), 1, fp) != 1)err_sys(“fwrite‏error”); Formatted I/O scanf, fscanf, sscanf functions #include &lt;stdio.h&gt;int scanf(const char *format, ...);int fscanf(FILE *stream, const char *format, ...);int sscanf(const char *str, const char *format, ...); Use fgets, then parse the string printf, fprintf, sprintf functions: #include &lt;stdio.h&gt;int printf(const char *format, ...);int fprintf(FILE *stream, const char *format, ...);int sprintf(char *str, const char *format, ...); Reposition a stream fseek, ftell, rewind functions: #include &lt;stdio.h&gt;int fseek(FILE *stream, long int offset, int whence);long ftell(FILE *stream);void rewind(FILE *stream); fgetpos, fsetpos functions ( Introduced in ANSI C): #include &lt;stdio.h&gt;int fgetpos(FILE *fp, fpos_t *pos);int fsetpos(FILE *fp, const fpos_t *pos); Flush a stream 把流里的数据立刻写入文件 #include &lt;stdio.h&gt;int fflush(FILE *stream); Stream and File Descriptor 确定流使用的底层文件描述符: #include &lt;stdio.h&gt;int fileno(FILE *fp); 根据已打开的文件描述符创建一个流: #include &lt;stdio.h&gt;FILE *fdopen(int fildes, const char *mode); Temporary File Create a name for a temporary file: #include &lt;stdio.h&gt;char *tmpnam(char *s); 返回值: 指向唯一路径名的指针 Create a temporary file: #include &lt;stdio.h&gt;FILE *tmpfile(void); 返回值: 若成功为文件指针，若出错为NULL Advanced System Calls Handling file attributes stat/fstat/lstat, ... Handling directory stat/fstat/lstat functions Get file status #include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;unistd.h&gt;int stat(const char *filename, struct stat *buf);int fstat(int filedes, struct stat *buf);int lstat(const char *file_name, struct stat *buf); Return: 0 if success; -1 if failure struct stat struct stat &#123;mode_t st_mode; /*file type &amp; mode*/ino_t st_ino; /*inode number (serial number)*/dev_t st_rdev; /*device number (file system)*/nlink_t st_nlink; /*link count*/uid_t st_uid; /*user ID of owner*/gid_t st_gid; /*group ID of owner*/off_t st_size; /*size of file, in bytes*/time_t st_atime; /*time of last access*/time_t st_mtime; /*time of last modification*/time_t st_ctime; /*time of last file status change*/long st_blksize; /*Optimal block size for I/O*/long st_blocks; /*number 512-byte blocks allocated*/&#125;; Test macros for file types Defined in &lt;sys/stat.h&gt; Macro File type S_ISREG regular file S_ISDIR directory S_ISCHR character special file S_ISBLK block special file S_ISFIFO fifo S_ISLNK() symbolic link S_ISSOCK socket File lock File lock 可以保证文件的并发安全访问 分类： 记录锁： 可以锁定文件的部分区域甚至字节 劝告锁 检查，加锁由应用程序自己控制 强制锁 检查，加锁由内核控制 影响[open()， read()， write() 等 共享锁： 读锁，不能加排他锁 排他锁：写锁，不能加读锁或其他写锁 特殊类型： 共享模式强制锁 租借锁 标志位 mount -o mand /dev/sdb7 /mnt super_block s_flags MS_MANDLOCK fcntl记录锁 用于记录锁的fcntl函数: #include &lt;unistd.h&gt;#include &lt;fcntl.h&gt;int fcntl(int fd, int cmd, struct flock *lock); 返回值: 若成功则依赖于cmd，若出错为-1 struct flock struct flock&#123;...short l_type; /* Type of lock: F_RDLCK, F_WRLCK, F_UNLCK */short l_whence; /* How to interpret l_start: SEEK_SET, SEEK_CUR,SEEK_END */off_t l_start; /* Starting offset for lock */off_t l_len; /* Number of bytes to lock */pid_t l_pid; /* PID of process blocking our lock (F_GETLK only) */...&#125; cmd参数 cmd参数的取值： F_GETLK：获得文件的封锁信息 F_SETLK：对文件的某个区域封锁或解除封锁 F_SETLKW：功能同F_SETLK, wait方式 其它封锁命令 lockf函数： #include &lt;sys/file.h&gt;int lockf(int fd, int cmd, off_t len)","categories":[{"name":"Technology","slug":"Technology","permalink":"http://lyk-love.cn/categories/Technology/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://lyk-love.cn/tags/Linux/"}]},{"title":"make and Makefile","slug":"make-and-Makefile","date":"2022-05-05T03:43:04.000Z","updated":"2022-09-26T06:39:34.942Z","comments":true,"path":"2022/05/05/make-and-Makefile/","link":"","permalink":"http://lyk-love.cn/2022/05/05/make-and-Makefile/","excerpt":"Outline: make Makefile","text":"Outline: make Makefile make GNU make是一个命令工具，是一个用来控制软件构建过程的自动化管理工具。make通过称为Makefile的文件来完成并自动维护编译工作, makefile定义了系统各模块间的依赖关系，make解释makefile中的指令 make所在路径: $(MAKE) make [-f filename] [targetname] make会在当前目录下找名字叫&quot;Makefile&quot;或&quot;makefile&quot;的文件, 根据Makefile中定义的规则来执行命令 Makefile中第一条规则定义的target是默认target。 直接执行make，执行的就是默认target make install需要 root 权限 如果 config 的时候使用 root 权限，则编译后产生的所有文件都需要root权限 # automake方式./configure #生成新的makefilemakemake installmake uninstallmake cleanmake distclean# 退回到configure之前(删除makefile) example： TOPDIR = ../include $(TOPDIR)Rules.makEXTRA LIBS += :EXEC = $(INSTALL_DIR)/helloOBJS = hello.o # make uninstall之后系统中源代码仍然存在# 变量定义，makefile可以include别的makefileall: $(EXEC) # 默认执行make all $(EXEC): $(OBJS) $(CC) $(LDFLAGS) -0 $@ $(OBJS) $(EXTRA_ LIBS) # gcc的别名CC，$@明确了目标文件放置位置install: $(EXP_ INSTALL) $(EXEC) $(INSTALL_ DIR) # make install执行的指定目标clean: -rm -f $(EXEC) *.elf*.gdb *.o Makefile 概述 Makefile文件由一系列规则（rules）构成。每条规则的形式如下： &lt;target&gt; : &lt;prerequisites&gt; [tab] &lt;commands&gt; target是一个目标文件，可以是Object File，也可以是执行文件 prerequisites是要生成target所需要的文件或是目标 command是make需要执行的命令。(可以是任意的Shell命令) &quot;目标&quot;是必需的，不可省略；&quot;前置条件&quot;和&quot;命令&quot;都是可选的，但是两者之中必须至少存在一个 每条规则就明确两件事：构建目标的前置条件是什么，以及如何构建 Makefile文件的规则 hello : main.o kbd.o gcc -o hello main.o kbd.omain.o : main.c defs.h cc -c main.ckbd.o : kbd.c defs.h command.h cc -c kbd.cclean : rm edit main.o kbd.o # 伪目标 target 一个目标（target）就构成一条规则。目标通常是文件名，指明Make命令所要构建的对象，比如上文的 a.txt 。目标可以是一个文件名，也可以是多个文件名，之间用空格分隔 如果Make命令运行时没有指定目标，会执行Makefile文件的第一个目标作为默认目标 make 该命令执行Makefile的第一个目标 伪目标 除了文件名，目标还可以是某个操作的名字，这称为&quot;伪目标&quot;（phony target），伪目标不是文件名，而是一个操作的名字 clean: rm *.o 上面代码的目标是clean，属于&quot;伪目标 &quot;，作用是删除对象文件: make clean &quot;伪目标&quot;的取名不能和文件名重名 例如，如果当前目录中，正好有一个文件叫做clean，那么这个make clean不会执行。因为Make发现clean文件已经存在，就认为没有必要重新构建了，就不会执行指定的rm命令 为了避免和文件重名的这种情况，可以使用.PHONY来显示地指明一个目标是&quot;伪目标&quot;，向make说明，不管是否有这个文件，这个目标就是&quot;伪目标&quot; .PHONY: cleanclean: rm *.o temp 声明clean是&quot;伪目标&quot;之后，make就不会去检查是否存在一个叫做clean的文件，而是每次运行都执行对应的命令。像.PHONY这样的内置目标名还有不少，可以查看手册 伪目标一般没有依赖的文件，但也可以为伪目标指定所依赖的文件 伪目标同样可以作为&quot;默认目标&quot;，只要将其放在第一个 多目标 当多个目标同时依赖于一个文件，并且其生成的命令大体类似，可以使用$@表示目前规则中所有的目标的集合 举例: bigoutput littleoutput : text.ggenerate text.g -$(subst output,,$@) &gt; $@ # 将$@中的output替换成空#上述规则等价于bigoutput : text.g generate text.g -big &gt; bigoutputlittleoutput : text.g generate text.g -little &gt; littleoutput prerequisites prerequisite通常是一组文件名，之间用空格分隔。它指定了target是否重新构建的判断标准：make会检查prerequisite, 只要有一个prerequisite不存在，或者有过更新（前置文件的last-modification时间戳比目标的时间戳新），target就需要重新构建 如果prerequisite也不存在，make就会查找生成该prerequisite（此时是作为目标）的规则，这是个递归的过程 result.txt: source.txt cp source.txt result.txt 上面代码中，构建 result.txt 的前置条件是 source.txt 。如果当前目录中，source.txt 已经存在，那么make result.txt可以正常运行，否则必须再写一条规则，来生成 source.txt source.txt: echo &quot;this is the source&quot; &gt; source.txt 上面代码中，source.txt后面没有前置条件，就意味着它跟其他文件都无关，只要这个文件还不存在，每次调用make source.txt，它都会生成 make result.txtmake result.txt 上面命令连续执行两次make result.txt。第一次执行会先新建 source.txt，然后再新建 result.txt。第二次执行，Make发现 source.txt 没有变动（时间戳晚于 result.txt），就不会执行任何操作，result.txt 也不会重新生成 如果需要生成多个文件，往往采用下面的写法: source: file1 file2 file3 上面代码中，source 是一个伪目标，只有三个前置文件，没有任何对应的命令 make source 执行make source命令后，就会一次性生成 file1，file2，file3 三个文件。这比下面的写法要方便很多: make file1make file2make file3 commands 命令（commands）表示如何更新目标文件，由一行或多行的Shell命令组成。它是构建&quot;目标&quot;的具体指令，它的运行结果通常就是生成目标文件 每行命令之前必须有一个tab键。如果想用其他键，可以用内置变量.RECIPEPREFIX声明: .RECIPEPREFIX = &gt;all:&gt; echo Hello, world 上面代码用.RECIPEPREFIX指定，大于号（&gt;）替代tab键。所以，每一行命令的起首变成了大于号，而不是tab键 需要注意的是，每行命令在一个单独的shell中执行 , 这些Shell之间没有继承关系（因而普通变量不可见） var-lost: export foo=bar echo &quot;foo=[$$foo]&quot; 上面代码执行后（make var-lost），取不到foo的值。因为两行命令在两个不同的进程执行。一个解决办法是将两行命令写在一行，中间用分号分隔: var-kept: export foo=bar; echo &quot;foo=[$$foo]&quot; 另一个解决办法是在换行符前加反斜杠转义: var-kept: export foo=bar; \\ echo &quot;foo=[$$foo]&quot; 最后一个方法是加上.ONESHELL:命令: .ONESHELL:var-kept: export foo=bar; echo &quot;foo=[$$foo]&quot; Makefile文件的语法 makefile描述模块间的依赖关系 注释 井号（#）在Makefile中表示注释。 # 这是注释result.txt: source.txt # 这是注释 cp source.txt result.txt # 这也是注释 回显（echoing） 默认情况下，每执行一条 makefile 中的命令之前，Shell 终端都会显示出这条命令的具体内容，除非该命令用分号分隔而紧跟在依赖关系后面，我们称之为&quot;回显&quot;。如果不想显示命令的具体内容，我们可以在命令的开头加上@，这种情况通常用于 echo 命令 test: # 这是测试 执行上面的规则，会得到下面的结果 $ make test# 这是测试 在命令的前面加上@，就可以关闭回声 test: @# 这是测试 现在再执行make test，就不会有任何输出 由于在构建过程中，需要了解当前在执行哪条命令，所以通常只在注释和纯显示的echo命令前面加上@ test: @# 这是测试 @echo TODO 通配符 通配符（wildcard）用来指定一组符合条件的文件名。Makefile 的通配符与 Bash 一致，主要有星号（*）、问号（？）和 [...] 。比如， *.o 表示所有后缀名为o的文件。 clean: rm -f *.o 模式匹配 Make命令允许对文件名，进行类似正则运算的匹配，主要用到的匹配符是%。比如，假定当前目录下有 f1.c 和 f2.c 两个源码文件，需要将它们编译为对应的对象文件。 %.o: %.c 等同于下面的写法。 f1.o: f1.cf2.o: f2.c 使用匹配符%，可以将大量同类型的文件，只用一条规则就完成构建。 变量和赋值符 Makefile 允许使用等号自定义变量。 txt = Hello Worldtest: @echo $(txt) 上面代码中，变量 txt 等于 Hello World。调用时，变量需要放在 $( ) 之中。 调用Shell变量，需要在美元符号前，再加一个美元符号，这是因为Make命令会对美元符号转义。 test: @echo $$HOME 有时，变量的值可能指向另一个变量。 v1 = $(v2) 上面代码中，变量 v1 的值是另一个变量 v2。这时会产生一个问题，v1 的值到底在定义时扩展（静态扩展），还是在运行时扩展（动态扩展）？如果 v2 的值是动态的，这两种扩展方式的结果可能会差异很大。 为了解决类似问题，Makefile一共提供了四个赋值运算符 （=、:=、？=、+=），它们的区别请看StackOverflow。 VARIABLE = value# 在执行时扩展，允许递归扩展。VARIABLE := value# 在定义时扩展。VARIABLE ?= value# 只有在该变量为空时才设置值。VARIABLE += value# 将值追加到变量的尾端。 内置变量（Implicit Variables） Make命令提供一系列内置变量，比如，$(CC) 指向当前使用的编译器，$(MAKE) 指向当前使用的Make工具。这主要是为了跨平台的兼容性，详细的内置变量清单见手册。 output: $(CC) -o output input.c 自动变量（Automatic Variables） Make命令还提供一些自动变量，它们的值与当前规则有关。主要有以下几个。 （1）$@ $@指代当前目标，就是Make命令当前构建的那个目标。比如，make foo的 $@ 就指代foo。 a.txt b.txt: touch $@ 等同于下面的写法。 a.txt: touch a.txtb.txt: touch b.txt （2）$&lt; $&lt; 指代第一个前置条件。比如，规则为 t: p1 p2，那么$&lt; 就指代p1。 a.txt: b.txt c.txt cp $&lt; $@ 等同于下面的写法。 a.txt: b.txt c.txt cp b.txt a.txt （3）$? $? 指代比目标更新的所有前置条件，之间以空格分隔。比如，规则为 t: p1 p2，其中 p2 的时间戳比 t 新，$?就指代p2。 （4）$^ $^ 指代所有前置条件，之间以空格分隔。比如，规则为 t: p1 p2，那么 $^ 就指代 p1 p2 。 （5）$* $* 指代匹配符 % 匹配的部分， 比如% 匹配 f1.txt 中的f1 ，$* 就表示 f1。 （6）$(@D) 和 $(@F) $(@D) 和 $(@F) 分别指向 $@ 的目录名和文件名。比如，$@是 src/input.c，那么$(@D) 的值为 src ，$(@F) 的值为 input.c。 （7）$(&lt;D) 和 $(&lt;F) $(&lt;D) 和 $(&lt;F) 分别指向 $&lt; 的目录名和文件名。 所有的自动变量清单，请看手册。下面是自动变量的一个例子。 dest/%.txt: src/%.txt @[ -d dest ] || mkdir dest cp $&lt; $@ 上面代码将 src 目录下的 txt 文件，拷贝到 dest 目录下。首先判断 dest 目录是否存在，如果不存在就新建，然后，$&lt; 指代前置文件（src/%.txt）， $@ 指代目标文件（dest/%.txt）。 判断和循环 Makefile使用 Bash 语法，完成判断和循环。 ifeq ($(CC),gcc)libs=$(libs_for_gcc)elselibs=$(normal_libs)endif 上面代码判断当前编译器是否 gcc ，然后指定不同的库文件。 LIST = one two threeall: for i in $(LIST); do \\ echo $$i; \\ done# 等同于all: for i in one two three; do \\ echo $i; \\ done 上面代码的运行结果。 onetwothree 函数 Makefile 还可以使用函数，格式如下。 $(function arguments)# 或者$&#123;function arguments&#125; Makefile提供了许多内置函数，可供调用。下面是几个常用的内置函数。 （1）shell 函数 shell 函数用来执行 shell 命令 srcfiles := $(shell echo src/&#123;00..99&#125;.txt) （2）wildcard 函数 wildcard 函数用来在 Makefile 中，替换 Bash 的通配符。 srcfiles := $(wildcard src/*.txt) （3）subst 函数 subst 函数用来文本替换，格式如下。 $(subst from,to,text) 下面的例子将字符串&quot;feet on the street&quot;替换成&quot;fEEt on the strEEt&quot;。 $(subst ee,EE,feet on the street) 下面是一个稍微复杂的例子。 comma:= ,empty:=# space变量用两个空变量作为标识符，当中是一个空格space:= $(empty) $(empty)foo:= a b cbar:= $(subst $(space),$(comma),$(foo))# bar is now `a,b,c&#x27;. （4）patsubst函数 patsubst 函数用于模式匹配的替换，格式如下。 $(patsubst pattern,replacement,text) 下面的例子将文件名&quot;x.c.c bar.c&quot;，替换成&quot;x.c.o bar.o&quot;。 $(patsubst %.c,%.o,x.c.c bar.c) （5）替换后缀名 替换后缀名函数的写法是：变量名 + 冒号 + 后缀名替换规则。它实际上patsubst函数的一种简写形式。 min: $(OUTPUT:.js=.min.js) 上面代码的意思是，将变量OUTPUT中的后缀名 .js 全部替换成 .min.js 。 预定义变量 $&lt; 第一个依赖文件的名称 $? 所有的依赖文件，以空格分开，这些依赖文件的修改日期比目标的创建日期晚 $+ 所有的依赖文件，以空格分开，并以出现的先后为序，可能包含重复的依赖文件 $^ 所有的依赖文件，以空格分开，不包含重复的依赖文件 $* 不包括扩展名的目标文件名称 $@ 目标的完整名称 $% 如果目标是归档成员，则该变量表示目标的归档成员名称 edit : main.o kbd.o command.o display.o \\ insert.o search.o files.o utils.o gcc -o edit main.o kbd.o command.o display.o \\ insert.o search.o files.o utils.omain.o : main.c defs.h gcc -c main.ckbd.o : kbd.c defs.h command.h gcc -c kbd.ccommand.o : command.c defs.h command.h gcc -c command.cdisplay.o : display.c defs.h buffer.h gcc -c display.cinsert.o : insert.c defs.h buffer.h gcc -c insert.csearch.o : search.c defs.h buffer.h gcc -c search.cfiles.o : files.c defs.h buffer.h command.h gcc -c files.cutils.o : utils.c defs.h gcc -c utils.cclean : rm edit main.o kbd.o command.o display.o \\ insert.o search.o files.o utils.oOBJECTS = main.o kbd.o command.o display.o \\insert.o search.o files.o utils.oedit : $(OBJECTS) gcc -o edit $(OBJECTS)main.o : main.c defs.h gcc -c main.ckbd.o : kbd.c defs.h command.h gcc -c kbd.ccommand.o : command.c defs.h command.h gcc -c command.cdisplay.o : display.c defs.h buffer.h gcc -c display.cinsert.o : insert.c defs.h buffer.h gcc -c insert.csearch.o : search.c defs.h buffer.h gcc -c search.cfiles.o : files.c defs.h buffer.h command.h gcc -c files.cutils.o : utils.c defs.h gcc -c utils.cclean : rm edit $(OBJECTS) 多目标扩展 语法&lt;targets ...&gt;: &lt;target-pattern&gt;: &lt;prereq-patterns ...&gt; &lt;commands&gt; 例子 目标从$object中获取 &quot;%.o&quot;表明要所有以&quot;.o&quot;结尾的目标，即&quot;foo.o bar.o&quot;，就是变量$object集合的模式 依赖模式&quot;%.c&quot;则取模式&quot;%.o&quot;的&quot;%&quot;，也就是&quot;foo bar&quot;，并为其加下&quot;.c&quot;的后缀，于是依赖的目标就是&quot;foo.c bar.c&quot; objects = foo.o bar.oall: $(objects)$(objects): %.o: %.c $(CC) -c $(CFLAGS) $&lt; -o $@# 等价于如下foo.o : foo.c $(CC) -c $(CFLAGS) foo.c -o foo.obar.o : bar.c $(CC) -c $(CFLAGS) bar.c -o bar.o 编写方法： 遍历.c文件中的头文件依赖树，把每一个依赖的头文件都放到后面！gcc里面有参数。 不写.h的话：第一次编译连接不会有问题，但是若头文件发生更新，并不会重新编译 多目标扩展 语法：&lt;targets ...&gt;: &lt;target-pattern&gt;: &lt;prereq-patterns ...&gt;&lt;commands&gt;... 举例 objects = foo.o bar.oall: $(objects)$(objects): %.o: %. c$(CC) -c $(CFLAGS) $&lt; -o $@ 目标从$object中获取 &quot;%.o&quot;表明要所有以&quot;.o&quot;结尾的目标，即&quot;foo.o bar.o&quot;，就是变量$object集合的模式 依赖模式&quot;%.c&quot;则取模式&quot;%.o&quot;的&quot;%&quot;，也就是&quot;foo bar&quot;，并为其加下&quot;.c&quot;的后缀，于是依赖的目标就是&quot;foo.c bar.c&quot; 上述规则等价于 foo.o : foo.c$(CC) -c $(CFLAGS) foo.c -o foo.obar.o : bar.c$(CC) -c $(CFLAGS) bar.c -o bar.o Makefile执行顺序 make在当前目录下查找Makefile/makefile make查找Makefile中第一个target 如果该target不存在或者该target的依赖文件中至少有一个比target新，则继续执行指令 如果target的依赖文件不存在，则找到生存该依赖的规则，执行该规则 make根据依赖文件生成target","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://lyk-love.cn/tags/Linux/"}]},{"title":"Popular Compilers","slug":"Popular Compilers","date":"2022-05-05T03:41:07.000Z","updated":"2022-09-26T06:39:34.937Z","comments":true,"path":"2022/05/05/Popular Compilers/","link":"","permalink":"http://lyk-love.cn/2022/05/05/Popular%20Compilers/","excerpt":"Outline: GCC Clang/LLVM MinGw Crgwin MSVC","text":"Outline: GCC Clang/LLVM MinGw Crgwin MSVC GCC Official GCC： GNU Compiler Collection，是GNU开发的编译器， 用于Unix &amp;&amp; Unix like的OS 原名为GNU C Compiler，因为它原本只能处理C. 经过发展，现在的GCC已经可以处理如下语言: C, C++, Objective-C Fortran Ada Go D GCC是Linux和BSD家族的OS的标准编译器。 OSX用的是apple clang 一般来讲，符号链接cc指向了平台的默认编译器. Linux上cc指向gcc, OSX上指向的是clang gcc配套的调试工具是gdb, 使用的链接器是ld, 一般用make构建 gcc生成的代码不跨平台 Command Options 官网 GCC Command Options Overall Options gcc [option] [file] 注意，[option]和[file]中间可以不加空格，比如可以gcc -l[library] gcc xx.c默认会把文件输出到当前目录下的a.out gcc编译cpp文件时只会默认链接C标准库，不默认链接CPP标准库. 因此编译CPP文件需要加选项-lstdc++, 要么就使用g++编译 编译完之后，用其他的机器调试可能是不行的，因为file的路径一般是不一样的。 option Description -c 只汇编不链接，生成可重定向目标文件.o -S 只编译不汇编，生成汇编代码.s -E 只预处理不编译，生成预处理后文件.i -o [file] 将编译后文件输出到[file] -v 打印编译器内部各编译过程的命令行信息 -DMACRO[=DEFN] 定义MACRO宏(针对#define) --help --version Options for Debugging option Description -g 在每一个编译完的二进制码上打上文件名和行号的标签，供gdb使用 -O/On[n] 在程序编译、链接过程中进行优化处理. 0 级不优化,默认是2级. 优化和调试不兼容，所以不要同时使用-g 和-O 选项 -D 在预处理时添加#define指令, 比如-DAA=2相当于在添加了#define AA 2 编译优化： 对于不同版本的gcc，n的取值范围及其对应的优化效果可能并不完全相同，比较典型的范围是0-2或0-3. 不同的优化级别对应不同的优化处理工作。例如：优化选项“-O1”主要进行线程跳转和延迟退栈两种优化；优化选项“-O2”除了完成所有的“-O1”级别的优化之外，还要进行一些额外的调整工作，例如处理器指令调度等；优化选项“-O3”则还包括循环开展和其他一些与处理器特性相关的优化工作。 源代码的语句编译成的汇编码可能是多条语句，是一对多的关系 调试器：在执行编译后的二进制码，二进制码会被打标签，记录哪一个源代码的哪一行编译而来的。 调试的时候仍然使用的是本地编译好的二进制文件 Options for Directory Search option Description -Idir 在头文件的搜索路径列表中添加dir目录, 其优先级比系统头文件目录高. 如果有多个 -I 选项，按从左到右的顺序搜索，最后才是系统头文件目录 -iquotedir 和-I dir相同，但只对用引号括起来的头文件有效，如#include &quot;file&quot;; --sysroot dir 用 dir 作为头文件和库的根目录, 例如, 如果编译器正常情况下在/usr/include 找头文件，在 /usr/lib 中找库文件, 开启该选项之后，更改为dir/usr/include and dir/usr/lib -Ldir 在库文件的搜索路径列表中添加dir目录 Options for Linking option Description -static 只链接静态库 -shared 这是默认选项. 链接静态和动态库, 动态库优先 -l [library] 链接名为[library]的库文件. -l和[library]之间可以没有空格. 比如-lm就链接了名为libm.a的库文件 Options for Warning option Description -Wall 会在标准输出上打印warning信息 -w 关闭所有warning -werror 把所有的warning信息转换为错误信息，并在warning发生时终止编译过程 Options for Archtecture option Description -mieee-fp/-mno-ieee-fp 使用/不使用IEEE标准进行浮点数的比较 -msoft-float 输出包含浮点库调用的目标代码 -mshort 将int类型作为16位处理，相当于short int -mrtd 强行将函数参数固定的函数用ret NUM返回，节省调用函数的一条指令 -mcpu=type 针对不同的CPU使用相应的CPU指令。可选择的type有i386、i486、pentium、i686等 Searching Header File gcc 在编译时查找头文件的顺序： 先查找-I指定的目录 然后找gcc的环境变量 C_INCLUDE_PATH，CPLUS_INCLUDE_PATH，OBJC_INCLUDE_PATH 再找默认目录(取决于OS，有些OS中这些目录不存在)： /usr/include/usr/local/include gcc的一系列自带目录, 例如: /usr/include/c++/4.8.5 Searching Library File 库文件就是编译好的二进制文件(.o)， 相比头文件，使用库文件可以避免暴露源代码，并且减少编译时间。 对库文件的链接分为静态链接和动态链接 gcc 在编译时查找库文件的顺序： 先查找-L指定的目录 再找gcc的环境变量LIBRARY_PATH 再找默认目录( 同上，取决于OS)： /lib/lib64/usr/lib/usr/lib64/usr/local/lib/usr/local/lib64 g++ g++: GNU的CPP编译器，其实就是把gcc的前端换掉，后端不变 gcc会根据文件后缀名.c/.cpp将文件分别当作C/CPP来编译. g++统一将文件当作CPP编译 gcc编译cpp文件时只会默认链接C标准库，不默认链接CPP标准库. g++默认页链接CPP标准库 g++的命令选项和gcc相同 Clang/LLVM Clang官网 Clang+LLVM： LLVM是一个可以跨平台的编译器后端。 Clang是支持LLVM的编译器前端。Clang+LLVM组合是一个完整的编译器tool chain（暴打gcc） Clang：Clang是LLVM的C, Objective-C, and C++语言的前端，且也只支持这几种语言 Clang/LLVM使用的调试工具是LLDB, 链接器是lld, 构建工具是CMAKE clang和clang++的关系相当于gcc和g++的关系，一般我们用clang++编译CPP，当然也可以用clang -lstdc++ OSX上的clang是Apple Clang， 这是个clang的发行版，具体有啥区别我也不知道 Clang/LLVM vs gcc clang的调试信息非常简洁明了，gcc尽管随着不断发展，也在这方面赶上clang，但还有一定差距 clang/LLVM的源代码用CPP书写，结构清晰, 几乎是编译器架构的教科书。 gcc是C写的，而且用了一种类似Lisp的编程范式，有大量的元编程，非常难懂 对于CPP文件，clang的编译速度比gcc快。 不过对于c文件，二者没什么差别。 Linux内核的代码里面有很多奇奇怪怪的语法，是和gcc妥协写的，clang目前好像还不能编译linux内核。 另外，对于CPP模板元编程的报错信息，clang比gcc优雅很多 Criteria gcc Clang/LLVM License GNU GPL Apache 2.0 Supported language standards C++23 in experimental stage, C++20 fully complaint. gcc官网上的CPP标准支持情况 C++17 support available. C++20和23都只是部分支持 Clang官网上的CPP标准支持情况 Generated Code Characteristics Efficient with a lot of compiler options to play around with Efficient due to the SSA form used by LLVM backend Language independent type system No Yes (One of the design goal for LLVM) Build tool Make based CMake Parser Previously Bison LR. Now recursive descent. Hand-written recursive descent Linker LD lld Debugger GDB LLDB 查看编译器支持的CPP标准: // test.cpp#include&lt;iostream&gt; using namespace std;int main()&#123; //也有人会打印__STDC_VERSION__, 但这个宏不是被所有编译器支持的， 比如g++就不支持 cout &lt;&lt; __cplusplus &lt;&lt; endl;&#125; clang++ test.cpp -lstdc++ -o test &amp;&amp; ./test g++ test.cpp -lstdc++ -o test &amp;&amp; ./test 截至目前，Gcc对CPP23标准的支持只是实验性的，参见官网: C++23 features are available since GCC 11. To enable C++23 support, add the command-line parameter -std=c++2b to your g++ command line. Or, to enable GNU extensions in addition to C++23 features, add -std=gnu++2b. Important: Because the ISO C++23 standard is still evolving, GCC's support is experimental. No attempt will be made to maintain backward compatibility with implementations of C++23 features that do not reflect the final standard. LLVM Basic 创建一个C语言文件test.c #include &lt;stdio.h&gt;int main() &#123; printf(&quot;hello world\\n&quot;); return 0;&#125; 编译生成可执行文件： clang test.c -o test 运行可执行文件 生成llvm字节码文件： clang -O1 -emit-llvm test.c -c -o test.bc 生成LLVM 的汇编代码 .ll 文件(可视化字节码文件): clang -O1 -emit-llvm test.c -S -o test.ll 运行字节码文件： lli test.bc .ll文件也可以用lli来执行 将 .bc 文件转化为 .ll 文件: llvm-dis test.bc 将 .ll 文件转化为 .bc 文件: llvm-as test.ll 编译字节码文件为汇编文件： llc test.bc -o test.s 将 .bc 或 .ll 文件转化为本机平台的汇编代码： llc test.bcllc test.ll `` `` MinGw MinGW: Minimalist GNU For Windows. Window平台的编译器, 事实上它不只是一个编译器, 还包含了Linux(事实上是GNU)的一系列工具(包括编译器GCC和开发工具gawk, bison等), 使用MinGw可以在Win上使用类似Linux的开发工具链来构建Windows应用. 而且交叉编译Linux和WIndows代码也更方便 MinGw包含了一系列头文件和库文件, 允许人们在没有第三方动态链接库的情况下使用 GCC（GNU Compiler C）产生 Windows32 程序。 MinGW把源码中Unix-like OS API调用通过头文件翻译替换成相应的Windows API调用的编译环境 MinGW与Linux下广泛使用的GNU近乎完全兼容，这意味着，在Linux下如何编译源代码，在MinGW中也可以以完全相同的方式编译. 本质上讲，MinGw是为了给那些因为不喜欢使用Unix/Unix like而使用WIn的人一个和GNU大体相符的编译环境的 Cygwin Cygwin: 一个Windows下Unix-like模拟环境，具体说就是Unix-like接口（OS API，命令行）重定向层，其目的是不修改软件源码仅重新编译就可以将Unix-like系统上的软件移植到Windows上（这个移植也许还算不上严格意义上的无缝移植） Cygwin是让Windows拥有Unix-like环境的软件, 它里面安装了GCC, 作为其编译器 Cygwin可以比MingW移植更多的软件到Windows上，对Linux接口模拟比MingW全面 Cygwin模拟的Unix-like环境依然不是真实的Unix-like环境， 比如说，Cygwin依然只能打开exe文件，没法打开elf文件 MSVC MSVC: 微软开发的WIndows native的编译器和运行时, MSVC只能编译出WIndows平台的应用","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[{"name":"Compilers","slug":"Compilers","permalink":"http://lyk-love.cn/tags/Compilers/"}]},{"title":"Debugger","slug":"Debugger","date":"2022-05-05T03:40:59.000Z","updated":"2022-09-26T06:39:34.928Z","comments":true,"path":"2022/05/05/Debugger/","link":"","permalink":"http://lyk-love.cn/2022/05/05/Debugger/","excerpt":"Outline: GDB LLDB","text":"Outline: GDB LLDB GDB 官网 GDB: GNU Debugger，是和GCC配套的Debugger， 当然现在也支持Clang/LLVM 要使用GDB，需要GCC编译时开始-g选项，生成GDB所需的调试信息 Debug步骤： 设置断点 监视变量值 单步执行 修改变量值 commands GDB Tutorial Syntax: gdb [-help] [-nx] [-q] [-batch] [-cd=dir] [-f] [-b bps] [-tty=dev] [-s symfile] [-e prog] [-se prog] [-c core] [-x cmds] [-d dir] [prog[core|procID]] 使用gdb [executable-file]打开文件进行调试 command 解释 break/tbreak 设置断点，可以是行号、函数名及地址(以*开头) tbreak: 设置临时断点 run 执行当前调试的程序 list 列出源代码的一部分 next 执行一条语句但不进入函数内部 step 执行一条语句，是函数则进入函数内部 display 显示表达式的值 print 临时显示表达式的值 kill 中止正在调试的程序 LLDB LLDB( LLVM Debugger ):是Clang/LLVM的配套Debugger. 在OSX上， Xcode的默认编译器就是Clang/LLVM，Debugger就是LLDB LLDB和GDB用法差不多，详见GDB 和 LLDB command的差别","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[{"name":"Compilers","slug":"Compilers","permalink":"http://lyk-love.cn/tags/Compilers/"}]},{"title":"Hard Link && Symbol Link","slug":"Hard-Link-Symbol-Link","date":"2022-05-05T02:07:49.000Z","updated":"2022-09-26T06:39:34.930Z","comments":true,"path":"2022/05/05/Hard-Link-Symbol-Link/","link":"","permalink":"http://lyk-love.cn/2022/05/05/Hard-Link-Symbol-Link/","excerpt":"Outline: link symlink","text":"Outline: link symlink 硬链接( link ) 对应的shell命令：ln 对应系统调用link 详解 硬链接只是对同一个inode创建了新的引用 &gt; ln far far2 &gt; ls -i 2536724 far 2536724 far2 //inode number一样 事实上，文件名都只是对inode的链接 创建文件时，实际上是先创建inode，然后将人类可读的名称链接到该文件，并将这个键值对存入目录 不能创建目录的硬链接，因为会在目录树中成环 inode number在不同文件系统中不唯一，因此硬链接不能跨文件系统 硬链接会增加文件的引用计数，也就是ls -l里看到的那个 code Create a new link to (make a new name for) a file: #include &lt;unistd.h&gt;int link(const char *oldpath, const char *newpath); Return: 0 if success; -1 if failure Delete a name and possibly the file it refers to: #include &lt;unistd.h&gt;int unlink(const char *pathname); Return: 0 if success; -1 if failure 软链接( symlink ) 也称为符号链接 对应shell命令：ln -s ln -s far far3 对应系统调用symlink 详解 符号链接是一个不同类型的文件, 它的内容是被链接文件的文件名 ls显示，类型为l OS将截获对符号链接文件的访问，,依据符号链接中的文件名去读真正的目标文件 优点： 可链接目录， 可跨文件系统链接（因为只存储了目标路径） 缺点:搜索文件路径开销大,需要额外的空间查找存储路径 &gt; ls -aldrwxr-xr-x 3 lyk lyk 4096 11月 26 22:14 .drwxr-xr-x 4 lyk lyk 4096 11月 26 22:00 ..-rwxrwxrwx 2 lyk lyk 6 11月 26 21:41 far-rwxrwxrwx 2 lyk lyk 6 11月 26 21:41 far2lrwxrwxrwx 1 lyk lyk 3 11月 26 22:14 far3 -&gt; far //软链接，内容为目标文件名“far”，是三字节 code Create a symbolic link (named newpath which contains‏the‏sting‏”oldpath”): #include &lt;unistd.h&gt;int symlink(const char *oldpath, const char *newpath); Return: 0 if success; -1 if failure Read value of a symbolic link: #include &lt;unistd.h&gt;int readlink(const char *path, char *buf, size_t bufsiz); Return: the count of characters placed in the buffer if success; -1 if failure","categories":[{"name":"Technology","slug":"Technology","permalink":"http://lyk-love.cn/categories/Technology/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://lyk-love.cn/tags/Linux/"}]},{"title":"OS Thread Sheduling Algorithm","slug":"OS-Thread-Sheduling-Algorithm","date":"2022-04-16T23:13:57.000Z","updated":"2022-09-26T06:39:34.936Z","comments":true,"path":"2022/04/17/OS-Thread-Sheduling-Algorithm/","link":"","permalink":"http://lyk-love.cn/2022/04/17/OS-Thread-Sheduling-Algorithm/","excerpt":"Outline: workload scheduling metrics 处理器调度层次 各种算法...","text":"Outline: workload scheduling metrics 处理器调度层次 各种算法... workload Def: a fully-operational scheduling discipline 为了方便下面各种算法的说明， 我们目前对work load做出如下假设： 每个工作运行相同的时间（工作就是进程） 所有的工作同时到达 一旦开始，所有工作保持运行直到完成 所有工作只使用CPU（即，不执行IO操作） 所有工作的运行时间已知 scheduling metrics 性能指标： 周转时间： 任务完成 - 任务到达 响应时间：任务首次运行（服务时间） - 任务到达 用于度量交互性，因为分时系统的用户会面对终端，这要求终端交互性好 公平 处理器调度层次 高级调度（长程调度、作业调度）: 决定能否加入到执行的进程池中 中级调度（平衡负载调度）： 决定主存中的可用进程集合 低级调度（短程调度、 进程调度）：决定哪个进程占用处理器执行 优化周转时间算法 用于批处理作业。下述三个算法都面向“周转时间”，但是当衡量其“响应时间”时，下述算法表现都不好。 对于SJF而言，只要短进程源源不断来, 长进程就饿死 FCFS First Come First Service: 非抢占,直到某个进程运行结束,依次调度接下来的,偏爱长作业 推翻假设1后（可能出现长作业），该算法fail SJF Shortest Job First: 非抢占，先运行最短的任务 再推翻假设2,运行进程不同时到达，此时短任务可能会晚到达，由于算法非抢占，短任务必须等待长任务执行完，该算法fail STCF Shortest Time-to-Completion First: 最短完成时间优先: 抢占式SJF,推翻了假设3 优化响应时间算法 用于分时系统 RR Round Robin:抢占,time slice结束时,当前进程放入就绪队列,然后切换到队列中的下个进程 “抢占”是指进程之间。 在同一time slice内，进程不抢占 time slice必须是时钟中断周期的倍数 time slice太短也不好，因为context switch有成本 该算法在“周转时间”表现不佳 结合IO的STCF 推翻假设4,现在运行程序执行IO。此时假设有两个工作A和B,每项占用50msCPU，但每A运行10ms会发起10ms的IO请求，而B只使用CPU 此时可以把A的每个10ms工作视为独立的工作，因此系统先调度10ms的A，然后调度50ms的B，而10ms后新的A会提交，因此会抢占B执行，这样做实现了overlap overlap： 一个进程在另一个进程等待IO时使用CPU MLFQ 推翻假设5后，进程运行时间不可知，之前的算法失效 Multi-Level Feedback:多级反馈调度: 抢占式,每当进程被抢占时就降级 建立多个不同优先级的就绪进程队列，多个就绪进程队列之间按照优先级调度 高优先级的就绪进程, 分配的时间片短 同一就绪进程队列中的进程的优先数和时间片相同, 按照FCFS调度 工作进入系统时放在最高优先级 一旦工作用完了其在某一层中的时间片，它就被抢占，自己移入低一级队列 进程只有被抢占后才降级，因此如果一共只有一个进程，该进程不会降级 problem： 如果短工作不断到来，长进程可能饿死 改进： 经过一段时间S, 将系统中所有工作重新加入最高优先级队列 比例份额 确保每个工作获得一定比例的CPU时间，实现公平性 份额调度最大的缺点在于难以确定份额，即依赖假设5 彩票调度 每个进程持有一定数量的彩票，该进程的彩票数占总彩票数的百分比代表它占有某个资源的份额 CPU每个时间片随即抽取一个数，拥有该数对应彩票的进程会被调度一个时间片 例如工作A和B分别有75, 25个彩票，对应数字0 - 74, 75 - 99. OS抽取数字序列为：63, 85, 70, 39, 76, 17, 95 工作调度次序为：A, B, A, A, B, A, B 随机， 优点是实现简单，不依赖任何全局量 PROBLEM：基于概率，因此有可能出现异常情况 步长调度 每个进程都具有步长(stride), 和一个行程（pass）值。 步长： 一个大数 / 进程的彩票数 初始时所有行程值为0 每次调度当前具有最小行程值的进程，每当进程运行一个时间片后后， 行程值 += 步长 进程在时间片内不被抢占 current = remove_min(queue); //pick client with min passschedule(current);// use resource for quantumcurrent -&gt; pass += current -&gt; stride;// compute next pass using strideinsert( queue, current );// put back into the queue 例子：假设A,B,C彩票数分别为100,50,250, 其步长值分别为100, 200, 40 初始时所有步长为零，随即调度工作，假设A被调度，执行一个时间片，更新其行程值为100. 然后选择B, 运行后更新其行程值为200； 然后选择C,运行后更新其行程值为40 然后算法选择最小的工作，它会再调度2次C, C的行程值增加到120. 然后调度A ... 行程值（A） （步长 = 100） 行程值（A） （步长 = 100） 行程值（A） （步长 = 100） 被调度进程 0 0 0 A 100 0 0 B 100 200 0 C 100 200 40 C 100 200 80 C 100 200 120 A 200 200 120 C 200 200 160 C 可以看到，在这段时间内， A, B, C分别调度2, 1， 4次，与其彩票数占比相符合 彩票调度只能在概率上实现比例（因此有概率翻车），而步长调度可以直接控制比例 PROBLEM： 步长调度需要维护全局状态：行程值。 如果中途有新进程加入，则新进程的行程值为0,它会独占CPU HRRN HRRN(highest response radio next)高响应比优先**: 非抢占式** 响应比 = (等待时间+服务时间) / 服务时间 多处理器调度 缓存一致性： 一种方案： 总线窥探（bus snooping）: 每个缓存都监听链接所有缓存和内存的总线，在发现内存访问。 同步问题： 加锁 缓存亲和度： 同意进程在同一CPU运行时，由于有cache而运行得快 SQMS 单队列调度（Single-Queue Multiprocessor Scheduling）：所有工作放入一个全局的单调队列。每个CPU从队列中拿取工作 当然，可以将工作以时间片为单位存储进队列，这样CPU每次都运行一个time slice的工作 缓存亲和度问题： 假设有CPU 0-3, 队列中工作序列为A,B,C,D,E, 则： CPU0 CPU1 CPU2 CPU3 A B C D E A B C D E A B C D E A 可以看到亲和度很差 MQMS 多队列调度（Multi-Queue Multiprocessor Scheduling）: 每个CPU拥有自己的队列。 不同队列可以采用不同的调度规则。OS依据一些启发式规则将新工作放入某个队列 每个CPU之间调度相互独立","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"OS Basic","slug":"OS-Basic","permalink":"http://lyk-love.cn/tags/OS-Basic/"}]},{"title":"OS Page Sheduling Algorithm","slug":"OS-Page-Sheduling-Algorithm","date":"2022-04-16T23:13:46.000Z","updated":"2022-09-26T06:39:34.935Z","comments":true,"path":"2022/04/17/OS-Page-Sheduling-Algorithm/","link":"","permalink":"http://lyk-love.cn/2022/04/17/OS-Page-Sheduling-Algorithm/","excerpt":"Outline: 各种页面调度算法...","text":"Outline: 各种页面调度算法... OPT算法(Belady算法) 当要调入新页面时，替换掉距现在最长时间后再访问的页 OPT是页面替换算法的理想情况，无法实现，但可以用来衡量其他算法 [ FIFO算法 略 FIFO算法的Belady异常：更多的页框导致了更高的缺页率，页框为3和4的时候 LRU算法 淘汰最近最少用的那一页，即那些刚被使用过的页面，可以马上还要被使用到 LRU算法实现比较困难 [ LFU算法 LFU: lest frequent usage, 最不常用 淘汰最近一段时间内访问次数较少的页面，对OPT的模拟性比LRU更好 算法过程：基于时间间隔中断，并给每一页设置一个计数器，时间间隔中断发生后，所有计数器清0，每访问页1次就给计数器加1，选择计数最小的页面淘汰 CLOCK算法 每个页面设置一个访问位，再将内存中的页面都通过链接指针链接成一个循环队列， 设置一个指针指向下一个换页位置 工作流程 页调入内存时，其访问位置为1，指针前移（如果内存之前没有页，现在调入一页，那么指针指向下一页） 访问内存的页时，无论该页的访问位是1还是0，都将其访问位置为1，指针不动（因为没有增加新页） 内存已满，又有页要调入内存（即要淘汰页面）时，从指针当前指向的页开始扫描循环队列 把所遇到的引用标志位是1的页面的访问位清0并跳过 把所遇到的引用标志位是0的页面淘汰，并换页，指针前移 CLOCK算法的例子 灰色和星号代表1，蓝色和无星号代表0 局部最佳页面替换算法(MIN) 假设进程在时刻t访问某页面，如果该页面不在内存中，导致一次缺页，把该页面装入一个空闲页帧。不论发生缺页与否，算法在每一步要考虑引用串，如果该页面在时间$[t, t + \\tau ]$内未被再次引用，那么就移出；否则，该页被保留在进程驻留集中 $\\tau$为一个系统常量，间隔$[t, t + \\tau ]$称作滑动窗口 。下面的示例中$\\tau = 3$ 间隔是闭区间 示例，假设t0时刻内存中已有页P4， 此时进程要访问P4: 注意， 在t时刻遇到Px，时，要将其添加到驻留集（因为滑动窗口有左闭区间t，因此t时刻遇到的页一定不会在下一时刻被淘汰），那添加到驻留集的时间肯定是下一时刻（t+1） 工作集置换算法(WS) 工作集算法是局部最佳页面替换算法的模拟实现，不向前查看页面引用串，而是基于程序局部性原理向后看 工作集也就是向后看的滑动窗口( 记为$[t-\\Delta, t]$ ) 所看到的页面集合， 记为$W[t-\\Delta, t]$ $\\Delta$是系统常量,称为&quot;工作集窗口尺寸&quot;. 工作集中所包含的页面数目称为&quot;工作集尺寸&quot; 下面的示例中Δ=3","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"OS Basic","slug":"OS-Basic","permalink":"http://lyk-love.cn/tags/OS-Basic/"}]},{"title":"OS Virtualization","slug":"OS-Virtualization","date":"2022-04-16T22:38:59.000Z","updated":"2022-09-26T06:39:34.936Z","comments":true,"path":"2022/04/17/OS-Virtualization/","link":"","permalink":"http://lyk-love.cn/2022/04/17/OS-Virtualization/","excerpt":"Outline: CPU虚拟化 内存虚拟化","text":"Outline: CPU虚拟化 内存虚拟化 CPU虚拟化 进程 进程是虚拟化的CPU 进程映像包括: 进程控制块( PCB) 进程程序块 进程数据块 内核栈 在x86上，执行TRAP时， CPU会将一些寄存器保存到该进程的内核栈上， 从TRAP返回将弹出这些值，并恢复user mode 进程三状态图： Linux父进程与子进程在终止时没有相互依赖关系。即爹死了儿子也可以活着, 只是其父进程变为init进程( init 进程是系统的第一个进程，PID=1) 进程API 注意，fork()和exec()是分离的，这使得程序可以在fork()之后，exec()之前运行代码，最典型的例子是shell的workflow，参见Using Shell fork（） //5_1.c#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;unistd.h&gt;int main()&#123; printf( &quot;hello world (pid: %d)\\n&quot;, (int) getpid() ); int rc = fork(); if(rc &lt; 0) &#123; fprintf( stderr, &quot;fork failed \\n&quot; ); exit(1); &#125; else if(rc == 0) &#123; printf(&quot;hello, I am child (pid: %d)\\n&quot;, (int)getpid()); &#125; else&#123; printf(&quot;hello, I am parent of %d (pid: %d)\\n&quot;, rc, (int)getpid() ); &#125; return 0;&#125; hello world (pid: 5951)hello, I am child (pid: 5957)hello, I am parent of 5957 (pid: 5951) fork()创建进程 子进程不会从 main()开始执行，而是从fork()处返回，就像它自己调用了fork() 子进程几乎完全拷贝了父进程，拥有和父进程相同的地址空间，寄存器，PC等，但它从fork()获得的返回值不同 fork()返回值： ERRORS： -1 子进程： 0 父进程： 新创建的子进程的PID wait（） //5_2.c#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;unistd.h&gt;#include&lt;sys/wait.h&gt;int main()&#123; printf( &quot;hello world (pid: %d)\\n&quot;, (int) getpid() ); int rc=fork(); if(rc &lt; 0) &#123; fprintf( stderr, &quot;fork failed \\n&quot; ); exit(1); &#125; else if(rc == 0) &#123; printf(&quot;hello, I am child (pid: %d)\\n&quot;, (int)getpid()); &#125; else&#123; int wc = wait(NULL); printf(&quot;hello, I am parent of %d (wc: %d) (pid: %d)\\n&quot;, rc, wc, (int)getpid() ); &#125; return 0;&#125; hello world (pid: 6747)hello, I am child (pid: 6762)hello, I am parent of 6762 (wc: 6762) (pid: 6747) wait()会在子进程运行结束后才返回 如果父进程先运行，它会马上调用wait() exec（） //5_3.c#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;unistd.h&gt;#include&lt;string.h&gt;#include&lt;sys/wait.h&gt;int main()&#123; printf( &quot;hello world (pid: %d)\\n&quot;, (int) getpid() ); int rc = fork(); if(rc &lt; 0) &#123; fprintf( stderr, &quot;fork failed \\n&quot; ); exit(1); &#125; else if(rc == 0) &#123; printf(&quot;hello, I am child (pid: %d)\\n&quot;, (int)getpid()); char *myargs[3]; myargs[0] = strdup(&quot;wc&quot;);//program: &quot;wc&quot;(word count) myargs[1] = strdup(&quot;5_3.c&quot;);//argument: file to count myargs[2] = NULL; // marks end of array execvp( myargs[0], myargs ); printf(&quot;this shouldn&#x27;t print put&quot;); &#125; else&#123; int wc = wait(NULL); printf(&quot;hello, I am parent of %d (wc: %d) (pid: %d)\\n&quot;, rc, wc, (int)getpid() ); &#125; return 0;&#125; hello world (pid: 7615)hello, I am child (pid: 7634) 32 98 810 5_3.chello, I am parent of 7634 (wc: 7634) (pid: 7615) exec(): replaces the current process image with a new process imag. 将当前进程的内容替换为不同的程序（wc） 对exec()的成功调用永远不会返回，因为子进程的内容是新的程序 execvp(const char *file, char *const argv[]): one of exec() family The initial argument for these functions is the name of a file that is to be executed. The char *const argv[] argument is an array of pointers to null-terminated strings that represent the argument list available to the new program. The first argument, by convention, should point to the filename associated with the file being executed. The array of pointers must be terminated by a null pointer.( 因此有arg[2] = NULL ) 插叙 Shell fork()和exec()分离, 使得程序可以在fork()之后, exec()之前运行代码. Limited directed execution OS首先（在启动时）设置trap table并开启时钟中断 （都是特权操作），然后仅在受限模式下运行进程。 只在执行特权操作，或者当进程需要切换时，才需要OS干预 OS重获CPU控制权 如果一个进程在CPU上运行，那么OS无法运行。 因此OS需要重获CPU的控制权 等待系统调用： 进程通过syscall主动放弃CPU, 比如yield 时钟中断： 时钟设备可以产生中断，产生中断时，当前进程停止，OS中的 interrupt handler运行，此时OS重获CPU控制权 硬件在发生中断时需要为当前进程保存状态 context switch 上下文切换（ context switch）: OS获得控制权后，需要觉得是否切换进程，如果要切换，那么需要上下文切换 OS为当前进程保存状态，并为即将执行的进程恢复状态 &quot;状态&quot;: 寄存器，在该进程的内核栈中 事实上，除了寄存器，cache, TLB和其他硬件的状态也被切换，因此context switch的成本可能非常高昂 本质上是为了确保最后从陷阱返回时，不是返回到之前运行的进程，而是继续执行另一个进程 插叙 系统调用 我们对系统调用的调用，实际上是对C lib中对应函数的函数调用，这些函数遵循了内核的调用约定（如将参数推到栈，执行TRAP，返回控制权等）实现了系统调用。 当然，这些C lib中的函数是汇编写的 插叙 中断 在指令执行周期最后增加一个微操作，以响应中断，CPU在完成执行阶段后，如果允许中断，则进入中断阶段 中断处理过程： 保护CPU状态 分析被中断进程的PSW中断码字段，识别中断源 分别处理发生的中断事件 Thread Scheduling 详见OS Thread Sheduling Algorithm 内存虚拟化 我们的编写和编译程序时假定内存从0开始，事实上，程序执行时，OS会决定其在物理内存中的实际加载地址 前者称为地址空间（虚拟空间），后者称为物理空间 Address Space Address Space of Linux process ld, ls等命令实际上会调用execve( /usr/bin/COMMAND ) execve()只接受绝对路径 进程的地址空间 == 内存中若干连续的“段” mmap可以将内存中的某一段映射到文件中的某一段 进程中的代码和数据是mmap从内存中映射的 可以接受文件描述符 void *mmap(void *addr, size_t len, int prot, int flags, int fildes, off_t off); 查看进程的地址空间： pmap 动态链接的程序比静态链接的小，并且链接得快， 用pmap分别查看其内容: 静态链接程序的地址空间中有其链接库的内容（二进制文件的代码、数据、bss等） 动态链接程序的地址空间中只有其链接库的指针 可以看到地址空间的高位有三个段：vvar, vdso, vsyscall，用于virtual system call virtual system call: 不陷入内核的系统调用 pmap实际上打印了/proc/PID/maps的一部分信息 通过strace pmap XX可以看到pmap调用了openat( XX, \\proc\\PID\\maps ) Segmaentation 在MMU中引入不止一个基址/界限寄存器对，而是给每个逻辑段一对，这可以将每个段独立地载入物理内存。 由于只有已用的内存才在物理内存中分配空间，因此可以容纳巨大的地址空间 段：在内存空间中的一段连续定长（段有段界限）区域 引用非法地址就会引发段错误 分段会造成外部碎片 示例：该地址空间内分为代码, 堆, 栈三段, 然后映射到物理内存 假设要引用虚拟地址100（在代码段中），MMU将代码段基址加上偏移量（100）得到实际的地址100 + 32KB = 32868 假设要引用虚拟地址4200（在堆中），因为堆在虚拟地址4K开始，那么物理地址中的偏移量其实是4200 - 4K, 所以物理地址应该是 4200 - 4K + 34KB 硬件还需要知道段的增长方向，因为有些段，如栈就是反向增长的。（哪些段会往哪边增长，这可以通过增加标记位，也可以约定俗成）假设要引用虚拟地址15KB（在栈中），地址空间中的偏移是1KB，这意味着栈增长了1KB，而物理内存中栈基址是28KB，增长1KB（反向地）就是27KB。因此对应物理内存27KB 访问非法的地址就会报segmentation fault 示例：按如上的计算方式，我们需要知道段段基址，那自然就需要先知道段号，一般会在地址空间中分出高位表示段号。 假设地址空间为14位，前两位表示段号 # 段号掩码，二进制11刚好过滤出前两位段号#define SEG_MASK 0x3000# 段内偏移所在的位数#define SEG_SHIFT 12# 段内偏移的掩码，二进制的0xFFF刚好过滤出后十二位的段内偏移#define OFFSET_MASK 0xFFFint Bound[]; # 段界限寄存器int Base[]; # 段基址寄存器//得到段号SegmentNum = ( VirtualAddress &amp; SEG_MASK ) &gt;&gt; SEG_SHIFT;//得到段内偏移Offset = VirtualAddress &amp; OFFSET_MASK;if( Offset &gt;= Bound[ SegmentNum ] )&#123; RaiseException（ PROTECTION_FAULT ）；&#125;else&#123; //得到物理地址 PhysicalAddr = Base[ Segment ] + Offset; //访问该地址 Register = AccessMemory( PhysicalAddr );&#125; 共享 可以增加几位保护位，来表示段的权限，比如可以将代码段标记为只读， 同样的代码就可以被多个进程共享 Free Space Management 由于分段会把内存分为不同大小的单元（即不规则的内存块），造成外部碎片，因此需要空闲空间管理算法对内存进行管理 这里我们只讨论解决外部碎片（即分段）的空闲空间管理算法，假定算法管理的是一块块连续的字节区域（即内存块） 这里只考虑堆中的内存分配，即malloc，free操作 空闲列表 typedef struct node_t&#123; int size; struct node_t *next;&#125;node_t; 空闲列表就是一个链表，每个节点都记录了一块没有被分配的空间，假设有下面的 30 字节的堆: 这个堆对应的空闲列表会有两个元素，一个描述第一个 10 字节的空闲区域(字节 0~9)， 一个描述另一个空闲区域(字节 20~29): 很明显，任何大于 10 字节的分配请求都会失败(返回 NULL)，因为 没有足够的连续可用空间。 如果是小于10B的请求，那么就从列表的某个节点（比如第二个）中分割一块内存 但是，对于这个(小)堆，如果应用程序调用 free(10)，归还堆中间的空间，空闲列表会变成： 尽管整个堆现在完全空闲，但它似乎被分割成了 3 个 10 字节的区域。这时， 如果用户请求 20 字节的空间，简单遍历空闲列表会找不到这样的空闲块，因此返回失败. 为此，空闲列表应该能自动合并： 头块 头块： typedef struct header_t&#123; int size; int magic;&#125;header_t; free(void* ptr)函数没有指定块大小的参数，因为它假定，对于给定的指针，内存分配库可以确定要释放空间的大小，从而将其放回free list 一般这通过分配头块来实现。 每次分配内存块时，在其前面分配一个头块，保存额外信息，这样在（通过指针，也就是内存块起始位置）释放该内存块的时候，通过查询指针前面的头块，就知道了内存块的信息，比如大小，然后根据这些信息来释放内存块（头块也顺便被释放） 当然，这意味着malloc/free时，分配/释放的的内存大小，是想要分配/释放给用户的内存大小 + 头块大小 整体逻辑如下（假定内存从低位向高位分配，所以头块在内存块的前面）： void free(void *ptr)&#123; header_t *header_ptr = (void*)ptr - sizeof(header_t); // 根据分配给用户的内存指针，减去头块大小，获得头块的指针 //读取头块的信息 //... //释放头块和内存块 //...&#125; 头块 + 空闲列表的实现 假定要管理4KB的内存块，它是个堆，先初始化堆，加入空闲列表的头节点（free list的节点大小是8B）： node_t *head = mmap( NULL, 4096, PRPT_READ | PROT_WRITE, MAP_ANON | MAP_PRIVATE, -1, 0 );head -&gt; size = 4096 - sizeof(node_t);head -&gt; next = NULL; 执行这段代码之后，free list的状态只有一个节点，记录的空闲大小为 4088（因为已经分配了一个free list的节点，占了8B） head 指针指向这块区域的起始地址， 假设位于16KB(尽管任何虚拟地址都可以)。堆看起来如图 17.3 所示 假设有一个 100 字节的内存请求。为了满足这个请求，库首先要找到一个足够大小的块。因为目前只有一个 4088 字节的块，所以选中这个块。然后，这个块被分割(split) 为两块:一块足够满足请求(以及头块，如前所述)，一块是剩余的空闲块。假设记录头块为 8 个字节(一个整数记录大小，一个整数记录幻数)，堆中的空间如图 17.4 所示 至此，对于 100 字节的请求，库从原有的一个空闲块中分配了 108 字节，返回指向它的一个指针(在上图中用 ptr 表示)，并在其之前连续的 8 字节中记录头块信息，供未来的 free()函数使用。同时将列表中的空闲节点缩小为 3980 字节(4088−108)。 之后的内存分配以此类推 如果用户程序通过 free(ptr)归还一些内存，那无非是让head指向ptr - 8,读取8字节的头块，得到要释放的内存信息，然后释放头块+紧跟的内存块： 这个堆堆起始地址是16KB 假设要free(16500)（ 即16384( 16KB ) + 前一块的108 ），也就是图上的sptr指针, 则令head指向sptr前的头块，得到sptr（开头的）内存块的信息，然后删除头块和sptr内存块 Paging 分页将一个进程的地址空间分割成固定大小的单元，称为page, 并将物理内存也分割成相同的固定大小的单元，称为frame, 每个frame装一个page, 将page和frame编号 虚拟页号（ virtual page number, VPN ）， 因为地址空间都属于虚拟内存（虚拟空间），因此称为“虚拟”页号 物理帧号（ physical ）：因为帧都处于物理内存（物理空间）中，因此称为“物理”帧号 每个进程都有一个页表(page table), 页表就是页表项的列表。 每个页表项( page table entry, PTE )存储了一个page到 frame 的映射（即虚拟页号到物理页号到映射） 页表项的索引就等于VPN， 比如VPN为2， 那就对应着页表中下标为2的PFN 页表基址寄存器（ page table base register ）: 存储了页表的起始位置的物理地址，用于访问PTE： // 得到VPNVPN = ( VirtualAddress &amp; VPN_MASK ) &gt;&gt; SHIFT;// VPN就是PTE的索引，得到PTE在页表中的偏移，加上页表的起始地址，就是该PTE的物理地址PTEAddr = PageTableBaseRegister + (VPN + sizeof(PTE)); 对于一个虚拟地址，它由两部分组成： VPN + 业内偏移。 只要查询页表，找到PTE（VPN就是PTE的下标），读取PTE，将VPN转换为PFN，再加上业内偏移，就得到了物理地址： offset = VirtualAddress &amp; OFFSET_MASK PhysAddr = (PFN &lt;&lt; SHIFT) | offset// Extract the VPN from the virtual addressVPN = (VirtualAddress &amp; VPN_MASK) &gt;&gt; SHIFT// Form the address of the page-table entry (PTE)PTEAddr = PTBR + (VPN * sizeof(PTE))// Fetch the PTEPTE = AccessMemory(PTEAddr) // Check if process can access the page if (PTE.Valid == False) RaiseException(SEGMENTATION_FAULT)else if (CanAccess(PTE.ProtectBits) == False) RaiseException(PROTECTION_FAULT)else // Access is OK: form physical address and fetch it offset = VirtualAddress &amp; OFFSET_MASK PhysAddr = (PTE.PFN &lt;&lt; PFN_SHIFT) | offset Register = AccessMemory(PhysAddr) TLB Translation Look_aside Buffer 可以把部分PTE存入TLB，对每次内存访问，先查看TLB，看是否有期望的转换映射，有的话（TLB hit）就直接得到了PFN，不需要查页表。 没有的话(.TLB miss )就继续查页表，得到PTE，然后将该PTE写入TLB，再重试查TLB的指令，这次会命中(hit)，得到PFN TLB存放了PTE集合，PTE只对页表有效，页表只对所属的进程有效，因此TLB只对所属进程有效。 context switch时，要刷新TLB 刷新TLB会导致每次上下午切换后，都会有大量TLB miss， 为此，TLB实际上会存储多个进程（即多个页表）的PTE，并增加一个地址空间标识符字段（相当于PID），不同进程的PTE就用地址空间标识符来区分 除此之外，TLB项还有一些其他的控制位，比如有效位 TLB的有效位和页表的有效位不同，如果PTE无效，表面该页没有被进程申请使用，访问该页是非法的；而TLB项无效，仅仅表明该TLB项不是有效的地址映射 VPN = (VirtualAddress &amp; VPN_MASK) &gt;&gt; SHIFT (Success, TlbEntry) = TLB_Lookup(VPN)if (Success == True) // TLB Hit if (CanAccess(TlbEntry.ProtectBits) == True) //查TLB控制位，看该TLB项是否有效 Offset = VirtualAddress &amp; OFFSET_MASK PhysAddr = (TlbEntry.PFN &lt;&lt; SHIFT) | Offset //得到物理地址 AccessMemory(PhysAddr) else RaiseException(PROTECTION_FAULT) 10 //该TLB项无效，进入异常处理程序，其实一般就是进入下一步的页表读取步骤 // TLB Miss，进入常规的页表读取步骤else PTEAddr = PTBR + (VPN * sizeof(PTE)) PTE = AccessMemory(PTEAddr) if (PTE.Valid == False) RaiseException(SEGMENTATION_FAULT) else if (CanAccess(PTE.ProtectBits) == False) RaiseException(PROTECTION_FAULT) else TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits) RetryInstruction() 段页式 段页式就是将分段和分页结合，给进程的每个逻辑段分配一个页表。 此时段基址寄存器指向的就不是段的物理基址，而是段对应的页表的物理基址 示例， 假设 32 位虚拟地址空间包含 4KB 页面，并且地址空间分为 4 个段。在这个例子中，我们只使用 3 个段:代码，堆，栈 用地址空间的前两位表示段号。假设 00 是未使用的段，01 是代 码段，10 是堆段，11 是栈段。因此，虚拟地址如下所示: 当进程正在运行时，每个段的基址寄存器都包含该段的线性页表的物理地址。因此，系统中的每个进程现在都有 3 个与其关联的页表。在上下文切换时，必须更改这些寄存器，以反映新运行进程的页表的位置。 在 TLB 未命中时(假设硬件管理的 TLB，即硬件负责处理 TLB 未命中)，硬件使用分段位(SN)来确定段号（也确定了要用哪个基址和界限寄存器对）。然后硬件将段基址寄存器中的物理地址（就是页表的物理地址）与 VPN 结合起来， 形成页表项(PTE)的地址: SN = (VirtualAddress &amp; SEG_MASK) &gt;&gt; SN_SHIFT VPN = (VirtualAddress &amp; VPN_MASK) &gt;&gt; VPN_SHIFT AddressOfPTE = Base[SN] + (VPN * sizeof(PTE)) 多级页表 多级页表就是把页表本身也分页，每个页就是页表的sublist.对页表页，用页目录(.page directory )来管理，页目录的每一项就是页目录项（.page directory entry， PDE ）, 它存储了虚拟页号 - 页帧号的映射，以及对应页表页的有效位。 我们只讨论两级页表，更高级的页表可以以此类推 注意，这里“虚拟页号 - 页帧号映射“中的页帧号，指的是页表页的所在的页帧号。因为页表分页了，每一页自然就是物理内存中的物理帧，PDE就是将虚拟页号转换成页表页的物理帧号，根据虚拟页号来读取页表页。 因此，实际上PDE是“VPN - 页表页”的映射 “有效位”是面向页表页的，而一个页表页“有效”，指的是该页表页（就是PTE的集合）中至少一个PTE有效。 反之，一个无效的页表页就是所有PTE都无效，该页表页会被分配PDE，但不会再 好处是，假设一个页表有100项，可以每10项一页，分10页， 其中有七页都无效（即70个PTE都为空），按照传统的页表，我依然要分配100项的空间，但是按照多级页表，只需要为三页（30项）分配空间 反向页表 传统页表是每个进程一个，而反向页表是整个系统一个。每个PTE带有所属进程的标识符。 要搜索反向页表，需要借助散列表等数据结构 虚拟内存 通过设置交换空间，可以将内存容量（在逻辑上）扩大，用户看到的不是实际内存大小，而是虚拟内存大小 交换空间 可以在磁盘上分配一块空间用于用于物理页的移入和移出，这称为swap space，当然我们会假设OS以页为单位对swap space读取/写入 示例，假设一个 4 页的物理内存和一个 8 页的交换空间。3 个进程(进程 0、进程 1 和进程 2)主动共享物理内存。但 3 个中的每一个， 都只有一部分有效页在内存中，剩下的在硬盘的交换空间中。第 4 个进程(进程 3)的所有页都被交换到硬盘上，很明显它目前没有运行。有一块交换空间是空闲的。可以看出，使用交换空间让系统假装内存比实际物理内存更大： page fault 页错误实际上不算错误，页错误意思是找到的页不在物理内存中，需要从磁盘中换出来，但这个访问本身对用户来说是合法的。 Anyway，页错误的处理流程是： PFN = FindFreePhysicalPage()if (PFN == -1) // no free page found PFN = EvictPage() // os必须为将要换入的页找到一个物理帧，如果没有这样的物理帧，我们将踢出一些 物理页 DiskRead(PTE.DiskAddr, pfn) // sleep (waiting for I/O)PTE.present = TruePTE.PFN = PFNRetryInstruction()// update page table with present // bit and translation (PFN)// retry instruction 内核虚拟内存空间 内核虚拟空间是每个用户地址空间的一部分 可以把一部分页表放在内核的虚拟内存中，不会随着context switch而刷新，这样就提升了速度，也减少了用户空间的内存压力 放在内核虚拟空间的页表不会被切换，这也意味着其寄存器（基址/界限寄存器）不会被刷新 page scheduling 详见OS Page Sheduling Algorithm","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"OS Basic","slug":"OS-Basic","permalink":"http://lyk-love.cn/tags/OS-Basic/"}]},{"title":"Spring Boot Intro","slug":"Spring-Boot-Basic","date":"2022-04-13T20:38:17.000Z","updated":"2022-09-26T06:39:34.940Z","comments":true,"path":"2022/04/14/Spring-Boot-Basic/","link":"","permalink":"http://lyk-love.cn/2022/04/14/Spring-Boot-Basic/","excerpt":"Outline: Spring Boot工程 常用配置 Spring Boot Features Spring Boot基于Spring提供了开箱即用的一组套件，可以简化spring开发，还提供了基于java的、面向REST的微服务框架","text":"Outline: Spring Boot工程 常用配置 Spring Boot Features Spring Boot基于Spring提供了开箱即用的一组套件，可以简化spring开发，还提供了基于java的、面向REST的微服务框架 Spring Boot工程 Spring Boot工程目录结构： springboot-hello├── pom.xml├── src│ └── main│ ├── java│ └── resources│ ├── application.yml│ ├── logback-spring.xml│ ├── static│ └── templates└── target Spring Boot要求main()方法所在的启动类必须放到根package下，命名不做要求，这里我们以Application.java命名，它的内容如下： @SpringBootApplicationpublic class Application &#123; public static void main(String[] args) throws Exception &#123; SpringApplication.run(Application.class, args); &#125;&#125; 启动Spring Boot应用程序只需要加上注解@SpringBootApplication，该注解实际上又包含了： @SpringBootConfiguration @Configuration @EnableAutoConfiguration @AutoConfigurationPackage @ComponentScan 这样一个注解就相当于启动了自动配置和自动扫描 application.yml Spring Boot默认的配置文件是采用YAML格式的application.yml ,当然你也可以继续沿用spring的application.properties spring: application: name: $&#123;APP_NAME:unnamed&#125; datasource: url: jdbc:hsqldb:file:testdb username: sa password: driver-class-name: org.hsqldb.jdbc.JDBCDriver hikari: auto-commit: false connection-timeout: 3000 validation-timeout: 3000 max-lifetime: 60000 maximum-pool-size: 20 minimum-idle: 1 配置的优先级 和其他程序一样， 命令行手动指定的优先级最高。 对于配置文件而言，优先级取决于其所在位置，按优先级从高到低： 外置,在相对于应用程序运行目录的/config子目录里。 外置,在应用程序运行的目录里。 内置,在config包内。 内置,在Classpath根目录 同一目录下，application.yml &gt; application.properties 环境变量 在配置文件中，我们经常使用如下的格式对某个key进行配置： app: db: host: $&#123;DB_HOST:localhost&#125; user: $&#123;DB_USER:root&#125; password: $&#123;DB_PASSWORD:password&#125; 这种$&#123;DB_HOST:localhost&#125;意思是，首先从环境变量查找DB_HOST，如果环境变量定义了，那么使用环境变量的值，否则，使用默认值localhost。 这使得我们在开发和部署时更加方便，因为开发时无需设定任何环境变量，直接使用默认值即本地数据库，而实际线上运行的时候，只需要传入环境变量即可： DB_HOST=10.0.1.123 DB_USER=prod DB_PASSWORD=xxxx java -jar xxx.jar yaml文件格式 使用缩进表示层级关系，不允许使用Tab键，只允许使用空格 # 表示注释，从这个字符一直到行尾，都会被解析器忽略。 对象，键值对，使用冒号结构表示 animal: pets hash: { name: Steve, foo: bar } 数组,一组连词线开头的行，构成一个数组 - Cat - Dog - Goldfish 行内表示法：animal: [Cat, Dog] 常用配置 让服务器监听不同的端口： server:port: 8000 使用https： 使用JDK的keytool工具来创建一个密钥存储(keystore) keytool -keystore mykeys.jks -genkey -alias tomcat -keyalg RSA 配置文件如下： server: port: 8443 ssl: key-store:file:///path/to/mykeys.jks key-store-password: letmein key-password: letmein server.ssl.key-store 属性指向密钥存储文件的存放路径。这里用了一个file://开头的URL, 从文件系统里加载该文件。你也可以把它打包在应用程序的JAR文件里,用classpath: URL来 引用它。server.ssl.key-store-password和server.ssl.key-password设置为创建该文 件时给定的密码。 配置单数据源 数据源配置可以用Bean， 但更方便的做法是通过配置文件,以Mysql为例： spring: datasource: url: jdbc:mysql://localhost/readinglist username: dbuser password: dbpass 通常你都无需指定JDBC驱动,Spring Boot会根据数据库URL识别出需要的驱动,但如果识别出问题了,你还可以设置spring.datasource.driver-class-name属性: spring: datasource: url: jdbc:mysql://localhost/readinglist username: dbuser password: dbpass driver-class-name: com.mysql.jdbc.Driver 在自动配置 DataSource Bean的时候,Spring Boot会使用这里的连接数据。 DataSource Bean是一个连接池,如果Classpath里有Tomcat的连接池DataSource,那么就会使用这个连接池; 否则,Spring Boot会在Classpath里查找以下连接池: HikariCP Commons DBCP Commons DBCP 2 这里列出的只是自动配置支持的连接池,你还可以自己配置DataSource Bean,使用你喜欢的各种连接池 配置多数据源 配置多数据源和单数据源没什么区别，但是如果使用了Mybatis等ORM框架，记得要额外配置一下ORM的扫描规则。 大概步骤为： （如果需要的话）引入新数据库的驱动的依赖 在配置文件里面配置新的数据库连接 新增配置类，在里面配置ORM框架的扫描规则 （如果需要的话）使用新的连接池 引入数据库驱动依赖 如果你新增的数据库数据源和目前的数据库不同，记得引入新数据库的驱动依赖，比如 MySQL 和 PGSQL。 &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.postgresql&lt;/groupId&gt; &lt;artifactId&gt;postgresql&lt;/artifactId&gt; &lt;version&gt;42.2.7&lt;/version&gt;&lt;/dependency&gt; 配置数据库连接 因为数据源要有一个默认使用的数据源，最好在名称上有所区分（这里使用 primary 作为主数据源标识） ########################## 主数据源 ##################################spring.datasource.primary.jdbc-url=jdbc:mysql://127.0.0.1:3306/demo1?characterEncoding=utf-8&amp;serverTimezone=GMT%2B8spring.datasource.primary.driver-class-name=com.mysql.jdbc.Driverspring.datasource.primary.username=rootspring.datasource.primary.password=########################## 第二个数据源 ###############################spring.datasource.datasource2.jdbc-url=jdbc:mysql://127.0.0.1:3306/demo2?characterEncoding=utf-8&amp;serverTimezone=GMT%2B8spring.datasource.datasource2.driver-class-name=com.mysql.jdbc.Driverspring.datasource.datasource2.username=rootspring.datasource.datasource2.password=# mybatismybatis.mapper-locations=classpath:mapper/*.xmlmybatis.type-aliases-package=com.wdbyte.domain 注意，配置中的数据源连接 url 末尾使用的是 jdbc-url. 因为使用了 Mybatis 框架，所以 Mybatis 框架的配置信息也是少不了的，指定扫描目录 mapper 下的mapper xml 配置文件。 配置Mybatis的扫描路径 到目前为止， Mybatis 多数据源和单数据源写法唯一的区别就是 Mapper 接口使用不同的目录分开了，那么这个不同点一定会在数据源配置中体现 主数据源 开始配置两个数据源信息，先配置主数据源，配置扫描的 MapperScan 目录为 com.wdbyte.mapper.primary /** * 主数据源配置 * * @author niujinpeng * @website: https://www.wdbyte.com * @date 2020/12/19 */@Configuration@MapperScan(basePackages = &#123;&quot;com.wdbyte.mapper.primary&quot;&#125;, sqlSessionFactoryRef = &quot;sqlSessionFactory&quot;)public class PrimaryDataSourceConfig &#123; @Bean(name = &quot;dataSource&quot;) @ConfigurationProperties(prefix = &quot;spring.datasource.primary&quot;) @Primary public DataSource dataSource() &#123; return DataSourceBuilder.create().build(); &#125; @Bean(name = &quot;sqlSessionFactory&quot;) @Primary public SqlSessionFactory sqlSessionFactory(@Qualifier(&quot;dataSource&quot;) DataSource dataSource) throws Exception &#123; SqlSessionFactoryBean bean = new SqlSessionFactoryBean(); bean.setDataSource(dataSource); bean.setMapperLocations(new PathMatchingResourcePatternResolver().getResources(&quot;classpath:mapper/*.xml&quot;)); return bean.getObject(); &#125; @Bean(name = &quot;transactionManager&quot;) @Primary public DataSourceTransactionManager transactionManager(@Qualifier(&quot;dataSource&quot;) DataSource dataSource) &#123; return new DataSourceTransactionManager(dataSource); &#125; @Bean(name = &quot;sqlSessionTemplate&quot;) @Primary public SqlSessionTemplate sqlSessionTemplate(@Qualifier(&quot;sqlSessionFactory&quot;) SqlSessionFactory sqlSessionFactory) &#123; return new SqlSessionTemplate(sqlSessionFactory); &#125;&#125; 和单数据源不同的是这里把 dataSource sqlSessionFactory transactionManager sqlSessionTemplate 都单独进行了配置，简单的 bean 创建，下面是用到的一些注解说明。 @ConfigurationProperties(prefix = &quot;spring.datasource.primary&quot;)：使用spring.datasource.primary 开头的配置。 @Primary ：声明这是一个主数据源（默认数据源），多数据源配置时必不可少。 @Qualifier：显式选择传入的 Bean。 第二个数据源 第二个数据源和主数据源唯一不同的只是 MapperScan 扫描路径和创建的 Bean 名称，同时没有 @Primary 主数据源的注解。 /** * 第二个数据源配置 * * @author niujinpeng * @website: https://www.wdbyte.com * @date 2020/12/19 */@Configuration@MapperScan(basePackages = &#123;&quot;com.wdbyte.mapper.datasource2&quot;&#125;, sqlSessionFactoryRef = &quot;sqlSessionFactory2&quot;)public class SecondDataSourceConfig &#123; @Bean(name = &quot;dataSource2&quot;) @ConfigurationProperties(prefix = &quot;spring.datasource.datasource2&quot;) public DataSource dataSource() &#123; return DataSourceBuilder.create().build(); &#125; @Bean(name = &quot;sqlSessionFactory2&quot;) public SqlSessionFactory sqlSessionFactory(@Qualifier(&quot;dataSource2&quot;) DataSource dataSource) throws Exception &#123; SqlSessionFactoryBean bean = new SqlSessionFactoryBean(); bean.setDataSource(dataSource); bean.setMapperLocations(new PathMatchingResourcePatternResolver().getResources(&quot;classpath:mapper/*.xml&quot;)); return bean.getObject(); &#125; @Bean(name = &quot;transactionManager2&quot;) public DataSourceTransactionManager transactionManager(@Qualifier(&quot;dataSource2&quot;) DataSource dataSource) &#123; return new DataSourceTransactionManager(dataSource); &#125; @Bean(name = &quot;sqlSessionTemplate2&quot;) public SqlSessionTemplate sqlSessionTemplate(@Qualifier(&quot;sqlSessionFactory2&quot;) SqlSessionFactory sqlSessionFactory) &#123; return new SqlSessionTemplate(sqlSessionFactory); &#125;&#125; 注意：因为已经在两个数据源中分别配置了扫描的 Mapper 路径，如果你之前在 SpringBoot 启动类中也使用了 Mapper 扫描注解，需要删掉。 连接池 其实在多数据源改造中，我们一般情况下都不会使用默认的 JDBC 连接方式，往往都需要引入连接池进行连接优化，不然你可能会经常遇到数据源连接被断开等报错日志。其实数据源切换连接池数据源也是十分简单的，直接引入连接池依赖，然后把创建 dataSource 的部分换成连接池数据源创建即可 下面以阿里的 Druid 为例，先引入连接池数据源依赖。 &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt;&lt;/dependency&gt; 添加 Druid 的一些配置。 spring.datasource.datasource2.initialSize=3 # 根据自己情况设置spring.datasource.datasource2.minIdle=3spring.datasource.datasource2.maxActive=20 改写 dataSource Bean 的创建代码部分。 @Value(&quot;$&#123;spring.datasource.datasource2.jdbc-url&#125;&quot;)private String url;@Value(&quot;$&#123;spring.datasource.datasource2.driver-class-name&#125;&quot;)private String driverClassName;@Value(&quot;$&#123;spring.datasource.datasource2.username&#125;&quot;)private String username;@Value(&quot;$&#123;spring.datasource.datasource2.password&#125;&quot;)private String password;@Value(&quot;$&#123;spring.datasource.datasource2.initialSize&#125;&quot;)private int initialSize;@Value(&quot;$&#123;spring.datasource.datasource2.minIdle&#125;&quot;)private int minIdle;@Value(&quot;$&#123;spring.datasource.datasource2.maxActive&#125;&quot;)private int maxActive;@Bean(name = &quot;dataSource2&quot;)public DataSource dataSource() &#123; DruidDataSource dataSource = new DruidDataSource(); dataSource.setUrl(url); dataSource.setDriverClassName(driverClassName); dataSource.setUsername(username); dataSource.setPassword(password); dataSource.setInitialSize(initialSize); dataSource.setMinIdle(minIdle); dataSource.setMaxActive(maxActive); return dataSource;&#125; 这里只是简单的提一下使用连接池的重要性，Druid 的详细用法还请参考官方文档 配置日志 默认情况下,Spring Boot会用Logback(http://logback.qos.ch)来记录日志,并用INFO级别输 出到控制台。在运行应用程序和其他例子时,你应该已经看到很多INFO级别的日志了。 一般来说,你不需要切换日志实现;Logback能很好地满足你的需要。但是,如果决定使 用Log4j或者Log4j2,那么你只需要修改依赖,引入对应该日志实现的起步依赖,同时排除掉 Logback。 以Maven为例,应排除掉根起步依赖传递引入的默认日志起步依赖,这样就能排除 Logback了: &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-logging&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 在Gradle里,在 configurations下排除该起步依赖是最简单的办法: configurations &#123; all*.exclude group:&#x27;org.springframework.boot&#x27;, module:&#x27;spring-boot-starter-logging&#x27;&#125; 排除默认日志的起步依赖后,就可以引入你想用的日志实现的起步依赖了。在Maven里可 以这样添加Log4j: &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-log4j&lt;/artifactId&gt;&lt;/dependency&gt; 在Gradle里可以这样添加Log4j: compile(&quot;org.springframework.boot:spring-boot-starter-log4j&quot;) 要完全掌握日志配置,可以在Classpath的根目录(src/main/resources)里创建logback.xml文 件。下面是一个logback.xml的简单例子: &lt;configuration&gt;&lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt;&lt;encoder&gt;&lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n&lt;/pattern&gt;&lt;/encoder&gt;&lt;/appender&gt;&lt;logger name=&quot;root&quot; level=&quot;INFO&quot;/&gt;&lt;root level=&quot;INFO&quot;&gt;&lt;appender-ref ref=&quot;STDOUT&quot; /&gt;&lt;/root&gt;&lt;/configuration&gt; 除了日志格式之外,这个Logback配置和不加logback.xml文件的默认配置差不多。但是,通 过编辑logback.xml,你可以完全掌控应用程序的日志文件。哪些配置应该放进logback.xml这个话 题不在本书的讨论范围内,请参考Logback的文档以了解更多信息。 即使如此,你对日志配置最常做的改动就是修改日志级别和指定日志输出的文件。使用了 Spring Boot的配置属性后,你可以在不创建logback.xml文件的情况下修改那些配置。 要设置日志级别,你可以创建以logging.level开头的属性,后面是要日志名称。如果根 日志级别要设置为WARN,但Spring Security的日志要用DEBUG级别,可以在application.yml里加入 以下内容: logging: level: root: WARN org: springframework: security: DEBUG 另外,你也可以把Spring Security的包名写成一行: logging: level: root: WARN org.springframework.security: DEBUG 现在,假设你想把日志写到位于/var/logs/目录里的BookWorm.log文件里。使用 logging. path和loggin.file属性就行了: logging: path: /var/logs/ file: BookWorm.log level: root: WARN org: springframework: security: DEBUG 假设应用程序有/var/logs/的写权限,日志就能被写入/var/logs/BookWorm.log。默认情况下, 日志文件的大小达到10MB时会切分一次。 与之类似,这些属性也能在application.properties里设置: logging.path=/var/logs/logging.file=BookWorm.loglogging.level.root=WARNlogging.level.root.org.springframework.security=DEBUG 如果你还是想要完全掌控日志配置,但是又不想用logback.xml作为Logback配置的名字,可 以通过logging.config属性指定自定义的名字: logging: config: classpath: logging-config.xml 虽然一般并不需要改变配置文件的名字,但是如果你想针对不同运行时Profile使用不同的日 志配置(见3.2.3节),这个功能会很有用。 配置HTTPS 用JDK的keytool工具来创建一个密钥存储(keystore): keytool -keystore mykeys.jks -genkey -alias tomcat -keyalg RSA 将生成的密钥（的路径）放在配置文件中: server: port: 8443 ssl: key-store: file:///path/to/mykeys.jks Spring Boot Features Spring Boot提供了许多Spring没有的新特性 AutoConfiguration Spring Boot提供了自动装配XxxAutoConfiguration，使得许多组件被自动化配置并创建, 这通过自动扫描+ 条件装配@Conditional实现, 步骤为: 引入各种依赖（由于是spring boot， 因此一般是起步依赖 ）， 依赖中会有XxxAutoConfiguration类 Spring Boot启动时会自动扫描所有的XxxAutoConfiguration 对于每个XXAutoConfiguratio, 它一般带有注解: @ConditionalOnClass（XX.class）： 在classpath中能找XX； @EnableConfigurationProperties(XX.class)： 在当前Bean的定义中能找到唯一的DataSource； 各种@Conditional... @Import(XXConfiguration）:注入某个配置类 如果符合条件，该XXConfiguration就会被装配。而各种Bean的实际创建由其导入的XXeConfiguration完成 XXConfiguration一般会带有注解@ConditionalOnMissingBean(XX.class)，即不存在XX的Bean时，就会自动注入，这就完成了Bean的自动装配 自动配置的细节 例如，引入spring-boot-starter-jdbc后，启动时： DataSourceAutoConfiguration：自动创建一个DataSource，其中配置项从application.yml的spring.datasource读取 DataSourceTransactionManagerAutoConfiguration：自动创建了一个基于JDBC的事务管理器 JdbcTemplateAutoConfiguration：自动创建了一个JdbcTemplate 因此，我们自动得到了一个DataSource、一个DataSourceTransactionManager和一个JdbcTemplate 类似地，当我们引入spring-boot-starter-web时，自动创建了： ServletWebServerFactoryAutoConfiguration：自动创建一个嵌入式Web服务器，默认是Tomcat DispatcherServletAutoConfiguration：自动创建一个DispatcherServlet HttpEncodingAutoConfiguration：自动创建一个CharacterEncodingFilter WebMvcAutoConfiguration：自动创建若干与MVC相关的Bean ... 引入第三方pebble-spring-boot-starter时，自动创建了： PebbleAutoConfiguration：自动创建了一个PebbleViewResolver 条件装配的细节 我们观察JdbcTemplateAutoConfiguration，它的代码如下： @Configuration(proxyBeanMethods = false)@ConditionalOnClass(&#123; DataSource.class, JdbcTemplate.class &#125;)@ConditionalOnSingleCandidate(DataSource.class)@AutoConfigureAfter(DataSourceAutoConfiguration.class)@EnableConfigurationProperties(JdbcProperties.class)@Import(&#123; JdbcTemplateConfiguration.class, NamedParameterJdbcTemplateConfiguration.class &#125;)public class JdbcTemplateAutoConfiguration &#123;&#125; 当满足条件： @ConditionalOnClass：在classpath中能找到DataSource和JdbcTemplate； @ConditionalOnSingleCandidate(DataSource.class)：在当前Bean的定义中能找到唯一的DataSource； 该JdbcTemplateAutoConfiguration就会起作用。实际创建由导入的JdbcTemplateConfiguration完成： @Configuration(proxyBeanMethods = false)@ConditionalOnMissingBean(JdbcOperations.class)class JdbcTemplateConfiguration &#123; @Bean @Primary JdbcTemplate jdbcTemplate(DataSource dataSource, JdbcProperties properties) &#123; JdbcTemplate jdbcTemplate = new JdbcTemplate(dataSource); JdbcProperties.Template template = properties.getTemplate(); jdbcTemplate.setFetchSize(template.getFetchSize()); jdbcTemplate.setMaxRows(template.getMaxRows()); if (template.getQueryTimeout() != null) &#123; jdbcTemplate.setQueryTimeout((int) template.getQueryTimeout().getSeconds()); &#125; return jdbcTemplate; &#125;&#125; 创建JdbcTemplate之前，要满足@ConditionalOnMissingBean(JdbcOperations.class)，即不存在JdbcOperations的Bean。 覆盖自动装配 由于Spring Boot自动装配功能通过自动扫描+条件装配实现， 想要覆盖Spring Boot的自动配置,只需编写一个显式的配置。根据条件@ConditionalOnMissingBean([XX.class])， Spring Boot就不会再创建一个重复的Bean Starter Dependencies &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; starter是一组依赖集合，用户不需要再单独依赖，只需要依赖Starter，就可以一站式解决所需的依赖 简单来说，starter是帮用户简化配置的， 在“约定大于配置”的理念下，starter把繁琐的配置交给自己，而把简单的交给用户。 用户也可以覆盖默认配置 “约定大于配置”：starter使用ConfigurationProperties来保存配置，且配置都有默认值，用户可以覆盖 ConfigurationProperties还使得所有的配置属性被聚集到一个文件中（一般在resources目录下的application.properties/yml），这样我们就告别了Spring项目中XML地狱 注意，不同的starter是为了解决不同的依赖，所以它们内部的实现可能会有很大的差异，例如jpa的starter和Redis的starter可能实现就不一样 parent starter的版本号不需要指定，而是继承自父级依赖spring-boot-starter-parent， parent是一个特殊的starter，用于管理所有子starter的依赖, 也就是说，只需要指定parent的版本，不需要关心普通starter的版本 &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.0.RELEASE&lt;/version&gt;&lt;/parent&gt; 示例 普通soringboot项目的依赖： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;springBootLearn&lt;/groupId&gt; &lt;artifactId&gt;springBootLearn&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.0.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; devtools gradle: compile &quot;org.springframework.boot:spring-boot-devtools&quot; maven: &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;&lt;/dependency&gt; Spring Boot以依赖的形式引入了一组开发者工具,包括如下功能： 自动重启: 当Classpath里的文件发生变化时,自动重启运行中的应用程序。 LiveReload支持:对资源的修改自动触发浏览器刷新。 远程开发:远程部署时支持自动重启和LiveReload。 默认的开发时属性值:为一些属性提供有意义的默认开发时属性值 当应用程序以完整打包好的JAR或WAR文件形式运行时,开发者工具会被禁用,所以没有必要在构建包前移除这个依赖 自动重启 激活了devtools后,Classpath里对文件做任何修改都会触发应用程序重启 为了让重启速度够快,不会修改的类(比如第三方JAR文件里的类)都加载到了基础类加载器里,而应用程序的代码则会加载到一个单独的重启类加载器里。检测到变更时,只有重启类加载器重启 有些Classpath里的资源变更后不需要重启应用程序。像Thymeleaf这样的视图模板可以直接编辑,不用重启应用程序。在/static或/public里的静态资源也不用重启应用程序,所以Spring Boot开发者工具会在重启时排除掉如下目录:/META-INF/resources、/resources、/static、/public和/templates。 可以设置spring.devtools.restart.exclude属性来覆盖默认的重启排除目录: 例如,只排除/static和/templates目录: spring: devtools: restart: exclude: /static/**,/templates/** 关闭自动重启: spring: devtools: restart: enabled: false 还可以设置一个触发文件,必须修改这个文件才能触发重启。例如,在修改名为.trigger的文件前你都不希望执行重启: spring: devtools: restart: trigger-file: .trigger LiveReoload 用于web页面，devtools集成了LiveReload, Sprign Boot启动时会启动一个内嵌的LiveReload服务器,在资源文件变化时会触发浏览器刷新 不过很少有人开发MVC，这功能也没啥用 禁 用 内 嵌 的 LiveReload 服 务 器 ： spring: devtools: livereload: enabled: false 默认的开发时属性 有些配置属性通常在开发时设置,从来不用在生产环境里。比如视图模板缓存,在开发时最好关掉,这样你可以立刻看到修改的结果。但在生产环境里,为了追求更好的性能,应该开启视图模版缓存。默认情况下,Spring Boot会为其支持的各种视图模板(Thymeleaf、Freemarker、Velocity、Mustache和Groovy模板)开启缓存选项。但如果存在Spring Boot的开发者工具,这些缓存就会禁用 这就是说在devtools激活后,如下属性会设置为false: spring.thymeleaf.cache spring.freemarker.cache spring.velocity.cache spring.mustache.cache spring.groovy.template.cache 这样一来,就不用在开发时(在一个开发时使用的Profile配置里)禁用它们了 Spring Boot CLI http://start.spring.io是Spring Boot提供的项目构建工具，位于Web端， 它的命令行访问工具是Spring Boot CLI Spring Boot CLI实际上会请求http://start.spring.io， 因此二者是一个东西 Spring Boot CLI也不是万能的，比如它无法指定root package name ( 默认是 &quot;demo&quot;) 下载SpiringBoot Cli Mac: brew tap spring-io/tap brew uninstall springboot brew install spring-boot 使用 Initialize a new project using Spring Initializr (start.spring.io): spring init 可以指定项目依赖： $ spring init -dweb,jpa,security --dependenciesor -d --build gradle: 默认项目构建工具是maven，可以指定使用gradle -packaging or -p : 默认构建为JAR包，可以指定构建为WAR包： -p war -n, --name &lt;String&gt;: Project name; infer application name 查看命令帮助： $ spring help init Profiles Profile本身是Spring提供的功能，它和@Conditional一样，属于条件装配，表示一个环境的概念，如开发、测试和生产这3个环境： native test production 或者按git分支定义master、dev这些环境： master dev 在启动一个Spring应用程序的时候，可以传入一个或多个环境，例如： -Dspring.profiles.active=test,master Spring Boot对Profiles的支持在于，可以在application.yml中配置多个环境： spring: application: name: $&#123;APP_NAME:unnamed&#125; datasource: url: jdbc:hsqldb:file:testdb username: sa password: dirver-class-name: org.hsqldb.jdbc.JDBCDriver hikari: auto-commit: false connection-timeout: 3000 validation-timeout: 3000 max-lifetime: 60000 maximum-pool-size: 20 minimum-idle: 1pebble: suffix: cache: falseserver: port: $&#123;APP_PORT:8080&#125;---spring: profiles: testserver: port: 8000---spring: profiles: productionserver: port: 80pebble: cache: true 注意到分隔符---，最前面的配置是默认配置，不需要指定Profile，后面的每段配置都必须以spring.profiles: xxx开头，表示一个Profile。上述配置默认使用8080端口，但是在test环境下，使用8000端口，在production环境下，使用80端口，并且启用Pebble的缓存 如果不指定任何Profile，那么Profile实际上是default，可以从Spring Boot启动日志看出 启用profile： spring: profiles: active:production Actuator Actuator会把它能收集到的所有信息都暴露给JMX。此外，Actuator还可以通过URL挂载一些endpoint(作为web接口)，通过它们了解应用程序运行时的内部状况 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; Actuator默认把所有访问点暴露给JMX，但出于安全原因，只有health和info会暴露给Web, 所有默认端点增加了/actuator前缀 例如，可以查看health: http://localhost:8080/actuator/health 注意，这是Actuator 2.x 中的新特性， 在 Actuator 1.x中，默认开放所有端点，也没有/actuator前缀 要暴露更多endpoint给Web，需要在application.yml中加上配置： management: endpoints: web: exposure: include: info, health, beans, env, metrics Actuator 提供了 13 个接口，可以分为三大类： 应用配置类：获取应用程序中加载的应用配置、环境变量、自动化配置报告等与Spring Boot应用密切相关的配置类信息。 度量指标类：获取应用程序运行过程中用于监控的度量指标，比如：内存信息、线程池信息、HTTP请求统计等。 操作控制类：提供了对应用的关闭等操作类功能 HTTP 方法 路径 描述 GET /autoconfig 提供了一份自动配置报告，记录哪些自动配置条件通过了，哪些没通过 GET /configprops 描述配置属性(包含默认值)如何注入Bean GET /beans 描述应用程序上下文里全部的Bean，以及它们的关系 GET /dump 获取线程活动的快照 GET /env 获取全部环境属性 GET /env/{name} 根据名称获取特定的环境属性值 GET /health 报告应用程序的健康指标，这些值由HealthIndicator的实现类提供 GET /info 获取应用程序的定制信息，这些信息由info打头的属性提供 GET /mappings 描述全部的URI路径，以及它们和控制器(包含Actuator端点)的映射关系 GET /metrics 报告各种应用程序度量信息，比如内存用量和HTTP请求计数 GET /metrics/{name} 报告指定名称的应用程序度量值 POST /shutdown 关闭应用程序，要求endpoints.shutdown.enabled设置为true GET /trace 提供基本的HTTP请求跟踪信息(时间戳、HTTP头等) /beans /beans接口会返回一个 JSON 文档，描述上下文里每个 bean 的情况，包括其 Java 类型以及注入的其它 bean bean：Spring 应用程序上下文中的 Bean 名称或 ID。 resource：.class 文件的物理位置，通常是一个 URL，指向构建出的 JAR 文件。这会随着应用程序的构建和运行方式发生变化。 dependencies：当前 Bean 注入的 Bean ID 列表。 scope：Bean 的作用域（通常是单例，这也是默认作用域）。 type：Bean 的 Java 类型。 /autoconfig /autoconfig接口能告诉你为什么会有这个 bean ，或者为什么没有这个 bean 提供endpoint可访问，如： /health，查看这个微服务的健康状况 /bean，创建的bean /env，查看环境变量信息","categories":[{"name":"Technology","slug":"Technology","permalink":"http://lyk-love.cn/categories/Technology/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://lyk-love.cn/tags/Spring/"}]},{"title":"String Algorithms","slug":"String-Algorithms","date":"2022-04-10T17:49:04.000Z","updated":"2022-09-26T06:39:34.940Z","comments":true,"path":"2022/04/11/String-Algorithms/","link":"","permalink":"http://lyk-love.cn/2022/04/11/String-Algorithms/","excerpt":"Outline: String Sort Tries Substring Search Regex Data Compress 目前只更新到String Sort","text":"Outline: String Sort Tries Substring Search Regex Data Compress 目前只更新到String Sort String Sort 因为字符串天然就有序（ascii， unicode序），因此可以用桶排序的方法，进行key indexed counting, 这样排序不需要比较，也就能达到线性时间 这里我用的序都是ascii序，当然你也可以自定义一个符号表，然后用符号表里的序 class StringSortStrategy&#123;public: void LSD( vector&lt;string&gt; &amp;strs); void MSD( vector&lt;string&gt; &amp;strs); void Quick3string( vector&lt;string&gt; &amp;strs);private: void LSD( vector&lt;string&gt; &amp;strs , int W); void MSD( vector&lt;string&gt; &amp;strs, vector&lt;string&gt; &amp;aux, vector&lt;int&gt; &amp;count, const int lo, const int hi, const int d ); void Quick3string( vector&lt;string&gt; &amp; strs, const int lo, const int hi, const int d );&#125;; void StringSortStrategy::LSD( vector&lt;string&gt; &amp;strs)&#123; const int W = strs[0].length(); LSD( strs, W );&#125;void StringSortStrategy::MSD( vector&lt;string&gt; &amp;strs)&#123; const int R = 256; vector&lt;int&gt; count(R+2); int N = strs.size(); vector&lt;string&gt; aux(N); MSD(strs, aux, count, 0, N - 1, 0);&#125;void StringSortStrategy::Quick3string(vector&lt;string&gt; &amp;strs)&#123; int N = strs.size(); Quick3string( strs, 0, N-1, 0 );&#125; 工具函数 char CharAt(const string s, int i) &#123; if (i &lt; s.length()) return s[i]; else return -1;&#125;void exch(string &amp;s1, string &amp;s2) &#123; string tmp = s1; s1 = s2; s2 = tmp;&#125;bool Less(string s1, string s2, int d) &#123; return s1.substr(d).compare(s2.substr(d)) &lt; 0;&#125; void InsertionSort(vector&lt;string&gt; &amp;a, int lo, int hi, int d) &#123; for (int i = lo; i &lt;= hi; i++) for (int j = i; j &gt; lo &amp;&amp; Less(a[j], a[j - 1], d); j--) exch(a[j], a[j - 1]);&#125; LSD LSD算法对等长的字符串数组从右往左按key进行排序，确保稳定性。 由于是key indexed counting, 不需要元素之间两两比较，因此可以达到线性复杂度 思路是把所有元素按key放在对应的桶里面，桶与桶之间按桶内的元素数拉开距离，然后对于每个桶，对其每个元素进行位置的分配。最后将结果写回到原来的strs数组 例如， 当前要将所有字符串的第0个字符（这里都是从右往左的，也就是倒数第0个字符）作为key进行排序，假设key为A的字符串有3个， key为B的字符串有5个，key为C的字符串有8个 可以装三个桶，key为A的桶有三个元素，key为B的桶有5个元素，key为C的桶有8个元素； 然后桶之间拉开距离，A桶和B桶之间距离为5， B桶和C桶间的距离为8； 然后给每个桶内的元素分配坐标，对于B桶中五个元素，就把它们按在原strs中的顺序（这一步确保了算法的稳定性）分配到A，B桶之间的空间中，其他桶亦如此，这样所有字符串就排好了序； 接着把上述结果写回到原strs中； 再对下一个key位置进行迭代（这里就是第1个字符） 注意count[0]必须存0值，表示下标分配时，最小桶的起始下标从0开始分配。比如0桶存放在count[1],0桶元素的下标分配就在count[0]和count[1]之间的空间中 /*** @param strs: 要排序的字符串列表，LSD要求所有字符串都是等长的（当然不等长的话修改算法也可以排序，但是一般我们 直接用MSD）* @param W : 要排序的key的范围，比如W=3，就代表对从右往左的三位进行排序*/void StringSortStrategy::LSD(vector&lt;string&gt; &amp;strs , int W)&#123; const int R = ALPHABET_SIZE; const int N = strs.size(); int len = strs[0].length(); vector&lt;string&gt; aux(N); for( int w = len - 1; w &gt;= len - W; w-- ) &#123; int count[ R+1 ] = &#123;&#125;; for( int i = 0 ; i &lt; N; i++ ) // count frequencies, 装满桶 &#123; count[CharAt(strs[i], w) + 1]++; &#125; for( int i = 1; i &lt; R+1; i++ )//桶之间拉开距离 &#123; count[i] += count[i-1]; &#125; for( int i = 0; i &lt; N; i++ )//桶内元素在空间中进行分配 &#123; int r = CharAt(strs[i],w); aux[ count[ r ]++ ] = strs[i]; &#125; for( int i = 0; i &lt; N; i++ )//写回 &#123; strs[ i ] = aux[i]; &#125; &#125;&#125; MSD MSD同样是key indexed count的桶排序，但是是从左向右扫，并且能允许不同长度的字符串 MSD可以看作是从左往右的LSD，并且将空key（因为字符串长度可能不够了，那一位的key可能为空）放在一个特别的空桶上，其余性质不变。 因此MSD也是稳定, 线性时间的 由于要处理空key，我们将空key的key定义为-1 (见CharAt()函数 )， 本来按LSD算法，桶的大小要存放在下一个count中，比如key为A的元素的个数（即A桶的大小）存放在count[B]。 由于count[0]必须是0值，所以只能把-1桶放在count[1], 0桶放在count[2]... 相当于所有count后延一位.这样, -1取代0成为符号表最初的元素，-1桶元素的下标也就从count[0]和count[1]的空间中分配。 总体思路和LSD一模一样，只不过是从左到右的，并且count要后延一位 注意到MSD对短字符串的处理性能不佳，因此对于短字符串( hi - lo &lt;= M )就直接用插入排序了。 这一步还顺便处理了递归的终止条件，即hi - lo &lt;= 0 的情况 void StringSortStrategy::MSD( vector&lt;string&gt; &amp;strs, vector&lt;string&gt; &amp;aux, vector&lt;int&gt; &amp;count, const int lo, const int hi, const int d )&#123; const int M = 3, R = 256; if (hi &lt;= lo + M) &#123; InsertionSort(strs, lo, hi, d); return; &#125; for( int i = 0; i &lt; R + 2; i++ ) &#123; count[i] = 0; &#125; for( int i = lo; i &lt;= hi; i++ ) &#123; count[ CharAt(strs[i],d) + 2 ]++; &#125; for( int i = 1; i &lt; R+2; i++ ) &#123; count[i] += count[i-1]; &#125; for( int i = lo; i &lt;= hi; i++ ) &#123; aux[ count[ CharAt( strs[i],d ) + 1 ]++ ] = strs[i]; &#125; for( int i = lo; i &lt;= hi; i++ ) &#123; strs[i] = aux[i - lo]; &#125; for( int i = 0; i &lt; R; i++ ) &#123; MSD( strs, aux, count, lo + count[i], lo + count[i+1] - 1, d+1 ); &#125;&#125; 三向排序 三向排序混合了桶排序（MSD）和快排， 由于引入了快排，所以是不稳定排序，但依然是线性时间 和MSD的区别是，MSD将每个key作为一个桶，而三向排序通过快排只引入三个桶 ---- key小于指定key的桶( 放到lt左边) key等于指定key的桶（lt ～ ht ） 和key大于指定key的桶（ht右边） 对于key种类很少的情况，可以用三向排序，这样产生的桶更少，子数组就更少 和传统快排的区别在于，快排要左右分别开始扫，扫到符合要求的元素就分别停下，这样做是为了方便两边对换；但是对于三向排序，不需要两边对换（只需要把元素换到左/右界之外），所以也没必要从两边开始扫。一遍就够了 void StringSortStrategy::Quick3string(vector&lt;string&gt; &amp;strs, const int lo, const int hi, const int d)&#123; if( hi &lt;= lo ) return; int v = CharAt(strs[lo],d); //pivot int lt = lo, gt = hi; //桶的界限，小桶会放到lt左边，大桶放到gt右边，与指定key相等的桶就放到lt，gt之间 int i = lo+1; while( i &lt;= gt ) //循环可保证，lt之前的值永远小于v， gt之后的值永远大于v &#123; int t = CharAt(strs[i],d); if( t &lt; v ) exch(strs[i++], strs[lt++]); else if( t &gt; v ) exch( strs[i], strs[gt--] ); //不能保证换过来的gt之后的值就是v了，因此不能i++，需要再次进循环 else i++; &#125; Quick3string(strs, lo, lt-1, d); if( v &gt; 0 ) Quick3string(strs, lt, gt , d + 1 ); Quick3string( strs, gt + 1, hi, d );&#125;","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[]},{"title":"MYSQL Basic","slug":"MYSQL-Basic","date":"2022-04-09T16:45:48.000Z","updated":"2022-09-26T06:39:34.934Z","comments":true,"path":"2022/04/10/MYSQL-Basic/","link":"","permalink":"http://lyk-love.cn/2022/04/10/MYSQL-Basic/","excerpt":"Outline: CS Archetecture Apps Config Concurrency Control Lock Transaction MVCC Sotrage Engine ref: Mysql是怎样运行的, 高性能Mysql","text":"Outline: CS Archetecture Apps Config Concurrency Control Lock Transaction MVCC Sotrage Engine ref: Mysql是怎样运行的, 高性能Mysql CS Archetecture Mysql采用CS模式，CS之间通过TCP通信，Server默认监听3306端口，也可以手动指定 如果CS都运行在同一台机器上，则默认使用Unix Socket通信， Server默认监听的Socket为/var/run/mysqld/mysqld.sock，也可以手动指定 和Servlet一样，每当一个Client连接到Server， 后者都会创建一个线程与该Client交互，当Client退出后，该线程会被缓存起来，并被之后的Client复用 Server拥有一个存储引擎API，来屏蔽底层存储引擎的差异。 Server通过API和存储引擎通信 连接管理 连接建立后，Mysql Server会接收到Client的请求，请求（即查询语句）是文本格式 解析与优化 查询缓存 由于维护缓存需要性能开销，Mysql 8.0开始已经没有查询缓存了 查询缓存就是对查询语句的结果做缓存，由于查询语句是文本格式，所以任何字符上的不同都会导致cache miss. 任何更新表的操作都会刷新缓存 语法解析 相当于编译过程 查询优化 优化结构生成执行计划 优化器不关心表的存储引擎，但后者又对查询有影响，因此优化器会请求存储引擎来提供信息 存储引擎 查看Server支持的存储引擎: show engines Transactions列表示是否支持事务处理 Savepoints列表示是否支持事务的部分回滚 指定存储引擎（默认是Innodb）： CREATE TABLE [table_name](...)ENGINE = [engine_name] 修改表的存储引擎： ALTER TABLE [table_name] ENGINE = Innodb 查看表的存储结构： SHOW CREATE TABLE [table_name]]\\G Apps Mysql是一组程序，包括了可执行文件，shell脚本等 Linux下，Mysql安装目录：/usr/bin ❯ ls /usr/bin | grep mysqlmysqlmysql_config_editormysql_migrate_keyringmysql_secure_installationmysql_ssl_rsa_setupmysql_tzinfo_to_sqlmysql_upgrademysqladminmysqlanalyzemysqlbinlogmysqlcheckmysqld_multimysqld_safemysqldumpmysqldumpslowmysqlimportmysqloptimizemysqlpumpmysqlrepairmysqlreportmysqlshowmysqlslap 一般安装时会自动将这些可执行文件添加到环境变量PATH Mysql Server mysqld: Mysql Server程序, 运行该程序就启动了一个Mysql Server进程，但该文件一般不常用，我们一般用脚本启动 -P: 指定Server监听的端口，默认是3306 --default-storage-engine=[engine_name]: 指定默认存储引擎 --skip_networking:禁止使用TCP通信，即Client无法用ip来与Server通信 mysqld_safe: 是一个Shell脚本，间接调用mysqld并持续监控Server的运行状态，并将Server的出错信息输出到错误日志 对于传递给mysqld_safe的启动选项，如果它处理不了，会被转发到mysqld mysqld_nulti: Shell脚本，用于启动或停止多个Server实例 Mysql Client mysql: Mysql Client程序，用法为： mysql -h [hostname] -u [username] -p [password] -h: Server进程所在的主机的域名or IP地址，如果Server就运行在本机，可以省略这个参数, 长参数形式为：--host=[hostname] -u: 用户名, 长参数形式为：--user=[username], 缺省则为当前Linux登陆用户 -p: 密码, 长参数形式为：--password=[password] -P: 指定需要连接的Server的端口，默认是3306 退出mysql：exit, quit Config 命令行指定的配置只对当次启动生效，可以在mysql配置文件里指定配置 命令行的配置会覆盖配置文件的配置 配置文件中，个人的配置会覆盖全局的配置 配置文件 Mysql配置文件位置： Global: /etc/mysql/my.cnf Personal: ~/.my.cnf 配置文件分多个组， [server], [client], [mysqld]这些， 对应名字的程序/脚本启动时会读取对应组的部分， 比如mysqld会读取[mysqld]的配置， mysqld_safe会读取[mysqld_safe]，当然，mysqld_safe会调用mysqld, 因此也会间接读取[mysqld] 所有Server程序都会读取[server] 所有Client程序都会读取[client] 如果一个程序间接调用了另一个程序，则后者的配置也会被读取， 如 mysqld_safe会读取[mysqld_safe]，由于mysqld_safe会调用mysqld, 因此也会间接读取[mysqld] 系统变量 通过命令行和配置文件进行的配置，会被读取为系统变量，系统变量的作用域分为GLOBAL和SESSION GLOBAL：对Server有效， 每次Server启动时，根据配置来初始化GLOBAL变量 SESSION：对当次会话有效，即只对某个Client的连接有效。 SESSION变量在连接时根据相应GLOBAL变量的值来初始化 更改GLOBAL变量的值只会影响到之后接入的SESSION， 之前的SESSION变量不会被改变 不是所有的系统变量都具有这两种作用范围，有些只具有GLOBAL范围，如max_connections;有些只具有SESSION范围，insert_id 有些系统变量是只读的，如version 查看系统变量： SHOW [scope] VARIABLES [LIKE [pattern]] [scope]: 取值为GLOBAL, SESSION， 即查看指定作用域的变量 并发控制 每种存储引擎都可以实现自己的锁策略和锁粒度 Mysql Server不管理事务，事务由下层的存储引擎实现（当然这只是理论上，事实上server层可能会加table lock） 不要使用多个存储引擎， 否则在一个事务中混合了采用多个存储引擎的表，有的支持事务，有的不支持，就会出现各种问题 Innodb采用两阶段锁定协议，在事务执行时随时都可以锁定，锁只有在执行COMMIT或ROLLBACK后才会释放，且所有锁都在同一时刻释放 InnoDB使用锁机制和MVCC进行并发控制, 并通过next-key locking防止幻读 Lock Innodb处理死锁的方案：将持有最少行级排他锁的事务进行回滚 下面介绍一些基本的锁策略: table lock Myisam使用表锁 存储引擎管理自己的锁，但server也可能会主动加table lock Server会对ALTER TABLE之类的语句使用table lock，而忽略存储引擎的锁机制 row lock Innodb使用行级锁 行级锁的粒度更细，对并发的支持更好，开销也更大， row lock只在存储引擎中实现，与Server无关 Transaction ACID atomicity consistency: 数据库总是从一个一致性的状态转换到另一个一致性的状态 isolation: 一个事务在提交之前，对其他事务不可见 duality: 事务提交后必须被持久化（持久性也分不同的级别，没有策略能保证100%对持久性） 隔离级别 READ UNCOMMITTED: 事务中的修改，即使未提交， 对其他事物也是可见的。 事务可以读取未提交的数据，也称为脏读 READ COMMITTED: 满足了隔离性（一个事务从开始直到提交之前，所做的任何修改对其他事务不可见）， 避免了脏读，存在不可重复读、幻读问题 可重复读：在同一个事务中多次读取同样的记录的结果是一致的 MVCC对该级别的实现就是每次进行普通的select查询，都会产生一个新的快照(不同时间，当前活跃的事务不同，行记录最近一次更新的事务ID也可能不同)。相当于二级锁协议，进行读操作需要加读锁，读完就释放锁 REPEATABLE READ: Mysql默认级别。 避免了脏读和不可重复读，但存在幻读（ Phantom Read ）问题 幻读：当某个事物读取某个范围内的记录时， 另一个事务又在该范围插入了新的纪录， 当之前的事务再次读取该范围的记录时，会产生幻行 Innodb和Xtradb通过Next-Key Locks算法的间隙锁和记录锁解决了幻读 MVCC对该级别的实现就是在当前事务中只有第一次进行普通的select查询，才会产生快照，此后这个事务一直使用这一个快照进行查询，相当于三级锁协议，进行读操作需要加读锁，事务结束才释放 SERIALIZABLE: 强制事务串行执行 Mysql能识别所有隔离级别，Innodb也支持所有隔离级别 设置当前会话隔离级别，在下一个事务开始时生效： SET TRANSACTION ISOLATION LEVEL READ COMMITTED; 可以在配置文件中设置GLOBAL的隔离级别 Mysql的事务 Mysql默认采用自动提交模式，每个查询都是一个事务， 可以修改AUTOCOMMIT变量来启用/禁用该模式 禁用自动提交后， 所有查询都在一个事务中，直到显示地执行COMMIT提交或ROLLBACK回滚 该模式对不支持事务的表，如Myisam, Memory等， 没有影响 某些命令在执行前会强制执行COMMIT提交当前的事务 ALTER TABLE, LOCK TABLE 事务日志 事务日志采用追加方式，这样日志就在磁盘中占用连续的区域，写日志非常快 预写式日志：先写入日志，再写入数据，两次写磁盘 MVCC ref: MVCC Multi Version Concurrency Control: 多版本并发控制，支持MVCC的数据库表中每一行数据都可能存在多个版本，对数据库的任何修改的提交都不会直接覆盖之前的数据，而是产生一个新的版本与老版本共存，通过读写数据时读不同的版本来避免加锁阻塞（写写还是要阻塞等待，因为事务对数据进行更新时会加上排他锁)，MVCC的具体实现依赖于存储引擎，这里只介绍Innodb的MVCC 实现：在每个表中添加三个隐藏字段以及事务在查询时创建快照（read view），以及建立数据版本链(Undo log) InnoDB支持多版本数据，在更新或者删除数据时，并不会立马删除原有行记录，而是将旧版本存入回滚段中的Undo log内，并通过回滚指针形成一个数据链，可以通过这个指针访问链上的历代数据版本，正是这种机制使得数据库数据产生了多个版本，为通过MVCC进行快照读提供了可能 并不是所有的查询都是进行快照读，使用普通的select 语句进行查询时会生成快照，进行快照读；使用select … lock in share mode，select … for update，insert，update，delete 语句等语句进行查询或者更新时还是会使用锁机制，进行锁阻塞。 使用MVCC的作用(意义)是非阻塞的解决了事务读写冲突，提高了并发性能 MVCC工作在READ COMMITED和REPEATABLE READ两种隔离级别下 三个隐藏字段 InnoDB会为每个使用InnoDB存储引擎的表添加三个隐藏字段，用于实现数据多版本以及聚集索引： A 6-byte DB_TRX_ID field: 对该行的最后一次插入/更新的事务号， 删除也是一种更新，只是标记一下该行的deleted bit 设置删除位并不会真的进行物理删除，当InnoDB丢弃为删除而编写的更新撤消日志记录时，它才会物理删除相应的行及其索引记录。此删除操作称为清除，速度非常快 这意味着每行都有一个事务号 A 7-byte DB_ROLL_PTR field: called the roll pointer. The roll pointer points to an undo log record written to the rollback segment. If the row was updated, the undo log record contains the information necessary to rebuild the content of the row before it was updated. 回滚指针，指向当前记录行的undo log信息(存储该数据的前一个版本) A 6-byte DB_ROW_ID field: 随着新行插入而单调递增的行ID。InnoDB使用聚簇索引，数据存储是以聚簇索引字段的大小顺序进行存储的，当表没有主键或唯一非空索引时，innodb就会使用这个行ID自动产生聚簇索引。如果表有主键或唯一非空索引，聚簇索引就不会包含这个行ID了。该字段跟MVCC关系不大 Read View read view是读视图，其实就是一种快照，里面记录了系统中当前活跃事务的ID以及相关信息，主要用途是用来做可见性判断，判断当前事务是否有资格访问该行数据(详情下解)。read view有多个变量，这里只介绍关键部分： trx_ids: 活跃事务列表，也就是Read View开始创建时其他未提交的活跃事务的ID列表。例如事务A在创建read view(快照)时，数据库中事务B和事务C还没提交或者回滚结束事务，此时trx_ids就会将事务B和事务C的事务ID记录下来 low_limit_id： 目前出现过的最大的事务ID+1，即下一个将被分配的事务ID up_limit_id： 活跃事务列表trx_ids中最小的事务ID，如果trx_ids为空，则up_limit_id 为 low_limit_id，虽然该字段名为up_limit,但在trx_ids中的活跃事务号是降序的，所以最后一个为最小活跃事务ID 如果当前行的事务号小于up_limit_id, 则说明该行的最后一次更新在当前的Read View中最早的事务开始之前已经提交了 creator_trx_id： 当前创建read view的事务的ID Undo log Undo log中存储老版本数据，当一个事务需要读取记录行时，如果当前记录行不可见，可以通过roll pointer顺着undo log链找到满足其可见性条件的记录行版本 在InnoDB里，undo log分为如下两类： insert undo log : 事务对insert新记录时产生的undo log, 只在事务回滚时需要, 并且在事务提交后就可以立即丢弃。 update undo log : 事务对记录进行delete和update操作时产生的undo log，不仅在事务回滚时需要，快照读也需要，只有当数据库所使用的快照中不涉及该日志记录，对应的回滚日志才会被purge线程删除 Purge线程：上文提到了InnoDB删除一个行记录时，并不是立刻物理删除，而是将该行数据的DB_TRX_ID字段更新为做删除操作的事务ID，并将删除位deleted_bit设置为true(已删除)，将其放入update undo log中。为了节省磁盘空间，InnoDB有专门的purge线程来清理deleted_bit为true的记录。purge线程自己也维护了一个read view，如果某个记录的deleted_bit为true，并且DB_TRX_ID相对于purge线程的read view可见，那么这条记录一定是可以被安全清除的。 MVCC更新行的原理 MVCC机制下实现更新还是会用到排他锁，但由于我们读的时候可以通过快照读，读多个版本避免了使用共享锁，因此可以使得读事务不会因为写事务阻塞。MVCC的优越性在于事务需要读行记录的时候不会因为有事务在更新该行记录而阻塞,事务在写行记录时也不会因为有事务在读数据而阻塞。 更新原理: 假设我现在需要修改行记录A,他们的修改过程如下， MVCC更新行记录A时会先用排他锁锁住该行记录A； 然后将该行记录复制到update undo log中，生成旧版本行记录B； 使行记录A的回滚指针指向这条旧版本B，再在行记录A中修改用户需要修改的字段，并将DB_TRX_ID字段更新为更新这条记录的事务ID； 最后提交事务。(用户需要修改的字段指的是业务字段，比如我们要修改name等) 通过回滚指针，形成了一条当前行记录指向历代旧版本行记录的链表，通过这条链表，我们就可以查询该行记录的多个旧版本: MVCC查询行的原理 InnoDB中，事务在第一次进行普通的select查询时，会创建一个read view(快照)，用于可见性判断，事务只能查询到行记录对于事务来说可见的数据版本。可见性判断是通过行记录的DB_TRX_ID(最近一次插入/更新/删除该行记录的事务ID)以及read view中的变量比较来判断。 查询过程如下: 如果 $\\mathrm{DB \\underline{ } TRX \\underline{ }ID} &lt; \\mathrm{up\\underline{ }limit \\underline{ } id}$ 表明这个行记录最近一次更新在当前事务创建快照之前就已经提交了，该记录行的值对当前事务是可见的，当前事务可以访问该行记录，跳到步骤(4)。 如果$\\mathrm{DB \\underline{ } TRX \\underline{ }ID} \\ge \\mathrm{low \\underline{ } limit \\underline{ } id}$ 表明这个行记录最近一次更新是快照创建之后才创建的事务完成的，该记录行的值对当前事务是不可见的，当前事务不可以访问该行记录。因此当前事务只能访问比该行记录更旧的数据版本。通过该记录行的 DB_ROLL_PTR 指针，找到更旧一版的行记录，取出更旧一版的行记录的事务号DB_TRX_ID，然后跳到步骤(1)重新判断当前事务是否有资格访问该行记录。 如果$\\mathrm{up\\underline{ }limit \\underline{ } id} \\le \\mathrm{DB \\underline{ } TRX \\underline{ }ID} &lt; \\mathrm{low \\underline{ } limit \\underline{ } id} $ 则表明对这个行记录最近一次更新的事务可能是活跃列表中的事务也可能是已经成功提交的事务(事务ID号大的事务可能会比ID号小的事务先进行提交)，比如说初始时有5个事务在并发执行，事务ID分别是1001~1005，1004事务完成提交，1001事务进行普通select的时候创建的快照中活跃事务列表就是1002、1003、1005。因此up_limit_id就是1002， low_limit_id就是1006。对于这种情况，我们需要在活跃事务列表中进行遍历(因为活跃事务列表中的事务ID是有序的，因此用二分查找)，确定DB_TRX_ID是否在活跃事务列表中。 若不在，说明对这个行记录最近一次更新的事务是在创建快照之前提交的事务，此行记录对当前事务是可见的，也就是说当前事务有资格访问此行记录，跳到步骤(4)。 若在，说明对这个行记录最近一次更新的事务是当前活跃事务，在快照创建过程中或者之后完成的数据更新，此行记录对当前事务是不可见的(若可见则会造成脏读、不可重复读等问题)。因此当前事务只能访问该行记录的更旧的版本数据。通过该记录行的 DB_ROLL_PTR 指针，找到更旧一版的行记录，取出更旧一版的行记录的事务号DB_TRX_ID，然后跳到步骤(1)重新判断当前事务是否有资格访问该行记录。 可以访问，将该行记录的值返回 当前读和快照读 快照读:使用普通的select 语句进行查询时会生成快照，进行快照读，快照读不会上锁，根据可见性判断，来决定是读取该行记录的最新版本还是旧版本。(只有使用普通的select语句进行查询才会用到快照读，才享受到了MVCC机制的读写非阻塞的优越性) 当前读:使用select … lock in share mode，select … for update，insert，update，delete 语句等语句进行查询或者更新时，会使用相应的锁进行锁定，查询到的肯定数据库中该行记录的最新版本。 示例 存储引擎 Mysql将每个数据库（即schema）保存为数据目录下的一个字目录，创建表时，会在数据库字目录下创建一个和表同名的.frm文件保存表的定义 查看表的相关信息： show table status like &#x27;[pattern]&#x27;\\G; 或者查看INFORMATION_SCHEMA 功能 MylSAM MEMORY InnoDB Archive 存储限制 256TB RAM 64TB None 支持事务 No No Yes No 支持全文索引 Yes No No No 支持树索引 Yes Yes Yes No 支持哈希索引 No Yes No No 支持数据缓存 No N/A Yes No 支持外键 No No Yes No Innodb Mysql默认的存储引擎 Innodb的数据存储在tablespace中，tablespace是由Innodb管理的黑盒 Myisam 不支持事务和行级锁，只使用表锁 Myisam将表分别存储为数据文件(.myd)和索引文件(.myi) Myisam可以创建压缩表，如果对表不再进行修改操作，那么可以讲该表压缩.这可以极大地减少空间占用 工具：myisampack 压缩表也支持索引，但索引是只读的 压缩表是按行压缩的，不是整表压缩。因此解压也只需要解压某些行 CSV 可以将CSV文件和Mysql的表做转换 Memory 所有数据都在内存，没有磁盘IO 即重启后数据会丢失，但表结构会保留 支持Hash索引 使用table lock，并发程度低 Mysql的临时表一般是Memoey表 临时表是使用CREATE TEMPORARY TABLE创建的表，可以采用任何存储引擎，Memory只是默认采用的存储引擎 转换表的引擎 ALTER TABLE ALTER TABLE [table-name] ENGINE=[engine-name] 先将表导出（mysqldump），然后修改sql文件的CREATE TABLE语句的存储引擎选项，还要修改表名（因为同一个数据库不允许重名的表） 创建一个新的存储引擎的表，然后用INSERT...SELECT来导入数据","categories":[{"name":"Technology","slug":"Technology","permalink":"http://lyk-love.cn/categories/Technology/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://lyk-love.cn/tags/Database/"}]},{"title":"MYSQL Management","slug":"MYSQL-Management","date":"2022-04-09T16:44:41.000Z","updated":"2022-09-30T17:37:54.953Z","comments":true,"path":"2022/04/10/MYSQL-Management/","link":"","permalink":"http://lyk-love.cn/2022/04/10/MYSQL-Management/","excerpt":"Outline: Mysql Management Mysql Database Management Mysql Table Management","text":"Outline: Mysql Management Mysql Database Management Mysql Table Management Mysql Management USE *数据库名* : 选择要操作的Mysql数据库，使用该命令后所有Mysql命令都只针对该数据库。 mysql&gt; use RUNOOB;Database changed SHOW DATABASES: 列出 MySQL 数据库管理系统的数据库列表。 mysql&gt; SHOW DATABASES;+--------------------+| Database |+--------------------+| information_schema |fuke| RUNOOB || cdcol || mysql || onethink || performance_schema || phpmyadmin || test || wecenter || wordpress |+--------------------+10 rows in set (0.02 sec) SHOW TABLES: 显示指定数据库的所有表，使用该命令前需要使用 use 命令来选择要操作的数据库。 mysql&gt; use RUNOOB;Database changedmysql&gt; SHOW TABLES;+------------------+| Tables_in_runoob |+------------------+| employee_tbl || runoob_tbl || tcount_tbl |+------------------+3 rows in set (0.00 sec) SHOW COLUMNS FROM *数据表*: 显示数据表的属性，属性类型，主键信息 ，是否为 NULL，默认值等其他信息。 mysql&gt; SHOW COLUMNS FROM runoob_tbl;+-----------------+--------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-----------------+--------------+------+-----+---------+-------+| runoob_id | int(11) | NO | PRI | NULL | || runoob_title | varchar(255) | YES | | NULL | || runoob_author | varchar(255) | YES | | NULL | || submission_date | date | YES | | NULL | |+-----------------+--------------+------+-----+---------+-------+4 rows in set (0.01 sec) SHOW INDEX FROM *数据表*: 显示数据表的详细索引信息，包括PRIMARY KEY（主键）。 mysql&gt; SHOW INDEX FROM runoob_tbl;+------------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |+------------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| runoob_tbl | 0 | PRIMARY | 1 | runoob_id | A | 2 | NULL | NULL | | BTREE | | |+------------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+1 row in set (0.00 sec) SHOW TABLE STATUS [FROM db_name] [LIKE 'pattern'] \\G: 该命令将输出Mysql数据库管理系统的性能及统计信息。 mysql&gt; SHOW TABLE STATUS FROM RUNOOB; # 显示数据库 RUNOOB 中所有表的信息mysql&gt; SHOW TABLE STATUS from RUNOOB LIKE &#x27;runoob%&#x27;; # 表名以runoob开头的表的信息mysql&gt; SHOW TABLE STATUS from RUNOOB LIKE &#x27;runoob%&#x27;\\G; # 加上 \\G，查询结果按列打印 创建用户帐户 CREATE USER: CREATE USER user_account IDENTIFIED BY password; user_account: 格式为username@hostname localhost: 只允许从本机连接 %: 相当于通配, 但是不能匹配到 locahost 例如,要允许mysqladmin用户帐户从yiibai.com主机的任何子域连接到数据库服务: CREATE USER mysqladmin@&#x27;%.yiibai.com&#x27;IDENTIFIED by &#x27;mypassword&#x27;; 如果只写username@%, 则相当于允许 locahost外的所有ip 也就是说, username@%和username@localhost是两个用户! 可以不写hostname, 此时相当于% password: 在IDENTIFIED BY子句中指定. password必须是明文, 它会被MySQL加密 空密码就是IDENTIFIED BY '' 新创建的用户只能登录到mysql, 没有其他权限 要注意引号'', 特别当用户帐户包含特殊字符(如_或%)时, 比如, 如果你写了&quot;username@hostname&quot;, 这其实是一整个username, 而没有包含hostname, 而hostname会被默认设置为% 这样的用户帐户，MySQL将创建一个username@hostname的用户，并允许用户从任何主机进行连接，这可能不是您预期的。 导出/入数据库 导出： mysqldump -u [user] [database_name] &gt; [filename].sql 导入: ymysql&gt; source c:\\temp\\mysqlsampledatabase.sql 给用户授权 GRANT ALL ON *.* TO &#x27;super&#x27;@&#x27;localhost&#x27; WITH GRANT OPTION; ON *.*: 表示MySQL中的所有数据库和所有对象 .之前的部分表示数据库, .后面的部分表示表, 例如database.table, testdb.offices等等 WITH GRANT OPTION: 允许super@localhost向其他用户授予权限 授权后需要刷新权限: flush privileges 查看用户权限 show grants for [user] 修改用户名和密码 在登陆MySQL的情况下 Plan A: 通过sql命令修改密码 命令格式：set password for 用户名@localhost = password('新密码'); 新版本mysql 命令： alter user 用户名@localhos identified by &#x27;新密码&#x27;; 如果密码为空，则不要加identified by ''子句 例子： set password for root@localhost = password(&#x27;123&#x27;); # oralter user &#x27;root&#x27;@&#x27;localhost&#x27; identified by &#x27;123&#x27;; Plan B: 用UPDATE直接修改user表 使用mysql数据库 use mysql; 更改user表中指定用户的密码 update user set password=password(&#x27;123&#x27;) where user=&#x27;root&#x27; and host=&#x27;localhost&#x27;; 权限刷新 flush privileges; 在没有登陆的情况下 mysqladmin 命令: mysqladmin -u用户名 -p旧密码 password 新密码 例子：将root用户的密码由123456改为123 mysqladmin -u root -p 123456 password 123 MYSQL Database Management MySQL创建数据库 我们可以在登陆 MySQL 服务后，使用 create 命令创建数据库，语法如下: CREATE DATABASE 数据库名; drop 命令删除数据库 drop 命令格式： drop database &lt;数据库名&gt;; MYSQL Table Management 创建表 CREATE TABLE table_name (column_name column_type); 以下例子中我们将在 RUNOOB 数据库中创建数据表runoob_tbl： CREATE TABLE IF NOT EXISTS `runoob_tbl`( `runoob_id` INT UNSIGNED AUTO_INCREMENT, `runoob_title` VARCHAR(100) NOT NULL, `runoob_author` VARCHAR(40) NOT NULL, `submission_date` DATE, PRIMARY KEY ( `runoob_id` ))ENGINE=InnoDB DEFAULT CHARSET=utf8; 实例解析： 如果你不想字段为 NULL 可以设置字段的属性为 NOT NULL， 在操作数据库时如果输入该字段的数据为NULL ，就会报错。 AUTO_INCREMENT定义列为自增的属性，一般用于主键，数值会自动加1。 PRIMARY KEY关键字用于定义列为主键。 您可以使用多列来定义主键，列间以逗号分隔。 ENGINE 设置存储引擎，CHARSET 设置编码。 删除表 DROP TABLE table_name ; 查看表 列出当前数据库的所有表：SHOW TABLES; 查看表的结构: DESC [table_name] 这里DESC是describe的缩写 还有个DESC是descend的缩写： select ename,sal from emp order by sal desc; 手动指定按照薪水由大到小排序（降序关键字desc） 查看创建表的SQL语句：SHOW CREATE TABLE [table_name]; 插入记录 MySQL 表中使用 INSERT INTO SQL语句来插入数据。 你可以通过 mysql&gt; 命令提示窗口中向数据表中插入数据，或者通过PHP脚本来插入数据。 以下为向MySQL数据表插入数据通用的 INSERT INTO SQL语法： INSERT INTO table_name ( field1, field2,...fieldN ) VALUES ( value1, value2,...valueN ); 如果数据是字符型，必须使用单引号或者双引号，如：&quot;value&quot;。 如果数据是DATE类型， 则不需要输入连字符， 如2019-08-01， 则插入20190721 修改字段 如果要给students表新增一列birth，使用： ALTER TABLE students ADD COLUMN birth VARCHAR(10) NOT NULL; 要修改birth列，例如把列名改为birthday，类型改为VARCHAR(20)： ALTER TABLE students CHANGE COLUMN birth birthday VARCHAR(20) NOT NULL; 要删除列，使用： ALTER TABLE students DROP COLUMN birthday; RENAME用于表的重命名：RENAME 或 RENAME TO MODIFY用于字段类型的修改：MODIFY COLUMN &lt;列名&gt; &lt;类型&gt; ALTER用于对字段类型、默认值的修改：ALTER COLUMN &lt;列名&gt; &lt;类型&gt; SET DE***T &lt;默认值&gt; CHANGE用于对列名及类型的修改：CHANGE COLUMN &lt;旧列名&gt; &lt;新列名&gt; &lt;类型&gt;","categories":[{"name":"Technology","slug":"Technology","permalink":"http://lyk-love.cn/categories/Technology/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://lyk-love.cn/tags/Database/"}]},{"title":"OS Persistence","slug":"OS-Persistence","date":"2022-04-04T01:36:01.000Z","updated":"2022-09-26T06:39:34.936Z","comments":true,"path":"2022/04/04/OS-Persistence/","link":"","permalink":"http://lyk-love.cn/2022/04/04/OS-Persistence/","excerpt":"Outline: I/O Devices File File System Locality and FFS Crush-Consistency Problem Ref: OSTEP","text":"Outline: I/O Devices File File System Locality and FFS Crush-Consistency Problem Ref: OSTEP I/O Devicecs CPU通过内存总线连接到内存 图像和其他高性能IO设备连接到常规的IO总线（如PCI） 最慢的设备，如键盘、鼠标等，连接到外围总线 I/O系统各层软件及其功能 Unbuffered I/O &amp; Buffered I/O Unbuffered I/O: read/write -&gt;System calls File descriptor Not in ANSI C, but in POSIX.1 and XPG3 Buffered I/O Implemented in standard I/O library (属于库函数，而不是系统调用) 处理很多细节, 如缓存分配, 以优化长度执行I/O等. Stream -&gt; a pointer to FILE RAID 略 File 进程是虚拟化的CPU, 地址空间是虚拟化的内存， 而文件和目录就是虚拟化的外部存储设备 文件：线性字节数组，每个文件都有一个低级名称：inode number 目录：目录是特殊的文件，本身也有低级名称inode number，其内容为（用户可读名字， 低级名称）对的列表。 目录的每个对，即每个条目dentry，都指向文件或其他目录 目录层次结构从根目录/开始 Linux文件的细节详见Linux Programming File System 整体组织 我们实现极简版的VSFS（ Very Simple File System ）， VVFS与Linux的VFS（虚拟文件系统，提供了统一的文件系统模型，详见Linux Basic）大致相同： 磁盘分块（block），文件系统由一系列块组成 假定有64块，每块4KB. 数据块为最后56个，inode表占5个，两种位图各占一个，超级块占一个 superblock：位于第一块， 记录关于该文件系统的信息, 包括inode和数据块数量，inode表的起始地址等。 和一些标识文件系统类型的magic number 位图（bitmap）：记录inode或数据块是否已分配的数据结构，有inode bitmap和data bitmap inode表：就是 inode数组 数据块 超级块 inode bit map data bitmap inode table datablock 1 1 1 5 56 挂载文件系统时，OS首先读取superblock，初始化各种参数，然后将该卷添加到文件系统树中 也就是说挂载文件系统时，超级块必定被加载到内存里 文件组织： inode index node， named by Ken Thompson inode：保存给定文件的元数据的结构 由inumber隐式引用, 给定inumber，可以计算磁盘上相应节点的位置 假设： inode表大小为20KB（5个4KB块） 每个inode256字节，因此有80个inode； 超级块0KB开始，inode map从4KB开始， data map从8KB开始，inode table从12字节开始 要读取inode number=32 过程： 先得到inode表的起始地址：12KB 再加上此inode在表内的偏移量：32 * 256B = 8192B， 12KB + 8192B= 20KB 由于磁盘不是字节可寻址的，而是由可寻址扇区组成（512B），因此为了获取inumber=32的inode块，文件系统将请求物理节点号40（ 20KB / 512B= 40 ）,获得期望的inode块 #通用算法： inumber -&gt; sector numberblk = ( inumber * sizeof(inode_t) )/ blockSize; # 得到该inode所在的块号sector = ((blk*blockSize)+ inodeStartAddr)/ sectorSize; 多级索引 为了支持大文件，inode中除了直接指针外，还允许间接指针。 间接指针指向一个间接块（磁盘的数据块区域）。 假设一个块是4KB，磁盘地址空间是4Byte， 那就增加了1024个指针。 假设inode有12个直接指针和一个间接指针，则一个inode可以支持$(12+ 1 \\times 1024) \\times 4 \\mathrm{KB}$ 按这个逻辑，还可以分配一个二重间接块，存放指向间接块的指针， inode存放一个二重间接指针， 这个指针就可以表示$1024 \\times 1024 \\mathrm{KB}$ 注意到，使用了多级索引的文件系统的文件分配模型是一个不平衡树，这个设计的初衷是： 大部分文件都是小文件， 因此只需要对大文件进行特殊的设计， 小文件让直接指针指向就好了 还有一种基于链表的方法，即inode只需存储一个指向第一个块的指针， 而数据区每个数据块的末尾都会有一个指向该文件下一个数据块的指针。但是这种方式对于某些workload效果不好，比如随机访问。 windows采用FAT（File Allocation Table）, 指向下一块的指针不存在当前数据块中， 而是存在FAT（位于内存）中 目录组织 目录是特殊的文件，它也有inode， 但是目录的内容（即数据块的内容）是目录条目（称为dentry )与该条目对应inode的映射 打开目录，首先是打开目录的文件描述符 目录基本上是一个&lt;dentry， inode number&gt;的列表。 其中条目名称还包括条目的记录长度（名称的总字节数 + 所有的剩余空间）和字符串长度（即条目名称的实际长度）。 每个目录还有两个额外的条目： .和.. 用于表示当前目录和父目录 目录就是特殊的文件， 它也有inode， 目录的数据块的内容就是以上说的&lt;条目名称， inode number&gt;的列表 假设某目录（i number = 5）中有三个文件（foo, bar, foo bar）， i number分别为12，13和24，则dir在磁盘上的数据是： inum reclen strlen name 5 4 2 . 2 4 3 .. 12 4 4 foo 13 4 4 bar 24 8 7 foobar 删除一个文件会在目录中留下一段空白空间， 一般是将该文件对应条目的inum设为一个保留的inum（例如0） 目录的内容虽然位于数据块，但它一般被认为是元数据， 因此在写入日志时，会被当作元数据（而不是物理数据）处理 空闲空间管理 文件系统必须记录哪些inode和数据块是空闲的，这样在分配新文件/新目录时，就可以使用空闲的inode和数据块， 这就是空闲空间管理 VFSFS中使用位图进行空闲空间管理， 然而也有别的方法， 比如空闲列表和B树等 空闲列表：（超级块中的有一个空闲指针，指向第一个空闲块，此后每个空闲块内部都有指向下一个空闲块的指针 文件访问 假设文件系统已经挂载，要读取文件/foo/bar, 要读取该文件（也就是该文件的数据块），需要先找到该文件的inode， 文件系统必须遍历路径名，才能找到inode， 所有遍历都从文件系统的根目录开始。 即文件系统会先读入inode为2的块， 然后找到/foo的inumber和块，最后找到/foo/bar的inumber和块 一般而言，根目录的inode number为2 这种遍历方式会导致， 访问文件导致的IO与路径长度成正比，路径上的每个目录都会被读取 read()系统调用不会查询位图，因为只有要分配空间时（比如write()）才需要查询分配结构 缓存 如前所述，每次文件访问都会读取路径上的所有目录，为此可以用缓存，将数据保留在内存中， 分为读缓存和写缓存 写缓存： 显然，写入数据最终必须要写入磁盘，这看上去和缓存没什么关系。然而，写缓存可以将一组写入操作编成一批（batch）, 再延迟写入， 一次延迟写入就处理一批写操作 为了避免缓存，可以用fsync()来强制写入磁盘， 甚至可以不使用文件系统，直接使用原始磁盘接口（raw disk interface）来写入数据（数据库就经常这么干， 因为数据库坚持自己控制一切） Locality and FFS 内存是随机访问的，但目前为止的磁盘都是顺序访问的，OS通过抽象，把磁盘抽象成了内存，让程序以为所有空间都是随机访问的，这就导致了某些workload会导致磁盘性能不佳，因为毕竟底层是顺序访问 解决这个问题，需要文件系统面向磁盘设计。为此，Berkley的一个小组设计了FFS（Fast File System），它将磁盘分为一些组，称为柱面组（或称为块组），将两个有关联的块放到同一分组，这样访问的时候可以提高效率 举例来说，FFS将文件的inode所在块和数据块放在一起（因为这二者必定有关联），避免长时间寻道 “有关联”还可以建立在局部性原理上，例如，FFS将同一目录下的所有文件尽量放在一组，因为按照局部性原理，这些文件被经常一起访问 由于大文件无法全部放入一个分组，因此FFS会将某些块（比如inode块）分配到一个组， 而大块房贷单独的分组。这会导致磁盘碎片化。 当然，允许的块越大，这种碎片也就越少 FFS的贡献： 引入了面向磁盘的文件系统 引入了软链接 引入了rename()系统调用 Crush-Consistency Problem 崩溃一致性问题 ： 更新持久数据结构时发生崩溃，解决方案有fsck和日志 向文件写数据时，要更新至少三个块：更新的inode（比如，要增加新的指针）， 更新的数据块和更新的数据位图 这三个块的写入操作，无论哪个出问题，都会导致崩溃后不一致 在元数据日志中，我们会看到， 解决崩溃一致性的核心，就是先写入被指对象，再写入指针对象，这样能保证数据的正确性 示例 示例：对于如下文件系统结构，有一个inode位图，一个数据位图，一个inode表（包含8个inode）和一个数据块表（包含8个数据块） （ 这里inode bmap和data bmap实际指向了下标3和5，应该是图画错了 ） 可以看到，已经分配了一个inode（inumber=2），它在inode位图中标记， 单个分配的数据块Da（数据块4）也在数据位图（记为B[v1]， 表示第一个版本)）中标记， inode表示为I[v1]，即该inode的第一个版本，I[v1]的内容有： owner :lykpermissions :read-writesize :1pointer :4pointer: :nullpointer: :nullpointer: :null... 文件大小为1 （ 有一个数据块 ）， 第一个直接指针指向块4， 且所有其他直接指针都是null 假如要向文件追加内容，比如要增加一个数据块， 此时要更新至少三个块：更新的inode（比如，要增加新的指针）， 更新的数据块和更新的数据位图： 我们希望更新后的inode（用I[v2]表示）内容如下： owner :lykpermissions :read-writesize :2pointer :4pointer: :5pointer: :nullpointer: :null... 更新后的数据位图(记为B[v2])要变成： 00001100 新增的数据块记为Db 我们希望最终的文件系统如下所示： 然而，对I[v2], B[v2], Db 这三个块的写入操作，无论哪个出问题，都会导致崩溃后不一致 FSCK File System Checker 一个UNIX工具，在文件系统挂载之前执行， 可以保证，fsck检查结束后，文件系统时一致的 fsc会检查超级块，空闲块，inode状态，inode链接等信息， 事实上，fsck会扫描整个磁盘（另一方面，出现不一致的可能只是几个数据块）， 因此fsck的代价非常大 Journalist Linux的ext2文件系统没有日志 ，日志是由ext3引入的 带有日志的ext3文件系统如下所示： 可以看到，这里对块/柱面进行了分组 Data Journaling 数据日志就是将要更新的物理内容也写进日志里 对于之前的示例，加入数据日志后，文件系统的日志区域如下所示： 这里写了五个块： 事务开始TxB： 此更新的相关信息，以及事务标识符（TID） 物理日志： 这里占三个块，就是更新的确切物理内容 事务结束TxE： 也会包含TID 更新文件系统分为三个步骤： 日志写入： 将食物的内容（包括TxB， 元数据和数据）写入日志，等待这些写入完成 日志提交： 将事务提交块（包括TxE）写入日志，事务被认为已提交（committed） 加检查点： 将更新内容（元数据和数据）写入磁盘 释放： 一段时间后，通过更新日志超级块（不是主文件系统的超级块）， 在超级块中标识该事物为空闲 这一步和事务的原子性没有关系，只是为了重用日志空间。日志空间如果满了，就无法提交事务了，因此日志被实现为循环数据结构，一旦事务被加检查点，文件系统就应该释放它在日志中占用的空间，允许重用日志空间 要达到这个目的有很多方法，比如在日志超级块中标记最新和最旧的事务 注意，如果将TxE在日志写入阶段一并提交（即没有日志提交步骤），那么在日志写入阶段发生崩溃时（即将以上五个块发生崩溃时），会出现问题。 因此， 日志总是要分为日志写入和提交两阶段，来确日志的原子性 崩溃恢复： 如果在步骤2之前崩溃，那可以跳过这个更新（事务），因为没有该事物的日志，崩溃后的系统根本不知道这个事务的存在 如果在步骤3之前崩溃，系统只需要replay日志中的食物，这称为redo logging 如果在加检查点时发生崩溃，处理方式如步骤2 缺点： 数据日志需要将待更新数据写入磁盘两次，一次写入日志空间，一次写入真正的待更新区域，这是巨大的开销，为此，我们一般使用元数据日志 Metadata Journaling 元数据日志与数据日志几乎相同，但是物理数据没有写入日志，而是直接写入文件系统 但是，物理数据不能在事务提交后再写入文件系统，否则即使replay日志，也无法恢复数据（因为此时的日志中没有物理数据），所以，应该首先进行数据写入，这可以保证指针永远不会指向垃圾， 其核心理念是先写入被指对象，再写入指针对象： 物理数据写入 日志元数据写入： 将开始块TxB和元数据写入日志 日志提交 加检查点元数据： 将元数据更新的内容写入文件系统 释放 元数据日志的缺点是块复用问题， 因为目录的内容被当作元数据而不是物理数据，这意味着（在元数据日志中）目录的内容会放在日志空间。 假设某目录的内容在日志空间中占用块1000， 随后用户删除该目录，并释放块1000， 最后用户创建了新文件，并复用了块1000（此时该块位于物理数据空间）， 此时，在日志提交后，加检查点完成之前发生了崩溃，在replay阶段， 会重放日志中所有内容， 那么新文件的内容（也就是块1000）就会被恢复为目录的内容！ 这都是因为目录的内容被视作元数据，而不是物理数据，保留在日志空间 Linux ext3点解决方案是加入撤销（revoke）指令，在上例中，删除目录将导致revoke指令被添加到日志，在replay时，被revoke的数据（这里是目录的内容）将不会被重放","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"OS Basic","slug":"OS-Basic","permalink":"http://lyk-love.cn/tags/OS-Basic/"}]},{"title":"Hexo","slug":"Hexo","date":"2022-03-27T02:28:03.000Z","updated":"2022-09-26T09:08:13.867Z","comments":true,"path":"2022/03/27/Hexo/","link":"","permalink":"http://lyk-love.cn/2022/03/27/Hexo/","excerpt":"Outline Hexo, Next, Markdown","text":"Outline Hexo, Next, Markdown Intro Hexo版本: 6.2.0 Next版本: 8.12.3 Hexo官网 NeXt Tutor, 里面有几乎全部的主题配置教程 最新版(v8)NeXt Github仓库 Next Official Blog Awesome NexT : 有很多别人的Next博客, 非常值得借鉴 本文档参考了: 比较全的Hexo + Next搭建教程, 其中有些内容有点老了 Hexo是流行的静态博客框架, 同类的还有Hugo和Jekyl. Hexo的优点是, 它的Next主题比较好看. 本文介绍Hexo + Next的配置. 主题文件都存放在&lt;hexo-dir&gt;/themes/, 例如Next主题就位于&lt;hexo-dir&gt;/themes/next/_config.yml. 但是, 由于下面讲的多主机同步的原因, 实际起作用的主题配置文件是&lt;hexo&gt;/_cofig.next.yml Hexo的配置文件位于Hexo目录的_config.yml Next的配置目录位于&lt;hexo&gt;/_cofig.next.yml. 在主题配置中, 除非特殊说明, 编辑都是主题配置文件. 同理, 对于Hexo的配置, 除非特殊说明, 编辑的都是Hexo配置文件 Hexo Commands 命令 描述 hexo init [folder] 初始化网站 hexo new [layout] 新建文章, 默认是“post”, 我通过default_layout: draft设置为新建到“draft” hexo publish [layout] 发布草稿 hexo g[enerate] 生成静态文件. hexo s[erver] 启动本地服务器. 服务器会监听文件变化并自动更新 hexo d[eploy] 在安装了deploy git 插件后, 可以生成本地文件并远程部署到GitPage. 再也不用hexo d -g了 hexo clean 清理数据库和静态文件 hexo list 列出站点信息 hexo version 版本信息 hexo d -g 生成并部署 Hexo搭建 安装 Hexo: npm install -g hexo-cli 在本地指定文件夹 &lt;folder&gt; 中建立项目: hexo init &lt;folder&gt;cd &lt;folder&gt;npm install 新建完成后，指定文件夹 &lt;folder&gt; 的目录如下： .├── _config.yml├── package.json├── scaffolds├── source| ├── _drafts| └── _posts└── themes 文件/文件夹 作用 _config.yml 网站的配置文件 package.json 应用程序的信息 scaffolds 模版文件夹。当您新建文章时，Hexo 会根据 scaffold 来建立文件 source 资源文件夹是存放用户资源的地方。除 posts 文件夹之外，开头命名为 (下划线)的文件 / 文件夹和隐藏的文件将会被忽略。Markdown 和 HTML 文件会被解析并放到 public 文件夹，而其他文件会被拷贝过去 themes 主题文件夹。Hexo 会根据主题来生成静态页面 安装主题, 见下文 部署到GitPage GitPage 允许你将你的博客创建为一个 GitHub Project，通过 your-account.github.io 这样的特殊项目名称与 GitPage 进行关联，然后，你只需要像平时一样 commit 你的博文到 GitHub 上就 OK 了，GitPage 会自动将你的更新部署出去. 注意, 私有仓库的Gitpages也是可以公共访问的, 所以千万不要把含有敏感信息的私有仓库作为GitPages 安装 deploy git 插件实现一键自动部署: npm install hexo-deployer-git --save 在 GitHub 创建一个名为&lt;username&gt;.github.io.git的仓库。 在主题配置文件_config.yml中修改仓库地址, 注意, 为了后文说的多主机同步. 我的仓库有两个分支. master用于存放生成的网页文件, hexo存放源文件. 部署当然是push网页文件, 也就是master分支: deploy: type: git repo: github: git@github.com:LYK-love/LYK-love.github.io.git branch: master GitPage可以关联到项目的任意分支, 我们要到仓库的Settings -&gt; Code and automation -&gt; Pages里, 把Pages关联到master分支. 这样我们部署到master的网页文件就可以同步到Pages上. 执行hexo d即可部署到 GitHub 仓库。 新增或修改主题配置后部署时请执行 hexo clean &amp;&amp; hexo d 注：这里使用 ssh 协议而非 http，所以请先确保您已经在 GitHub 添加了公钥. 自定义域名 首先你需要去域名注册商（阿里云腾讯云等）买一个域名 在根站点下source目录中添加CNAME文件，文件内容为您购买的域名xxx.com，不要添加www、mail等子域例如www.xxx.com或mail.xxx.com 前往域名控制台解析此域名到github.io，根据 gitpage 的自定义域名要求，他们建议解析到github.io的数字 ip 地址，即151.101.129.147 在控制台设置域名解析，添加 A 记录指向 151.101.129.147 即可 命令行执行hexo d发布站点到 GitHub 库，这时在 Git 库应该就能看到 CNAME 文件，至此自定义域名设置完毕，现在使用xxx.com即可访问站点 写作 默认新建文章都是posts, 改成新建为drafts: default_layout: draft 这样hexo new新建的就都是草稿了 文章能会有多个类别, 分类具有顺序性和层次性, 有3种不同的编写方式 # 第一种categories: - Java - Servlet# 第二种categories: [Java, Servlet]# 第三种categories: -[Java] -[Servlet] 前一、二种书写方式的作用一致，表示该文章分类于Java/Servlet下，起到了子分类的作用 第三种书写方式起到了多分类的作用，表示该文章分类于Java和Servlet下 标签没有层次性: categories:- Diarytags:- PS3- Games Configuring Author Edit Hexo config file and set the value of author to your nickname. Hexo config file# Siteauthor: Configuring Description Edit Hexo config file and set the value of description to your description, which can be a sentence you like. Hexo config file# Sitedescription: Enabling Theme Like all Hexo themes, after you download it, open Hexo config file, find theme option, and change its value to next (or another theme directory name). Edit Hexo config file: theme: next Now you have installed NexT theme and enabled it. The following steps will help you verify whether NexT is enabled correctly. NeXt配置 进阶配置 Schemes: 目前觉得Mist比较好看 # Schemes# scheme: Musescheme: Mist# scheme: Pisces# scheme: Gemini 设置语言: 我使用默认的英语. 如果要使用汉语, 可以编辑主题配置文件: language: zh-CN 首页文章显示摘要: 在文章中适当位置插入 &lt;!--more--&gt;，该位置之前的部分即为摘要，会显示在首页中. sidebar社交链接: 邮箱前要加mailto:, 这还是我用开发者工具查别人的网站发现的..( 其实mailto是html中发送email的代码 ) social: GitHub: https://github.com/LYK-love || fab fa-github E-Mail: mailto:191820133@smail.nju.edu.cn || fa fa-envelope sidebar社交链接的图案: social_icons: enable: true icons_only: false transition: false 添加建站时间, 不加的话就会显示当前年份: footer: # Specify the date when the site was setup. If not defined, current year will be used. since: 2021 关掉闪烁: quicklink: enable: false Toc: Table of Contents in the Sidebar. 主要是sidebar里面的标题要不要自动进行数字编号. 默认为true. 我不喜欢编号： toc: enable: true # Automatically add list number to toc. number: false Configuring Favicon By default the Hexo site use NexT favicons in [hexo-site]/themes/next/source/images/ directory with different size for different device. You can replace them with your own favicons. 但是, 如果使用npm安装Next, 则主题文件夹是[hexo-site]/node_modules/hexo-theme-next, 无法进行版本管理. 因此, Hexo也支持将图片放在[hexo-site]/source/images/. 我也强烈推荐这么做, 这样就可以进行版本管理了: 先手动创建文件夹: cd [hexo-site]/sourcemkdir images 后续在Next配置文件中使用images路径来找到图片: 由于themes/next/source/images/和source/images/都可以放图片, 该操作实际上会扫描这两个文件夹 mkdir images 设置站点图标: favicon: # small: /images/favicon-16x16-next.png small: /images/white_flower1.jpg # medium: /images/favicon-32x32-next.png medium: /images/white_flower1.jpg # apple_touch_icon: /images/apple-touch-icon-next.png apple_touch_icon: /images/white_flower1.jpg 设置菜单: 真不知道about页面有啥用 menu: home: / || home # about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat# Enable / Disable menu icons / item badges.menu_settings: icons: true badges: false 设置sidebar avatar, 并且让图片圆形显示 # Sidebar Avataravatar: # Replace the default image and set the url here. url: /images/white_flower1.jpg # If true, the avatar will be displayed in circle. # 圆形显示 rounded: true # If true, the avatar will be rotated with the cursor. rotated: false 头像必须存放在&lt;next&gt;/themes/source/images/ 搜索服务 使用本地搜索，按以下步骤配置： 安装 hexo-generator-searchdb 插件：加了这个之后,博客生成时间要慢好几秒, 不过也是值得的 npm install hexo-generator-searchdb --save 编辑Hexo配置文件 _config.yml： search: path: search.xml field: post format: html limit: 10000 编辑Next配置文件： # Local searchlocal_search: enable: true 百度统计 也可以添加Google统计, 都差不多 开通百度统计帐号: 在 百度统计 注册帐号. 帐号注册成功后，在侧边栏账户管理 -&gt; 网站列表，点击右侧新增网站按钮 添加网站域名/网站首页信息后，点击确定按钮，百度统计会提供一段JS脚本用于嵌入 &lt;script&gt;var _hmt = _hmt || [];(function() &#123; var hm = document.createElement(&quot;script&quot;); hm.src = &quot;https://hm.baidu.com/hm.js?&lt;app-id&gt;&quot;; var s = document.getElementsByTagName(&quot;script&quot;)[0]; s.parentNode.insertBefore(hm, s);&#125;)();&lt;/script&gt; 粘贴上面代码中的`, 复制到主题配置文件: baidu_analytics: &lt;app-id&gt; Next主题已对百度统计进行配置优化，因此只需要编辑配置文件, 填写app-id. 对于更一般的情况, 需要把上面的JS代码添加到网站全部页面的 &lt;/head&gt; 标签前. 如果代码安装正确，一般20分钟后，可以查看网站分析数据. Math Next数学公式支持官方文档 Next Mathjax高级特性 目前的Latex渲染引擎有 MathJax 和 Katex, MathJax完美支持Latex, Katex速度更快, 但是有些语法不支持. 综合来看还是选择Mathjax. NexT默认使用的markdown渲染引擎是hexo-renderer-marked, 它不支持Mathjax，不支持插件扩展，不支持emoji表情, 因此我们需要卸载它, 并替换为别的引擎. 注意, 不同的渲染引擎是不能共存的( hexo-filter-mathjax除外, 它是server端的渲染引擎 ), 因此在使用一个渲染引擎之前, 需要卸载掉其他全部的渲染引擎/ Engines Next支持的渲染引擎: Mathjax If you use MathJax to render Math Equations, you can choose one of the Markdown renderers below: hexo-renderer-pandoc 支持Mathjax语法, 不仅可以渲染markdown, 还支持textile, reStructedText和许多其他格式, 仍然不支持emoji表情 内建的汇总文件db.json将来可能会非常大, 同步到 Github 可能会比较慢, 博客内建的搜索功能也可能会变得非常慢. 亲测, 博客生成和博客的搜索功能都慢得离谱, 而且它和Next的集成有问题, 无法正确把生成的HTML文件的标题加入Anchor. hexo-renderer-kramed: 基于hexo-renderer-marked二次开发的渲染器，完善了对Mathjax的支持, 仍然不支持插件的扩展，不支持emoji表情. 亲测它比hexo-renderer-pandoc快一点. 但是有bug, 需要自己配置. 由于它太老了,就不推荐了. hexo-renderer-markdown-it：支持MathJax, 并可以通过插件支持KeTex. 支持Markdown以及CommonMark语法. 支持插件配置, 支持标题带安全的id信息 支持脚注（上标, 下标, 下划线） 我最后选择hexo-renderer-markdown-it hexo-renderer-markdown-it-plus: 支持Katex插件并默认启用 hexo-filter-mathjax: Server side MathJax Renderer Plugin for Hexo. 要使用它,需要卸载掉除hexo-renderer-marked ( 用于渲染markdown )之外的LaTex引擎. 它的缺点是有些语法不支持, 而且无法渲染目录里的LaTex. KaTex If you use KaTeX to render Math Equations, you can choose one of the Markdown renderers below: hexo-renderer-markdown-it-plus: 没用过 hexo-renderer-markdown-it: 亲测它也支持Mathjax, 并支持Hexo支持的Mathjax特性 支持MathJax 支持Latex步骤: 卸载hexo-renderer-marked, 安装hexo-renderer-pandoc hexo-renderer-markdown-it: npm un hexo-renderer-marked # npm i hexo-renderer-pandoc --save 有问题, 废弃npm i hexo-renderer-markdown-it --save 如果选择hexo-renderer-pandoc , 还需要额外安装pandoc for Mac: brew install pandoc 开启mathjax: math: # Default (false) will load mathjax / katex script on demand. # That is it only render those page which has `mathjax: true` in front-matter. # If you set it to true, it will load mathjax / katex script EVERY PAGE. every_page: false mathjax: enable: true # Available values: none | ams | all tags: ams # ams: 开启公式自动编号 per_page: 设置为false, 这样只会渲染添加了mathjax: true的文章 在低版本的NeXt,这句话上面的注释是反的, 即“false”只会渲染指定文章. 在需要渲染Latex的文章的Front-matter里打开mathjax开关，如下： ---title: index.htmldate: 2016-12-28 21:01:30tags:mathjax: true-- Configuring hexo-renderer-markdown-it hexo-renderer-markdown-it的默认配置是无法正确给标题添加anchor的, 需要做一些修改, 并将配置添加到Hexo中. 编辑Hexo配置文件 _config.yml, 插入以下内容: # Config of hexo-renderer-markdown-itmarkdown: preset: &#x27;default&#x27; # 渲染器默认预设 # &quot;commonmark&quot;: 使用严格 CommandMark 规定. # &quot;default&quot;: 默认配置, 类似于 GFM # &quot;zero&quot;: 禁用所有预设. render: html: true xhtmlOut: false # 将 HTML 内容渲染为 XHTML 的形式 (XHTML 语法非常严格, 比如原 HTML 中的 &lt;br&gt; 标签必须要使用 &lt;br/&gt; 这样的形式进行 &quot;自闭和&quot;) 可能会出现兼容性问题. langPrefix: &#x27;language-&#x27; breaks: true # true 则将所有换行渲染为 &lt;br&gt; 标签 # 这种行为不属于 CommandMark 和 GFM. linkify: true # true 则自动解析链接并添加为 &lt;a&gt; 标签, false 则将链接渲染为文本. typographer: false # 默认 true # 自动转义各种排版用字符, 如 ©. 这甚至会转义掉LaTex中的字符, 所以不能开启 quotes: &#x27;“”‘’&#x27; # 当 typographer 定义为 true 时的自动转换引号的行为, quotes: &#x27;“”‘’&#x27; 则表示将 &quot;123&quot; &#x27;123&#x27;转换为 “123” ‘123’ enable_rules: disable_rules: plugins: anchors: level: 1 # 开始创建锚点的等级, 默认为2,表示从 H2 开始创建一直到 H6(最后). collisionSuffix: &#x27;&#x27; # 如果遇到重复的锚点 ID 为其添加数字编号时在这个数字后添加的后缀. permalink: true # 默认为false, 需要更改为true, 来创建一个除标题外带有固定地址的的锚点标签. permalinkClass: &#x27;header-anchor&#x27; permalinkSide: &#x27;left&#x27; # 设定为 right 则会在标题后添加固定链接. permalinkSymbol: &#x27;&#x27; # 更改为空字符串 case: 0 # 转换锚点 ID 中的字母为大写或小写 # &quot;0&quot; 不转换, &quot;1&quot; 为小写, &quot;2&quot; 为大写. “不转换”是为了方便手写Anchor separator: &#x27;-&#x27; # 用于替换空格的符号. # 默认为 &quot;-&quot; # images: # lazyload: false # prepend_root: false # post_asset: false 当然你也可以直接更改依赖的代码, 但是这样做无法进行版本管理, 所以不推荐: 进入包目录: cd [hexo-site]/node_modules/hexo-renderer-markdown-it 编辑index.js: hexo.config.markdown.anchors = Object.assign(&#123; level: 2, collisionSuffix: &#x27;&#x27;, permalink: true, //更改为true permalinkClass: &#x27;header-anchor&#x27;, permalinkSide: &#x27;left&#x27;, permalinkSymbol: &#x27;&#x27;, //更改为空字符串 case: 0, separator: &#x27;-&#x27;&#125;, hexo.config.markdown.anchors); 公式自动编号和引用 To enable this feature, you need to set mathjax.tags to ams in NexT config file. math: mathjax: enable: true # Available values: none | ams | all tags: ams 为了使用这项功能，一般来说，你必须把所使用的 LaTeX 公式放在 equation 环境里面，采用旧的方法（也就是说，仅仅把公式的每一边用两个 $ 符号包含起来）是无效的。如何引用公式？你只需要在书写公式的时候给公式一个 \\label&#123;&#125; 标记（tag），然后在正文中，可以使用 \\ref&#123;&#125; 或者 \\eqref&#123;&#125; 命令来引用对应的公式。使用 \\eqref&#123;&#125; 是推荐的方式，因为如果你使用 \\ref&#123;&#125;，公式在文中的引用编号将没有圆括号包围。下面介绍几种常见的公式编号例子. 对于简单的公式，使用下面的方式给公式一个标记， $$\\begin&#123;equation&#125;\\label&#123;eq1&#125;e=mc^2\\end&#123;equation&#125;$$ 然后，在正文中，你可以轻松引用上述公式，一个简单的例子如下： 著名的质能方程 $\\eqref&#123;eq1&#125;$ 由爱因斯坦提出 ... Multi-line Equations 对于多行公式，在 equation 环境中，你可以使用 aligned 环境把公式分成多行， $$\\begin&#123;equation&#125;\\label&#123;eq2&#125;\\begin&#123;aligned&#125;a &amp;= b + c \\\\ &amp;= d + e + f + g \\\\ &amp;= h + i\\end&#123;aligned&#125;\\end&#123;equation&#125;$$ Multiple Aligned Equations 要对齐多个公式，我们需要使用 align 环境。align 环境中的每个公式都有自己的编号： $$\\begin&#123;align&#125;a &amp;= b + c \\label&#123;eq3&#125; \\\\x &amp;= yz \\label&#123;eq4&#125;\\\\l &amp;= m - n \\label&#123;eq5&#125;\\end&#123;align&#125;$$ Exclude Equations from Numbering 在 align 环境中，如果你不想给某个或某几个公式编号，那么在这些公式后面使用 \\nonumber 命令即可。例如： $$\\begin&#123;align&#125;-4 + 5x &amp;= 2+y \\nonumber \\\\ w+2 &amp;= -1+w \\\\ ab &amp;= cb\\end&#123;align&#125;$$ Use \\tag to Tag Equations 有时，你可能会希望采用更加奇特的方式来标记和引用你的公式，你可以通过使用 \\tag&#123;&#125; 命令来实现，例如： $$x+1\\over\\sqrt&#123;1-x^2&#125; \\tag&#123;i&#125;\\label&#123;eq_tag&#125;$$ 如果你想要了解更多信息，请访问 MathJax 关于公式编号的官方文档。同时，你也可以阅读 这篇文档 来获取更多细节信息。 字体 默认字体(font: false)是温软雅黑, 还挺好看的. 当然也可以自己改字体. font: enable: false 低版本的Next的font: false的中文巨丑, 将font设为true,之后比原来好看了.. 但我也不知道新字体是啥 代码块 代码高亮: 这个网站可以预览所有高亮效果：传送门 我使用highlight作为高亮引擎, 高亮用arduino-light 我没有设置代码块行号 主题配置文件: codeblock: # Code Highlight theme # All available themes: https://theme-next.js.org/highlight/ theme: # light: default # dark: stackoverflow-dark light: arduino-light dark: arduino-light prism: light: vs dark: vs # Add copy button on codeblock copy_button: enable: true # Available values: default | flat | mac style: flat Hexo项目配置文件: highlight: enable: true line_number: false auto_detect: false tab_replace: &#x27;&#x27; wrap: true hljs: falseprismjs: enable: false preprocess: true line_number: true tab_replace: &#x27;&#x27; 图像缩放 NexT集成了多种图像显示工具, 包括FancyBox和MediumZoom, 主要功能是图像缩放. 开启MediumZoom: # A JavaScript library for zooming images like Medium.# Warning: Do not enable both `fancybox` and `mediumzoom`.# For more information: https://medium-zoom.francoischalifour.commediumzoom: true 图像懒加载 npm install hexo-lazyload --save lazyload: true 访客人数&amp;&amp;文章阅读次数 NexT主题已集成了不蒜子的访客人数和文章阅读统计功能: # Show Views / Visitors of the website / page with busuanzi.# Get more information on http://ibruce.info/2015/04/04/busuanzibusuanzi_count: enable: true total_visitors: true total_visitors_icon: user fa fa-user total_views: true total_views_icon: eye fa fa-eye post_views: true post_views_icon: eye fa fa-eye 在首页显示文章的阅读次数 点击全文阅读显示阅读次数 在底部可以看见访客人数和文章阅读次数 网页加载加速 使用hexo-filter-optimize来提升网页加载速度: 下载插件: npm install hexo-filter-optimize 编辑配置文件: filter_optimize: enable: true # remove the surrounding comments in each of the bundled files remove_comments: false css: # minify all css files minify: true # bundle loaded css files into one bundle: true # use a script block to load css elements dynamically delivery: true # make specific css content inline into the html page # - only support the full path # - default is [&#x27;css/main.css&#x27;] inlines: excludes: js: # minify all js files minify: true # bundle loaded js files into one bundle: true excludes: # set the priority of this plugin, # lower means it will be executed first, default of Hexo is 10 priority: 12 back2top button back2top button非常好看, 默认是添加的: back2top: enable: true # Back to top in sidebar. button默认出现在左下角, 如果这里为true,就会出现在sidebar里面(头像, 目录下面),很难察觉 # 所以我设为false sidebar: false # Scroll percent label in b2t button. # 让to-top的小箭头随时显示数值, 我觉得这样破坏阅读体验, 所以为false scrollpercent: false 这里有个插件, 不过已经不需要了. 自带的就很好看. 版权 选择sidebar, 会在sidebar出现一个小徽章, 不怎么碍眼. 如果选择post, 版权信息会出现在文章底部, 很难看. # Creative Commons 4.0 International License.# See: https://creativecommons.org/about/cclicenses/creative_commons: # Available values: by | by-nc | by-nc-nd | by-nc-sa | by-nd | by-sa | cc-zero license: by-nc-sa # Available values: big | small size: small sidebar: true post: false # You can set a language value if you prefer a translated version of CC license, e.g. deed.zh # CC licenses are available in 39 languages, you can find the specific and correct abbreviation you need on https://creativecommons.org language: 一些不想用的美化 以下美化我都不想用, 只是给出添加方式. 添加版权信息: 版权信息很丑, 所以我没加 creative_commons: license: by-nc-sa sidebar: true post: true 添加评论系统. 我不想加 夜间模式: 这是永久的, 不能手动切换, 所以我也不想加. darkmode: true Reading progress bar: 五颜六色的,影响观看: reading_progress: Reading progress bar enable: false 多主机同步 多主机同步主要的坑在于主题的管理 多分支 在安装插件后,hexo d会生成网页文件, 并将其部署到GitHub和GitPage. 但是不会把源文件也push到github. 我们需要: 在Github建两个分支,分别管理生成的网页文件和源文件: master: 用于存放hexo d 部署的网页文件 hexo: 用于项目源文件, 包括文档源文件 在Github仓库-&gt;Settings-&gt;Branches-&gt;Default branch中将默认分支设为Hexo. 这样每次手动push源文件,都到hexo分支. 当然也可以每次都手动指定,这样很蠢: git push origin hexo 而我们的hexo d会把生成的网页文件同步到master分支. 这是之前配置hexo d插件时设置的: deploy: type: git repo: github: git@github.com:LYK-love/LYK-love.github.io.git branch: master 多主机同步的问题 我们看到, Hexo添加主题的一般流程是 clone 主题到对应的 themes目录中，然后编辑Hexo的配置文件. 但是, 主题是一个独立的github仓库, 有自己的 .git 文件夹, 也就是说本地的主题项目是嵌套在Hexo项目内的字仓库. 当push本地仓库时, 是不会push子仓库的, 证据就是, 在github上查看网站项目仓库, 点进&lt;repository&gt;/themes/, 会发现next文件夹有个很奇怪的名字: [next @ XXXXX, 点击它, 会跳转到该主题文件夹对应的github项目. 这说明本地的主题文件夹是一个子项目, 从来没有随着父项目一起被push到github上. 因此, 当有新主机clone你的hexo仓库时( 比如它要参与多主机同步 ), 它clone下的主题文件夹是空的. 比如, 我用next主题, 当我买了台新电脑, 想要在它上面同步我开发的hexo. clone下我的网站, 发现项目内themes/next为空. 这样就无法正确生成网站页面了. 该Bug的表面结果是: hexo生成的index.html也为空( 0kb ) 主题文件夹不会被push, 且每次主题更新时,都会被overwritten, 所以不要更改主题文件夹到任何内容. 对于主题配置文件. Next官方提供Alternate Theme Config机制来让用户自定义主题配置: 把主题配置文件复制到Hexo项目目录下, 取名为 _config.[name].yml. Replace [name] with the value of theme option in Hexo config file. For NexT theme, the file name is _config.next.yml by default 现在主题的配置文件就会读取Hexo项目目录的_config.[name].yml, 而不是主题目录的_config.yml. 由于位于Hexo项目目录下, _config.[name].yml会随着每次的push被push到Hexo项目的仓库. 不用担心子项目问题 多主机同步时, 新主机只需clone整个项目: git clone https://github.com/LYK-love/LYK-love.github.io 对于CSS之类的文件, 反我是不会改的, 所以无所谓同步. 对于图片, 之前讲了,可以放在项目的/source/images/进行同步( 而不是主题的themes/next/images). 当然为了保险, 我也另外在项目文件夹内备份了图片文件. 下文的**@Deprecated 同步步骤**是被废弃的方案, 它使用git modules, 这种方案新建了主题仓库, 然后用git module同步整个主题文件夹, 问题在于这样做就没法进行主题的更新了, 因此废弃 @New 同步步骤 老主机只需git pull就行了. 对于本地还没有hexo项目的新主机, 需要: clone自己的Hexo项目并初始化: git clone https://github.com/LYK-love/LYK-love.github.iocd LYK-love.github.ionpm install 可能会遇到报错: ERROR Cannot find module &#x27;hexo&#x27; from &#x27;/Users/lyk/Documents/LYK-love.github.io&#x27;&lt;Snip&gt;ERROR Try running: &#x27;rm -rf node_modules &amp;&amp; npm install --force&#x27; 只需按照提示操作即可: rm -rf node_modules &amp;&amp; npm install --force 安装主题 由于下文介绍的Next主题的Alternate Theme Config机制, 主题配置文件已经在Hexo项目文件夹中被我们clone下来了, 也就是说已经同步了, 万事大吉. 由前文知, hexo-renderer-markdown-it的配置也放在主题配置文件中进行同步了. 但是, images等文件没有同步, 我把images放在Hexo项目文件夹下, 需要手动把它copy到主机文件夹的source/images中 @Deprecated 同步步骤 注: 该方案已经被废弃 Ref: 在 hexo 中使用 git submodules 管理主题 在Hexo多主机同步时, 我们当然希望自己的主题配置文件也同步. Bad Practice: 由于本地的主题项目没办法push, 新主机就只能每次只clone 自己Hexo项目, 然后重新clone官方的主题. 这样做是愚蠢的. 多台机子上开发, 每台的本地都是不同的主题项目, 每次生成网页文件,样式都不一样. 因此, 我们需要有一个自己的主题项目, 来对主题也进行版本管理. 可以fork官方主题项目, 但我为了方便, 直接创建了自己的主题项目. Old Hosts 对于已经加入多主机同步的主机来说, 如果本地更改了主题. 那么每次除了push Hexo项目文件, 还得把再把主题项目文件也push. 否则主题配置的更改是没法同步到Github的. push子项目: cd ./themes/nextgit add . &amp;&amp; git commit -m&quot;XXX&quot;git push 要在pull Hexo项目时顺便拉取子项目( 主题项目 ), 这称为update submodule: # 把项目的子项目也pull下来git submodule update --init 当然也可以pull Hexo后手动再pull子项目,这样很蠢: cd ./themes/nextgit pull New Hosts 对于要加入多主机同步的新主机来说, 要把主题文件当作git submodule, 在初始化阶段, 先同步Hexo, 再同步主题. clone自己的Hexo项目并初始化: git clone https://github.com/LYK-love/LYK-love.github.iocd LYK-love.github.ionpm install 将主题作为submodule添加进来: cd blog-hexogit submodule add https://github.com/LYK-love/next themes/next git submodule add &lt;sub-module-registry&gt;: git 会将主题项目( 也就是我的next项目 )作为一个submodule, clone 到 themes/hexo 中. 同时 hexo 项目中会生成一个 .gitmodules 文件, 这个配置文件中保存了项目 URL 与已经拉取的本地目录之间的映射. .gitmodules 文件内容: &gt; cat .gitmodules[submodule &quot;themes/next&quot;] path = themes/next url = git@github.com:LYK-love/next.git update submudule, 第一次update时要加--init选项: git submodule update --init 也可在 clone 父项目时直接使用 git clone --recursive , git 也会pull所有的子项目. 现在新主机已经加入多主机同步, 变成Old Hosts了. Bugs hexo g 会生成静态文件, 但是，如果你的目录下有失效的软链接， 就不会生成文件。 因此请删除所有的失效软链接 ref： Fixing Hexo Not Generating Files hexo的markdown源代码避免出现跨级标题结构， 这里的跨级指的是不能从一个一级标题直接跟三级标题；二级标题后紧跟的子标题级别必须是三级标题 文章的Front-matter是YAML格式, 因此冒号后面必须有一个英文空格: title: XXcategories: XXtags: XX 否则报错: YAMLException: can not read a block mapping entry; a multiline key may not be an implicit key 如果表格多了行/列,在显示时会很丑 极其罕见的Bug, 花了我大半天: Hexo和Next分别更新, 结果Latex不能显示, hexo g巨慢, 页面闪烁, back2top小箭头图表消失等等等等... 最好他居然神奇地好了. 我猜是package.json冲突了. 不过具体原因我也不知道... .气死我了. Hexo Doc 网上没有关于安装指定版本的Hexo的教程. 我的做法是抄一份指定版本的package.json然后npm install Version 查看本地Hexo版本: hexo version 查看有哪些落后的版本: npm outdated Install npm install -g hexo-cli Ungrade 安装hexo时需要安装hexo-cli(它包含了hexo在内的一大堆依赖), 而升级hexo只需升级所有插件: npm install -g npm-upgrade npm-upgrade: 升级作为dependency的hexo 查看是否更新成功: hexo version Uninstall Uninstall: npm uninstall hexo NeXt Doc NexT 每个月都会发布新版本 安装文档 Version NeXt &lt; 8的版本好像没办法查看 Next &gt;=8 之后, 每次hexo s/d时在命令行的输出里都有Next版本信息. 此外hexo version也会显示next版本. Installation 由于我不对主题做版本管理, 也就不新开仓库了. 无论是下载还是更新Next, 都要先更新到最新的Hexo. Using npm npm install hexo-theme-next@latest npm会把主题文件夹下载到/node_modules/hexo-theme-next Using git git clone https://github.com/next-theme/hexo-theme-next themes/next 注意, 如果在themes/下已经存在了主题文件夹, 则Hexo会忽略node_modules/中可能存在的/主题文件夹. 也就是说要么用git, 要么用npm, 两者不能共存 Upgrade 记得备份old主题文件夹的文件, 把旧主题文件夹rename为next-old. 当然, 由于该文件夹一般什么都不会改,所以不备份也没啥关系 已经采用了Alternate Theme Config, 因此可以平滑地升级: git pull Markdown 页面内跳转 Anchor Auto 只需要使用hexo-renderer-markdown-it, 并修改其配置文件, 就可以使文章Header自带Anchor Manual 例子: Markdown的一个标题: // in markdown:# Ha hadadads 会被Hexo渲染成: //in html&lt;id = &quot;ha-ha&quot;&gt;&lt;h1&gt;dadads&lt;/h1&gt; 空格会被转换为连字符, 大写会被转换为小写. 由于我在hexo-renderer-markdown-it中的配置, 空格会被转换为-, 而大小写是不转换的 如果有重名的标题(即使处于不同的标题层次), 就会在html的标签的id属性中予以区分: // in markdown:# hahadadads# Heihei## Haha //重名了asa //in html&lt;id = &quot;haha-1&quot;&gt;&lt;h1&gt;dadads&lt;/h1&gt;&lt;id = &quot;haha-2&quot;&gt; //用数字后缀区分了&lt;h1&gt;asa&lt;/h1&gt; 因此, 只需要在markdown中写: [显示的内容](#标题) 生成的Html是: &lt;a href=&quot;标题&quot;&gt;显示的内容&lt;/a&gt; 这就引用了对应的标题: # 标题 可以看到, 这是基于Html的标签id匹配的, 而Markdown标题生成的Html标签的id和标题级别没有关系, 只和标题名字有关系. 所以 [显示的内容](#KKK) 可以引用到: # Haha## KKK 中的二级标题KKK","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[]},{"title":"Docker Stack","slug":"Docker-Stack","date":"2022-03-25T04:48:35.000Z","updated":"2022-09-26T06:39:34.929Z","comments":true,"path":"2022/03/25/Docker-Stack/","link":"","permalink":"http://lyk-love.cn/2022/03/25/Docker-Stack/","excerpt":"Outline: Basic Idea Commands docker-stack.yml Docker Stack是Docker Compose的进化版, 是Docker原生的部署和管理多服务应用的方案, 默认集成在Docker引擎中, 且提供了简单的声明式接口对应用进行部署和全生命周期管理","text":"Outline: Basic Idea Commands docker-stack.yml Docker Stack是Docker Compose的进化版, 是Docker原生的部署和管理多服务应用的方案, 默认集成在Docker引擎中, 且提供了简单的声明式接口对应用进行部署和全生命周期管理 Basic Idea 虽然是Docker Compose的进化版, 但是Stack是Docker原生的, 和Swarm一样 事实上, Docker Stack依赖于Docker Swarm, Stack将被部署到Swarm上 对应用的任何变更都应该通过Stack文件进行声明, 然后用docker stack deploy部署 Stack文件是Stack的配置的唯一声明, 所有Stack相关的改动都要体现在Stack文件中 不要用命令来修改 Docker Stack不支持build, 意味着在部署Stack之前, 所有镜像必须提前build, 这一点不同于Docker Compose 具体细节就不赘述了, 用的时候再查 Commands 部署 docker stack deploy 列出Stack 列出Swarm集群的全部Stack: docker stack ls 查看Stack详情 docker stack ps &lt;stack&gt; 删除Stack 从Swarm中移除Stack docker stack rm &lt;stack&gt; 只会删除网络和服务, 不会删除密钥和卷 docker-stack.yml Docker Stack根据Stack文件部署应用, 也就是docker-stack.yml docker-stack.yml就是docker-compose.yml, 唯一的区别就是Docker Stack文件的version要大于3.0 二者文件格式相同 项目地址 version: &quot;3.2&quot;services: reverse_proxy: image: dockersamples/atseasampleshopapp_reverse_proxy # 唯一的必填项,指定镜像 ports: # 指定端口映射 - &quot;80:80&quot; - &quot;443:443&quot; secrets: - source: revprox_cert # 定义了两个密钥,不同于网络, 密钥必须在顶级关键字secrets下定义, 且必须在系统上已存在 target: revprox_cert - source: revprox_key target: revprox_key networks: # 该网络要么已经存在，要么在networks一级key中指定, 后者会让Docker创建该网络 - front-tier database: image: dockersamples/atsea_db environment: # 向容器中注入环境变量, 一般都将它们以密钥形式传递 POSTGRES_USER: gordonuser POSTGRES_DB_PASSWORD_FILE: /run/secrets/postgres_password POSTGRES_DB: atsea networks: - back-tier secrets: - postgres_password deploy: # 定义服务约束 placement: constraints: - &#x27;node.role == worker&#x27; # 当前服务只会运行在Swarm的worker上 appserver: image: dockersamples/atsea_app networks: - front-tier - back-tier - payment deploy: replicas: 2 # 含义同docker swarm update_config: parallelism: 2 failure_action: rollback placement: constraints: - &#x27;node.role == worker&#x27; restart_policy: # 定义了Swarm针对容器异常退出的重启策略 condition: on-failure# # 如果容器以failure退出, 会立即重启当前容器,重启最多重试三次, delay: 5s # 每次都会等待120s来检测是否启动成功, 每次重启间隔5s max_attempts: 3 window: 120s secrets: - postgres_password visualizer: image: dockersamples/visualizer:stable ports: - &quot;8001:8080&quot; stop_grace_period: 1m30s volumes: - &quot;/var/run/docker.sock:/var/run/docker.sock&quot; deploy: update_config: failure_action: rollback placement: constraints: - &#x27;node.role == manager&#x27; payment_gateway: image: dockersamples/atseasampleshopapp_payment_gateway secrets: - source: staging_token target: payment_token networks: - payment deploy: update_config: failure_action: rollback placement: constraints: - &#x27;node.role == worker&#x27; - &#x27;node.labels.pcidss == yes&#x27;networks: front-tier: back-tier: payment:` driver: overlay driver_opts: encrypted: &#x27;yes&#x27;secrets: postgres_password: external: true # 指定在Stack部署之前, 该密钥必须存在 staging_token: external: true revprox_key: external: true revprox_cert: external: true 关键字 顶级关键字: version: 代表Compose文件格式的版本号. 要应用于Stack, 需要 &gt; 3.0 services networks: 默认用overlay网络, 因为是Swarm模式 secrets: external: 在Stack部署之前, 该密钥必须存在 网络 Docker Stack首先会检查并创建networks, 因为服务依赖于网络 默认情况下, overlay网络的控制层是加密的, 如果要加密数据层, 需要在Stack文件中driver_opts下指定encrypted: 'yes' 服务 服务只有一个必填的下级关键字image, 指定构建服务副本所需的镜像 其余下级关键字: Docker: 指定Docker Registry, 默认是Docker Hub ports: 定义端口映射, 注意由于是Swarm模式, 默认网络是Ingress模式, port项可以简写, 如果指定用Host模式, 则port项要写完整格式 secrets: 定义了密钥, 不同于网络, 密钥必须已经在顶级关键字secrets下定义, 且必须在系统上已存在 envirenment: 向容器中注入环境变量, 一般都将它们以密钥形式传递 deploy: 定义部署约束, 比如节点ID, 节点名称, 节点角色, 还有更新约束等待 更新约束就是docker service uodate 的那些参数, replicas之类","categories":[{"name":"Technology","slug":"Technology","permalink":"http://lyk-love.cn/categories/Technology/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://lyk-love.cn/tags/Docker/"}]},{"title":"Docker Volume","slug":"Docker-Volume","date":"2022-03-25T01:42:45.000Z","updated":"2022-09-26T06:39:34.929Z","comments":true,"path":"2022/03/25/Docker-Volume/","link":"","permalink":"http://lyk-love.cn/2022/03/25/Docker-Volume/","excerpt":"Outline: Basic Idea Commands 示例 第三方卷驱动 在集群节点间共享存储","text":"Outline: Basic Idea Commands 示例 第三方卷驱动 在集群节点间共享存储 Basic Idea docker可以进行非持久化和持久化存储 非持久化存储: 属于容器的一部分, 与容器的生命周期一致 默认情况下, 容器的所有存储都使用非持久化存储 持久化存储: 用户可以创建docker卷, 将卷挂载到容器上 这首先需要将主机的文件挂载到docker卷, 默认会用主机的 &lt;docker镜像存放位置&gt;/volumes/&lt;卷名&gt;/_data目录 这意味着可以在主机的文件系统里查看卷的数据 docker卷与容器是解耦的. 删除容器,卷不会被删除 本地存储默认位于&lt;docker镜像存放位置&gt;/&lt;storage-driver&gt; docker镜像存放位置: 默认情况下 在 /var/lib/docker 存储驱动: 可以自己查看deamon.json, 一般都是overlay2 Commands 创建卷 docker volume create &lt;vol-name&gt; -d: 指定驱动, 默认是local 列出卷 docker volume ls 查看卷 docker volume inpect &lt;vol-name&gt; 输出形如: [ &#123; &quot;CreatedAt&quot;: &quot;2022-03-25T01:02:47+08:00&quot;, &quot;Driver&quot;: &quot;local&quot;, //默认驱动是local &quot;Labels&quot;: null, &quot;Mountpoint&quot;: &quot;/home/lyk/data/docker/volumes/bizvol/_data&quot;, // docker主机被挂载的文件系统位置 &quot;Name&quot;: &quot;bizvol&quot;, &quot;Options&quot;: null, &quot;Scope&quot;: &quot;local&quot; // 默认是local,只能用于当前docker主机的容器 &#125;] 删除卷 删除所有未被使用的卷: docker volume prune 删除指定卷( 不能删除正在被使用的卷 ): docker volume rm &lt;vol&gt; 将卷挂载到容器 docker命令: docker container run \\-dit --name &lt;contaienr&gt; \\ --mount source=&lt;vol&gt;,target=&lt;container-mount-point&gt; \\--restart=always \\&lt;image&gt; --mount: 挂载卷, 如果没有该卷则会创建 source: 要被挂载的docker卷 target: 卷被挂载到的容器内目录 --restart=always: 容器总是自动重启，生产环境中建议使用此特性 Dockerfile: VOLUME &lt;container-mount-point&gt; 这个方案没什么用,因为Dockerfile一般只能指定相对的主机目录, 这就意味着挂载点在不同主机间会不一样, 得每次部署都手动修改主机目录Docker 查看容器的挂载卷情况 docker inspect &lt;container&gt; | grep Mounts -A 20 示例 创建一个容器,并挂载一个卷到其/vol目录: docker container run -dit --name voltainer \\--mount source=bizvol,target=/vol \\ # bizvol卷不存在,因此会被自动创建alpine 进入该容器: docker container exec -it &lt;contaienr&gt; sh 在容器中的/vol目录下存储数据: echo &quot;This will last&quot; &gt; /home/lyk/data/docker/volumes/bizvol/_data/file1 查看上一步操作: cat /home/lyk/data/docker/volumes/bizvol/_data/file1# 输出为: This will last 删除该容器: docker container rm -f voltainer 这并不会删除docker卷 查看docker主机文件系统中对应该卷的目录: sudo cat /home/lyk/data/docker/volumes/bizvol/_data/file1# 输出为: This will last# 可见卷保留了原始数据, 也证明了卷没有被删除 下面将bizvol 卷挂载到新的服务/容器 docker service create \\--name hellcat \\--mount source=bizvol,target=/vol \\alpine sleep 1d 由于没有指定--replicas, 因此服务只会部署一份副本. 找到该服务运行的节点, 在节点上找到该服务对应的容器: docker service ps hellcat 进入该容器 查看挂载点下的数据: sudo cat /home/lyk/data/docker/volumes/bizvol/_data/file1# 输出为: This will last# 可见可以将卷挂载给其他容器 第三方卷驱动 Docker可以通过插件方式集成第三方卷驱东, 这可以为Docker集成外部存储系统,并使用这些系统的特性: 块存储: 适用于小块数据的随机访问 文件存储: 包括Azure文件存储和Amazon EFS 对象存储: 适用于较大且长期存储的,很少变更的二进制数据, 通常是根据内容寻址 在集群节点间共享存储 Docker可以继承外部存储系统, 而后者中, 如LUN, NFS等能提供集群间节点共享存储 但是, 所有的共享存储都会面临缓存一致性, 数据一致性等问题,这需要在应用程序中控制. Docker自身无法控制这点","categories":[{"name":"Technology","slug":"Technology","permalink":"http://lyk-love.cn/categories/Technology/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://lyk-love.cn/tags/Docker/"}]},{"title":"Docker Swarm","slug":"Docker-Swarm","date":"2022-03-24T23:44:59.000Z","updated":"2022-10-01T15:20:36.996Z","comments":true,"path":"2022/03/25/Docker-Swarm/","link":"","permalink":"http://lyk-love.cn/2022/03/25/Docker-Swarm/","excerpt":"Outline: Basic Idea Commands Swarm Service Swarm Lock","text":"Outline: Basic Idea Commands Swarm Service Swarm Lock Preface Docker Swarm是一个Docker容器集群编排方案, 同类型的还有K8S, 简而言之, K8S &gt; Docker Swarm. 尽管如此, 但Docker Swarm由Docker原生支持, 而且比K8S简单, 学习它可以初步了解一下容器编排., 可以当作K8S的简易入门. 也就是说Docker Swarm就是个玩具... 本章需要结合Docker Network理解 使用Swarm, 需要打开Swarm所需端口： TCP/2377： Swarm的集群管理默认使用2377端口 TCP/7946, UDP/7946: Swarm的节点发现使用7946端口 Basic Idea Docker Swarm由Docker原生支持, 能够创建Dokcer容器集, 并进行编排 node node: 一个集群( Swarm )由多个node组成, 一个node就是一台docker主机. node分为Manager和worker Manager: 负责集群的控制面板( Control Plane ), Manager Worker Swarm node采取主从方式, 有多个Manager, 但同一时刻有且仅有一个Manager处于Active, 处于Active的Manager称为leader, 其余Manager称为follower leader是唯一一个对Swarm进行控制的节点, 如果follower接收到了Swarm命令, 它会将其转发到主节点 leader, follower是Raft的术语, Swarm采用了Raft共识算法 Raft Docker Swarm采用了Raft共识算法, 以下是两个原则: 部署奇数个Manager. 如果Manager之间分区, 掌握多数Manager的分区就会对Swarm进行管理; 反之,如果偶数个分区,会出现Manager数量相同的情况,无法进行决策,称为Split-Brain Manager数量控制在3~5个. 更多的Manager意味着要花更长的时间达成共识 Manager可以进行分区, 但是各分区之间的网络一定要确保畅通 service Docker Swarm的最小调度单元是服务, 也就是Docker Compose里定义的&quot;服务&quot; 服务就是容器的组合, 而且提供了一些高级特性, 比如一些配置文件 etcd etcd( European Toiletries and Cosmetics Database) : a distributed, consistent key-value store for shared configuration and service discovery, 许多分布式系统都采用了etcd, 包括K8S和Docker Swarm etcd位于所有管理节点上, 保存了Swarm的配置和状态 etcd运行在内存, 并保持数据的最新状态 token 任何node加入Swarm的唯一凭证就是准入令牌&lt;token&gt; token的格式为: PREFIX - VERSION - SWARM ID - TOKEN PREFIX: 永远是SWMTKN VERSION: Swarm的版本 SWARM ID: Swarm认证信息的哈希值 TOKEN: 其内容决定了该令牌是Manager还是worker的准入令牌 因此,对于指定了加入Swarm的角色的token, 除了TOKEN字段外,其他字段应该相同 TLS Docker Swarm采用TLS, 每个node都有自己的客户端证书, 证书的更新周期默认是90天 查看CA配置: docker system info# 输出为Swarm: active NodeID: 0b51jn6a7l72c7rky9k7w3b1d Is Manager: true ClusterID: 47zrkbd66toz8zo0fyqgk5oyv &lt;SNIP&gt; CA Configuration: Expiry Duration: 4 weeks # 证书过期时间 Force Rotate: 0 Autolock Managers: false Root Rotation In Progress: false&lt;SNIP&gt; 指定外部CA docker swarm init加 --external-ca参数来指定外部CA 更新证书的更新周期 可以用docker swarm update 配置证书的更新周期: # 更新到30天docker swarm update --cert-expiry 720h CA配置 docker swarm ca 使用时用--help来查看具体功能 Commands 只有manager能查看集群信息, worker无权查看, 因此docker service ls之类的命令只有Manager能用 创建Swarm 创建一个新的Swarm, 执行该命令的节点会成为第一个Manager, 也是根CA节点, 且切换到Swarm模式, : docker swarm init --advertise-addr &lt;ip:port&gt;: 指定该节点公布给外界的ip和端口( 用于让其他服务器加入swarm ). 这是可选的,但建议总是手动设置这两个属性 可以指定一个节点上没有的ip用于负载均衡 端口默认是2377/tcp 最好一定要指定一个公网ip. 否则默认会公布一个内网ip, 例如 10.0.16.7, 假如其它服务器不在同一个内网, 是无法通过该节点的令牌( 带有其公布的ip )加入swarm的 --listen-addr &lt;ip:port&gt;: 指定用于承载Swarm流量的ip和端口, 通常与--advertise-addr相匹配 --autolock: 开启swarm锁 --external-ca: 指定外部CA 获取准入令牌 在Manager上, 获取加入到该Swarm的准入令牌( token ): docker swarm join-token manager # 获得的token能让其他节点作为manager加入docker swarm join-token worker # 获得的token能让其他节点作为worker加入 更新准入令牌 废除原有的准入令牌,再发布新令牌 docker swarm join-token --rotate manager # 更新已发布的manager的准入令牌docker swarm join-token --rotate worker # 同理 新令牌和旧令牌只有TOKEN字段不同, SWARM ID还是相同的 可以废除当前令牌, 再发布新的 加入Swarm 指定节点加入某Swarm docker swarm join --token &lt;token&gt; &lt;target-ip:port&gt; 可以看到, 节点以什么身份加入Swarm, 完全取决于所给的Token, 节点自己没有自定义的权利 查看Swarm 列出Swarm中所有节点: docker node ls 仅限Manager使用, worker无权查看集群状态 MANAGER STATUS 一列没有任何显示的是worker ID列显示星号( * )的是执行该命令的节点 注意, 节点名就是主机名. 如果你觉得主机名没有可读性, 可以改名, 然后重启docker: hostnamectl set-hostname acs-services-node1service docker restart STATUS: Ready：集群中的节点 Down：集群中 leave 的节点 MANAGER STATUS 无值：表示该节点是worker Leader：集群的第一个管理者，管理整个集群，编排决策 Reachable：属于管理节点，当Leader节点不可用后，该节点有权利竞选Leader Unreachable：管理节点不可用，无法与其它管理节点连接(节点退出集群) availability Active：活动的节点，可以被调度器分配任务 Pause：不能分配新任务，已存在的任务继续运行 Drain：不能分配新任务，已存在的任务会被停止，并将这些任务调度到在可用节点上： 离开Swarm 节点主动离开swarm: docker swarm leave --force: Manager leave需要加--force 删除节点 有时只能在其他节点上强制删除另一些节点( 比如, 某些节点是云服务器, 已经过期了, 此时没办法登录这些节点让它们主动leave, 只能在其他节点上删除它 ): docker node rm &lt;noce&gt; 仅能删除worker, 如果要删除manager, 需要降级为worker. 或者也可以用-rm 更改节点角色 将manager角色降级为worker: docker node demote &lt;hostname&gt; 将worker角色升级为manager: docker node promote &lt;hostname&gt; 创建Service 创建新服务: docker service create --name &lt;service-name&gt; \\--replicas &lt;num-of-replica&gt; \\&lt;image&gt; docker service create 格式与docker container run 类似 --replicas: 告知Docker应该总是有n个此服务的副本, 这定义了期望状态 默认为1 Leader会按要求在Swarm中实例化n个副本, 服务副本使用相同的镜像和配置 Manager也会作为worker运行 -p, --publish port：&lt;host-port&gt;:&lt;container-port&gt;， 宿主机端口映射到容器端口，由于是swarm模式，该宿主机端口会监听在集群所有节点上 注意，只有拥有该镜像的节点才会被调度到运行该服务, 可以从docker service ps &lt;service&gt;中看到，某些节点“拒绝了任务调度，因为”： No such image: &lt;imaeg&gt; 即使该service创建失败, 也依然会占用端口等资源. --resolve-image never : 在很少见的情况下, 加了这个选项可以避免平台不支持的问题, 不过我没遇到过 swam集群中如果管理节点使用一个本地镜像创建服务,在给子节点分配任务时, 子节点就无法从公共docker hub上拉取到这个自建的镜像. 可以使用自建的本地registry. 也就是说, swarm集群没法使用本地镜像, 即使集群中全都是manager也不行, 因为manager也是worker; 甚至即使集群中只有一个节点, 它是manager, 且它本地有同名镜像, 也依然会使用registry的而不是本地的. 一个例子是, 我在macbook上build了镜像 lyklove/frontend_volatile_reborn:latest, 然后push到了dockerhub, 结果在linux服务器上使用: docker service create \\--name frontend_volatile_reborn_svc \\--network volatile_reborn -p 81:80 \\--replicas 1 \\lyklove/frontend_volatile_reborn:latest 报错: 1/1: no suitable node (unsupported platform on 1 node) 显然,这说明了我service基于的镜像是mac的, 不是linux的, 尽管我的在服务器上也有同名的该镜像. 当然, 你也可以使用自建的本地registry, 不过也太麻烦了, 还是每次都push到dockerhub吧. e.g. ❯ docker service create --name web-fe \\&gt; -p 4000:8080 \\&gt; --replicas 5 \\&gt; nigelpoulton/pluralsight-docker-ci Swarm会持续确保服务的实际状态和期望状态一致(也就是K8S中的调协循环) Swarm会在后台轮询检查所有服务, 来持续比较服务的实际状态和期望状态 查看Service 列出Swarm中所有运行中的服务: docker service ls 查看service在哪个node: docker service ps vo 进一步查看Swarm中某个服务的信息, 比如哪些节点在运行该服务实例: docker service ps &lt;service&gt; 在运行任务的节点上运行docker ps也能看到这个service对应的容器 查看服务的详细信息: docker service inspect --pretty &lt;service&gt; --pretty: 文本更可读 改变Service 对运行中的服务的属性进行变更, 这要求节点都处于overlay网络. docker service update &lt;service&gt; --autolock=true: 开启swarm锁 滚动更新: docker service update \\--image &lt;new-image&gt; \\ # 指定新镜像--update-parallelism &lt;num-of-replica-per-update&gt; \\ # 每次更新的副本数--update-delay 20s uber-svc # 每次更新的延迟&lt;service&gt; 所有update操作都是持久的, 可以用docker service inspect 查看 服务扩缩容 对服务副本个数进行增减 docker service scale 删除Service 删除服务, 该命令不会要求确认 docker service rm 配置CA docker swarm ca 服务滚动更新 docker service update \\--image &lt;new-image&gt; \\ --update-parallelism &lt;num&gt; \\ --update-delay &lt;time&gt; uber-svc \\&lt;service-to-update&gt; --update-parallelism &lt;num&gt;： 每次更新num个副本 --update-delay &lt;time&gt; # 每次更新有times 延迟 查看Service日志 查看服务的日志 docker service logs Swarm Service 创建Service 创建新服务: 副本模式 vs 全局模式 服务的复制模式: 副本模式( replicated ): 默认模式, 会部署期望数量的副本,并尽可能均匀地讲个副本分布在各个集群中 这意味着一个主机可能被部署多个副本 全局模式了每个节点上仅运行一个副本 扩缩容 docker service scale=&lt;expected-number&gt; 删除服务 docker service rm &lt;service-name&gt; 验证服务是否被删除: docker service ls 只有Manager可用 滚动更新 滚动更新需要节点处于overlay网络中. 如果位于bridge网络中, 则其他节点是不会收到更新的 先创建一个overlay网络,: docker network create -d overlay uber-net 由Docker Network得, 一个Swarm节点直到在覆盖网络上启动容器时,才会将自己接入该网络, 因此,此时在其他节点上,看不到该网络 然后创建一个服务,部署4个副本, 连接到overlay网络 ❯ docker service create --name uber-svc \\--network uber-net \\ # 接入overlay网络-p 80:80 --replicas 4 \\nigelpoulton/tu-demo:v1 此时在其他节点上启动了容器,因此能看到该网络 ❯ docker service update \\--image nigelpoulton/tu-demo:v2 \\ # 指定新镜像--update-parallelism 2 \\ # 每次更新两个副本--update-delay 20s uber-svc # 每次更新有20s延迟uber-svc # 对uber-svc 进行更新 此时在其他节点上docker container ls 可以看到已经运行了新版本的镜像 Swarm Security Docker Swarm提供了许多安全特性( 许多已经被前文提到 ) , 包括: 加密 node ID 基于TLS的认证 安全准入令牌 周期性证书更新 CA配置 加密集群存储: 目前的存储基于etcd, 且会在Swarm manager间自动复制, 存储默认加密 加密网络 Swarm Lock 重启旧的Manager, 进行备份恢复等操作都会对集群造成影响, 为此, Docker Swarm提供了锁机制, 这会强制要求重启的管理节点在提供一个集群解锁码之后才能接入集群 需要手动开启 开启锁 在创建Swarm时开启锁: docker swarm init --autolock 在已有Swarm上开启锁: docker swarm update --autolock=true 上锁后会得到解锁码: ❯ docker swarm update --autolock=trueSwarm updated.To unlock a swarm manager after it restarts, run the `docker swarm unlock`command and provide the following key: SWMKEY-1-RAjjfCURYShYusB+KEJRL3RDzz6B9hA1z48tmqaTJWsPlease remember to store this key in a password manager, since without it youwill not be able to restart the manager. 验证锁效果 开启锁后, 重启一个管理节点: service docker restart docker node ls 输出为: error response from daemon: Swarm is encrypted and needs to be unlocked before it can be used. Please use &quot;docker swarm unlock&quot; to unlock it. 解锁 docker swarm unlock# 然后会要求你输入解锁码 Swarm实战: volatile 我们要将前端服务volatile_frontend_svc(容器监听80端口)部署到集群，对外暴露集群的81端口 Server Config 主机名 主机ip 角色 lyk阿里云服务器 ** master，CICD工作节点 lyk华为云服务器 ** master lyk腾讯云服务器 ** master 3台Master， 3台Worker（ Master也同时作为Worker，因此实际上只有三台主机 ） Prerequists 前置准备：所有节点必须打开： UDP/4789： 绑定到VTE TCP/2377： Swarm的集群管理默认使用2377端口 TCP/7946, UDP/7946: Swarm的节点发现使用7946端口 TCP81: 暴露所有运行前端server的node的的81端口 步骤 先在阿里云主机上创建第一个master节点： docker swarm init --advertise-addr **:2377 填该master节点的公网ip 生成令节点作为master加入集群的令牌： docker swarm join-token manager 该命令的输出即为令牌 在其他节点上使用上述令牌，使其作为master加入该swarm集群。 成功后执行下述命令，查看集群中的节点： docker node ls 在任意master上创建overlay网络，名为volatile: docker network create -d overlay volatile 一定要先创建网络, 否则其他节点无法加入该网络 在master上基于镜像创建新服务volatile_frontend_svc，并使用网络volatile: docker service create --name volatile_frontend_svc \\--network volatile \\-p 81:80 \\--replicas 3 \\lyklove/volatile_frontend:latest 这里设置服务实例数为3 我们将集群的81端口映射到了容器的80端口。 因此访问集群的任意节点的81端口的流量最终都会被转发到运行了该服务副本的节点 （后续）滚动更新: docker service update --image lyklove/volatile_frontend:new --update-parallelism 2 --update-delay 1s volatile_frontend_svc 基于新镜像lyklove/volatile_frontend:new 更新服务，并在其他节点上也进行服务更新 该命令可以在任意拥有该新镜像的master节点上执行 集成CICD 可以发现，CICD只需要将服务打包成镜像，然后利用该镜像滚动更新就行了 Jenkins脚本jenkinsfile.groovy: // ...// 根据代码构建新镜像stage(&quot;update service by built image&quot;)&#123; sh &quot;docker service update --image $&#123;IMAGE_TO_RUN&#125; --update-parallelism 2 --update-delay 2s $&#123;SERVICE_NAME&#125;&quot; &#125; 注意, jenkinsfile里只写了docker service update, 而没有create. 因此, 需要先手动在server上create service, 后续cicd时才能够update 集群使用 通过[host-ip]:81访问前端 其中host-ip可以是集群中任意节点的ip Swarm实战: volatile_reborn Server Config 由于华为云服务器过期了, 这次只有两个节点. 这也意味着只能有一个专业的worker, 否则会发生brain-split 主机名 主机ip 角色 lyk腾讯云服务器 ** master, CICD工作节点 lyk阿里云服务器 ** master 1台Master， 2台Worker(算上Master) Steps 照常配置服务器端口 把之前volatile时期配置的华为云的node删掉. 由于server已经登陆不上了, 没法主动leave, 只能在其他节点上删除: docker node rm k8s-master //k8s-master是华为云服务器的hostname Create Token: docker swarm init --advertise-addr 123.56.20.222:2377 Let other nodes join the swarm with this token Create overlay network on any master, 名为volatile_reborn: docker network create -d overlay volatile_reborn Create frontend service on manager: docker service create \\--name frontend_volatile_reborn_svc \\--network volatile_reborn -p 81:80 \\--replicas 2 lyklove/frontend_volatile_reborn:latest-linux Create backend service on manager: docker service create \\--name backend_volatile_reborn_svc \\--network volatile_reborn -p 8000:8000 \\--replicas 1 lyklove/backend_volatile_reborn:latest-linux Create eureka service on manager: docker service create \\--name backend_eureka_volatile_reborn_svc \\--network volatile_reborn -p 8001:8001 \\--replicas 1 lyklove/backend_eureka_volatile_reborn:latest-linux 后续滚动更新和集成cicd都和volatile类似 Problems swarm面对一个镜像名, 似乎会优先使用dockerhub的镜像? --resolve-image never","categories":[{"name":"Technology","slug":"Technology","permalink":"http://lyk-love.cn/categories/Technology/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://lyk-love.cn/tags/Docker/"}]},{"title":"Docker Network","slug":"Docker-Network","date":"2022-03-23T05:39:44.000Z","updated":"2022-09-26T06:39:34.929Z","comments":true,"path":"2022/03/23/Docker-Network/","link":"","permalink":"http://lyk-love.cn/2022/03/23/Docker-Network/","excerpt":"Outline: Basic Idea Docker DNS服务器 Commands Networks","text":"Outline: Basic Idea Docker DNS服务器 Commands Networks Basic idea Docker使用了Linux的Namespaces技术来进行资源隔离，如PID Namespace隔离进程，Mount Namespace隔离文件系统，Network Namespace隔离网络等。一个Network Namespace提供了一份独立的网络环境，包括网卡、路由、Iptable规则等. 与其他的Network Namespace隔离 因此，一个独立的网络就是一个独立的Network Namespace 因此，严格意义上讲，下文介绍的Host， None根本不算网络，因为他们没有自己的Network Namespace Docker网络架构由三部分组成： CNM： Container Network Model, 容器网络模型，是设计标准 Libnetwork：是CNM的具体实现，被Docker采用 驱动：Go编写，实现了CNM中列举的核心组件 Docker DNS服务器 Docker sever有一个DNS服务器，可以为容器提供DNS解析功能， 也就是说，在容器网络中， 容器名相当于域名 Docker DNS服务器记录了全部容器名称和ip地址的映射 每个启动时使用了--name或--net-alias的Docker容器和Swarm服务， 都会将自己的名称和IP地址注册到Docker DNS服务器 每个Docker容器都有自己的本地DNS解析器，相当于DNS代理服务器。像正常的网络一样，如果DNS解析器在本地缓存中没有找到对应映射，就会向Docker DNS服务器发起递归查询 CNM 定义了三个基本要素： Sandbox： 就像容器中的容器，其中运行着独立的网络栈。 被放在容器内部，为容器提供网络连接 终端： 相当于虚拟网卡， 负责将沙盒连接到网络 网络：就是软件实现的网桥 虽然容器A,B运行于同一个主机，但是由于有不同的沙盒，所以网络堆栈不同 容器A, B可以通信，因为都接入了网络A. 容器B的两个终端属于不同网络， 没有交换机无法通信 Libnetwork 实现了CNM定义的三大组件 Driver Docker封装的内置驱动( on Linux )： Bridge Overlay Macvlan 第三方也可以编写外部驱动 每个驱动都负责其上所有网络资源的创建和管理 Libnetwork支持同时激活多个网络驱动。 这意味着Docker环境可以支持一个庞大的异构网络 Commands 创建网络 注意到只能用对应驱动创建网络， 而None，Host， Container这些特殊的“网络”，不需要驱动，也就没法用驱动创建 创建容器网络： docker network create [OPTIONS] [network_name] -driver, -d: 驱动程序管理网络( 默认 bridge ) -d bridge: 创建bridge网络（仅限linux），这还会在主机内核中创建一个新的网桥， 查看liunx网桥： brctl show -d overlay： overlay网络， 控制层默认用了TLS加密， 可以再指定-o encrypted对数据层加密 --subnet=&lt;subnet1&gt; --subnet=&lt;subnet2&gt; ...: 创建具有多个子网的覆盖网络（覆盖网络可以划分子网）， Docker负责子网间的路由 -p, --publish: 指定端口映射 查看网络 查看已有的网络： docker network ls 查看指定网络的详细信息： docker network inspect [network_name] 删除网络 删除Docker主机上指定的网络 docker network rm &lt;network&gt; 删除Docker主机上所有未使用的网络 docker network prune 创建容器并定制DNS 启动新容器，向其添加自定义的DNS服务器和域名： docker container run -it --name &lt;dontainer-name&gt; \\--dns=&lt;nameserver-ip&gt; \\--dns-search=&lt;domain-name&gt; \\&lt;image&gt; &lt;command&gt; Networks None， Host， Containers网络比较特殊，我们一般也不用 none 就是没有网络，使用此网络的主机没有ip地址，处于完全隔离的状态 host Host网络即容器直接使用宿主机的网络，与主机在相同的网络命名空间下，使用相同的网络栈，容器可以直接使用主机的所有端口 即不进行任何网络的虚拟化 container container网络下的容器都共享相同的Network Namespace， 但除此之外，其他的namespace依然隔离 这意味着容器共享了网络栈 这意味着两个容器可以互相ping通 单机桥接网络 单机： 该网络只能在单个主机上运行，且只能连接所在主机上的容器 桥接：该网络是交换机的软件实现 每个Docker主机都有一个默认的单机桥接网络， 在linux上名为bridge, windows上名为nat 除非在容器创建时指定参数--network. 默认情况下，所有容器都会连接到该网络 linux主机上的bridge网络由Bridge驱动创建， 而Bridge底层基于Linux内核的Linux Bridge技术. 默认的bridge网络被映射到docker0Linux网桥, 容器连接到该网络后，docker就会从docker0子网中分配一个 IP 给容器使用，并设置 docker0 的 IP 地址为容器的默认网关 创建单机桥接网络： docker network create -d bridge [name] 容器通信 同一网络中的容器可以通过容器名来ping通， 因为容器内部运行了一个本地的DNS解析器，将请求转发到docker内部DNS服务器， 后者记录了容器启动时通过--name或--net-alias 指定的名称与容器之间的映射 示例 ❯ docker container run -it --name c1 \\--network localnet \\alpine sleep 1d❯ docker container run -it --name c2 \\--network localnet \\alpine sh 端口映射 容器启动时可以使用端口映射： docker container run -p &lt;host_port&gt;:&lt;container_port&gt; &lt;image&gt; -p, --publish: 指定端口映射 多机覆盖网络 覆盖网络是个二层容器网络, 允许单个网络包含多个主机，这样不同主机上的容器就可以在链路层实现通信 使用overlay驱动， 即需要指定-d overlay 覆盖网络的控制层默认是加密的，可以指定-o encrypted对数据层加密 overlay网络需要Swarm模式 overlay网络中，所有节点必须打开： UDP/4789： 绑定到VTE， 详见下文“原理” Swarm所需端口： TCP/2377： Swarm的集群管理默认使用2377端口 TCP/7946, UDP/7946: Swarm的节点发现使用7946端口 容器通信 只有当运行中的容器连接到覆盖网络时，该网络才变成可用状态 这意味着在集群中却没有运行（连接到覆盖网络的）容器的节点，是看不到覆盖网络的 此时的docker network ls 看不到覆盖网络 当然，创建该网络的节点肯定能看到该网络 一个Swarm节点直到在覆盖网络上启动容器时，才会自动将自身加入到覆盖网络 此时的docker network ls 可以看到覆盖网络 覆盖网络在网络层为容器提供了完全的抽象 覆盖网络内的容器在网络层直接通信 ， traceroute的跳数是1, 尽管这两个容器所在主机可能不在同一个物理网络， 跳数不止1 “直接通信” ‘跳数为1“ 的原因是Swarm默认是Ingress网络，这是个完全图，任意两个节点是直连的 覆盖网络内的容器可以通过网络内的子网ip通信，也可以用容器名通信（ 参见下文《Ingress网络》 ） 原理 隧道端点VTEP (VXLAN Tunnel End Point - an entity which originates and/orterminates VXLAN tunnels) VNI(VXLAN Network Identifier) Docker使用VXLAN隧道技术， 基于已经存在的三层网络来创建虚拟的二层网络 VXLAN基于现有的三层网络创建隧道 注意： overlay网络可以实现三层路由，即可以创建一个包含多个子网的overlay网络， Docker负责子网间的路由: # 在Sandbo中创建2个虚拟交换机（不再只有1个）， 默认支持路由docker network create --subnet=&lt;11.1.1.0/24&gt; --subnet=&lt;11.1.1.0/24&gt; -d overlay &lt;network-name&gt;` 容器的Sandbox内部会创建一个名为Br0的虚拟交换机和一个VTEP， 后者一边连到Br0， 一边连到主机的网络栈， 主机网络栈中的终端可以从所连接的网络中获得IP地址（主机的IP）， 并以UDP Socket的方式绑定到4789端口 每个容器都有自己的虚拟以太网(veth)适配器, 并接入本地的Br0 假设node1上的容器为C1, node2上的容器为C2， C1 ping C2 的ip 10.0.0.4： 该请求通过veth接口发送到Br0, Br0的ARP映射表中没有与当前目的ip对应的MAC地址， 因此它会将该frame发送到其上的所有端口 VTEP接口连接到Br0,它知道这个frame的映射， 于是VTEP将自己的MAC地址返回给Br0， 这是个代理ARP响应 和计网一模一样， 只返回下一跳的MAC地址，一步一步迭代 VTEP接口知道C2, 是因为所有新启动的容器都会将自己的网络详情用网络内置的Gossip协议发送给相同Swarm集群内的其他节点 Br0收到返回结果，它更新自己的ARP映射， 将10.0.0.4映射到本地VTEP的MAC地址 Br0 将该frame转发到VTEP， 后者将frame封装成UDP包， 设置目的IP字段为node2的VTEP的IP地址，设置目的UDP Socket端口为4789 封装就是把 VXLAN Header信息（记录了VLAN到VXLAN的映射）添加到frame, 每个VLAN都对应一个VNID, 以便包被解析后可以被转发到正确的VLAN 包到达node2后， node2的内核发现目的端口为 UDP 4789, 还知道存在VTEP绑定到该端口， 内核就将包发给VTEP VTEP读取VNID, 解压该包， 根据VNID将包发给本地名为Br0的连接到VLAN（ 由VNID指定 ）的交换机， 在该交换机上， 包被发送给容器C2 示例 注：图中的主机ip请替换为自己的 构建Swarm 假设有两台Docker主机, 将他们配置为Swarm集群， 令node1为manager, node2为worker: # on node1:docker swarm init --advertise-addr &lt;node1-ip&gt; # on node2:# 需要打开2377/tcp, 7946/tcp, 7946/udpdocker swarm join --token &lt;token&gt; &lt;node1-ip&gt; 创建覆盖网络 在node1上创建网络： # 需要打开4789/udpdocker network create -d overlay &lt;network-name&gt; # 覆盖网络使用overlay驱动 创建了新的覆盖网络， 还包括了一个TLS加密的控制层 -o encrypted: 设置数据层加密 将服务连接到覆盖网络 # on node1:docker service create --name &lt;service-name&gt; \\--network &lt;network-name&gt; \\--replicas 2 \\ubuntu sleep infinity 创建了新服务，连接到了覆盖网络，且基于指定的镜像创建了两个副本（容器） 在这里，均在容器中用sleep来保持容器运行 当Swarm在覆盖网络上启动容器时，会自动将容器运行所在节点加入到网络中 node2上启动了容器， 因此node2加入到覆盖网络 测试覆盖网络 由后文“服务发现”知， Swarm网络是可以通过容器名来定位的， 这里用ip来定位只是为了测试 先查看被分配给覆盖网络的Subnet： docker network inspect uber-net [ &#123; &quot;Name&quot;: &quot;uber-net&quot;, &quot;Id&quot;: &quot;6arolzgpk57l5s6nn3uca25ho&quot;, &quot;Created&quot;: &quot;2022-03-23T03:24:24.651086709+08:00&quot;, &quot;Scope&quot;: &quot;swarm&quot;, &quot;Driver&quot;: &quot;overlay&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: &#123; &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: null, &quot;Config&quot;: [ &#123; &quot;Subnet&quot;: &quot;10.0.1.0/24&quot;, # 子网掩吗 &quot;Gateway&quot;: &quot;10.0.1.1&quot; &#125; ] &lt;Snip&gt; 看到覆盖网络的子网是10.0.0/24 然后分别查看两个node的对应容器的id和ip地址： 查看id： docker container ls 查看容器ip地址： docker container inspect \\--format=&#x27;&#123;&#123;range .NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;&#x27; \\ &lt;container&gt;# node1输出：10.0.1.4 # node2输出：10.0.1.3# 确实遵循子网掩码10.0.0/24 然后进入任一容器，ping另一个容器的ip,发现能ping通 使用traceroute跟踪路由信息，发现路由信息只有一跳，证明覆盖网络中，容器间通信是在网络层直连的 接入现有网络 Docker内置的Macvlan驱动可以给容器提供MAC和IP地址， 让容器在物理网络上成为“一等公民”： Macvlan 性能高，但是需要将主机网卡设为混杂模式，大部分公有云平台都不允许这么设置， 因此Macvlan在公有云平台不可行 Macvlan基于linux的Macvlan内核驱动， 因此支持linux内核的所有网络功能， 包括VLAN的Trunk Macvlan驱动在连接到目标网络前，需要设置： 子网信息 网关 可分配给容器的IP范围 主机使用的接口或子接口 示例 假设有一个物理网络，其上有两个VLAN： VLAN100： 10.0.0.0/24 ； VLAN200： 192.168.3.0/24 我们创建一个名为 macvlan100的 Macvlan网络，它连接到VLAN100： docker network create -d macvlan \\--subnet=10.0.0.0/24 \\--ip-range=10.0.0.0/25 \\--gateway=10.0.0.1 \\-o parent=eth0.100 \\macvlan100 Macvlan 采用标准的Linux子接口， 需要打上目标VLAN网络的ID, 在本例中是VLAN100, 所以子接口标记为.100( eth0.100 ) --ip-range: 设置Macvlan网络中可分配给容器的IP范围， 这些地址会被保留，不能用于其他节点或者DHCP, 因为没有任何管理功能来检查IP区域重合的问题 将容器部署到网络中： docker container run -d --name mactainer1 \\--network macvlan100 \\alpine sleep 1d 结果如下： Macvlan支持Trunk功能，这意味着可以在同一台主机创建多个VLAN网络： 服务发现 允许同一网络中的容器和Swarm服务通过名称互相定位 显然，这是通过Docker的DNS服务实现的， 原理和正常网络的DNS服务完全相同 自定义DNS规则 用户可以自定义Swarm服务和容器的查询规则， 这是通过向容器内部的/etc/resolv.conf添加条目实现的（不要问我为什么是resolv 而不是resolve） --dns: 添加自定义的DNS服务器， 向/etc/resolv.conf中追加这个列表 --dns-search: 指定自定义查询时所使用的域名， 向/etc/resolv.conf中追加这个域名 示例 启动新容器，并添加8.8.8.8DNS服务器， 同时指定dockercets.com作为域名添加到非完整查询中 ❯ docker container run -it --name c3 \\--dns=8.8.8.8 \\--dns-search=dockercets.com \\&gt; alpine sh 进入该容器，查看/etc/resolv.conf： / # cat /etc/resolv.confsearch dockercets.comnameserver 8.8.8.8 Ingress网络 Swarm支持两种服务发布模式： Ingress: 默认， 可以让Swarm集群内任意节点（即使没有运行该服务副本）都能访问该服务 Ingress模式底层是一个完全图（ 采用Service Mesh网络 ） Ingree网络会将流量平均分在所有服务副本之上 Host： Host模式发布的服务只能通过运行服务副本的节点来访问 这里的host模式不是之前提到的host网络 Host模式发布： docker service create -d --name svc1 \\--publish published=5000,target=80,mode=host \\nginx Ingress模式可以使用简单格式，即形如 -p 5000:80, 而Host模式必须使用上述的完整格式（逗号前后还不能有空格） published=5000: 服务通过5000端口向外部提供服务 target=80: 发送到published端口5000的请求， 会映射到服务副本的80端口 mode=host： 只有外部请求发送到运行了服务副本的节点才可以访问该服务 示例 该命令部署了一个svc1服务，连接到了overnet网络，并发布到5000端口 这里的Swarm节点全部接入了Ingress网络， 所以这个端口被发布到了Swarm范围内 集群确保到达Ingress网络中任意节点的5000端口的浏览，都会被路由到80端口的svc1 服务 当前svc1服务只部署了一个副本，因此流量全部分配到Node2 箭头展示了访问Node1的5000端口的浏览，通过Ingress网络，被路由到了Node2的正在运行的服务副本上 可以看到，外部流量可能访问任意一个Swarm节点，但是最终都会被路由到运行服务副本的节点","categories":[{"name":"Technology","slug":"Technology","permalink":"http://lyk-love.cn/categories/Technology/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://lyk-love.cn/tags/Docker/"}]},{"title":"Neuroscience L1","slug":"Neuroscience-L1","date":"2022-03-22T20:42:25.000Z","updated":"2022-09-26T06:39:34.934Z","comments":true,"path":"2022/03/23/Neuroscience-L1/","link":"","permalink":"http://lyk-love.cn/2022/03/23/Neuroscience-L1/","excerpt":"Ref: NEUROSCIENCE --- DALE PURVES","text":"Ref: NEUROSCIENCE --- DALE PURVES AI与神经科学 人工智能不能照搬神经科学（ CREATE, NOT MIMIC ） 注意“力”（ ATTENTION ） 现有的CNN模型通过快速地扫描图像，逐步将注意力转移到图像中的下一个位置 注意力亦指向意识内部（ SPOTLIGHT） 内容寻址 注意力亦指向意识内部（ IMAGE） generative models 系统以增量的方式构建图像， 每次只关注“心理画布”的一部分 情景记忆（ EPISODIC MEMORY ） 智能行为要依赖多种记忆系统 * 基于强化的记忆系统(reinforcement-based mecanisms) * 基于实例的记忆系统( instance- based mecanisms )， 也称为情景记忆 神经科学基本概念 基因 ( gene )和基因组( genomics ) 基因：DNA序列 基因组：某个物种或个体的全部DNA序列 基因型( genotype )和表型( phenotype ) 基因型为表型编码，但不能决定表型 基因型本身不能解释大脑的运作 Orginization of the human nervous system 中枢神经系统( central nervous system, CNS )和外部神经系统( peripheral nervous system, PNS ) Internal and external encironment→ 感受器 → 中枢神经系统 → 运动组件（第一部分由内脏神经系统和肠道神经系统组成， 第二部分由运动神经系统组成 ） → Effections CELLULAR DIVERSITY IN THE NERVOUS SYSTEM 高尔基染色法 只能染一部分神经元 尼式染色 神经元细胞 Soma（ 细胞胞体 ） Axon（ 轴突 ） 发出信息 Dendrite （ 树突 ） 接收信息 Spine（ 树突脊 ） GLIA（ 胶质细胞， 非神经元细胞 ） Astrocyte（ 星型胶质细胞 ） Oligodendrocyte ( 寡突胶质细胞 ) Microglia（ 小胶质细胞 ） NEURAL CIRCUIT( 神经环路 ) Sensor neuron Motor neuron（ 运动神经元 ） Interneuron( 中间神经元 )","categories":[{"name":"Natural Science","slug":"Natural-Science","permalink":"http://lyk-love.cn/categories/Natural-Science/"}],"tags":[{"name":"Neuroscience","slug":"Neuroscience","permalink":"http://lyk-love.cn/tags/Neuroscience/"}]},{"title":"Spring JDBC and data source config","slug":"Spring-JDBC-and-data-source-config","date":"2022-03-21T23:38:49.000Z","updated":"2022-09-26T06:39:34.940Z","comments":true,"path":"2022/03/22/Spring-JDBC-and-data-source-config/","link":"","permalink":"http://lyk-love.cn/2022/03/22/Spring-JDBC-and-data-source-config/","excerpt":"Outline: Spring DAO DataSource JDBC ref: Spring In Action","text":"Outline: Spring DAO DataSource JDBC ref: Spring In Action Spring DAO 数据访问对象 （data access object）DAO == Repository 异常体系 SQLException 发生异常时难以恢复 难确定异常类型 是一个check异常 Hibernate异常 定义了许多具体异常 对业务对象的侵入 Spring所提供的平台无关的持久化异常 DataAccessException， 属于runtimeException,属于uncheck异常 具体异常，方便定位问题 隔离具体的数据库平台 模板方法模式 模板方法定义过程的主要框架，template 管理资源 事务控制 处理异常 变化的部分，回调（callback），自定义的数据访问代码 Spring提供的模板 JdbcTemplate HibernateTemplate JpaTemplate DataSource 配置数据源 四种方式： 使用JNDI数据源 （Java 命名与目录接口（Java Naming and Directory Interface） ） 在Web容器中配置JNDI参数，定义一个数据源 Spring中配置JNDI数据源 &lt;jee:jndi-lookup id=“dataSource” jndi-name=“/jdbc/**DS”resource-ref=“true”/&gt; 连接池的数据源 通过JDBC驱动程序定义数据源 DriverManagerDataSource：注意没有进行池化处理 SingleConnectionDataSource:只有一个连接的池 使用Spring配置嵌入式数据源 &lt;jdbc:embedded-database&gt; 可以创建数据表和初始化数据 用于开发和测试 以JDBC为例： @Configuration@ComponentScan@PropertySource(&quot;jdbc.properties&quot;) //通过@PropertySource(&quot;jdbc.properties&quot;)读取数据库配置文件；public class AppConfig &#123; @Value(&quot;$&#123;jdbc.url&#125;&quot;) //通过@Value(&quot;$&#123;jdbc.url&#125;&quot;)注入配置文件的相关配置； String jdbcUrl; @Value(&quot;$&#123;jdbc.username&#125;&quot;) String jdbcUsername; @Value(&quot;$&#123;jdbc.password&#125;&quot;) String jdbcPassword; @Bean //创建一个DataSource实例，它的实际类型是HikariDataSource，创建时需要用到注入的配置； DataSource createDataSource() &#123; HikariConfig config = new HikariConfig(); config.setJdbcUrl(jdbcUrl); config.setUsername(jdbcUsername); config.setPassword(jdbcPassword); config.addDataSourceProperty(&quot;autoCommit&quot;, &quot;true&quot;); config.addDataSourceProperty(&quot;connectionTimeout&quot;, &quot;5&quot;); config.addDataSourceProperty(&quot;idleTimeout&quot;, &quot;60&quot;); return new HikariDataSource(config); &#125; @Bean //创建一个JdbcTemplate实例，它需要注入DataSource，这是通过方法参数完成注入的。 JdbcTemplate createJdbcTemplate(@Autowired DataSource dataSource) &#123; return new JdbcTemplate(dataSource); &#125;&#125; 数据库配置文件 // jdbc.properties：# 数据库文件名为testdb:jdbc.url=jdbc:hsqldb:file:testdb# Hsqldb默认的用户名是sa，口令是空字符串:jdbc.username=sajdbc.password= 使用profile选择数据源 建立开发、测试、生产环境的不同数据源 配置文件+注解， 原理和SpringBoot一样， SpringBoot采用yml格式，比Spring的Profile简易很多，这里就不介绍Spring的了 JDBC 使用步骤 如上文所示 首先要创建并管理一个DataSource实例，表示数据库连接池； 然后实例化一个JdbcTemplate来操作JDBC JdbcTemplate查询的内部实现： 从全局DataSource实例获取Connection实例； 通过Connection实例创建PreparedStatement实例； 执行SQL语句，如果是查询，则通过ResultSet读取结果集，如果是修改，则获得int结果。 Spring JDBC框架 JDBC模板：资源管理和异常处理 JdbcTemplate NamedParameterJdbcTemplate","categories":[{"name":"Technology","slug":"Technology","permalink":"http://lyk-love.cn/categories/Technology/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://lyk-love.cn/tags/Spring/"}]},{"title":"Spring ORM","slug":"Spring-ORM","date":"2022-03-21T23:12:27.000Z","updated":"2022-09-26T06:39:34.940Z","comments":true,"path":"2022/03/22/Spring-ORM/","link":"","permalink":"http://lyk-love.cn/2022/03/22/Spring-ORM/","excerpt":"Outline: object-relational mapping Hibernate、MyBatis JPA (Java Persistence API) APPENDIX： 一些好用的ORM Tips ref: Spring In Action","text":"Outline: object-relational mapping Hibernate、MyBatis JPA (Java Persistence API) APPENDIX： 一些好用的ORM Tips ref: Spring In Action JDBC Hibernate JPA MyBatis DataSource SessionFactory EntityManagerFactory SqlSessionFactory Connection Session 带有@PersistenceContext注解的EntityManager代理类 SqlSession Hibernate 配置 获得org.hibernate.Session接口的实现类, 这需要我们创建一个LocalSessionFactoryBean，它会自动创建一个SessionFactory 在Hibernate中，Session是封装了一个JDBC Connection的实例，而SessionFactory是封装了JDBC DataSource的实例，即SessionFactory持有连接池，每次需要操作数据库的时候，SessionFactory创建一个新的Session，相当于从连接池获取到一个新的Connection 在hibernate4，我们一般用：org.springframework.orm.hibernate4.LocalSessionFactoryBean Hibernate作为ORM框架，可以替代JdbcTemplate，但Hibernate仍然需要JDBC驱动，所以，我们需要引入JDBC驱动、连接池，以及Hibernate本身， 并配置DataSource 定义映射关系：XML、注解（JPA、Hibernate） 配置数据源等 创建DataSource、引入JDBC配置文件，以及启用声明式事务： @Configuration@ComponentScan@EnableTransactionManagement@PropertySource(&quot;jdbc.properties&quot;)public class AppConfig &#123; @Bean DataSource createDataSource() &#123; ... &#125;&#125; SessionFactory 使用org.springframework.orm.hibernate4.LocalSessionFactoryBean @BeanLocalSessionFactoryBean createSessionFactory(@Autowired DataSource dataSource) &#123; var props = new Properties();//hibernateProperties属性配置了Hibernate如何进行操作的细节 props.setProperty(&quot;hibernate.hbm2ddl.auto&quot;, &quot;update&quot;); // 表示自动创建数据库的表结构，生产环境不要使用 props.setProperty(&quot;hibernate.dialect&quot;, &quot;org.hibernate.dialect.HSQLDialect&quot;);//指示Hibernate使用的数据库是HSQLDB props.setProperty(&quot;hibernate.show_sql&quot;, &quot;true&quot;); //让Hibernate打印执行的SQL，这对于调试非常有用 var sessionFactoryBean = new LocalSessionFactoryBean(); sessionFactoryBean.setDataSource(dataSource); // 扫描指定的package获取所有entity class: sessionFactoryBean.setPackagesToScan(&quot;com.itranswarp.learnjava.entity&quot;); sessionFactoryBean.setHibernateProperties(props); return sessionFactoryBean;&#125; 查询 三类查询： HQL:hibernate query language，即hibernate提供的面向对象的查询语言 select/update/delete…… from …… where …… group by …… having …… order by …… asc/desc QBC查询: query by criteria 完全面向对象的查询 本地SQL查询 @Repository 的作用 @Component 转换成Spring的统一异常 @Bean public BeanPostProcessor persistenceTranslation()&#123; return new PersistenceExceptionTranslationPostProcessor(); &#125; MyBatis MyBatis是半自动的ORM，只负责把ResultSet自动映射到Java Bean，或者自动填充Java Bean参数，但仍需自己写出SQL 可以用注解或XML配置， 后者比较繁琐，不介绍了 application-**.yml中的配置 配置数据源 mapper-locations指定 定义接口（使用注解@Mapper） mapper/***Mapper.xml 配置 配置数据源等 SqlSessionFactory 使用MyBatis的核心就是创建SqlSessionFactory，这里我们需要创建的是SqlSessionFactoryBean： @BeanSqlSessionFactoryBean createSqlSessionFactoryBean(@Autowired DataSource dataSource) &#123; var sqlSessionFactoryBean = new SqlSessionFactoryBean(); sqlSessionFactoryBean.setDataSource(dataSource); return sqlSessionFactoryBean;&#125; 因为MyBatis可以直接使用Spring管理的声明式事务，因此，创建事务管理器和使用JDBC是一样的： @BeanPlatformTransactionManager createTxManager(@Autowired DataSource dataSource) &#123; return new DataSourceTransactionManager(dataSource);&#125; Mapper 和Hibernate不同的是，MyBatis使用Mapper来实现映射，而且Mapper必须是接口。我们以User类为例，在User类和users表之间映射的UserMapper编写如下： public interface UserMapper &#123; @Select(&quot;SELECT * FROM users WHERE id = #&#123;id&#125;&quot;) User getById(@Param(&quot;id&quot;) long id);&#125; @MapperScan Mybatis将Mapper接口动态代理，生成实现类。 要识别到Mapper接口，有两种方法： 使用@Mapper标识Mapper接口， 比较繁琐： @Mapperpublic interface UserDAO &#123; //代码&#125; 在某个配置类中启用@MapperScan()，并指定Mapper接口所在的包， 该包下的所有接口都会被动态代理： @MapperScan(&quot;com.itranswarp.learnjava.mapper&quot;)...其他注解...public class AppConfig &#123; ...&#125; 可以扫描多个包： @MapperScan(&#123;&quot;com.kfit.demo&quot;,&quot;com.kfit.user&quot;&#125;) 这个注解实际上会生成MapperFactoryBean，后者会自动创建包下所有Mapper的实现类 ： Mapper语法 在定义了接口方法后，还需要明确写出查询的SQL, SQL的每个参数都与方法参数按名称对应 例如，方法参数id的名字通过注解@Param()标记为id，则SQL语句里将来替换的占位符就是#&#123;id&#125; 如果有多个参数，那么每个参数命名后直接在SQL中写出对应的占位符即可： @Select(&quot;SELECT * FROM users LIMIT #&#123;offset&#125;, #&#123;maxResults&#125;&quot;)List&lt;User&gt; getAll(@Param(&quot;offset&quot;) int offset, @Param(&quot;maxResults&quot;) int maxResults); SELECT MyBatis将ResultSet的每一行转换为Domain实例， 转换规则当然是按列名和属性名对应。如果列名和属性名不同, 需要用别名： 对于SELECT语句： -- 列名是created_time，属性名是createdAt:SELECT id, name, email, created_time AS createdAt FROM users INSERT MyBatis插入时， 需要将对象的属性转换成列： @Insert(&quot;INSERT INTO users (email, password, name, createdAt) VALUES (#&#123;user.email&#125;, #&#123;user.password&#125;, #&#123;user.name&#125;, #&#123;user.createdAt&#125;)&quot;)void insert(@Param(&quot;user&quot;) User user); 在SQL中以#&#123;obj.property&#125;的方式写占位符 如果表的id是自增主键，那么，我们在SQL中不传入id，但希望获取插入后的主键，需要再加一个@Options注解： @Options(useGeneratedKeys = true, keyProperty = &quot;id&quot;, keyColumn = &quot;id&quot;)@Insert(&quot;INSERT INTO users (email, password, name, createdAt) VALUES (#&#123;user.email&#125;, #&#123;user.password&#125;, #&#123;user.name&#125;, #&#123;user.createdAt&#125;)&quot;)void insert(@Param(&quot;user&quot;) User user); keyProperty：JavaBean 的属性 keyColumn： 数据库的主键列名 UPDATE 执行UPDATE和DELETE语句相对比较简单，我们定义方法如下： @Update(&quot;UPDATE users SET name = #&#123;user.name&#125;, createdAt = #&#123;user.createdAt&#125; WHERE id = #&#123;user.id&#125;&quot;)void update(@Param(&quot;user&quot;) User user); DELETE @Delete(&quot;DELETE FROM users WHERE id = #&#123;id&#125;&quot;)void deleteById(@Param(&quot;id&quot;) long id); 使用Mapper 在Service层直接注入Mapper, 使用Mapper提供的方法 @Component@Transactionalpublic class UserService &#123; // 注入UserMapper: @Autowired UserMapper userMapper; public User getUserById(long id) &#123; // 调用Mapper方法: User user = userMapper.getById(id); if (user == null) &#123; throw new RuntimeException(&quot;User not found by id.&quot;); &#125; return user; &#125;&#125; JPA JPA的宗旨是为POJO提供持久化标准规范 JPQL（Java Persistence Query Language) JPQL就是一种查询语言，具有与 SQL 相类似的特征 JPA语法大全 配置 步骤： 配置数据源等 创建EntityManagerFactoryBean， 它会生成一个SessionFactory 将SessionFactory注入到JpaTransactionManager， 以实现声明式事务 使用Hibernate时，我们需要创建一个LocalSessionFactoryBean，并让它再自动创建一个SessionFactory。使用JPA也是类似的，我们需要创建一个EntityManagerFactoryBean，并让它再自动创建一个EntityManagerFactory， EntityManagerFactory: 是个工厂Bean, 会创建创建EntityManager org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean ``org.springframework.orm.jpa.vendor.HibernateJpaVendorAdapter` javax.persistence.EntityManager EntityManager不是线程安全的， Spring遇到标注了@PersistenceContext的EntityManager会自动注入代理，该代理会在必要的时候自动打开EntityManager。换句话说，多线程引用的EntityManager虽然是同一个代理类，但该代理类内部针对不同线程会创建不同的EntityManager实例。因此，标注了@PersistenceContext的EntityManager可以被多线程安全地共享。 @PersistenceUnit @PersistenceContext 配置数据源等 在AppConfig中启用声明式事务管理，创建DataSource： @Configuration@ComponentScan@EnableTransactionManagement@PropertySource(&quot;jdbc.properties&quot;)public class AppConfig &#123; @Bean DataSource createDataSource() &#123; ... &#125;&#125; EntityManagerFactory @BeanLocalContainerEntityManagerFactoryBean createEntityManagerFactory(@Autowired DataSource dataSource) &#123; var entityManagerFactoryBean = new LocalContainerEntityManagerFactoryBean(); // 设置DataSource: entityManagerFactoryBean.setDataSource(dataSource); // 扫描指定的package获取所有entity class: entityManagerFactoryBean.setPackagesToScan(&quot;com.itranswarp.learnjava.entity&quot;); // 指定JPA的提供商是Hibernate: JpaVendorAdapter vendorAdapter = new HibernateJpaVendorAdapter(); entityManagerFactoryBean.setJpaVendorAdapter(vendorAdapter); // 设定特定提供商自己的配置: var props = new Properties(); props.setProperty(&quot;hibernate.hbm2ddl.auto&quot;, &quot;update&quot;); props.setProperty(&quot;hibernate.dialect&quot;, &quot;org.hibernate.dialect.HSQLDialect&quot;); props.setProperty(&quot;hibernate.show_sql&quot;, &quot;true&quot;); entityManagerFactoryBean.setJpaProperties(props); return entityManagerFactoryBean;&#125; JpaTransactionManager Spring Data JPA &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-jpa&lt;/artifactId&gt; &lt;version&gt;1.3.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; 加注解@EnableJpaRepositories 会扫描org.springframework.data.repository.Repository接口 继承接口org.springframework.data.jpa.repository.JpaRepository 编写自定义的查询方法 定义查询方法，无需实现 领域特定语言（domain-specific language，DSL)，spring data的命名约定 查询动词 + 主题 + 断言 查询动词：get、read、find、count 声明自定义查询 不符合方法命名约定时，或者命名太长时： @Query(“select ...”) 使用EntityManager直接低层实现 接口名+Impl的实现类 实体类 需要添加注解来告诉ORM如何把实体类映射到表记录 作为映射使用的JavaBean，所有属性都使用包装类型而不是基本类型（ Mybatis是个例外，这是因为它不是全ORM框架 ） 如果一个JavaBean被用于映射，我们就标记一个@Entity。默认情况下，实体类User映射的表名是user，如果实际的表名不同，例如实际表名是users，可以追加一个@Table(name=&quot;users&quot;)表示 对于主键，还需要用@Id标识，自增主键再追加一个@GeneratedValue，以便Hibernate能读取到自增主键的值 每个属性到数据库列的映射用@Column()标识，nullable指示列是否允许为NULL，updatable指示该列是否允许被用在UPDATE语句，length指示String类型的列的长度（如果没有指定，默认是255） 示例： @Entitypublic class User &#123; //映射到表名user @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(nullable = false, updatable = false) public Long getId() &#123; ... &#125; @Column(nullable = false, unique = true, length = 100) public String getEmail() &#123; ... &#125; @Column(nullable = false, length = 100) public String getPassword() &#123; ... &#125; @Column(nullable = false, length = 100) public String getName() &#123; ... &#125; @Column(nullable = false, updatable = false) public Long getCreatedAt() &#123; ... &#125;&#125; Appendex MySQL与JAVA数据类型对应关系 These MySQL Data Types Can always be converted to these Java types CHAR, VARCHAR, BLOB, TEXT, ENUM, and SET java.lang.String, java.io.InputStream, java.io.Reader, java.sql.Blob, java.sql.Clob FLOAT, REAL, DOUBLE PRECISION, NUMERIC, DECIMAL, TINYINT, SMALLINT, MEDIUMINT, INTEGER, BIGINT java.lang.String, java.lang.Short, java.lang.Integer, java.lang.Long, java.lang.Double, java.math.BigDecimal DATE, TIME, DATETIME, TIMESTAMP java.lang.String, java.sql.Date, java.sql.Timestamp MYSQL存URL最佳类型 MySQL 5.0.3及更高版本中VARCHAR的有效最大长度受最大行大小（65,535字节，在所有列之间共享）和使用的字符集的限制。 所以，存储url最佳类型为： &lt; MySQL 5.0.3 use TEXT &gt;= MySQL 5.0.3 use VARCHAR(2083)","categories":[{"name":"Technology","slug":"Technology","permalink":"http://lyk-love.cn/categories/Technology/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://lyk-love.cn/tags/Spring/"}]},{"title":"Spring Cache","slug":"Spring-Cache","date":"2022-03-21T21:40:40.000Z","updated":"2022-09-26T06:39:34.940Z","comments":true,"path":"2022/03/22/Spring-Cache/","link":"","permalink":"http://lyk-love.cn/2022/03/22/Spring-Cache/","excerpt":"Outline: Basic Idea 启用Spring缓存支持 提供缓存管理器 应用缓存 ref: Spring In Action","text":"Outline: Basic Idea 启用Spring缓存支持 提供缓存管理器 应用缓存 ref: Spring In Action Basic Idea 缓存实际上是一种面向切面的行为。Spring将缓存实现为一个切面。 在使用XML声明缓存规则时,这一点非常明显:我们必须要将缓存通知绑定到一个切点上 缓存编程步骤： 启用Spring缓存支持（java方式）： 提供缓存配置类，加上@EnableCaching 提供缓存管理器：给配置类提供一个CacheManager` 应用缓存：给需要缓存的方法加上对应的注解 启用Spring缓存支持 Spring启用缓存支持有两种方式: 注解驱动的缓存：在一个配置类上添加 @EnableCaching import org.springframework.cache.CacheManager;import org.springframework.cache.annotation.EnableCaching;import org.springframework.cache.concurrent.ConcurrentMapCacheManager;@Configuration@EnableCachingpublic class CachingConfig &#123; @Bean public CacheManager cacheManager() &#123; return new ConcurrentMapCacheManager(); //这里还声明了一个ConcurrentMapCacheManager的Bean &#125; &#125; 当你在配置类(@Configuration)上使用@EnableCaching注解时，会触发一个post processor，这会扫描每一个spring bean，查看是否已经存在注解对应的缓存。如果找到了，就会自动创建一个代理拦截方法调用，使用缓存的bean执行处理。. 参见. XML声明的缓存： 用Spring cache命名空间中的&lt;cache:annotation-driven&gt;元素来启用注解驱动的缓 存 &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:cache=&quot;http://www.springframework.org/schema/cache&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/cache http://www.springframework.org/schema/cache/spring-cache.xsd&quot;&gt; &lt;cache:annotation-driven /&gt;&lt;/beans&gt; 本质上,@EnableCaching和&lt;cache:annotation-driven&gt;的工作方式是相同的。它们都会创建一个切面(aspect)并 触发Spring缓存注解的切点(pointcut) 配置CacheManager Spring 内置的缓存管理器： SimpleCacheManager NoOpCacheManager ConcurrentMapCacheManager CompositeCacheManager EhCacheCacheManager Spring Data的缓存管理器： RedisCacheManager(来自于Spring Data Redis项目) GemfireCacheManager EhCache 对比Redis, 跑在同一个进程上， 速度更快 import net.sf.ehcache.CacheManager; //ehcache提供的CacheManagerimport org.springframework.cache.ehcache.EhCacheCacheManager;//Spring提供的EhCacheCacheManagerimport org.springframework.cache.ehcache.EhCacheManagerFactoryBean;@Configuration@EnableCachingpublic class CachingConfig &#123;// @Bean// public CacheManager cacheManager() &#123;// return new ConcurrentMapCacheManager();// &#125; @Bean //将ehCache提供的CacheManager注入到Spring的EhCacheManager public EhCacheCacheManager cacheManager(CacheManager cm) &#123; return new EhCacheCacheManager(cm); &#125; @Bean //这是个工厂Bean, 用来生成 Ehcache的CacheManager实例 ，后者被注入到cacheManager（Cacaemanager cm） public EhCacheManagerFactoryBean ehcache() &#123; EhCacheManagerFactoryBean ehCacheFactoryBean = new EhCacheManagerFactoryBean(); ehCacheFactoryBean.setConfigLocation( new ClassPathResource(&quot;spittr/cache/ehcache.xml&quot;)); return ehCacheFactoryBean; &#125; cacheManager()方法通过传入Ehcache的CacheManager实例， 创建了一个EhCacheCacheManager实例 Spring和ehcache都定义了CacheManager类型，我们需要将 ehcache的CacheManager注入到Spring的EhCacheCacheManager Spring提供了EhCacheManagerFactoryBean来生成EhCache的CacheManager。方法ehcache()会创建并返回一EhCacheManagerFactoryBean实例。因为它是一个工厂bean(即实现了Spring的FactoryBean接口),所以注册在Spring应用上下文中的并不是EhCacheManagerFactoryBean的实例,而是Ehcache的CacheManager的实例, 后者被注入到EhCacheCacheManager之中 EhCache自身也需要配置（通过XML）， 我们在创建EhCacheManagerFactoryBean的过程中, 通过setConfigLocation()方法,传入ClassPath-Resource,来指定EhCache XML配置文件相对于根类路径(classpath)的位置。 ehCache配置文件 ehCache官方文档 &lt;ehcache&gt; &lt;cache name=&quot;spittleCache&quot; //声明了一个名为spittleCache的缓存 maxBytesLocalHeap=&quot;50m&quot; //最大的堆存储为50MB timeToLiveSeconds=&quot;100&quot;&gt; // 存活时间为100秒 &lt;/cache&gt;&lt;/ehcache&gt; Redis 优点： 对比EhCache， 可以实现跨进程的缓存 缓存的条目是键值对,其中key描述了产生value的操作和参数。因此, Redis作为key-value存储,非常适合存储缓存 Spring Data Redis提供了RedisCacheManager, 它与一个Redis Server协作,并通过RedisTemplate将缓存条目存储到Redis中 @Configuration@EnableCachingpublic class CacheConfig &#123; @Bean public CacheManager cacheManager(RedisTemplate redisTemplate) &#123; return new RedisCacheManager(redisTemplate); //Redis缓存管理器Bean &#125; @Bean //Redis连接工厂Bean， 这也是一个工厂Bean public RedisConnectionFactory redisCF() &#123; return new JedisConnectionFactory(); //这里的工厂Bean的底层实现用的是JedisConnectionFactory &#125; @Bean //Redis Template Bean public RedisTemplate&lt;String, String&gt; redisTemplate(RedisConnectionFactory cf) &#123; RedisTemplate&lt;String, String&gt; redis = new RedisTemplate&lt;&gt;(); redis.setConnectionFactory(cf); return redis; &#125;&#125; cacheManager()方法通过传入RedisTemplate实例， 创建了一个RedisCacheManager实例 为了使用RedisCacheManager,我们需要RedisTemplate及RedisConnection Bean, 这同样用工厂Bean实现 使用多个缓存管理器 可以用Spring的CompositeCacheManager @Beanpublic CacheManager cacheManager(net.sf.ehcache.CacheManager cm, RedisTemplate&lt;String, Object&gt; redisTemplate) &#123; CompositeCacheManager compositeCacheManager = new CompositeCacheManager(); List&lt;CacheManager&gt; managers = new ArrayList&lt;CacheManager&gt;(); managers.add(new EhCacheCacheManager(cm)); managers.add(new RedisCacheManager(redisTemplate)); compositeCacheManager.setCacheManagers(managers);//添加缓存管理器，按添加顺序查找，没找到再往下找 return compositeCacheManager;&#125; 会从EhCacheCacheManager检查Ehcache, 然后从RedisCacheManager检查Redis 应用缓存 这里主要介绍用注解配置缓存 @Cacheable @Cacheable( cache_name ) 方法的结果会被存到指定的缓存中，下次采用相同的参数进行方法调用时，会使用缓存中的结果 例如： @Cacheable(&quot;spittleCache&quot;) Spittle findOne(long id);@Cacheable(&quot;books&quot;) 当findOne()被调用时,缓存切面会拦截调用并在缓存中查找之前 以名spittleCache存储的返回值。缓存的key是传递 到findOne()方法中的id参数。如果按照这个key能够找到值的话, 就会返回找到的值,方法不会再被调用。如果没有找到值的话,那么 就会调用这个方法,并将返回值放到缓存之中,为下一次调 用findOne()方法做好准备 可以加到接口的方法上， 这会导致所有实现该接口的类的对应方法都应用该注解 默认key生成： 默认key的生成按照以下规则： - 如果没有参数,则使用0作为key - 如果只有一个参数，使用该参数作为key - 如果又多个参数，使用包含所有参数的hashCode作为key 自定义key的生成： 当目标方法参数有多个时，有些参数并不适合缓存逻辑 比如： @Cacheable(&quot;books&quot;)public Book findBook(ISBN isbn, boolean checkWarehouse, boolean includeUsed) 其中checkWarehouse，includeUsed并不适合当做缓存的key.针对这种情况，Cacheable 允许指定生成key的关键属性，并且支持支持SpringEL表达式。（推荐方法） 再看一些例子： @Cacheable(cacheNames=&quot;books&quot;, key=&quot;#isbn&quot;)public Book findBook(ISBN isbn, boolean checkWarehouse, boolean includeUsed)@Cacheable(cacheNames=&quot;books&quot;, key=&quot;#isbn.rawNumber&quot;)public Book findBook(ISBN isbn, boolean checkWarehouse, boolean includeUsed)@Cacheable(cacheNames=&quot;books&quot;, key=&quot;T(someType).hash(#isbn)&quot;)public Book findBook(ISBN isbn, boolean checkWarehouse, boolean includeUsed)@Cacheable(cacheNames=&quot;books&quot;, key=&quot;#map[&#x27;bookid&#x27;].toString()&quot;)public Book findBook(Map&lt;String, Object&gt; map) 缓存的同步 sync： 在多线程环境下，某些操作可能使用相同参数同步调用。默认情况下，缓存不锁定任何资源，可能导致多次计算，而违反了缓存的目的。对于这些特定的情况，属性 sync 可以指示底层将缓存锁住，使只有一个线程可以进入计算，而其他线程堵塞，直到返回结果更新到缓存中。 例： @Cacheable(cacheNames=&quot;foos&quot;, sync=&quot;true&quot;)public Foo executeExpensiveOperation(String id) &#123;...&#125; 属性condition： 有时候，一个方法可能不适合一直缓存（例如：可能依赖于给定的参数）。属性condition支持这种功能，通过SpEL 表达式来指定可求值的boolean值，为true才会缓存（在方法执行之前进行评估）。 例： @Cacheable(cacheNames=&quot;book&quot;, condition=&quot;#name.length &lt; 32&quot;)public Book findBook(String name) 此外，还有一个unless 属性可以用来是决定是否添加到缓存。与condition不同的是，unless表达式是在方法调用之后进行评估的。如果返回false，才放入缓存（与condition相反）。 #result指返回值 例： @Cacheable(cacheNames=&quot;book&quot;, condition=&quot;#name.length &lt; 32&quot;, unless=&quot;#result.name.length &gt; 5&quot;&quot;)public Book findBook(String name) @CachePut @CachePut标注的方法在执行前不会去检查缓存中是否存在之前执行过的结果，而是每次都会执行该方法，并将执行结果以键值对的形式存入指定的缓存中。 @CachePut(cacheNames=&quot;book&quot;, key=&quot;#isbn&quot;)public Book updateBook(ISBN isbn, BookDescriptor descriptor) 注意：应该避免@CachePut 和 @Cacheable同时使用的情况。 @CacheEvict 移除缓存条目 @CacheEvict要求指定一个或多个缓存，使之都受影响。此外，还提供了一个额外的参数allEntries 。表示是否需要清除缓存中的所有元素。默认为false，表示不需要。当指定了allEntries为true时，Spring Cache将忽略指定的key。有的时候我们需要Cache一下清除所有的元素。 @CacheEvict(cacheNames=&quot;books&quot;, allEntries=true)public void loadBooks(InputStream batch) 清除操作默认是在对应方法成功执行之后触发的，即方法如果因为抛出异常而未能成功返回时也不会触发清除操作。使用beforeInvocation可以改变触发清除操作的时间，当我们指定该属性值为true时，Spring会在调用该方法之前清除缓存中的指定元素。 @CacheEvict(cacheNames=&quot;books&quot;, beforeInvocation=true)public void loadBooks(InputStream batch) @CacheConfig 有时候一个类中可能会有多个缓存操作，而这些缓存操作可能是重复的。这个时候可以使用@CacheConfig @CacheConfig(&quot;books&quot;)public class BookRepositoryImpl implements BookRepository &#123; @Cacheable public Book findBook(ISBN isbn) &#123;...&#125;&#125; @CacheConfig是一个类级别的注解，允许共享缓存的名称、KeyGenerator、CacheManager 和CacheResolver。 该操作会被覆盖。 用XML配置缓存 就是在XML里面指定缓存规则要应用到哪些方法， 避免在源代码里写@Cacheable(), 比较晦涩，不好用","categories":[{"name":"Technology","slug":"Technology","permalink":"http://lyk-love.cn/categories/Technology/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://lyk-love.cn/tags/Spring/"}]},{"title":"Training Day","slug":"Training-Day","date":"2022-03-19T01:13:40.000Z","updated":"2022-09-26T06:39:34.941Z","comments":true,"path":"2022/03/19/Training-Day/","link":"","permalink":"http://lyk-love.cn/2022/03/19/Training-Day/","excerpt":"一男的从家里出来，他看到门廊里有只蜗牛，他把它扔到了后院。 但蜗牛是不会死的，很快它就能爬了，千辛万苦，用了快一年，蜗牛又爬回了门廊。这个时候，男人刚好出门，看见这蜗牛，然后说：“你tm的什么毛病？”","text":"一男的从家里出来，他看到门廊里有只蜗牛，他把它扔到了后院。 但蜗牛是不会死的，很快它就能爬了，千辛万苦，用了快一年，蜗牛又爬回了门廊。这个时候，男人刚好出门，看见这蜗牛，然后说：“你tm的什么毛病？” Plot Alonzo因为惹怒了俄罗斯黑帮，所以要准备100万美元交钱保平安。于是Alonzo想出来一个局，由于他手下的人太狡猾，不好骗，于是他向上级提出纳新，此后一周他一直在谋划整个计划。 接下来Jake就上场了。Alonzo一整天的行动都是围绕利用这个新人展开的，他先故意带菜鸟去贩毒点，为的是拿到和自己无关的毒品和吸毒工具。（这里很容易让人迷惑他是个好人，其实他只是为了抢对方的吸毒工具）之后使用更厉害的毒品骗菜鸟吸食（这里黑人掉包了毒品，并不是大麻之类的低级毒品而是海洛因一类的化学毒品），目的是在接下来的环节可以抓住白人的话柄，时刻威胁他会为此丢了饭碗，不想白人警察的正义感让他救下来一个差点被强奸的女孩，让黑人的计划出现了意外的漏洞。 Roger是Alonzo的线人，Alonzo先带Jake去他家拜访，实则是摸底，因为他想要抢劫Roger。 此时Roger出于爱才，给Jake讲了蜗牛的故事，并告诉Jake：“什么时候懂了这个笑话，你就懂了这个地方”。 为了能去Roger家里“名正言顺”的拿到这笔钱，于是又要准备搜查令。而开搜查令需要买通那群高层，所以Alonzo故意去抓snoopy dogg，就是为了在男主面前演戏，要去找sandman，因为以Alonzo的人脉，没道理他不知道地盘上有一个sandman贩毒。抢了sandman四万美元去贿赂法官开搜查证明，之后去抢劫Roger一百万美元，并杀掉，到这一步，实际上Jake已经迷失自我，准备真的追随Alonzo。 但是Alonzo的局没有结束，他想要弄死Jake,因为之前在Roger家分赃时，以及之后车上的谈话，Alonzo已经明白了他蛊惑不了 Jake，所以斩草除根。 在车上，Alonzo为了消除Jake的负罪感，告诉Jake， Roger是一个大毒枭。其实后者是他的线人 其实即使Jake答应了，即使不被杀死，也还是会被Alonzo利用，来背一百万美元的锅 在拉美裔的地盘，Alonzo把Jake扔在那然后自己离开。其实就是要让人弄死他。后面“微笑哥”的话也证明这条，他说这就是business，意思就是只是要杀他，并不是私人恩怨。但是由于Jake阴差阳错白人救了微笑哥的妹妹，Jake没有死并且醒悟过来去抢了Alonzo的救命钱。 最后Jake黑化，他并没有上交这一百万美元，因为新闻没有提。Alonzo没有钱交赎金被乱枪打死。 Review 那个蜗牛的故事，其实门廊就是正义，男人就是Alonzo,蜗牛是Jake 蜗牛躺在门廊上，却跟了男人，后者很随意的把蜗牛从门廊上扔出去，打碎了他的壳（壳是蜗牛与残酷外界之间的保护，即坚守的原则与美好的幻想），蜗牛装死一会儿（接受现实中）后又起来了，继续朝着门廊（正义）爬着，以为这是要学会的法则，自己跟着黑警走是没错的。万番辛苦后，当他好不容易又走到了门廊处时，男人却说：你tm的什么毛病？（你壳都没了，底线都突破了，赤身裸体，还妄想着追寻正义这种东西？） Roger想提醒Jake，Alonzo从不在意蜗牛的死活与蜗牛的倔强，他很随意的打碎你的壳，不是为了特意教会你什么，只因为你对他就是随意的存在，是幼稚可笑的，用完就扔的。你相信了他，又爬到当初的门廊处时质问他正义所在时，只会收获一句你脑子瓦特了。 而男主当时的理解是，作为一名警察，要珍惜自己的smail and tears, 因为这是唯一别人夺不走的东西。这是典型的理想主义的想法，和Roger的本意是南辕北辙的 这部电影告诉我们要坚持自己的理念和原则，而且与其在深渊中挣扎，不如一开始就离开那。 即使一定要走向深渊，也千万不能相信社会上的任何人，因为你永远不知道，他们背地里是不是扭曲的怪物","categories":[{"name":"Art","slug":"Art","permalink":"http://lyk-love.cn/categories/Art/"}],"tags":[{"name":"Movie","slug":"Movie","permalink":"http://lyk-love.cn/tags/Movie/"}]},{"title":"Dockerizing","slug":"Dockerizing","date":"2022-03-19T01:04:45.000Z","updated":"2022-09-29T16:24:48.803Z","comments":true,"path":"2022/03/19/Dockerizing/","link":"","permalink":"http://lyk-love.cn/2022/03/19/Dockerizing/","excerpt":"Outline: 单体应用容器化 Dockerfile build image push image 多阶段构建 构建镜像优化 介绍了应用的容器化","text":"Outline: 单体应用容器化 Dockerfile build image push image 多阶段构建 构建镜像优化 介绍了应用的容器化 Intro 应用容器化步骤： 编写应用代码 创建Dockerfile,其中包括当前应用的描述，依赖以及如何运行这个应用 对该Dockerfile执行docker image build 等待Docker将应用程序构建到Docker镜像中 单体应用容器化 示例项目：https://github.com/LYK-love/psweb Dockerfile 构建上下文（Build Context）： 包含应用文件的目录 Dockerfile一般放在构建上下文的根目录下 Dockerfile首字母不能小写 Dockerfile: 除了 # 开头的注释行之外， 其他的每一行都是一条指令 指令： INSTRUCTION argument: 不区分大小写，一般INSTRUCTION大写 分为四部分：基础镜像信息、维护者信息、镜像操作指令和容器启动时执行指令 FROM alpineLABEL maintainer=&quot;nigelpoulton@hotmail.com&quot;# Install Node and NPMRUN apk add --update nodejs npm curl# Copy app to /srcCOPY . /srcWORKDIR /src# Install dependenciesRUN npm installEXPOSE 8080ENTRYPOINT [&quot;node&quot;, &quot;./app.js&quot;] Options FROM &lt;image&gt;： 将指定的镜像的作为要构建的镜像的基础镜像层，一般是OS LABEL &lt;tag&gt; &lt;tag&gt;：添加一些元数据，每个tag都是键值对 RUN &lt;command&gt; 或 RUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;]： 前者将在 shell 终端中运行命令，即 /bin/sh -c；后者则使用 exec 执行。指定使用其它终端可以通过第二种方式实现，例如 RUN [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;echo hello&quot;] 每条 RUN 指令都会在当前镜像层基础上执行指定命令， 并新建一个镜像层 ENV &lt;ENV_VARIABLE&gt;=&lt;str&gt;: 设置环境变量 COPY &lt;src&gt; &lt;dest&gt;： 复制本地主机的 &lt;src&gt;（为 Dockerfile 所在目录的相对路径，即构建上下文）到容器中的 &lt;dest&gt; WORKDIR [dir]: 为Dockerfile中尚未执行的指令设置工作目录 ENTRYPOINT 两种格式： ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] ENTRYPOINT command param1 param2（shell中执行）。 配置镜像以容器方式启动后默认运行的程序，并且不可被 docker run 提供的参数覆盖。 每个 Dockerfile 中只能有一个 ENTRYPOINT，当指定多个时，只有最后一个起效。 EXPOSE &lt;port&gt; [&lt;port&gt;...]：暴露容器端口. 一般不用写这个指令，在启动容器的时候自己映射端口. 写这个指令有如下好处: 告诉告诉镜像使用者,该镜像暴露的端口 如果使用随机端口映射运行容器，也就是 docker run -P ，会自动随机映射 EXPOSE 的端口 VOLUME [&quot;/data&quot;]： 创建一个可以从本地主机或其他容器挂载的挂载点，一般用来存放数据库和需要保持的数据等。 构建镜像 步骤 docker image build [选项] path docker daemon按行来读取path下（包括子目录）的 Dockerfile，并将该path下的所有内容发送给 Docker 服务端，由服务端来创建镜像, -t : 指定镜像的标签信息,即&lt;image&gt;, 注意镜像名必须是全小写 可以在构建时指定tag: docker build -t hangge_server:2.4 . -f: 指定dockerfile docker build -f /path/to/a/Dockerfile . 原理 增加镜像层 一般而言，如果指令会对镜像增改，那么会新建镜像层， 如果指令只是指示Docker如何构建或者如何运行应用程序，那么就只会增加镜像的元数据 查看image build的输出： ❯ docker image build -t web:latest .Sending build context to Docker daemon 82.43kBStep 1/8 : FROM alpinelatest: Pulling from library/alpine3d2430473443: Pull complete Digest: sha256:d6d0a0eb4d40ef96f2310ead734848b9c819bb97c9d846385c4aca1767186cd4Status: Downloaded newer image for alpine:latest ---&gt; e9adb5357e84Step 2/8 : LABEL maintainer=&quot;nigelpoulton@hotmail.com&quot; ---&gt; Running in 84a356f040f7Removing intermediate container 84a356f040f7 ---&gt; f84bda7d881dStep 3/8 : RUN apk add --update nodejs npm curl ---&gt; Running in 16254526f96c&lt;Snip&gt;Removing intermediate container 16254526f96cStep 4/8 : COPY . /src ---&gt; 7bee4035f9fbStep 5/8 : WORKDIR /src ---&gt; Running in 9773c1204206Removing intermediate container 9773c1204206 ---&gt; a9bea6558795Step 6/8 : RUN npm install ---&gt; Running in 2117e2800e7d 可以发现，对于Dockerfile中的每一个产生镜像层的指令， docker server会： 运行一个临时容器 在该容器中执行该指令 将指令执行结果保存为镜像层 删除临时容器 而对于不产生镜像层的指令， 不会生成临时容器 build cache docker image build会从顶层自上而下逐条执行Dockerfile中的指令， 对于每一条指令， Docker都会检查缓存中是否已经有与该指令对应的镜像层。 如果Cache hit, 并且会链接到这个镜像层，在此基础上继续构建； 如果Cache miss, 则会对剩余部分的指令设置缓存无效（ 这意味着Dockerfile接下来的指令将全部执行， 而不再尝试查找build cache ）， 并基于当前指令构建新的镜像层 一旦某条指令cache miss, 则之后的指令都不会使用缓存。 因此编写Dockerfile时， 尽量将易于导致镜像层改变的指令放到后面 --no-cahce=true： 强制忽略build cache 判断缓存命中（即镜像是否相同）的算法：计算每一个被构建文件的checksum， 将其与已有镜像层中同一文件的checksum进行对比 。 如果不同，则说明 cache miss squash image 正常来说， docker会构建多个镜像层， 并将它们合并为一个镜像 可以将镜像层手动合并， 这样更方便， 但是会导致被合并的镜像层无法被共享： docker image build --squash 例子： 可以看到，合并前的镜像层是独立的，可以只发送不同的镜像层， 但合并后，所有镜像层合并为一个镜像层， 所以每次都需要传输完整的镜像 示例 example: docker image build -t web:latest . 推送镜像 push首先需要当前用户登陆dockerhub push镜像需要如下信息： Registry: 默认docker.io Repository: 被推送镜像的REPOSITORY属性值 Tag: 默认latest preparation 假设镜像仓库名是web, 那么push后，镜像位于docker.io/web:latest,然而用户一般没有一级命名空间的权限, 因此需要为当前镜像重新打一个标签， 这个标签指定了要推送的用户空间： docker image tag &lt;image-qith-current-wothtag&gt; &lt;image-with-new-tag&gt;# 该命令会为镜像添加额外的标签，不会删除已有的标签. 可以通过`docker image ls` 查看，发现镜像拥有了两个标签 如果你的标签上的用户名不等于你当前登陆的docker hub id, push会失败： # 当前登陆用户为lyklove❯ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEhello-world latest feb5d9fea6a5 5 months ago 13.3kBlyk/hello-world latest feb5d9fea6a5 5 months ago 13.3kB# 错误示范❯ docker image push lyk/hello-world:latestThe push refers to repository [docker.io/lyk/hello-world] ## 可以看到docker试图push到lyk的目录下，但是当前用户是lyklove, 不具有用户lyk的权限， 因此push会失败e07ee1baac5f: Preparing denied: requested access to the resource is denied 步骤 先登陆docker hub： docker login 为当前镜像重新打一个标签（ 与当前dockerhub id相符）： docker image tag &lt;current-tag&gt; &lt;new-tag&gt; push镜像（以新标签标识的镜像）： docker image push [OPTIONS] &lt;image&gt; 示例 例子： 假设有一个镜像 web， 则 docker image push web 实际上会将镜像推送到docker.io/web:latest ( 默认Registry是docker.io, 默认tag是latest ) 但是， 我不可能有docker.io/这个以及命名空间的权限， 只能推送到我自己的二级命名空间(也就是用户的命名空间)： 假如我当前登陆的docker hub id为lyklove， 则我需要推送到docker.io/lyklove/web:latest 为此，需要给镜像改名： docker image tag web:latest lyklove/web:latest# 由于镜像web的默认标签名就是latest, 因此也可以:docker image tag web lyklove/web 最后将lyklove/web ( 或者lyklove/web:latest ) 推送到dockerhub docker image push lyklove/web:latest 查看镜像构建过程 查看在构建镜像的过程中执行了哪些指令： docker image history &lt;image&gt; 每行内容都对应Dockerfile的一条指令（自下而上, 最后执行的指令（如 ENTRYPOINT）最先显示） 对于示例项目web:latest, 可以看到只有Dokcerfile的 FROM， RUN和ADD指令添加了镜像层， 其他的指令只是新增了元数据信息： ❯ docker image history webIMAGE CREATED CREATED BY SIZE COMMENT8552b568ff4c 2 minutes ago /bin/sh -c #(nop) ENTRYPOINT [&quot;node&quot; &quot;./app… 0B 3669d3adfb1a 2 minutes ago /bin/sh -c #(nop) EXPOSE 8080 0B d0005c695ed3 2 minutes ago /bin/sh -c npm install 23.4MB a9bea6558795 2 minutes ago /bin/sh -c #(nop) WORKDIR /src 0B 7bee4035f9fb 2 minutes ago /bin/sh -c #(nop) COPY dir:09deb2ee65cb723fd… 44.9kB 6df93a7da909 2 minutes ago /bin/sh -c apk add --update nodejs npm curl 52.5MB f84bda7d881d 5 minutes ago /bin/sh -c #(nop) LABEL maintainer=nigelpou… 0B e9adb5357e84 30 hours ago /bin/sh -c #(nop) CMD [&quot;/bin/sh&quot;] 0B &lt;missing&gt; 30 hours ago /bin/sh -c #(nop) ADD file:cf4b631a115c2bbfb… 5.57MB 可以看到，第一行是执行的最后一条指令ENTRYPOINT。 一共产生了四个镜像层 查看镜像： docker image inspect &lt;image&gt; docker image inspect web:latest # 以示例项目为例, 可以看到确实只有四个镜像层 &lt;Snip&gt; &quot;RootFS&quot;: &#123; &quot;Type&quot;: &quot;layers&quot;, &quot;Layers&quot;: [ &quot;sha256:5e03d8cae8773cb694fff1d55da34a40d23c2349087ed15ce68476395d33753c&quot;, &quot;sha256:3dc92b603964ad1b75c9dde518d028676ded40c82858ee4d236e10ef0e3c02fb&quot;, &quot;sha256:04910df3fe981f716ad106dec89d8b667102690462e81efefef411273dad7d26&quot;, &quot;sha256:a46c93c283ea7d6611acc8d1422f0b20f40acf1ebadd9d51439e425f7a3dc18d&quot; ] &#125;, &lt;Snip&gt; 运行容器 docker container run -d --name c1 \\-p 80:8080 \\web:latest --name： 指定容器名 -p host_port:container_port: 指定将主机的端口映射到容器的端口 -P: 随机端口映射，容器内部端口随机映射到主机的高端口 -d: 后台运行容器，并返回容器ID -i: 以交互模式运行容器，通常与 -t 同时使用 -t: 为容器重新分配一个伪输入终端，通常与 -i 同时使用 -e username=&quot;ritchie&quot;: 设置环境变量 --env-file=[file]: 从指定文件读入环境变量 --expose=[port-num]-[port-num]: 开放（暴露）一个端口或一组端口； --rm: 退出时自动删除容器 多阶段构建 进行多阶段构建， 概念和Jenkinsfile、 Github Action workflow一样 多阶段构建使用一个Dockerfile, 其中包含多个FROM指令， 每个都是一个 Build Stage, 从0开始编号。 每个stage可以复用之前stage的构建结果（jar包， target文件之类的） 示例项目: https://github.com/LYK-love/atsea-sample-shop-app FROM node:latest AS storefrontWORKDIR /usr/src/atsea/app/react-appCOPY react-app .RUN npm installRUN npm run build # 构建出一个很大的node镜像FROM maven:latest AS appserverWORKDIR /usr/src/atseaCOPY pom.xml .RUN mvn -B -f pom.xml -s /usr/share/maven/ref/settings-docker.xml dependency:resolveCOPY . .RUN mvn -B -s /usr/share/maven/ref/settings-docker.xml package -DskipTests # 构建出一个很大的maven镜像FROM java:8-jdk-alpineRUN adduser -Dh /home/gordon gordonWORKDIR /staticCOPY --from=storefront /usr/src/atsea/app/react-app/build/ . # 从storefront阶段拉取一些文件过来， 复制到工作目录下WORKDIR /appCOPY --from=appserver /usr/src/atsea/target/AtSea-0.0.1-SNAPSHOT.jar . # 同上， 从appserver阶段拉取一些文件过来， ENTRYPOINT [&quot;java&quot;, &quot;-jar&quot;, &quot;/app/AtSea-0.0.1-SNAPSHOT.jar&quot;]CMD [&quot;--spring.profiles.active=postgres&quot;] # 构建出一个精简的镜像 COPY --from 指令： 从之前stage构建的镜像中仅复制生产环境所需要的文件， 这样镜像中就带有不会冗余文件（比如maven, node） 三个FROM指令构建出三个镜像， 用docker image build -t multi:stage进行构建， 它只会命名最后一个镜像， 我们就只需要将最后一个镜像push到生产环境： ❯ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEnode latest 36fad710e29d 2 weeks ago 991MB&lt;none&gt; &lt;none&gt; d9c9c532ae40 7 minutes ago 934MBmaven latest d833a10812ed 3 weeks ago 793MB&lt;none&gt; &lt;none&gt; db32cdd21a1a 31 minutes ago 1.15GBopenjdk 8-jdk-alpine a3562aa0b991 2 years ago 105Mmulti stage 040df44afa9a 7 minutes ago 211MB 可以看到， 1第一行是第一阶段拉取的镜像， 第二行是第一阶段生成的镜像； 第三，四，五，六行同理；并且第六行镜像被-t multi:stage命了名 还可以看到，只有最后一个镜像会被命名， 而其余的FROM指令生成的镜像都变成了玄虚镜像， 可以直接删除， 非常方便 构建镜像优化 ref:如何优化 node 项目的 docker 镜像, 这篇文章将构建镜像优化到了: 大小从 1.06G 到 73.4M 构建速度从 29.6 秒到 1.3 秒 我们以文中的node项目为例, 最初的Dockerfile如下: FROM node:14.17.3# 设置环境变量ENV NODE_ENV=productionENV APP_PATH=/node/app# 设置工作目录WORKDIR $APP_PATH# 把当前目录下的所有文件拷贝到镜像的工作目录下 .dockerignore 指定的文件不会拷贝COPY . $APP_PATH# 安装依赖RUN yarn# 暴露端口EXPOSE 4300CMD yarn start 基本操作 对于会新建镜像层的指令, 比如RUN, ENV.... 因此这些指令最好写成一行, 可以用 &amp;&amp;连接多个命令或用\\\\换行书写. 例如: ENV NODE_ENV=production \\ APP_PATH=/node/app 由于构建镜像时会逐层检查build cache, 因此最好把不经常变动的层提到前面去, 比如ENV 使用alpine 基础镜像层可以使用alpine, 这是一个超级小的Linux镜像. 上例的基础镜像层是node, 可以: 使用软件的alpine版本 dockerhub 查看 node 版本 对于node等基础软件,使用其alpine版本: FROM node:14.17.4-alpine 可以去 使用alpine linux 使用alpine linux作为基础镜像层,然后手动装node等基础软件. 该方法效果最显著. alpine使用apk作为包管理工具, 可以到 apk官网 查看apk包版本 需要注意alpine镜像版本. 比如, 如果使用镜像alpine:3.16, 而我需要的nodejs版本只存在于alpine3.13, 就会无法拉取该依赖 即: 一定要指定alpine版本. 不要选择 latest 版本( From alpine:latest) 其次, 有人会用阿里云的apk源, 此时也要注意选择alpine镜像的版本 FROM alpine:3.13 AS baseLABEL maintainer=&quot;LYK-love&quot;# RUN echo &quot;http://mirrors.aliyun.com/alpine/edge/main/&quot; &gt; /etc/apk/repositories \\# &amp;&amp; echo &quot;http://mirrors.aliyun.com/alpine/edge/community/&quot; &gt;&gt; /etc/apk/repositories \\RUN apk update \\ &amp;&amp; apk add --no-cache --update nodejs=14.20.0-r0 npm=14.20.0-r0 \\ &amp;&amp; npm config set registry http://r.cnpmjs.org/ --production# ... 后面的步骤不变 用户软件( node, yarn等 )也最好要指定版本, 下面的例子中使用方案2 注意: apk和其他工具不同, 不会在下载node时顺便下载npm, 所以如果使用npm, 需要手动下载: apk add --no-cache --update nodejs=14.17.4-r0 npm=8.19.1-r0 提前下载依赖 对于前端项目, 下载依赖在构建镜像时花了很大时间. 我们可以利用构建缓存, 先将package.json 文件单独提前拷贝到镜像，再装依赖，执行命令装依赖这层的前一层是拷贝 package.json 文件，因为安装依赖命令不会变化，所以只要 package.json 文件没变化，就不会重新执行 yarn 安装依赖，它会复用之前安装好的依赖. 示例: FROM alpine:latest# 使用 apk 命令安装 nodejs 和 yarn，如果使用 npm 启动，就不需要装 yarnRUN apk add --no-cache --update nodejs=14.17.4-r0 yarn=1.22.10-r0# 暴露端口EXPOSE 4300# 设置环境变量ENV NODE_ENV=production \\ APP_PATH=/node/app# 设置工作目录WORKDIR $APP_PATH# 拷贝 package.json 到工作跟目录下COPY package.json .# 安装依赖RUN yarn# 把当前目录下的所有文件拷贝到镜像的工作目录下 .dockerignore 指定的文件不会拷贝COPY . .# 启动命令CMD yarn start 利用多阶段构建 运行 node 程序只需要生产的依赖和最终 node 可以运行的文件，就是说我们运行项目只需要 package.js 文件里 dependencies 里的依赖，devDependencies 依赖只是编译阶段用的 比如 eslint 等这些工具在项目运行时是用不到的，再比如我们项目是用 typescript 写的，node 不能直接运行 ts 文件，ts 文件需要编译成 js 文件， 运行项目我们只需要编译后的文件和 dependencies 里的依赖就可以运行，也就是说最终镜像只需要我们需要的东西，任何其他东西都可以删掉，下面我们使用多阶段改写 Dockerfile: # 构建基础镜像 FROM alpine:3.16.2 AS base # 设置环境变量 ENV NODE_ENV=production \\ APP_PATH=/node/app # 设置工作目录 WORKDIR $APP_PATH # 安装 nodejs 和 yarn RUN apk add --no-cache --update nodejs=14.17.4-r0 yarn=1.22.10-r0# 使用基础镜像 装依赖阶段 FROM base AS install # 拷贝 package.json 到工作跟目录下 COPY package.json ./ # 安装依赖 RUN yarn# 最终阶段，也就是输出的镜像是这个阶段构建的，前面的阶段都是为这个阶段做铺垫 FROM base # 拷贝 装依赖阶段 生成的 node_modules 文件夹到工作目录下 COPY --from=install $APP_PATH/node_modules ./node_modules # 将当前目录下的所有文件（除了.dockerignore排除的路径），都拷贝进入镜像的工作目录下 COPY . . # 启动 CMD yarn start github 的 actions 构建镜像问题 github 提供的 actions，每次都是一个干净的实例，什么意思，就是每次执行，都是干净的机器，这会导致一个问题，会导致 docker 没法使用缓存，那有没有解决办法呢，我想到了两种解决办法： docker 官方提供的 action 缓存方案 我用的是 Github cache 方案 自托管 actions 运行机器 相当于 gitlab 的 runner 一样，自己提供运行器，自己提供的就不会每次都是干净的机器，详情看 actions 官方文档 先构建一个已经安装好依赖包的镜像，然后基于此镜像再次构建，相当于多阶段构建，把前两个阶段构建的镜像产物推送到镜像仓库，再以这个镜像为基础去构建后续部分。借助镜像仓库存储基础镜像从而达到缓存的效果（此方案来源于评论里的大佬） # 以这个镜像为基础去构建，这个镜像是已经装好项目依赖的镜像并推送到镜像仓库里，这里从镜像仓库拉下来FROM project-base-image:latestCOPY . .CMD yarn start复制代码 Examples Vue app Dockerfile 使用node的alpine: # build stage# FROM node:14.20.1-slim as build-stageFROM node:14.16.0-alpine3.13 AS build-stageLABEL maintainer=&quot;LYK-love&quot;WORKDIR /appCOPY package*.json ./RUN node -v &amp;&amp; npm -v \\ &amp;&amp; npm config set registry http://r.cnpmjs.org/ \\ &amp;&amp; npm install COPY . .RUN npm run build# production stage# FROM nginx:1.21.5 as production-stageFROM nginx:1.21.5-alpine as production-stageRUN nginx -vCOPY --from=build-stage /app/dist/ /usr/share/nginx/html/COPY --from=build-stage /app/default.conf /etc/nginx/conf.d/default.confEXPOSE 80CMD [ &quot;nginx&quot;,&quot;-g&quot;,&quot;daemon off;&quot; ] 我实验了一下, 如果全都使用标准镜像(node:14.20.1-slim + nginx:1.21.5 ), 则镜像总大小为151.42MB. 而全都使用alpine镜像后, 总大小为39.03MB, 这是惊人的提升. 使用alpine linux RUN apk update \\ &amp;&amp; apk add --no-cache --update nodejs=14.20.0-r0 npm=14.20.0-r0 \\ &amp;&amp; npm config set registry http://r.cnpmjs.org/ --production# ENV NODE_ENV productionFROM base AS build-stageWORKDIR /appCOPY package*.json ./RUN node -v &amp;&amp; npm -v \\ &amp;&amp; npm install # &amp;&amp; npm install -g @vue/cli@5.0.1COPY . .RUN npm run build# production stageFROM nginx:1.21.5-alpine as production-stageCOPY --from=build-stage /app/dist/ /usr/share/nginx/html/COPY --from=build-stage /app/default.conf /etc/nginx/conf.d/default.confEXPOSE 80CMD [ &quot;nginx&quot;,&quot;-g&quot;,&quot;daemon off;&quot; ] 总体大小为39.03MB, 感觉反而不如node-alpine呢...... 为啥啊?? .dockerignore #Dependency directory# https://www.npmjs.org/doc/misc/npm-faq.html#should-i-check-my-node_modules-folder-into-gitnode_modules.DS_Storedist# node-waf configuration.lock-wscript# Compiled binary addons (http://nodejs.org/api/addons.html)build/Release.dockerignoreDockerfile*docker-compose*# Logslogs*.log# Runtime data.idea.vscode*.suo*.ntvs**.njsproj*.sln*.sw*pids*.pid*.seed.git.hg.svn Commands build: docker build -t Frontend_VolatileReborn . run: docker run -it -p 8080:80 --rm --name Frontend_VolatileReborn Frontend_VolatileReborn:latest visit: localhost:8080","categories":[{"name":"Technology","slug":"Technology","permalink":"http://lyk-love.cn/categories/Technology/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://lyk-love.cn/tags/Docker/"}]},{"title":"Shell Script","slug":"Shell-Script","date":"2022-03-17T17:06:52.000Z","updated":"2022-09-26T06:39:34.939Z","comments":true,"path":"2022/03/18/Shell-Script/","link":"","permalink":"http://lyk-love.cn/2022/03/18/Shell-Script/","excerpt":"Outline: Variable String Array Quoting Mechanism Control Flow I/O redirection Function ..... ref: shell tutorial","text":"Outline: Variable String Array Quoting Mechanism Control Flow I/O redirection Function ..... ref: shell tutorial Shell Script 不同的Shell实现会有不同的方言，不过这很少见。 这里只介绍最标准的Shell Script语法， 能够被最常用的Shell(如Bash)接受 常见Shell指令 Variable assign variables in bash: foo=bar Note that foo = bar will not work since it is interpreted as calling the foo program with arguments = and bar. In general, in shell scripts the space character will perform argument splitting access the value of the variable: $foo 等价于 $&#123;foo&#125;， 花括号可以精确地界定变量名称的范围。 可以用read命令从标准输入接受数据并赋值:read val example: #! /usr/bin/env bashecho -n &quot;Enter your name:&quot;read nameecho &quot;hello $name&quot;exit 0 String Strings in bash can be defined with ' and &quot; delimiters, but they are not equivalent. Strings delimited with ' are literal strings and will not substitute variable values whereas &quot; delimited strings will. foo=barecho &quot;$foo&quot;# prints barecho &#x27;$foo&#x27;# prints $foo Array array initialization − array_name=(value1 ... valuen) assign array_name[index]=value access Array Values $&#123;array_name[index]&#125; Quoting Mechanism metacharacters Unix Shell provides various metacharacters which have special meaning while using them in any Shell Script and causes termination of a word unless quoted. * ? [ ] &#x27; &quot; \\ $ ; &amp; ( ) | ^ &lt; &gt; new-line space tab quoting The following table lists the four forms of quoting − Sr.No. Quoting &amp; Description 1 Single quoteAll special characters between these quotes lose their special meaning. 2 Double quoteMost special characters between these quotes lose their special meaning with these exceptions −$`$'&quot;\\ 3 BackslashAny character immediately following the backslash loses its special meaning. 4 Back quote (aka backtick)Everything you type between backticks is evaluated (executed) by the shell before the main command, and the output of that execution is used by that command, The Single Quotes: 其内容不转义, 相当于在每个字符前加 backslash The Double Quotes： 其内容转义 The Backslash: 取消其后面的一个字符的转义 The Backquotes：将其内容视作 command 并执行， 与后文的CMD substitution类似 var=`command` DATE=`date`echo $DATE#等价于后文的CMD substitutionecho $(DATE) 条件语句 As with most programming languages, bash supports control flow techniques including if, case, while and for. if syntax: if [ expression ] then Statement(s) to be executed if expression is true fi if [ expression 1 ]then Statement(s) to be executed if expression 1 is trueelif [ expression 2 ]then Statement(s) to be executed if expression 2 is trueelif [ expression 3 ]then Statement(s) to be executed if expression 3 is trueelse Statement(s) to be executed if no expression is truefi 紧凑形式： ; (同一行上多个命令的分隔符) example: if [ -f ~/.bashrc ]; then . ~/.bashrc fi #!/bin/shread -p &quot;Is this morning? Please answer yes or no: &quot; answerif [ &quot;$answer&quot; = &quot;yes&quot; ]; then echo “Good morning”elif [ &quot;$answer&quot; = &quot;no&quot; ]; then echo “Good afternoon” else echo “Sorry, $answer not recognized. Enter yes or no” exit 1 fiexit 0 case syntax: case word in pattern1 | pattern2) Statement(s) to be executed if pattern1 matches ;; pattern3) Statement(s) to be executed if pattern2 matches ;; pattern4) Statement(s) to be executed if pattern3 matches ;; *) Default condition to be executed ;;esac example: #!/bin/shoption=&quot;$&#123;1&#125;&quot; case $&#123;option&#125; in -f) FILE=&quot;$&#123;2&#125;&quot; echo &quot;File name is $FILE&quot; ;; -d) DIR=&quot;$&#123;2&#125;&quot; echo &quot;Dir name is $DIR&quot; ;; *) echo &quot;`basename $&#123;0&#125;`:usage: [-f file] | [-d directory]&quot; exit 1 # Command to come out of the program with status 1 ;; esac #!/bin/shread -p &quot;Is this morning? Please answer yes or no.&quot; answercase &quot;$answer&quot; in yes | y | Yes | YES) echo “Good morning!” ;; no | n | No | NO) echo “Good afternoon!” ;; *) echo “Sorry, $answer not recognized.” ;;esac exit 0 select in &amp;&amp; case in select in语句自带循环 select variable in value_listdo statementsdone variable: 表示变量 value_list: 取值列表 in: Shell关键字 select in 通常和 case in 一起使用，在用户输入不同的编号时可以做出不同的反应 example: #!/bin/shclearselect item in Continue Finishdo case &quot;$item&quot; in Continue) ;; Finish) break ;; *) echo &quot;Wrong choice! Please select again!&quot; ;; esac done 该命令的while版本： while [ &quot;$item&quot; != &quot;Finish&quot; ]; do read item case &quot;$item&quot; in &quot;Continue&quot;) ;; &quot;Finish&quot;) ;; *) echo &quot;Wrong choice! Please select again!&quot; ;; esac done 循环语句 for syntax： for var in word1 word2 ... wordNdo Statement(s) to be executed for every word.done example: for FILE in $HOME/.bash*do echo $FILEdone for f in *.png do mv -n $f $f.kkdone while syntax: while condition do statements done example: quit=nwhile [ &quot;$quit&quot; != &quot;y&quot; ]; do read menu_choicecase &quot;$menu_choice&quot; ina) echo &quot;a, continue...&quot;;;b) echo &quot;b, continue...&quot;;;q|Q) quit=y;;*) echo &quot;Sorry, choice not recognized.&quot;;;esac done a=0LIMIT=10while [ $a -le $LIMIT ] do a=$(($a+1)) if [ $a -gt 2 ] then break # Skip entire rest of loop. fi echo -n &quot;$a&quot;done 命令组合 分号串联： command1 ; command2 ; ... 条件组合: statement1 &amp;&amp; statement2 &amp;&amp; statement3 &amp;&amp; ... statement1 || statement2 || statement3 || 语句块 &#123; statement1 statement2 ...&#125; 或 &#123; statement1; statement2 ; ... ; &#125; I/O redirection 一般情况下，每个 Linux 命令运行时都会打开三个文件： 标准输入文件(stdin)：stdin的文件描述符为0，Unix程序默认从stdin读取数据。 标准输出文件(stdout)：stdout 的文件描述符为1，Unix程序默认向stdout输出数据。 标准错误文件(stderr)：stderr的文件描述符为2，Unix程序会向stderr流中写入错误信息 Discard the output $ command &gt; /dev/null The file /dev/null is a special file that automatically discards all its input. Discard both output of a command and its error output, $ command &gt; /dev/null 2&gt;&amp;1 a command normally writes its output to STDOUT use standard redirection to redirect STDERR to STDOUT 这里2&gt;&amp;1将标准错误（2）合并到标准输出（1）， 而标准输出已经被重定向到了/dev/null, 因此总体效果是，标准错误和输出都被重定向到了/dev/null file descriptor: 0 : STDIN 1: STDOUT 2: STDERR Sr.No. Command &amp; Description 1 pgm &gt; fileOutput of pgm is redirected to file会覆盖目标文件中原有的数据 2 pgm &lt; fileProgram pgm reads its input from file 3 pgm &gt;&gt; fileOutput of pgm is appended to file 4 n &gt; fileOutput from stream with descriptor n redirected to file 5 n &gt;&gt; fileOutput from stream with descriptor n appended to file 6 n &gt;&amp; mMerges output from stream n with stream m 7 n &lt;&amp; mMerges input from stream n with stream m 8 &lt;&lt; tagStandard input comes from here through next tag at the start of line 9 |Takes output from one program, or process, and sends it to another Here Document Here Document 目前没有统一的翻译，这里暂译为”嵌入文档“。Here Document 是 Shell 中的一种特殊的重定向方式，它的基本的形式如下： command &lt;&lt; delimiterdocumentdelimiter 它的作用是将两个 delimiter 之间的内容(document) 作为输入传递给 command 注意： 结尾的delimiter 一定要顶格写，前面不能有任何字符，后面也不能有任何字符，包括空格和 tab 缩进。 开始的delimiter前后的空格会被忽略掉。 下面的例子，通过 wc -l 命令计算 document 的行数： wc -l &lt;&lt; EOF This is a simple lookup program for good (and bad) restaurants in Cape Town.EOF 也可将 Here Document 用在脚本中，例如： #!/bin/bashcat &lt;&lt; EOF This is a simple lookup program for good (and bad) restaurants in Cape Town.EOF test test expression 或 [ expression ] [ (aka test) command the and [[ ... ]] test construct are used to evaluate expressions [是一条命令， 与test等价，大多数shell都支持。在现代的大多数sh实现中，[与test是builtin命令 []将其operand直接当作argument [[，是关键字，许多shell(如ash bsh)并不支持这种方式 [[]]将其operand进行参数引用，算术扩展和CMD substitution， 不需要手动转义等 test expression: test 1 -lt 2echo $?0test 1 -gt 2echo $?1 [ expression ]: echo &quot;Starting program at $(date)&quot; # Date will be substitutedecho &quot;Running program $0 with $# arguments with pid $$&quot;for file in &quot;$@&quot;; do grep foobar &quot;$file&quot; &gt; /dev/null 2&gt; /dev/null # When pattern is not found, grep has exit status 1 # We redirect STDOUT and STDERR to a null register since we do not care about them if [[ $? -ne 0 ]]; then echo &quot;File $file does not have any foobar, adding one&quot; echo &quot;# foobar&quot; &gt;&gt; &quot;$file&quot; fidone 与文件有关的条件测试 文件测试运算符的形式及功能 option parameter function -r 文件名 如文件存在并且是用户可读的，则测试条件为真 -w 文件名 如文件存在并且是用户可写的，则测试条件为真 -x 文件名 如文件存在并且是用户可执行的，则测试条件为真 -f 文件名 如文件存在并且是普通文件，则测试条件为真 -d 文件名 如文件存在并且是目录文件，则测试条件为真 -p 文件名 如文件存在并且是命名的FIFO文件，则测试条件为真 -b 文件名 如文件存在并且是块特殊文件，则测试条件为真 -c 文件名 如文件存在并且是字符特殊文件，则测试条件为真 -s 文件名 如文件存在并且文件长度大于0，则测试条件为真 -t 文件描述符 如文件被打开且文件描述符是与终端设备相关的，则测试条件为真，默认文件描述符是1 字符串测试 option parameter function -z s1 如果字符串s1的长度为0，则测试条件为真 -n s1 如果字符串s1的长度大于0，则测试条件为真 s1 如果字符串s1不是空字符串，则测试条件为真 =或== s1=s2 如果s1等于s2，则测试条件为真,“=”前后应有空格 != s1!=s2 如果s1不等于s2，则测试条件为真 &lt; s1&lt;s2 如果按字典顺序s1在s2之前，则测试条件为真 &gt; s1&gt;s2 如果按自定顺序s1在s2之后，则测试条件为真 数值测试 parameter function n1 -eq n2 如果整数n1等于n2，则测试条件为真 n1 -ne n2 如果整数n1不等于n2，则测试条件为真 n1 -lt n2 如果如果n1小于n2,则测试条件为真 n1 -le n2 如果如果n1小于或等于n2,则测试条件为真 n1 -gt n2 如果n1大于n2,则测试条件为真 n1 -ge n2 如果n1大于或等于n2,则测试条件为真 逻辑操作 parameter function ! expr 逻辑表达式求反 expr1 –a expr2 两个逻辑表达式“And“ expr1 –o expr2 两个逻辑表达式“Or“ Function syntax function_name()&#123; list of commands [ return value ]&#125; example: mcd () &#123; mkdir -p &quot;$1&quot; cd &quot;$1&quot;&#125; return code exit: 不仅会退出函数，还会退出执行该函数的shell return code:仅仅退出函数。和command的return code同 Function Call from Prompt 令shell加载函数定义： 可以将函数定义在主目录下的.profile，这样每次登陆后，在命令提示符后面输入函数名字就可以立即调用: func para1 para2 将函数定义写在一个文件( say test.sh ), 然后执行它 令shell删除函数定义： unset -f function_name 该命令也可用来令shell删除变量定义 反之，set可以用来定义变量 output Commands will often return output using STDOUT, errors through STDERR return code Commands have Return Code to report errors in a more script-friendly manner. 0 usually means everything went OK; anything different from 0 means an error occurred. return code， 你可以指定返回任何值 return code as bool value Return codes can be used to conditionally execute commands using &amp;&amp; (and operator) and || (or operator) . Commands can also be separated within the same line using a semicolon ;. The true program will always have a 0 return code and the false command will always have a 1 return code. false || echo &quot;Oops, fail&quot;# Oops, failtrue || echo &quot;Will not be printed&quot;#true &amp;&amp; echo &quot;Things went well&quot;# Things went wellfalse &amp;&amp; echo &quot;Will not be printed&quot;#true ; echo &quot;This will always run&quot;# This will always runfalse ; echo &quot;This will always run&quot;# This will always run CMD substitution 命令替换: $(CMD)执行命令，并将其输出作为一个变量 For example, if you do ``, the shell will first call ls and then iterate over those values. example: for file in $(ls)... #!/bin/shecho &quot;The current directory is $PWD&quot;echo &quot;The current directory is $(pwd)&quot;exit 0 arithmetic substitution $((...)) #!/bin/shx=0while [[ $x != 10 ]]; doecho $xx=$(($x+1)) doneexit 0 variable substitution 变量替换可以根据变量的状态（是否为空、是否定义等）来改变它的值 可以使用的变量替换形式： 形式 说明 ${var} 变量本来的值 ${var:-word} 如果变量 var 为空或已被删除(unset)，那么返回 word，但不改变 var 的值。 ${var:=word} 如果变量 var 为空或已被删除(unset)，那么返回 word，并将 var 的值设置为 word。 ${var:?message} 如果变量 var 为空或已被删除(unset)，那么将消息 message 送到标准错误输出，可以用来检测变量 var 是否可以被正常赋值。 若此替换出现在Shell脚本中，那么脚本将停止运行。 ${var:+word} 如果变量 var 被定义，那么返回 word，但不改变 var 的值。 #!/bin/shecho $&#123;var:-&quot;Variable is not set&quot;&#125;echo &quot;1 - Value of var is $&#123;var&#125;&quot;echo $&#123;var:=&quot;Variable is not set&quot;&#125;echo &quot;2 - Value of var is $&#123;var&#125;&quot;unset varecho $&#123;var:+&quot;This is default value&quot;&#125;echo &quot;3 - Value of var is $var&quot;var=&quot;Prefix&quot;echo $&#123;var:+&quot;This is default value&quot;&#125;echo &quot;4 - Value of var is $var&quot;echo $&#123;var:?&quot;Print this message&quot;&#125;echo &quot;5 - Value of var is $&#123;var&#125;&quot; result: Variable is not set1 - Value of var isVariable is not set2 - Value of var is Variable is not set3 - Value of var isThis is default value4 - Value of var is PrefixPrefix5 - Value of var is Prefix process substitution &lt;( CMD ) ： 执行 CMD 并将其输出重定向到一个临时文件， 用这个临时文件的名字替换 &lt;() This is useful when commands expect values to be passed by file instead of by STDIN. For example, diff &lt;(ls foo) &lt;(ls bar) will show differences between files in dirs foo and bar. operator Bourne shell didn't originally have any mechanism to perform simple arithmetic operations but it uses external programs, either awk or expr. #!/bin/shval=`expr 2 + 2`echo &quot;Total value : $val&quot; There must be spaces between operators and expressions. 2+2 ： wrong 2 + 2 ： right all the conditional expressions should be inside square braces with spaces around them [$a==$b] or [$a &lt;= $b]: wrong [ $a == $b ] or [ $a &lt;= $b ]: right When performing comparisons in bash, try to use double brackets [[ ]] in favor of simple brackets [ ] 因为表达式要先执行，因此它必须被包裹在backquote内 Arithmetic Operators Assume variable a holds 10 and variable b holds 20 then − Operator Description Example + (Addition) Adds values on either side of the operator expr $a + $b will give 30 - (Subtraction) Subtracts right hand operand from left hand operand expr $a - $b will give -10 * (Multiplication) Multiplies values on either side of the operator expr $a \\* $b will give 200 / (Division) Divides left hand operand by right hand operand expr $b / $a will give 2 % (Modulus) Divides left hand operand by right hand operand and returns remainder expr $b % $a will give 0 = (Assignment) Assigns right operand in left operand a = $b would assign value of b into a == (Equality) Compares two numbers, if both are same then returns true. [ $a == $b ] would return false. != (Not Equality) Compares two numbers, if both are different then returns true. [ $a != $b ] would return true. All the arithmetical calculations are done using long integers. Relational Operators These operators do not work for string values unless their value is numeric. For example, following operators will work to check a relation between 10 and 20 as well as in between &quot;10&quot; and &quot;20&quot; but not in between &quot;ten&quot; and &quot;twenty&quot;. Assume variable a holds 10 and variable b holds 20 then − Operator Description Example -eq Checks if the value of two operands are equal or not; if yes, then the condition becomes true. [ $a -eq $b ] is not true. -ne Checks if the value of two operands are equal or not; if values are not equal, then the condition becomes true. [ $a -ne $b ] is true. -gt Checks if the value of left operand is greater than the value of right operand; if yes, then the condition becomes true. [ $a -gt $b ] is not true. -lt Checks if the value of left operand is less than the value of right operand; if yes, then the condition becomes true. [ $a -lt $b ] is true. -ge Checks if the value of left operand is greater than or equal to the value of right operand; if yes, then the condition becomes true. [ $a -ge $b ] is not true. -le Checks if the value of left operand is less than or equal to the value of right operand; if yes, then the condition becomes true. [ $a -le $b ] is true. Boolean Operators The following Boolean operators are supported by the Bourne Shell. Assume variable a holds 10 and variable b holds 20 then − Operator Description Example ! This is logical negation. This inverts a true condition into false and vice versa. [ ! false ] is true. -o This is logical OR. If one of the operands is true, then the condition becomes true. [ $a -lt 20 -o $b -gt 100 ] is true. -a This is logical AND. If both the operands are true, then the condition becomes true otherwise false. [ $a -lt 20 -a $b -gt 100 ] is false. String Operators The following string operators are supported by Bourne Shell. Assume variable a holds &quot;abc&quot; and variable b holds &quot;efg&quot; then − Operator Description Example = Checks if the value of two operands are equal or not; if yes, then the condition becomes true. [ $a = $b ] is not true. != Checks if the value of two operands are equal or not; if values are not equal then the condition becomes true. [ $a != $b ] is true. -z Checks if the given string operand size is zero; if it is zero length, then it returns true. [ -z $a ] is not true. -n Checks if the given string operand size is non-zero; if it is nonzero length, then it returns true. [ -n $a ] is not false. str Checks if str is not the empty string; if it is empty, then it returns false. [ $a ] is not false. File Test Operators We have a few operators that can be used to test various properties associated with a Unix file. Assume a variable file holds an existing file name &quot;test&quot; the size of which is 100 bytes and has read, write and execute permission on − Operator Description Example -b file Checks if file is a block special file; if yes, then the condition becomes true. [ -b $file ] is false. -c file Checks if file is a character special file; if yes, then the condition becomes true. [ -c $file ] is false. -d file Checks if file is a directory; if yes, then the condition becomes true. [ -d $file ] is not true. -f file Checks if file is an ordinary file as opposed to a directory or special file; if yes, then the condition becomes true. [ -f $file ] is true. -g file Checks if file has its set group ID (SGID) bit set; if yes, then the condition becomes true. [ -g $file ] is false. -k file Checks if file has its sticky bit set; if yes, then the condition becomes true. [ -k $file ] is false. -p file Checks if file is a named pipe; if yes, then the condition becomes true. [ -p $file ] is false. -t file Checks if file descriptor is open and associated with a terminal; if yes, then the condition becomes true. [ -t $file ] is false. -u file Checks if file has its Set User ID (SUID) bit set; if yes, then the condition becomes true. [ -u $file ] is false. -r file Checks if file is readable; if yes, then the condition becomes true. [ -r $file ] is true. -w file Checks if file is writable; if yes, then the condition becomes true. [ -w $file ] is true. -x file Checks if file is executable; if yes, then the condition becomes true. [ -x $file ] is true. -s file Checks if file has size greater than 0; if yes, then condition becomes true. [ -s $file ] is true. -e file Checks if file exists; is true even if file is a directory but exists. [ -e $file ] is true. shell globbing 一些正则操作，用于文件名扩展 Wildcards: ?: match one character *: match any amount of characters For instance, given files foo, foo1, foo2, foo10 and bar, the command rm foo? will delete foo1 and foo2 whereas rm foo* will delete all but bar. Curly braces &#123;&#125; - 相当于笛卡尔积： a.&#123;py,cpp,java&#125;等价于 a.py a.cpp a.java convert image.&#123;png,jpg&#125;# Will expand toconvert image.png image.jpgcp /path/to/project/&#123;foo,bar,baz&#125;.sh /newpath# Will expand tocp /path/to/project/foo.sh /path/to/project/bar.sh /path/to/project/baz.sh /newpath# Globbing techniques can also be combinedmv *&#123;.py,.sh&#125; folder# Will move all *.py and *.sh filesmkdir foo bar# This creates files foo/a, foo/b, ... foo/h, bar/a, bar/b, ... bar/htouch &#123;foo,bar&#125;/&#123;a..h&#125;touch foo/x bar/y# Show differences between files in foo and bardiff &lt;(ls foo) &lt;(ls bar)# Outputs# &lt; x# ---# &gt; y env virable 使用 export 设置的变量就成为了环境变量，而没有使用 export 设置的则是自定义变量 环境变量 说明 $HOME 当前用户的登陆目录 $PATH 以冒号分隔的用来搜索命令的目录的列表 $PS1 命令行提示符，通常是”$”字符 (很多主题都会改掉$PS1) $PS2 辅助提示符，用来提示后续输入，通常是”&gt;”字符 $IFS 输入区分隔符。当shell读取输入数据时会把一组字符看成是单词之间的分隔符，通常是空格、制 表符、换行符等 parameter variable $0 - Name of the script $1 to $9 - Arguments to the script. $1 is the first argument and so on. 当n&gt;=10时，需要使用$&#123;n&#125;来获取参数 $@ - 全部参数组成的列表 $# - Number of arguments $? - Return code of the previous command $*： 全部参数连接成的字符串，按$IFS的第一个字符分割 $$ - Process identification number (PID) for the current script !! - Entire last command, including arguments. A common pattern is to execute a command only for it to fail due to missing permissions; you can quickly re-execute the command with sudo by doing sudo !! $_ - Last argument from the last command. If you are in an interactive shell, you can also quickly get this value by typing Esc followed by . or Alt+. $PATH 是由多个路径所组成的，并且用冒号进行了分隔 环境变量可以在其进程的子进程中继续有效，而自定义变量则无效 #我们在当前Shell进程中指定了var1变量[roc@roclinux ~]$ var1=&quot;hello&quot;[roc@roclinux ~]$ echo $var1hello#我们在当前Shell进程中又指定了var2变量[roc@roclinux ~]$ var2=&quot;world&quot;[roc@roclinux ~]$ echo $var2world#我们通过export发布var1[roc@roclinux ~]$ export var1 #我们进入到一个bash子进程中[roc@roclinux ~]$ bash #var1变量仍然有效, 而var2变量已经无效了[roc@roclinux ~]$ echo $var1 # 输出变量的值hello[roc@roclinux ~]$ echo $var2 # 什么也没有输出 export PATH=$PATH:/home/to/operation_tools 声明环境变量PATH, 值为之前的PATH的值（采用$进行赋值）， 再append :/home/to/operation_tools， 这里要append冒号，因为环境变量之间通过冒号隔开 scripts shebang Note that scripts need not necessarily be written in bash to be called from the terminal. For instance, here’s a simple Python script that outputs its arguments in reversed order: #!/usr/local/bin/pythonimport sysfor arg in reversed(sys.argv[1:]): print(arg) shebang: the character sequence consisting of #! at the beginning of a script in a Unix-like operating system shell会将shebang中#!之后的内容作为一个程序的路径，打开该程序， 将本script的路径当作参数传入（ 即，将整个script当作input传入shebang所指定的程序 ） For example, if a script is named with the path path/to/script, and it starts with the following line, #!/bin/sh, then the program loader is instructed to run the program /bin/sh, passing path/to/script as the first argument. In Linux, this behavior is the result of both kernel and user-space code.[9] The shebang line is usually ignored by the interpreter, because the &quot;#&quot; character is a comment marker in many scripting languages; some language interpreters that do not use the hash mark to begin comments still may ignore the shebang line in recognition of its purpose. shebang with env The shebang expects a full path to the interpreter to use so the following syntax would be incorrect: #!python Setting a full path like this might work: #!/usr/local/bin/python but would be non portable as python might be installed in /bin, /opt/python/bin, or wherever other location. Using env #!/usr/bin/env python is a method allowing a portable way to specify to the OS a full path equivalent to the one where python is first located in the PATH. scripts vs shell functions Some differences between shell functions and scripts that you should keep in mind are: Functions have to be in the same language as the shell, while scripts can be written in any language. This is why including a shebang for scripts is important. Functions are loaded once when their definition is read. Scripts are loaded every time they are executed. This makes functions slightly faster to load, but whenever you change them you will have to reload their definition. Functions are executed in the current shell environment( 可以简单理解为， function的所在路径是当前路径 ) whereas scripts execute in their own process. Thus, functions can modify environment variables, e.g. change your current directory, whereas scripts can’t. Scripts will be passed by value environment variables that have been exported using export As with any programming language, functions are a powerful construct to achieve modularity, code reuse, and clarity of shell code. Often shell scripts will include their own function definitions. Potpourri :： 空命令 .或source： 在当前shell session中执行脚本， 因此可以用于刷新当前shell环境: source ~/.bashrc break: 从for/while/until循环退出 execute shell scripts Shell script是能在命令行直接输入的，但仅会作用一次 执行脚本文件： 方法1: 直接指定sh来执行该脚本，不需要shebang，也不需要脚本有执行权限（因为该脚本直接作为参数传给了sh） sh script_file 方法2: 要shebang（指定解释器），要指定脚本路径（./必加）， 否则bash就会在环境变量中查找该脚本名，找不到就报错 chmod +x script_file ##(chown, chgrp optionally)./script_file 方法3: 在当前shell session中执行该脚本 source script_file or . script_file ​ 注意，方法1和2都是新开一个子shell session，在其中执行脚本，而方法三是在当前shell session中执行脚本","categories":[{"name":"Language","slug":"Language","permalink":"http://lyk-love.cn/categories/Language/"}],"tags":[{"name":"shell","slug":"shell","permalink":"http://lyk-love.cn/tags/shell/"}]},{"title":"Java Class","slug":"Java-Class","date":"2022-02-23T21:37:07.000Z","updated":"2022-09-26T06:39:34.931Z","comments":true,"path":"2022/02/24/Java-Class/","link":"","permalink":"http://lyk-love.cn/2022/02/24/Java-Class/","excerpt":"Outline RTTI Class对象 类型转换前先做检查 反射 动态代理 空对象 ref ： Thinking in Java","text":"Outline RTTI Class对象 类型转换前先做检查 反射 动态代理 空对象 ref ： Thinking in Java RTTI Java使用Class对象执行其RTTI Java类在必须时才加载 当程序创建第一个对类的静态成员的引用时，就会加载这个类 因此构造器也是静态成员 类加载 加载 -&gt; 验证 -&gt; 准备 -&gt; 解析 -&gt; 初始化 -&gt; 使用 -&gt; 卸载 类加载只有一次 初始化 静态变量初始化 static final变量不会初始化，直接赋值 静态变量赋值 执行静态代码块 public class TestClassLoader&#123; public static int k=0; static TestClassLoader t1 = new TestClassLoader(&quot;t1&quot;); public static TestClassLoader t2 = new TestClassLoader(&quot;t2&quot;); public static int i = print(&quot;i&quot;); public static int n = 99; public int j = print(&quot;j&quot;); static&#123; print(&quot;静态块&quot;); &#125; public TestClassLoader(String str) &#123; System.out.println((++k)+ &quot;: &quot; + str + &quot; i=&quot; + i + &quot; n=&quot; + n ); ++i; ++n; &#125; public static int print(String str) &#123; System.out.println((++k)+ &quot;: &quot; + str + &quot; i=&quot; + i + &quot; n=&quot; + n ); ++n; return ++i; &#125; public static void main(String[] args) &#123; new TestClassLoader(&quot;init&quot;); &#125;&#125; 上述程序输出为： 1: j i=0 n=02: t1 i=1 n=13: j i=2 n=24: t2 i=3 n=35: i i=4 n=46: 静态块 i=5 n=997: j i=6 n=1008: init i=7 n=101 Class loader: 先检查这个类的Class对象是否被加载 若未加载 一旦Class对象加载入内润，就被用来创建该类的所有对象 Class对象 生成类对象的引用： ``Class.forName(String className)`静态方法: 返回该类的Class引用，具有副作用，如果该类还没有加载就会加载该类。该方法会抛出异常。 Class t = Class.forName(&quot;java.lang.Thread&quot;) Params: className – the fully qualified name of the desired class. Returns: the Class object for the class with the specified name. Throws: LinkageError – if the linkage fails ExceptionInInitializerError – if the initialization provoked by this method fails ClassNotFoundException – if the class cannot be located Class.class（ 类字面常量 ）： 返回类对象的引用，没有副作用 object.getClass(): 获得对象的确切类型的Class引用 类名： object.getName(): 返回全限定名 object.getSimpleName(): 返回不含包名的类名 object.getCanonicalName(): 返回全限定名 创建对象： Class.newInstance()实例方法：“虚拟构造器”，能且仅能调用该类的public无参数构造方法 类型转换前先做检查 Class引用可以指向别的Class对象，这个错误在编译期不会发现。 使用泛型语法可以在编译期执行类型检查： Class intClass = int.class;Class&lt;Integer&gt; genericClass = int.class;genericClass = Integer.class; // same thingintClass = double.class; // 编译期不会报错genericIntClass = double.class; // illegal 判断类型是否兼容： if( Object obj_a instanceof Class class_b)&#123; &#125; * ```java public boolean isInstance(Object o); 判断类型是否相等： equals() == 反射 利用反射来查看类 getDeclaredXX() 方法可以无视访问权限 得到方法： Method getMethod(name, Class...)：获取某个public的Method（包括父类） Method getDeclaredMethod(name, Class...)：获取当前类的某个Method（不包括父类） Method[] getMethods()：获取所有public的Method（包括父类） Method[] getDeclaredMethods()：获取当前类的所有Method（不包括父类） 得到字段： Field getField(name)：根据字段名获取某个public的field（包括父类） Field getDeclaredField(name)：根据字段名获取当前类的某个field（不包括父类） Field[] getFields()：获取所有public的field（包括父类） Field[] getDeclaredFields()：获取当前类的所有field（不包括父类） 访问构造方法： getConstructor(Class...)：获取某个public的Constructor； getDeclaredConstructor(Class...)：获取某个Constructor； getConstructors()：获取所有public的Constructor； getDeclaredConstructors()：获取所有Constructor。 得到继承关系： Class getSuperclass()：获取父类类型； Class[] getInterfaces()：获取当前类实现的所有接口。 通过Class对象的isAssignableFrom()方法可以判断一个向上转型是否可以实现。 例子： 打印类的方法和构造方法： public class ShowMethods &#123; private static String usage = &quot;Please input a Class&quot;; private static Pattern pattern = Pattern.compile(&quot;\\\\w+\\\\.&quot;); public static void println(String str)&#123; System.out.println(str); &#125; public static void print(String str)&#123; System.out.print(str); &#125; public static void main(String[] args) &#123; if(args.length &lt; 1) &#123; println( usage ); &#125; int lines = 0; try&#123; Class&lt;?&gt; c = Class.forName( args[0] ); Method[] methods = c.getMethods(); Constructor[] constructors = c.getConstructors(); if(args.length == 1) &#123; for( Method method: methods ) &#123; println( pattern.matcher(method.toString()).replaceAll(&quot;&quot;) ); &#125; for(Constructor constructor: constructors ) &#123; println( pattern.matcher( constructor.toString() ).replaceAll(&quot;&quot;) ); &#125; lines += methods.length + constructors.length; &#125; else&#123; ; &#125; &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 利用反射来修改类 反射可以绕过几乎所有障碍（访问权限）来得到类的信息、修改类， 即使只发布字节码文件，依然可以通过javap这样的反编译工具查看源代码 javap -p: 显示所有成员，包括私有成员 下面给出两种防御措施，并给出破解方案, 基本思路是： 只要得到对象，就可以调用getClass()得到其实际类型（破解了类访问权限）， 再用getDeclaredXXX()得到其所有字段/方法（破解了字段/方法访问权限） 对非public字段/方法/构造方法的访问、修改，都要先setAccessible(true) 有些jvm有security manager,可能会拒绝setAccessible(true) 接下来就可以设置字段： Field.set(Object, Object)，其中第一个Object参数是指定的实例，第二个Object参数是待修改的值 final字段在被修改时是安全的， 运行时系统会在不抛任何异常的情况下接受修改尝试，但实际上不会发生任何修改 隐瞒类访问权限 防御措施 接口A只有方法f(): package typeinfo.packageaccess;public interface A &#123; void f();&#125; 只暴露HiddenC一个类，它产生A类型（实际是C类型）的对象， 但是调用者无法调用A接口之外的方法， 因为类名C是不可见的， 因此理论上g(),u()等不应该能被调用 class C implements A &#123; public void f()&#123; Utils.println( &quot;public C.f()&quot; ); &#125; public void g() &#123; Utils.println(&quot;public C.g()&quot;); &#125; void u()&#123; Utils.println(&quot;package C.u()&quot;); &#125; protected void v()&#123; Utils.println(&quot;protected C.v()&quot;); &#125; private void w()&#123; Utils.println(&quot;private C.w()&quot; ); &#125;&#125;public class HiddenC &#123; public static A makeA()&#123; return new C(); &#125;&#125; 破解方案 得到a的实际类型，然后进行方法调用 static void callHiddenMethod( Object a, String method_name ) throws Exception&#123; Method g = a.getClass().getDeclaredMethod(method_name); g.setAccessible(true); g.invoke(a);&#125; 主程序 package typeinfo.HiddenImplementation;import typeinfo.packageaccess.*;import java.lang.reflect.Method;public class HiddenImplementation &#123; public static void main(String[] args) throws Exception &#123; A a = HiddenC.makeA(); a.f(); //编译期错误，找不到类型C// if( a instanceof C)// &#123;// C c = (C)a;// c.g();// &#125; //绕过了包访问权限！！！ callHiddenMethod(a, &quot;g&quot;); callHiddenMethod(a, &quot;u&quot;); callHiddenMethod(a, &quot;v&quot;); callHiddenMethod(a, &quot;w&quot;); &#125;&#125; 隐瞒字段访问权限 破解方法： 通过getDeclaredField(field_name) 就可以得到私有字段 后续可以setAccessible(true)，set(Object, Object)对字段进行修改（ final字段不可被修改，因此是安全的 ） WithPrivateFinalField pf = new WithPrivateFinalField(); f = pf.getClass().getDeclaredField(&quot;s2&quot;); f.setAccessible(true); f.set(pf,&quot;No, you&#x27;re not!&quot; ); System.out.println(pf); 示例： 防御措施 class WithPrivateFinalField&#123; private int i = 1; private final String s1 = &quot;I&#x27;m totally safe&quot;; private String s2 = &quot;Am I safe?&quot;; public String toString()&#123; return &quot;i = &quot; + i + &quot;, &quot; + s1 + &quot;, &quot; + s2; &#125;&#125; 破解方案 public class ModifyingPrivateFields &#123; public static void main(String[] args) throws Exception &#123; WithPrivateFinalField pf = new WithPrivateFinalField(); System.out.println(pf); Field f = pf.getClass().getDeclaredField(&quot;i&quot;); f.setAccessible(true); f.setInt(pf,47); System.out.println(pf); f = pf.getClass().getDeclaredField(&quot;s1&quot;); f.setAccessible(true); f.set(pf,&quot;No, you&#x27;re not!&quot; ); System.out.println(pf); f = pf.getClass().getDeclaredField(&quot;s2&quot;); f.setAccessible(true); f.set(pf,&quot;No, you&#x27;re not!&quot; ); System.out.println(pf); &#125;&#125; javap查看源代码 可以看到，我们能看到private字段，因此保护是没有用的 ❯ javap -p /home/lyk/Projects/java_learning/out/production/java_learning/WithPrivateFinalField.classCompiled from &quot;ModifyingPrivateFields.java&quot;class WithPrivateFinalField &#123; private int i; private final java.lang.String s1; private java.lang.String s2; WithPrivateFinalField(); public java.lang.String toString();&#125; 动态代理 class DynamicProxyHandler implements InvocationHandler&#123; private Object proxied; public DynamicProxyHandler(Object proxied) &#123; this.proxied = proxied; &#125; public Object invoke(Object proxy, Method method, Object[] args ) throws Throwable &#123; System.out.println( &quot;**** proxy: &quot; + proxy.getClass() + &quot;, method: &quot; + method + &quot;, args: &quot; + args ); if(args != null) &#123; for(Object arg: args) &#123; System.out.println(&quot; &quot; + arg); &#125; &#125; return method.invoke(proxied, args); &#125;&#125;public class SimpleDynamicProxy &#123; public static void consumer(Interface iface) &#123; System.out.println(&quot;do &quot; + iface); &#125; public static void main( String[] args ) &#123; RealObject realObject = new RealObject(); consumer(realObject); Interface proxy = (Interface) Proxy.newProxyInstance( Interface.class.getClassLoader(), new Class[] &#123; Interface.class &#125;, new DynamicProxyHandler( realObject ) ); consumer(proxy); &#125;&#125; 在运行期动态创建一个interface实例的方法如下： 定义一个InvocationHandler实例，它负责实现接口的方法调用； 通过 Proxy.newProxyInstance() 创建 interface 实例，它需要3个参数： 使用的ClassLoader，通常就是接口类的ClassLoader； 需要实现的接口数组，至少需要传入一个接口进去； 用来处理接口方法调用的InvocationHandler实例。 将返回的Object强制转型为接口。 空对象 空对象，相比null好处是它更靠近数据，因为对象标识的是问题空间的实体 （ 感觉没啥用，增加了编程的复杂性 ）","categories":[{"name":"Language","slug":"Language","permalink":"http://lyk-love.cn/categories/Language/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://lyk-love.cn/tags/Java/"}]},{"title":"Network Layer","slug":"Computer Networking Network-Layer","date":"2022-02-21T00:33:00.000Z","updated":"2022-09-26T06:39:34.927Z","comments":true,"path":"2022/02/21/Computer Networking Network-Layer/","link":"","permalink":"http://lyk-love.cn/2022/02/21/Computer%20Networking%20Network-Layer/","excerpt":"Outline: Intro 虚电路和数据报网络 路由器工作原理 网际协议IP ICMP In-net Route Selection Internet Route Selection 广播和多播路由选择 VPN, NAT MPLS","text":"Outline: Intro 虚电路和数据报网络 路由器工作原理 网际协议IP ICMP In-net Route Selection Internet Route Selection 广播和多播路由选择 VPN, NAT MPLS Intro 计算机网络的不同层次采用不同设备互联： 物理层使用的中间设备： 转发器 链路层： 网桥( bridge ) 网络层：路由器 网络层以上： 网关( gateway ) linux上的gateway，我查了文档，说是第五层设备，但我找不到一个物理设备和它对应。。。 很多人也把路由器叫做网关， 比如Linux路由表中有个default（ 即default gateway），代表的是本机所属的网络 网络层的功能 网络层（第三层、IP层）有三个功能： 转发( forwarding )： 分组在单一的路由器中从一条入链路传送到一条出链路 是路由器的本地动作 硬件实现 路由选择( routing )：网络中决定分组从源到目的地所采取的端到端路径 涉及整个网络的路由器 软件实现 转发和路由选择经常混用 （某些网络层结构，如ATM, MPLS中有）连接建立： 从源到目的地沿着所选择的路径彼此握手，以便在分组传输之前就建立好状态 网络层的组件 网络层三大组件： IP协议 路由选择协议： RIP, OSPF, BGP ICMP协议：差错报告、路由器“信令” 因特网的网络层提供尽力而为服务，这可以理解为无服务。 当然除了因特网外还有其他的网络结构模型，比如ATM，就提供恒定比特率、可用比特率等服务 交换机和路由器 交换机： 进行分组转发，是链路层设备。 路由器：分组转发和路由选择，网络层设备 路由器具有转发表，路由器通过检查到达分组的首部字段的值来转发分组，然后使用该值在该路由器的转发表中索引查询 路由选择算法决定了插入转发表的项的内容 虚电路和数据报网络 运输层可以提供面向连接服务和无连接服务（TCP, UDP）, 网络层也类似，但与运输层相比又有不同： 网络层中，这些服务是主机到主机的服务，而运输层中是进程到进程的服务 网络层要么提供面向连接服务，称为虚电路网络；要么提供无连接服务， 称为数据报网络。 网络层不能同时提供这两种服务。 虚电路网络： ATM，帧中继 数据报网络： 因特网 运输层连接服务只在端系统中实现， 而网络层连接服务需要在端系统和路由器中实现 可以看到，虚电路网络和数据报网络是网络层的两个基本类型 虚电路网络 路由器需要为连接维持连接状态信息 每当建立和释放连接（虚电路），都要在转发表中增/删表项 虚电路的组成： 源到目的主机的路径 VC号， 延着该路径的每段链路的一个号码 该路径上每台路由器的转发表 VC号 虚电路分组将在它的首部携带一个VC号，VC号在每段链路都不同，从中间路由器的转发表表项更新： 注意， 一段链路可以有多条虚电路，每条虚电路有一个VC号， 因此属于某虚电路的分组在到达对应链路时，会获得唯一的VC号（ 不会与同一链路上其他虚电路的分组发生冲突 ） 示例： 假设主机A要与B创建虚电路，且网络给该虚电路选择路径 A-R1-R2-B,并为三段链路分配VC号 12, 22，32。 该分组离开主机A时，首部的VC号是12,而离开R1时，VC号变成了22；离开R2时，VC号是32 R1的转发表： 入接口 入VC号 出接口 出VC号 1 12 2 22 2 63 1 18 ... ... ... ... 每条虚电路的连接和建立都会让路径上所有路由器的转发表更新 VC号在每段链路都不同的原因： 逐链路代替VC号减少了VC号字段在分组首部中的长度 如过一条链路具有多条虚电路，那么约定互不冲突的VC号将花费大量时间 虚电路的三个阶段 虚电路的三个阶段： 虚电路建立：发送方运输层与网络层联系，指定接收方地址，网络层决定发送方到接收方的路径，为该路径上的每条链路决定一个VC号。 最后在路径上的每个路由器中增加一个表项 数据传输 虚电路拆除：网络层通知接收方的端系统，并更新路径上每个路由器的转发表以表明该虚电路不存在了 虚电路网络层的两个端系统之间路径上的路由器都要参与虚电路的建立，且每台路由器都完全知道经过它的所有虚电路 端系统向网络层发送的指示虚电路启动与终止的报文，以及路由器之间传递的用于建立虚电路（即修改路由表中的连接状态）的报文，称为信令报文（ signaling message ）,用于交换这些报文的协议称为信令协议（ signaling protocol ） 数据报网络 数据报网络中的路由器不需要维持连接状态信息（因为是无连接的，没有虚电路），但是要在转发表中维持转发状态信息。 装发表存储了目的地址到输出链路接口的映射，通过路由选择算法修改 转发表在将分组的目的地址（当然是IP地址）和转发表表项匹配时，采用最长前缀匹配 路由器工作原理 路由器结构 以下“端口“均指硬件端口，和套接字等软件端口不同 输入端口： 将输入的物理链路与路由器连接。 同时还要在这里完成查询转发表。 一般分组转发到交换结构，进而转发到输出端口；而控制分组（如携带路由选择协议信息的分组）转发到路由选择处理器 输入端口拥有转发表的影子副本，因此可以进行查找+转发 交换结构： 用于分组交换，有许多实现，比如经内存交换，经总线交换，经复杂的总线网络交换 输出端口 路由选择处理器： 执行路由选择协议， 维护路由表，并由此计算出转发表。 它还执行网络管理功能 输入/输出端口和交换结构共同实现了转发功能，称为路由器转发平面（ router forwarding plane ） 纯硬件实现，因此在纳秒级别 路由器选择处理器实现了控制功能（路由选择），称为路由器控制平面（ router control plane ） 软件实现并在路由选择处理器上执行，在毫秒级别 队列等待问题 在输入和输出端口都会形成分组队列，如果队列过长，耗尽了路由器的缓存，路由器就会丢弃分组，即丢包。 输入端口的排队不那么严重，因为可以将分组分配到多个输入端口。 输入端口的分组传输速度取决于交换结构的速度 对于输出端口，由于多个分组都可能争用同一个输出端口， 更容易发生排队，此时需要一个在输出端口的分组调度程序选出一个分组发送，以提供服务质量保证。常见的做法是主动队列管理 主动队列管理（ Active Queue Management, AQM ） ：在缓存填满前便丢弃（或在首部加标记）一个分组，以便向发送方提供一个拥塞信号 随即早期检测（ Random Early Detection, RED ）：AQM的最流行的一种实现。 为输出队列维护一个加权平均值，如果该值大于某个阈值，则丢弃后续到达的分组 路由表和转发表 路由表 路由表只存储三元素： 目的网络地址 （如果划分了子网）子网掩码 下一跳地址（Mac地址） IP地址和网络接口对应，因此我们所说的下一跳地址，就是下一跳的网络接口的地址 注意，路由表中只存储下一跳地址，而不是最终地址，地址其实就是网络接口的IP。 此外，路由表中还可以设置默认路由( Linux中称为默认网关default gateway )， 如果某个包的目的地址和路由表其他项都不匹配时，就匹配到默认路由 # 这个命令可以查看路由表，再次强调，我们在Linux内核中不区分路由表和转发表$ ip route show# 当目的地址与其他所有项都不匹配时，匹配到default gateway, 其对应的下一跳网络接口地址为172.31.0.1 # via后面跟的就是下一跳地址# dev后面跟的是default via 172.31.0.1 dev en0127.0.0.0/8 via 127.0.0.1 dev lo0127.0.0.1/32 via 127.0.0.1 dev lo0169.254.0.0/16 dev en0 scope link172.31.0.0/17 dev en0 scope link172.31.0.1/32 dev en0 scope link 转发表 结构： 路由表的三元素 主机方面的信息：比如输出端口信息、标记信息等 路由器的分组转发： 查看转发表，如果有对应的表项（ 目的IP在本网络， 或者目的IP的规则在路由表中被手动制定 ），就进行交付，否则就转发给本网络的默认路由 转发过程中需要将目的地址与子网掩码做与操作得到网络号， 注意网络号匹配采用最长前缀匹配 路由表和转发表的区别 转发表是根据路由表生成的， 可以看到，路由器只存储路由信息，和主机没有关系，它只描述网络链路状态和方向； 而转发表还要存储一些主机信息，它直接作用于数据包，在主机内部将一个数据包从一个端口导向另一端口。 对于程序员来说， 二者可以看成一个东西，因为Linux内核本身就不区分这两个表，每到一个数据包都会查路由表。二者只对于硬件工程师有差别。 在本文的剩余部分，我们不会区分这二者。 IP（v4） IP数据报 网络层的基本数据传输单元为IP数据报( IP Datagram ) 其中的关键字段如下： 版本号：4个比特。通过查看版本号，路由器可以确定如何解释IP数据报的剩余部分，因为不同IP版本使用不同数据报格式。 首部长度：4个比特。确定IP数据报数据部分实际从哪里开始。 服务类型：使不同类型的IP数据报（如要求时延低的数据报）能相互区别开。 数据报长度：16个比特，IP数据报的总长度（首部+数据），以字节计算。IP数据报的理论长度为65535字节，但实际很少超过1500字节。 标识，标志，片偏移：与IP分片有关。IPV6不允许路由器对分组分片。 寿命（TTL）：该字段确保数据报不会永远在网络中循环。数据报每过一个路由器，TTL减1，减到0就丢弃。 协议：仅在达到目的地才有用。指示IP数据报的数据部分交给哪个运输层协议，比如6表示交给TCP，17交给UDP。协议号是将网络层与运输层绑定到一起的粘合剂，端口号则用于运输层和应用层的绑定。 首部校验和：帮助路由器检测收到的IP数据报的比特错误。TCP/IP在运输层与网络层都执行差错检测。 源和目的IP地址：通过源主机通过DNS查找来决定目的地址。 选项：该字段允许IP首部被扩展。很少使用，所以数据报首部不包括该字段的信息 数据：该字段除了包含TCP和UDP外，也可重载其他类型的数据，如ICMP报文。 IP数据报的首部有20字节（假设无选项）。如果数据报承载一个TCP报文段，每个数据报共承载40字节的首部（有20字节的TCP首部）以及应用层报文 IP数据报分片 并非所有链路层协议都能承载相同长度的网络层分组 最大数据单元（ Maximum Transmission Unit, MTU ）: 一个帧能承载的最大数据量 一段路经上的不同链路可能有不同的链路层协议，因此有不同的MTU 分片： 将数据报中的数据部分分成较小的数据报，再封装成帧 片的组装是相当复杂的工作， 由端系统实现。这是为了保持网络的简单性 IPV6没有片偏移字段，根本上废止了分片 IPV4编址 IP要求每个主机和路由器借口拥有自己的IP地址， 每个IP地址为32 Bit 因此，一个IP地址与一个网络接口相关联，而不是与包括该接口的主机或路由器相关联 当一台主机同时连到两个网络，该主机就必须具有两个相应的IP地址，网络号不同 IP地址的编址方法，目前广泛采用的是CIDR， 早年采用的是分类编址。 我们先介绍传统的分类编址。 为了提高地址利用率， 分类编址还可以划分子网 分类编址 Classful addressing 将IP地址划分为若干个固定类，每一类地址都由网络号(net-id)和主机号(host-id)构成。 现代网络采用的都是CIDR，传统分类地址已经不用了。 不过分类编址在事实上还广泛存在， 比如CIDR中的mask位数取8、16、24就分别代表了分类网络中的A、B、C类地址块 网络号由IP管理机构分配，主机号由得到该网络号的单位自行分配 一个网络是指具有相同网络号的主机的集合，因此用转发器或网桥等连接起来的若干局域网仍属于一个网络，因为它们都具有相同的网络号。 具有不同网络号的局域网必须用路由器互联 路由器总是有至少2个不同网络号的IP地址，用于连接不同的网络 A、B、C类地址都是单播地址。D类地址用于多播 网络号全为0的IP表示“本网络” 网络号全0 + 主机号全0， 表示“本网络上的本主机” 网络号全0 + host-id， 表示“本网络上的某台主机host-id” 主机号全为0的IP表示“本主机所连接到的网络的地址”。 例如， 一个A类地址为5.6.7.8, 则该主机所在的网络地址就是5.6.7.0 主机号全为1的IP表示&quot;本网络上的所有主机的地址&quot;。 例如，一个B类地址为126.43.255.255， 它表示&quot;在网络126.43.0.0上的所有主机的地址&quot;。 A类地址的网络号127(即0111 1111)保留作为本地软件环回(loopback)地址。 如果主机发送一个IP数据报的目的地址为环回地址，则数据报根本不会被发送到网络，而是由本机的协议软件来处理。 CIDR表示的127.*.*.*/8，就是整个环回地址块（A类网络的127地址块） 比如本地如果新开了个虚拟机，给它分配的地址就在127.*.*.*/8里面选，通向该地址的包全部走lo0接口 此外，CIDR的127.0.0.1/32是个特殊的环回地址，可以看到它是32位网络号，整个地址就是个网络地址。虽然它已经被包含在了127.*.*.*/8内， 但Linux路由表中还是把这个地址单列了出来。 我也不知道为啥，可能是传统吧 $ ip route show &lt;snip&gt;# 可以看到，IPv4的网络号为127的A类网络(8位掩码，表示A类网络)下的地址，全部作为了环回地址127.0.0.0/8 via 127.0.0.1 dev lo0# 很明显它被包含在127.0.0.0/8，单路由表里还是把它单列了出来127.0.0.1/32 via 127.0.0.1 dev lo0&lt;snip&gt; IP地址的指派范围： 网络类别 最大可指派的网络数 第一个可指派的网络号 最后一个可指派的网络号 每个网络中的最大主机数 A 126( $2^7 - 2$ ) 1 126 $2^{24}-2$ B 16383( $2^{14} - 1$ ) 128.1 191.255 $2^{16} - 2$ C 2097151( $2^{21} - 1$ ) 192.0.1 223.255.255 254( $2^{8} - 2$ ) 不使用的特殊IP地址: net-id host-id 源地址使用 目的地址使用 意义 0 0 可以 不可 在本网络上的本主机 0 host-id 可以 不可 在本网络上的某台主机host-id 全1 全1 不可 可以 只在本网络进行广播(各路由器均不转发) net-id 全1 不可 可以 对net-id上的所有主机进行广播 127 非全0或全1的任何数 可以 可以 本地环回地址 A类地址 A类地址的net-id占1 Byte， 其中第一位固定为0， 因此只有7 bit可以使用。 由于全0的网络号表示“本网络”； 网络号127表示环回地址，因此可以指派的网络号有$2^7 - 2$个 A类地址的host-id占3字节， 由于全0的主机号表示“本主机所连接到的网络的地址”； 全1主机号表示该网络上的所有主机的地址，因此可以指派的主机号有$2^{24}-2$个 B类地址 B类地址的net-id占2 Byte， 但前面2位已经固定为10， 因此只有14 bit可以使用。 由于网络号字段后面的14位怎么分配也不会出现全0或全1， 因此这里不需要“减二”。 但是，事实上B类网络地址128.0.0.0是不指派的( 即网络号1000 0000 )， 可以指派的B类最小网络地址是128.1.0.0( 即网络号1000 0001 )， 所以可以指派的网络号有$2^{14} - 1$个 A类地址的host-id占2 Byte, 减去全0和全1， 可以指派的主机号有$2^{16} - 2$ C类地址 B类地址的net-id占3 Byte， 但前面3位已经固定为110， 因此只有21 bit可以使用。 同理，C类网络地址不需要“减二”， 但C类网络地址192.0.0.0也是不指派的( 即网络号1100 0000 0000)， 可以指派的C类最小网络地址是192.0.1.0( 即网络号1100 0000 0001 )， 所以可以指派的网络号有$2^{21} - 1$个 C类地址的host-id占1 Byte, 减去全0和全1， 可以指派的主机号有$2^{8} - 2$ 划分子网 使用IP的主机号的若干位进一步划分子网。划分子网后，新的网络地址为： net-id + 子网id 子网掩码：就是若干个1， 其数量为新的网络地址数量， 也就是网络号+子网号的位数 使用子网掩码后， 不管网络有没有划分子网，只要把子网掩码和IP地址进行与运运算，就能得到网络地址 如果一个网络不划分子网，其子网掩码就是默认子网掩码，也就是仅有“net-id”位 比如，A累地址的默认子网掩码是255.0.0.0， 或0xFF00 0000 CIDR 无类别域间路由选择Classless Inter Domain Routing, CIDR 取消了传统分类网络的概念 和子网掩码的区别：子网掩码只是局限于在某一个子网内或一个站点内使用，而CIDR是对全球路由系统可见的 这意味着在公网，路由器可以直接对CIDR地址块进行寻址，而不必对主机进行寻址，大大提升了效率 这称为路由聚合 相比之下，子网掩码只能在子网使用，因此传统的分类网络在公网只能按主机寻址 CIDR采用最长前缀匹配： 同一个IP，在路由表中可能会匹配多个网络号，此时从匹配结果中采用具有最长网络前缀的路由 获取IP地址 ISP从ICANN等机构获得地址 组织从ISP获得一组地址（得到公网IP） 组织内可以用DHCP等方式给主机配置IP地址（得到内网IP） IP地址与硬件地址 IP层使用IP地址，链路层使用MAC地址 硬件地址（aka MAC地址）是下一跳的地址， IP地址是目的主机的IP地址 例如，我要去北京，需要经过： 1. 车站。 2.地铁站。 3. 飞机场。 那么，我的目的IP地址始终是北京，而起始IP地址是车站，到了车站后，我的目的MAC地址变成了地铁站， 到了地铁站后，我的起始IP地址是飞机场 但是，如何在知道总目的地是北京的情况下，知道下一步是要去车站呢？ 即： 如何在知道目的IP地址的情况下，知道下一跳的MAC地址呢？ 这就需要地址解析协议ARP( Address Resolution Protocal ) 以上信息，即目的网络号和下一跳地址( Mac地址 )都被存储在路由表中 ARP IP层使用ARP协议来进行IP - MAC地址的映射 ARP过程 每个主机里都设有一个ARP高速缓存( ARP cache )，里面有本局域网上各主机和路由器的IP地址到硬件地址的映射表，且这个映射表经常动态更新 ARP高速缓存中每条cache都只存在一段时间，超过时间后就被删除 工作流程：假设A和B在同一局域网，当主机A向局域网上某个主机B发送IP数据报时，先在ARP cache中查看有无主机B的IP地址，若有，就可查出对应的硬件地址，反之，执行下列步骤，找到主机B的硬件地址： A的ARP进程在局域网广播一个ARP请求分组，包含A自己的硬件地址和IP地址，目的方硬件地址(未知时填0)，目的方IP地址 A为什么要发送自己的硬件地址和IP地址？ 当A向B发送IP数据报时，很可能不久后B也要向A发送。 为了减少通信量， A在发送其ARP请求分组时，会把自己的IP地址和硬件地址写进请求分组。 这样主机B收到该请求分组时，就会把A当这一映射写进B的ARP cache中 在本局域网上的所有主机上运行的ARP进程都收到此ARP请求分组 主机B收到广播，发现本机IP与查询IP一致，就向A发送ARP响应分组，其中包括自己的硬件地址和IP地址。 由于其余的所有主机的IP地址都与ARP请求分组中要查询的IP地址不一致，就会直接丢弃该分组。 ARP请求分组是多播，但ARP响应分组是单播 A收到回复的ARP响应分组后，将对应IP和硬件地址存入ARP cache中，方便下次使用 注意： ARP协议只解决同一局域网上IP地址和硬件地址映射， 如果A和B在不同局域网，网络通过路由器R1相连，那么ARP会把R1的物理地址返回给A， A就只管把R1的地址写进链路层帧里。 当链路层帧到达R1时， R1再进行ARP，此时就能得到B的物理地址，将数据最终发给B 所以ARP如果在本局域网查不到B的IP，就会查路由器的IP ARP工作过程对用户是透明的 ARP典型情况 四种使用ARP的典型情况： 发送方是主机，要把IP数据报发送到本网络上的一个主机，这时ARP找到目的主机的硬件地址 发送方是主机，要把IP数据报发送到另一个网络上的一个主机，这时ARP找到本网络上的一个路由器的硬件地址，A只管把路由器的物理地址写进MAC帧，并最终发给路由器。 再由路由器进行ARP，一步步转发通过各个网络， 最终到达B 发送方是路由器，要把IP数据报发送到本网络上的一个主机，这时ARP找到目的主机的硬件地址 发送方是路由器，要把IP数据报发送到另一个网络上的主机，这时ARP找到本网络上的一个路由器的硬件地址，剩下的工作由这个路由器完成 IP层分组转发 示例： 默认路由：就是给主机指定一个“当其他路由表项都不匹配时”匹配的路由 可以减少路由表所占用的空间和搜索路由表所用的时间，将不在路由表中的网络都连向默认路由 直接交付：主机H查找路由表，如果发现目的主机就在本网络上，则不经过任何路由器，而是直接交付 分组转发算法： 从数据报的首部提取目的主机的IP地址$D$，得出目的网络地址为$N$ 若N就是与此路由器直接相连的某个网络地址，则进行直接交付，不需要再经过其他的路由器，直接把数据报交付给目的主机（这里包括把目的主机地址D转换为具体的硬件地址，把数据报封装为MAC帧，再发送此帧）；否则就是间接交付, 执行(3) 若路由表中有目的地址为$D$的特定主机路由，则把数据报传送给路由表中所指明的下一跳路由器，否则执行(4) 若路由表中有到达网$N$的路由，则把数据报传送给路由表中所指明的下一跳路由器，否则执行(5) 若路由表中有一个默认路由，则把数据报传送给路由表中所指明的下一跳路由器，否则执行(6) 就是说，如果路由表项能匹配目的地址$D$最好，否则就匹配目的网络$N$，否则就匹配到默认路由 报告转发分组出错 IPV6 IPV6新特性： 地址长度从32位扩展到128位 简化高效的40字节首部 流标签与优先级： 某些分组（如音频、视频传输）可以被当作一个流，并打上特定的标签 版本：该4比特字段用于标识IP版本号。IPv6将该字段设为6。注意到将该字段值设置为4并不能创建一个合法的IPv4数据报。 流量类型：该8比特字段与我们在IPv4中看到的TOS字段的含义相似。 流标签：该20比特的字段用于标识一条数据报的流，能够对一条流中的某些数据报给出优先权，或者它能够用来对来自某些应用的数据报给出更高的优先权，以优于来自其他应用的数据报。 有效载荷长度：该16比特值作为一个无符号整数，给出了IPv6数据报中跟在定长的40字节数据报首部后面的字节数量。 下一个首部（指传输层协议的首部）：该字段标识数据报中的内容（数据字段）需要交付给哪些个协议（如TCP或UDP）。该字段使用与IPv4首部中协议字段相同的值。 跳限制： 转发数据报的每台路由器将对该字段的内容减一。如果跳限制计数达到0，则该数据包将被丢弃。 源地址和目的地址：128比特地址 数据：IPv6数据报的有效载荷部分。当数据报到达目的地时，该有效载荷就从IP数据报中移除，并交给在下一个首部字段中指定的协议处理。 IPV6弃用的特性： 分片和重新组装： Pv6不允许在中间路由器上进行分片与重新组装。这种操作只能在源与目的地执行。如果路由器收到的IPv6数据报因太大而不能转发到出链路上的话，则路由器只需丢掉该数据报，并向发送方发回一个分组太大的ICMP差错报文即可。于是发送方能够使用较小长度的IP数据报重新发送数据 首部校验和：因为因特网层中的运输层和数据链路层协议执行了校验操作，所以IPv6去除了校验 选项：选项字段不再是标准IP首部的一部分了。但是它并未消失，而是可能出现在IPv6首部中由下一个首部指出的位置上 从IPV4到IPV6的迁移 有两种方案： 双栈， 隧道 双栈： 使用双栈的IPV6节点具有完整的IPV4实现，即同时具有收发两种数据报的能力。 在与IPV4节点进行交互时采用IPV4, 与IPV6节点交互时采用IPV6 双栈节点需要知道所要交互的节点是否支持IPV4/6, 这可以通过DNS查询实现，如果一个节点支持IPV6,那么DNS返回一个IPV6地址，反之返回IPV4地址 IPV6数据报中一些字段在IPV4中没有，这意味着如果将其IPV6数据报转化为IPV4的，将会丢失一些信息。 对于路径A - B - C -D - E , 假设除了C节点是IPV4外，其他均是IPV6. 那么A-B的IPV6数据报在转换为IPV4时会不可逆地丢失信息，即使后来又被转换为IPV6数据报发向E 隧道：如果两台IPV6数据库之间采用IPV6交互，但它们中间经由IPV4路由器互联，那么可以将整个IPV6数据报放到IPV4的有效载荷中。 比如图中B和E，要使用IPV6交互，但是它们经由中间IPv4路由器关联 IPsec IPsec是提供安全性服务的新型网络层协议，向后兼容IPV4和IPV6 IPsec只需在两台互相通信的主机中可用，其他主机和路由器可以继续用普通的IPv4 IPsec是面向连接的。主机间建立会话后，发送的TCP和UDP报文段都享受IPsec的安全性服务 密码技术约定 IP数据报有效载荷的加密 数据完整性 初始鉴别。主机确信在数据报中的源IP地址是该数据报的实际源 ICMP（V4） ICMP协议用于在网络层进行差错报告 ICMP是网络层协议，但从体系结构上讲位于IP协议之上，因为ICMP报文是作为IP报文的有效载荷承载的。同样，当一台主机收到一个指明上层协议为ICMP的IP数据报时，它分解出该报文的内容交给ICMP（就像任何低层次协议到高层一样） ICMP报文有两种： ICMP差错报文 ICMP查询报文，查询报文总是成对出现 ICMP种类 ICMP type字段 code 描述 查询报文 0 0 回显回答（对ping的回答） 3 0 目的主机不可达 3 1 目的主机不可达 3 2 目的主机不可达 3 3 目的主机的端口不可达 3 6 目的网络未知 3 7 目的主机未知 4 0 源抑制（拥塞控制） 8 0 回显请求 9 0 路由器通告 查询报文 10 0 路由器发现 11 0 TTL过期 12 0 IP首部损坏 使用ICMP的程序 ping程序，就是发送一个ICMP类型8编码0的报文到指定主机。 看到该回显（echo）请求，目的主机发回一个类型0编码0的ICMP回显回答 大多数提出/IP实现支持在内核中实现ping服务器， 因此ping服务器不一定是个进程 traceroute程序： 向目的主机发送一系列普通的IP数据报，每个数据报都携带了具有一个不可达UDP端口号的UDP报文段 第一个数据报的TTL为1,第二个的为2, 依次类推。 该源主机也为每个数据报启动定时器。 当第n台路由器观察到这个数据报的TTL刚好过期，根据IP协议规则，就会丢弃该数据报并发送一个ICMP警告报文（类型11,编码0）。 该警告报文包含了路由器的名字与IP地址。 当该ICMP报文返回路由器时，源主机从定时器得到往返时延，从ICMP报文中得到第n台路由器的名字与地址。 而数据报最终将到达目的主机，由于它包含了一个具有不可达端口号的UDO报文段， 该目的主机将向源发送一个端口不可达的ICMP报文，当源主机收到这个报文时，就可以停止发送探测分组了 ICMPv6 ICMPv6：用于IPv6， 因为IPv6和IPv4都不保证数据的可靠交付， 需要ICMP来反馈差错信息 ARP和IGMP都被合并到了ICMPv6中 In-net Route Selection 原理：将网络抽象为图 依据算法是全局的还是分布式的，可以分为： 全局式路由选择算法, aka 链路状态算法（ Link State, LS ）： 该算法以所有节点之间的连通性和所有链路的费用作为输入 分布式路由选择算法：每个节点仅知道相邻链路的信息，通过迭代计算并于相邻节点交换信息，逐渐计算出到达目的节点的最低费用路径 最著名的有距离向量( Distance-Vector, DV )算法 LS算法 就是Dijkstra算法，需要有网络的全局信息 LS算法的缺陷： 链路振荡 振荡问题 假设网络出初始的链路费用和路由选择如下，路由费用等于路由负载 那么算法再次运行时， y会发现顺时针到达w的代价为1,低于逆时针的代价，其他节点亦如是，整个链路的方向变为顺时针 算法再次运行，再次发生上述状况，链路方向又变为顺时针 。。。 循环往复，整个链路在不断振荡 DV算法 距离向量（DV）算法是一种迭代的、异步的和分布式的算法，每个节点都要从一个或多个直接相邻邻居接受某些信息，执行计算然后将其计算结果分发给邻居。 分布式：每个结点都从一个或多个直接连接的邻居接收信息，通过对信息的计算，再把结果通告给邻居 迭代： 一直持续到邻居之间无更多信息要交换为止 异步： 不要求节点相互之间步伐一致地操作，而是当收到邻居发来的信息时进行操作即可。 Bellman-Ford 方程 Bellman-Ford 方程: 令$c(x,v)$ 表示 x 到邻居 v 的开销，$d_v(y) $表示从 节点 v 到目的地 y 的开销，我们需要在 x 的所有邻居中获得最小值。 $$ d_x(y) = min_v{ \\ c(x,v) + d_v(y) \\ } $$ 距离向量 距离向量: x 在 N 中到其他所有结点 y 的开销的估计值 $$ D_x = [D_x (y): \\ y ∈ N] $$ Bellman-Ford 方程为 DV 算法提供了理论基础。简单地说，也就是结点获得最短路径的下一跳，并且将该信息写入转发表中。这时我们也 每个节点x维护如下路由选择信息： 对每个邻居v,从x到直接相连邻居的费用为 节点x的距离向量，即$D_x = [D_x (y): \\ y ∈ N]$, 是 x 在 N 中到其他所有结点 y 的开销的估计值 它的每个邻居的距离向量，即对x的每个邻居v,有$D_v = [D_v (y): \\ y ∈ N]$ 算法步骤： 每个节点周期性向邻居发送它的距离向量副本。 当节点x从它的任何一个邻居v接收到一个新距离向量路由协定，就保存v的距离向量，并根据Bellman-Ford方程更新自己的距离向量如下： $$ D_x(y) = min_v{ \\ c(x,v) + d_v(y) \\ } \\quad 对N中的每个节点 $$ 如果x的距离向量因为这次更新被改变，x将向它的每个邻居发送其更新后的距离向量 已证明： 只要所有节点继续以异步方式执行此算法，每个费用估计$D_x (y)$收敛到 $d_x (y)$, $d_x (y)$ 为节点x到y的实际最低费用路径的费用 DV算法的缺点：无穷计数问题 无穷计数问题 图中有X、Y、Z三个节点。我们增加链路费用。 Y检测到它到X的路径费用由4增加到了60。此时节点Z的距离向量为：d(X) = 5, d(Y) = 1, d(Z) = 0。于是Y在更新向量时发现，咦，Z到X的距离只有5诶，那可以先到Z再到X，于是Y的距离向量更新为：d(x) = 5 + 1 = 6, d(Y) = 0, d(z) = 1。我们可以发现，这个逻辑显然是错误的，因为Z到X的距离为5的前提是要经过Y，但Y更新后的路径又要经过Z，这就形成了一个选路环路（routing-loop）问题。因为Y的距离向量更新了（虽然是错误的），但它还是向Z发送了更新报文。Z收到更新报文后，比较了下邻居们到X的距离，发现经过Y的路径距离为1 + 6 = 7，小于直接到X的距离，于是Z也更新的自己的距离向量，然后又将更新后的距离向量发给Y。Y收到后又更新向量为8，然后再发给Z。。。这样循环往复，更新报文在Y和Z之间传来传去，直到第44次迭代后，Z算出它经由Y的路径费用大于50为止。此时，Z最终确定到X的最短路径费用是直接到达X的费用50，而Y也得到了最短路径是经Z到X的费用51。 无穷计数问题： 链路费用增加时，路由选择形成环路，该选择过程会不断迭代直到判断开销过大不符合条件才会跳出 解决方案： 毒性逆转 毒性逆转： 如果z通过y路由选择到目的地x，则z将通告y：它（z） 到x的距离是无穷大的 。 只要z经y路由选择到x， z就持续地向y讲述这个谎言。y将相信z没有到x的路径，最终y将永远不会试图经由z路由选择到x。 毒性逆转并没有解决一般的无穷计数问题，当网络规模增大，涉及到了 3 个及以上的更多节点的环路，毒性逆转将不能探测到故障 （片面的）解决方案：参加RIP 层次路由选择 在互联网等大型网络中，一般把路由器组织为自治系统（ Autonomous System, AS ） 将路由器组织成AS的理由： 互联网上路由器规模庞大，路由选择信息的计算开销大，因此需要对路由器进行分组，降低计算复杂性 许多组织都希望管理自治 同一AS内，运行相同的AS内部路由选择协议（ RIP, OSPF ）； AS之间， 运行相同的AS间路由选择协议( BGP ) 网关路由器（ gateway router ）：在AS内的路由器，负责向在本AS之外的目的地转发分组 Internet Route Selection 自治系统内的路由选择协议 也称为内部网关协议（ interior gateway protocol ） RIP 就是AS内的DV算法，只不过定义了最大跳数（默认16）来解决无穷计数问题，这个方案太简陋了，因为还是要绕圈子， 而且这种定义跳数上限的行为，导致网络中不能存在距离大于16的节点，这导致RIP只能用于小型网络 RIP在传输层使用UDP OSPF 开放最短路径优先（ Open Shortest Path First, OSPF）：使用洪泛链路状态信息和Dijkstra最低开销路径算法。，每台路由器知道整个自治系统的完整拓扑图。 当且仅当链路状态发生变化时，向本AS中所有路由器发送信息（洪泛） RIP仅和相邻节点交换信息 RIP是周期性交换信息，不管链路状态有没有变化 发送的信息就是与本路由器相邻的所有路由器的链路状态，但这只是路由器所知道的部分信息 通过交换信息，所有路由器最终都能建立一个链路状态数据库，事实上就是全网（整个AS）的拓扑图，每个路由器都知道全网的拓扑结构 RIP的路由器仅知道下一跳距离和下一跳路由器 OSPF采用IP数据报进行传送， 相比UDP，报文更简短 OSPF的优点如下： 安全：OSPF路由器之间的交换可以进行认证，只有受信任的路由器才能参与AS内的OSPF协议，从而防止恶意入侵者； 多条相同路径的开销：当到达某目的地的多条路径具有相同的开销时，OSPF允许使用多条路径； 对单播与多播路由选择的综合支持：多播OSPF（MOSPF）提供对OSPF的简单扩展，以便提供多播路由选择； 支持在单个AS中的层次结构：一个OSPF自治系统能够将AS进一步划分为一个个区域，每个区域都允许自己的OSPF链路状态路由选择算法 面向区域，比面向AS的粒度更细 又比面向主机粒度更粗，在洪泛时可以减少链路负载 自治系统间的路由选择 BGP 边界网关协议BGP, 作用顾名思义。 BGP并非要寻找最短路径路由，而是比较好的路由。因为要考虑AS自身的策略（比如，中国这个AS不允许其报文经由外国再转发到本地，哪怕这条路由费用比较奥低） BGP是面向AS的，将AS视为节点 广播和多播路由选择 广播 最简单的实现方式是“N次单播”，缺点是需要发送方知道所有接收方的信息，这需要额外的协议来保证 N次单播会在到第一跳的链路上发送N个副本，效率低下（时间为N）且造成巨大的链路负载，但这可以通过转而让后续节点来生成和转发副本来解决。 第一跳仅需要发一份副本 无控制洪泛 洪泛（flooding）: 源节点向它的所有邻居发送分组副本，当某邻居接收了一个广播分组时，它复制该分组并向它的所有邻居（除了从其接收该分组的那个邻居）转发。 如果图有环，该方法会导致广播分组死循环 受控洪泛 原理：如果节点已经接收并且洪泛了某分组的较早副本，它就不应该继续洪泛该分组 序号控制洪泛 序号控制洪泛： 每个广播分组带有一个广播序号和源节点的地址。节点维护一个接收过的广播分组的广播序号列表。 节点只会复制并转发列表中没有对应序号的分组 反向路径转发 反向路径转发（ Reverse Path Forwarding, RPF ）: 当一台路由器接收到具有给定源地址的广播分组时，仅当该分组到达的链路正好位于它自己的返回其源的最短单播路径上，它才向其所有出链路传输报文；否则，路由器只是丢弃入分组而不向任何它的出链路转发分组 如图，假设粗线是源A到其它接收方的最低费用路径： A先广播一个源为A的分组到节点C, B B将向C, D转发它从节点A接收到的源为A的分组（因为A位于自己到A的最低费用路径上）， B将忽略从其它任何节点接收的源为A的分组 考虑C， C将从A和B接收源为A的分组， 而B不在C自己到A的最短路径上， 因此C将忽略来自B的任何源为A的分组 生成树广播 首先对网络构造出一个最小生成树 实现方法： 定义一个根节点 所有其它节点对根节点单播，直到到达根节点或根节点的生成树内的任意节点 整个单播路径上的所有节点就被加入进生成树 多播 多播数据报采用间接地址来编址， 用来表示一组接收方， 寻址到该组的分组被交付给所有与该组相关联的多播接收方 间接地址是D类地址 多播组：与多播地址相关联的一组接收方 显然多播地址只能用于目的地址，不能用于源地址 对多播数据报不会产生ICMP差错报文， 因此ping一个多播地址将无法收到响应 多播的两个组件： IGMP：将主机加入多播组 多播路由选择协议：负责多播数据报的路由选择 IGMP 与ICMP类似，封装在IP数据报中 IP协议号为2 三种报文类型： menbership_query: 由一台路由器向所有与主机相连的接口发送一个menbership_query报文，以确定该接口上主机已加入的所有多播组集合 路由器周期性地发送menbership_query menbership_report: 主机用一个menbership_report报文来响应menbership_report。 当一个应用程序首次加入一个多播组时，主机会之动向路由器发送menbership_report leave_group: 表明主机离开该多播组，该报文是可选的， 因为IGMP是软状态协议。 通过周期性地发送menbership_query，接收menbership_report来更新状态（指某主机加入了多播组）。 如果长时间没有收到menbership_report，那么该状态通过超时事件被删除 软状态协议， 通过某个端系统发送的周期性报文来更新状态。 状态能够在一次崩溃中丢失，接着自动地由后继的更新报文所恢复 多播路由选择协议 生成树算法，得到一棵包含所有多播组节点（即多播组对应的路由器）的树。 注意到，该树可能会包含一些非多播组节点，它们因此也会承载多播流量 VPN 机构可以为内部的主机自行分配IP地址。RFC1918指定了一些专用地址，只能用于内部网络： 10.0.0.0——10.255.255.255 172.16.0.0——172.31.255.255 192.168.0.0——192.168.255.255 在互联网中的所有路由器，对目的地址是专用地址的数据报一律不进行转发 专用网PN：采用专用地址的网络 虚拟专用网VPN（ Virtual Private Network ）: 用利用互联网来作为本机构各专用网之间的通信载体。。如果专用网的通信必须经过互联网，但又有保密的要求，那么所有通过互联网传送的数据都必须加密 尽管可能要经过互联网，但还是用于机构内部的通信，因此称为专用网 显然，一个机构的不同专用网要在公网上通信，每个专用网之间需要有至少一个代理路由器，具有公网IP 可以认为，VPN和PN的区别就是有公网IP VPN采用隧道技术，组织A内的主机可以采用专用地址与主机B通信，主机B可能在本专用网内，可能在另一个专用网，后一种情况下无非是将A的数据报发给本专用网的代理服务器为其做转发，整个过程对通信的双方透明明 NAT 网络地址转换NAT（Network Address Transition）：将专用地址映射为公用地址， 用于专用网内的主机和互联网上的主机通信 运行NAT的路由器称为NAT代理，显然，如果NAT代理具有n个公网IP，则最多允许n个内网主机联网。 同一时刻，一个内部主机持有一个公网IP，因此当NAT路由器收到互联网上的主机发来的SYN分组（见运输层）时，可以映射表来进行IP地址转换 由于一个公网IP可能会映射不同的内网主机， 外部主机将无法通过NAT代理与内网通信， 即通信只能由内网的主机发起。因此，专用网内部的主机无法当服务器用 这意味着NAT和P2P是矛盾的，因为P2P要求任何参与对等方A应当能够对任何其他参与对等方B发起一条TCP连接 NAPT：同时做IP映射和端口映射， 此时内网主机在代理时可通过端口来区分，哪怕它们映射到一个公网IP NAPT不仅要改变IP，还要改变端口号，后者属于运输层的范畴，因此NAPT不是“纯的”IP层协议。 而且端口号本来是用于进程编址的（详见运输层）， NAPT将其用于主机编址，属于滥用 MPLS 传统的IP网络中， 路由器对分组在转发表中进行最长前缀匹配， 找到下一跳的IP地址，这个过程需要软件计算路由表，速度太慢 转发等价类FEC（ Forwarding Equivalence Class ）: 路由器按照同样方式对待的IP数据报的集合（ 比如，具有相同的源和目的地址 ） 多协议标记交换MPLS（ MultiProtocol Label Switching ）: 给属于同一FEC的IP数据报打上相同的标记，路由器仅仅根据转发表来对打上标记的数据报进行转发， 这里的“路由器”只用到了转发表，没用到路由表，相当于交换机，即整个转发由硬件实现 硬件意味着开销小，且需要人为控制，这也称为流量工程 MPLS首部的位置 由于IPv4 首部没有多余的位置存放MPLS标记，MPLS首部实际上位于第二层和第三层之间: “打标记” 就是在帧首部和IP数据报首部之间插入一个4字节的MPLS首部，具体的标记值是一个整数， 位于“MPLS首部”字段中 .","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"Computer Networking","slug":"Computer-Networking","permalink":"http://lyk-love.cn/tags/Computer-Networking/"}]},{"title":"Unix Toolkit","slug":"Unix-Toolkit","date":"2022-02-19T14:04:37.000Z","updated":"2022-09-26T06:39:34.942Z","comments":true,"path":"2022/02/19/Unix-Toolkit/","link":"","permalink":"http://lyk-love.cn/2022/02/19/Unix-Toolkit/","excerpt":"介绍了Unix, Unix Like主机的常用操作 OS可以是各种LINUX发行版和Mac OSX，因此你可以看到各种包管理工具（yay,apt,yum, brew...）, 选自己用的就好","text":"介绍了Unix, Unix Like主机的常用操作 OS可以是各种LINUX发行版和Mac OSX，因此你可以看到各种包管理工具（yay,apt,yum, brew...）, 选自己用的就好 SSH 配置ssh免密登陆 主机上生成 ssh key： ssh-keygen -t rsa 腾讯云的服务器不会默认生成ssh key， 所以也不会有~/.ssh文件夹， 其他主机也无法通过ssh连接（ssh-copy-id）也不行。 因此腾讯云服务器需要先生成ssh key，才能使用 (这种情况很罕见)如果主机没有安装ssh，需要先安装openssh-server: apt install openssh-server 查看~/.ssh: ❯ ls -l ~/.sshtotal 20-rw-r--r-- 1 lyk lyk 27 2月 10 15:44 config-rw------- 1 lyk lyk 2602 2月 9 17:18 id_rsa-rw-r--r-- 1 lyk lyk 571 2月 9 17:18 id_rsa.pub-rw------- 1 lyk lyk 3926 2月 19 12:58 known_hosts-rw------- 1 lyk lyk 3182 2月 19 12:41 known_hosts.old authorized_keys: 存放远程免密登录的公钥,主要通过这个文件记录多台机器的公钥。(没有则手动创建该文件) id_rsa: 生成的私钥文件 id_rsa.pub: 生成的公钥文件 known_hosts: 已知的主机公钥清单 服务器上配置ssh免密登陆： 将本地 id_rsa.pub 文件的内容拷贝至远程服务器的 ~/.ssh/authorized_keys 如果服务器没有~/.ssh，则需要自己创建 也可以ssh-copy-id user@host 本机到本机的免密登陆 本机到本机也是需要配置免密登陆的，但是root@localhost需要额外配置，/etc/ssh/sshd_config中有一个属性为PermitRootLogin ,默认值no不允许进行密码登录，我们需要将其改为yes.： vim /etc/ssh/sshd_config 然后重启ssh服务: ubuntu： sudo service ssh restart manjaro: systemctl restart sshd.service mac: sudo launchctl load -w /System/Library/LaunchDaemons/ssh.plist ssh会因为各种各样的原因失败，排查ssh问题的命令: sshd -T 如果远程主机的公钥发生了变化, 那么需要重新纪录远程主机的公钥，这需要先把原来的记录清空： ssh-keygen -R [remote-host-ip] User Config 配置用户 详见后文 用户操作 添加用户： sudo adduser [username] 赋予新用户sudo权限 用adduser创建后的新用户是不能使用sudo的，因为还没有赋予相关root权限，执行以下代码赋予权限 sudo usermod -a -G adm usernamesudo usermod -a -G sudo username 或者将用户加到sudoers文件中: #添加sudo文件的写权限,命令是:chmod u+w /etc/sudoers#编辑sudoers文件vim /etc/sudoers#在 root ALL=(ALL) ALL,下面添加lyk ALL=(ALL) ALL lyk ALL=(ALL) ALL#最后删除sudo文件的写权限chmod u-w /etc/sudoers Disk Config fdisk：查看磁盘信息 sudo fdisk -l 卸载分区 #sudo umount [磁盘路径]sudo umount /dev/nvme0n1p11 将分区格式化为ext4类型 #（这里分区为/dev/nvme0n1p11）sudo mkfs.ext4 /dev/sdb1 挂载分区（临时） 挂载分区（ 这里挂载到到/data目录下 ） #sudo mount /dev/sdb1 /[目录名] （目录当然是已存在的）sudo mount /dev/sdb1 /data 这个挂载是临时的，重新开机就会丢失。 如果要开机自动启动挂载， 需要编辑/etc/fstab 查看硬盘和挂载分区等信息 df -h 这样就成功添加了一块硬盘并挂载到/data目录下了， 设置开机自动挂载 查询UUID ls -al /dev/disk/by-uuid#输出为：...... 88e7c2eb-82e6-48c2-a3d8-829c32468f1f -&gt; ../../nvme0n1p11... 可以查到对应分区nvme0n1p11的uuid为88e7c2eb-82e6-48c2-a3d8-829c32468f1f 编辑/etc/fstab(用来存放文件系统的静态信息的文件) sudo vim /etc/fstab 末尾加上UUID=刚刚复制的UUID /data ext defaults 0 0 UUID=刚刚复制的UUID /data ext4 defaults 0 0 Start Using 不仅主机需要安装软件, 有时容器也需要安装某些软件( 尤其是传统Unix命令行工具包). 对于某些非常精简的容器, 可能连ping这样的基本命令都没有，需要手动安装这些基本的命令. 如果OS是Ubuntu（ 云服务器或容器 ）， 需要先： apt-get update Terminal 安装终端模拟器, Zsh, Zsh主题, Zsh插件以及各种美化 Tmux Editor or IDE: 对于服务器而言, 大多数OS预装的vim都够用了. 自用的话还得是nvim Nvim Developing Environment VSCode Package Manager Unix CLI Tools Traditional Unix CLI Tools 各种开发环境: Java-Toolkit Clang/LLVM Toolkit 数据库: Database Toolkit 云原生工具安装: Cloud Native Toolkit 常用操作 clipboard pbcopy on OSX can Copy data from stdin to the clipboard. nvim a# write thoughtful responsecat a | pbcopy# cmd tab# paste to slackrm a xxd xxd - make a hexdump or do the reverse. 查看键盘输入的字符对应的16进制表示: xxd -ps Example 输入 &lt;Ctrl+b&gt; + c，其会显示该输入的 hex codes 为： ^Bc02630a 其中，02 代表 &lt;Ctrl+b&gt;，63 代表 c，而 0a 代表回车键 Aliases alias alias_name=&quot;command_to_alias arg1 arg2&quot; alias是一个command，接受一个参数， 因此=左右不能有空格 example # Make shorthands for common flagsalias ll=&quot;ls -lh&quot;# Save a lot of typing for common commandsalias gs=&quot;git status&quot;alias gc=&quot;git commit&quot;alias v=&quot;vim&quot;# Save you from mistypingalias sl=ls# Overwrite existing commands for better defaultsalias mv=&quot;mv -i&quot; # -i prompts before overwritealias mkdir=&quot;mkdir -p&quot; # -p make parent dirs as neededalias df=&quot;df -h&quot; # -h prints human readable format# Alias can be composedalias la=&quot;ls -A&quot;alias lla=&quot;la -l&quot;# To ignore an alias run it prepended with \\\\ls# Or disable an alias altogether with unaliasunalias la# To get an alias definition just call it with aliasalias ll# Will print ll=&#x27;ls -lh&#x27; alias是面向session的，session关闭也使得alias失效， 要想使alias持久化，可以将其写在shell的启动文件，如 .bashrc or .zshrc clear 清屏： clear 切换tty CTRL + ALT + Fn sudo chvt N # N: tty number, 1 represents the main tty ## 下载```shell# Download the contents of a URL to a file (named &quot;foo&quot; in this case):wget https://example.com/foo# Download the contents of a URL to a file (named &quot;bar&quot; in this case):wget -O bar https://example.com/foo 主机操作 查看主机名： hostname 更改主机名： vim /etc/hostname # 编辑该文件 添加域名映射： vim /etc/hosts 时间 查看时间： date 查看发行版 lsb_release -a 用户操作 创建新用户 sudo adduser [username] 切换用户 sudo su [username] 修改用户密码 sudo passwd user 删除用户 sudo userdel username: 仅仅删除用户，不删除用户的home目录文件 -r: 删除用户的home目录文件 当需要删除用户时可以使用以下指令 查看用户组 cat /etc/group","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://lyk-love.cn/tags/Linux/"}]},{"title":"Linux Basic","slug":"Linux-Basic","date":"2022-02-18T18:47:11.000Z","updated":"2022-09-26T06:39:34.933Z","comments":true,"path":"2022/02/19/Linux-Basic/","link":"","permalink":"http://lyk-love.cn/2022/02/19/Linux-Basic/","excerpt":"Outline: History Hardware File Name Partition BIOS &amp; UEFI File System 介绍了Linux系统的硬件基本知识，更多内容参见我的《OS Introduction》","text":"Outline: History Hardware File Name Partition BIOS &amp; UEFI File System 介绍了Linux系统的硬件基本知识，更多内容参见我的《OS Introduction》 History Unix 的前身是由贝尔实验室(Bell lab.)的 Ken Thompson 利用汇编语言写成的, 后来在 1971-1973 年间由 Dennis Ritchie 以 C 程序语言进行改写,才称为 Unix。 1977 年由 Bill Joy 释出 BSD (Berkeley Software Distribution),这些称为 Unix-like 的操作系统。 1984 年由 Andrew Tanenbaum 开始制作 Minix 操作系统,该系统可以提供原始码以及软件; 1984 年由 Richard Stallman 提倡 GNU 计划,倡导自由软件(Free software), 强调其软件可以『自由的取得、 复制、修改与再发行』 ,并规范出 GPL 授权模式, 任何 GPL(General Public License)软件均不可单纯仅贩卖 其软件,也不可修改软件授权。 1991 年由芬兰人 Linus Torvalds 开发出 Linux 操作系统。简而言之, Linux 成功的地方主要在于:Minix(Unix), GNU, Internet, POSIX 及虚拟团队的产生。符合 Open source 理念的授权相当多,比较知名的如 Apache / BSD / GPL / MIT 等。 Feature Linux是藉由 Minix 操作系统开发的,， 属于 Unix like ,没有版权纠纷 Linux 支持 POSIX ,因此很多 Unix 上的程序可以直接在 Linux 上运作 Hardware File Name Linux中一切皆文件，磁盘文件名形如/dev/sd[a-z]，虚拟机可能会使用 /dev/vd[a-z] Partition 分区就是对分区表进行配置，通常分区是以Cylinder为单位的『连续』磁盘空间。 现代也可以用sector为单位 分区表有两种格式： MBR, GPT MBR MBR磁盘分区是一种使用最为广泛的分区结构，它也被称为DOS分区结构，但它并不仅仅应用于Windows系统平台，也应用于Linux，基于X86的UNIX等系统平台。它位于磁盘的0号扇区（一扇区等于512字节），是一个重要的扇区（简称MBR扇区） 主要分区与延伸分区最多可以有四个(硬盘的限制) 延伸分区最多只能有一个(操作系统的限制) 逻辑分区是由延伸分区持续切割出来的分区槽; 能够被格式化后,作为数据存取的分区槽为主要分区与逻辑分区。延伸分区无法格式化; 逻辑分区的数量依操作系统而不同,在 Linux 系统中 SATA 硬盘已经可以突破 63 个以上的分区限制; 0号扇区结构 在MBR分区表中，一个分区最大的容量为2T，且每个分区的起始柱面必须在这个硬盘的前2T内。你有一个3T的硬盘，根据要求你至少要把它划分为2个分区，且最后一个分区的起始扇区要位于硬盘的前2T空间内。如果硬盘太大则必须改用GPT 标准MBR结构： MBR扇区(0号扇区)由四部分组成： 主引导记录（MBR）：一段引导代码，占MBR分区的前446字节，负责整个系统启动。如果引导代码被破坏，系统将无法启动。 Windows磁盘签名：占引导代码后面的4字节，是Windows初始化磁盘写入的磁盘标签，如果此标签被破坏，则系统会提示“初始化磁盘”。 MBR分区表：4个16字节的“磁盘分区表”(DPT), 可以分出四个主分区 所谓的『分区』就是配置分区表 可以将一个主分区作为扩展分区， 扩展分区可以继续分出逻辑分区 MBR结束标志：占MBR扇区最后2个字节，一直为“55 AA”， 检验主引导记录是否有效 主分区（MBR） 分区表示例，假设该硬盘的挂载文件名为/dev/sda ， 分区的挂载文件名为[硬盘文件名][分区id] 分区的文件名为硬盘名 + 后缀 例如，上述四个主分区的挂载文件名为： P1:/dev/sda1 P2:/dev/sda2 P3:/dev/sda3 P4:/dev/sda4 一个硬盘，只能有4个主分区， （恢复分区也占一个分区） （微软引导占一个） （微软OS占一个C盘分区） （剩下的一个主分区可以做扩展分区，做出最多4个逻辑分区） 在这样的情况下，是没法双系统的，因为4个主分区已经用完了 MBR用32位寻址，每个bit代表512字节，因此只能支持2T硬盘 扩展分区和逻辑分区 示例： 这里有两个主分区P1,P2, P2被用作扩展分区 主分区最多有4个，为了获得更多的分区，可以将一个主分区变成扩展分区，在扩展分区内进行逻辑分区 利用扩展分区的第一个扇区，可以分出逻辑扇区， 逻辑分区的分区信息存放在扩展引导记录（EBR）中 这里的EBR仅仅指EBR分区， 包括分区表和结束标志“55 AA”，没有引导代码 逻辑分区位于扩展分区内，其磁柱范围也当然在扩展分区的磁柱范围内，上例中就是101-401 最多有1个扩展分区（OS规定） [分区id]的前4都保留给主分区和扩展分区。 因此逻辑分区的分区id从5开始。上例中各分区文件名如下： P1:/dev/sda1 P2:/dev/sda2 L1:/dev/sda5 L2:/dev/sda6 L3:/dev/sda7 L4:/dev/sda8 L5:/dev/sda9 逻辑分区的数量依操作系统而不同, 在 Linux 系统中 SATA 硬盘已经可以突破 63 个以上的分区限制 GPT 全局唯一标识分区表（GUID Partition Table） 可以有很多主分区，也就不需要扩展分区和逻辑分区了 最大硬盘容量9.4ZB 缺点是浪费更多的磁盘空间 GPT分区表结构 简言之，前512字节保留，用于MBR。后512字节用于GPT Header，然后是分区表， 分区表在磁盘尾部又会备份一遍 LBA0: MBR相容区块 前512字节（0号扇区）有个保护MBR（用于防止不识别GPT的硬盘工具错误识别并破坏硬盘中的数据），这个MBR中只有一个类型为0xEE的分区，以此来表示这块硬盘使用GPT分区表。不能识别GPT硬盘的操作系统通常会识别出一个未知类型的分区，并且拒绝对硬盘进行操作；能够识别GPT分区表的操作系统会检查保护MBR中的分区表，如果分区类型不是0xEE或者MBR分区表中有多个项，也会拒绝对硬盘进行操作 LBA1： GPT表头记录 GPT头位于GPT磁盘的第二个磁盘，也就是1号扇区，该扇区是在创建GPT磁盘时生成，GPT头会定义分区表的起始位置，分区表的结束位置、每个分区表项的大小、分区表项的个数及分区表的校验和等信息。 LBA2～33： 实际记录分区信息处 分区表位于GPT磁盘的2-33号磁盘，一共占用32个扇区，能够容纳128个分区表项。每个分区表项大小为128字节。因为每个分区表项管理一共分区，所以Windows系统允许GPT磁盘创建128个分区 LBA34到-34： 分区区域 GPT分区区域就是用户使用的分区，也是用户进行数据存储的区域。分区区域的起始地址和结束地址由GPT头定义。 LBA-33~-2: 分区表备份 LBA-1： GPT头备份 GPT头有一个备份，放在GPT磁盘的最后一个扇区，但这个GPT头备份并非完全GPT头备份，某些参数有些不一样。复制的时候根据实际情况更改一下即可。 BIOS &amp; UEFI 计算机启动时，首先加载硬件驱动程序， 硬件驱动程序有： BIOS：对应分区格式MBR， 读取MBR分区 BIOS模式又称为Legacy UEFI： 对应分区格式GPT， 读取EFI分区 BIOS BIOS是写入到主板上的一个程序。主板上还有硬件CMOS， 是记录各项硬件参数且嵌入在主板上的储存器； BIOS是开机时,计算机会执行的第一个程序 UEFI UEFI (Unified Extensible Firmware Interface) 是BIOS的进化版，也称为UEFI BIOS UEFI对应分区格式GPT, 启动后读取EFI分区（EFI system partition， aka ESP） EFI是UEFI的1.0版本 UEFI用C编程，BIOS用汇编编程。 因此UEFI非常强大 BIOS缺点： BIOS不知道GPT,还需要GPT 提供兼容模式才能够读写这个磁盘装置 BIOS仅为 16 位的程序，功能太简单 UEFI 可以直接取得 GPT 的分区表,但保险起见，你最好依旧拥有BIOS boot的分区槽支持, 同时,为了与 windows 兼容,并且提供其他第三方厂商所使用的 UEFI 应用程序储存的空间,你必须要格式化一个 vfat 的文件系统, 大约提供 512MB 到 1G 左右的容量,以让其他 UEFI 执行较为方便 ESP EFI系统分区（EFI system partition）:GPT硬盘分区模式中的系统启动分区 FAT16或FAT32格式的物理分区， 其分区标识是EF (十六进制) 而非常规的0E或0C。 该分区在Windows操作系统下一般是不可见的。 ESP分区是一个独立于操作系统之外的分区，操作系统被引导后就不再依赖它。分区内存放引导管理程序、驱动程序、系统维护工具等。支持 EFI 模式的电脑需要从ESP启动系统，EFI固件可从ESP加载EFI启动程序和应用程序 boot loader boot loader用于加载OS内核， 由于每种OS的文件系统不一致，因此每种OS都有自己的boot loader boot loader位于MBR中，最大只有446字节 常用的boot loader是grub（version2） 位置 每个文件系统都会保留一块启动扇区（boot sector）来安装该OS的boot loader， 即OS都会默认安装一份boot loader到自己的文件系统中（位于根目录所在的文件系统的boot sector） LInux安装时，可以选择将boot loader安装到MBR，如果安装了，则MBR和boot sector都会保留一份boot loader Windows安装时默认会将boot loader也安装到MBR 功能 boot loader主要功能： 加载内核文件：直接指向可开机的程序区段来启动OS 提供选项：用户可以选择不同启动选项 转交其他loader： 将启动管理功能转交给其他loader 选项功能使得我们可以选择不同的内核来启动，而转交功能使我们可以加载其他地方（也就是其他boot sector）的loader 多boot loader ![image-20220504012023432](/Users/lyk/Library/Application Support/typora-user-images/image-20220504012023432.png) 如上图所示，我的 MBR 使用 Linux 的 grub2 这个开机管理程序，并且里面假设已经有了三个菜 单， 第一个菜单可以直接指向 Linux 的核心文件并且直接载入核心来开机;第二个菜单可以将开机管理程 控权交给 Windows 来管理，此时 Windows 的 loader 会接管开机流程，这个时候他就能够启动 windows 了。 第三个菜单则是使用 Linux 在 boot sector 内的开机管理程序，此时进入另一个grub 多重系统 windows的loader不具有转交功能，因此不能使用windows的loader启动linux的loader，也就是说，装多系统的时候，需要先装windows，再装linux 前文已提到，windows的boot loader会自动覆盖MBR扇区，那么如果后安装windows，启动扇区就被覆盖为windows的loader，而它无法转交给其他loader，即无法支持多系统 A boot loader loads and starts the Linux kernel Can pass boot parameters to the Linux kernel, such as device information Can optionally load an Initial Root Disk Can boot other operating systems as well  Common Boot loaders: LILO: Linux Loader GRUB: Grand Unified Boot Loader  Generally configured in /dev/sda, unless other boot loader is usd（存疑） LILO LILO: A Program that configures the MBR according to the configuration file. Must be run as root with the lilo command. lilo command Syntax: lilo [-v] [-v] [-C config-file] [-t] Configuration file: /etc/lilo.conf GRUB 目前都是grub2 grub是主流的boot loader程序，由于MBR扇区太小，最大才446字节，因此grub的配置文件没有放在MBR，而是放在/boot/grub 详见《GRUB》 GRUB Program stored in MBR (first stage) and in /boot/grub (1.5th and second stage) Understand file system structure; no need to activate a configuration as with LILO  Configuration file /boot/grub/grub.conf Installed in MBR with grub-install kernel 内核文件:/boot/vmlinuz 内核源代码：/usr/src/linux 模块 内核模块：/lib/modules/$(uname -r)/kernel 模块文件都以.ko为后缀 Linux发行版一般都会将非必要的且可以编译为模块的内核功能编译成模块， 比如各种设备驱动程序. linux内核可以动态加载内核模块，内核接管系统后，会尝试检测硬件并挂载根目录，来取得内核模块，这样才能利用它们提供的设备加载功能 不能把/lib和/放在不同的硬盘分区 由于担心影响到磁盘内的文件系统，启动过程中根目录一般以只读的形式挂载 因此，如果遇到OS不支持的新硬件，要么重新编译内核，并加入该硬件的驱动程序源码； 要么将该硬件的驱动程序编译成模块，在启动时加载该模块 模块依赖 一个模块A引用另一个模块B所导出的符号，则加载模块A之前必须先加载模块B，这称为模块依赖 模块依赖关系存放在/lib/modules/$(uname -r)/modules.dep， 要生成该文件，需要用depmod depmod depmod [-adeisvV][-m &lt;文件&gt;][--help][模块名称] 不加任何参数：分析当前内核的模块依赖(模块位于/lib/modules/$(uname -r)/kernel)，并重新写入modules.dep -A: 查找比 modues.dep 内还要新的模块，找到了才会更新modules.dep -a: 检查所有模块，如果命令中没有文件名称，这个选项默认是开启的 -e: 显示出目前已加载的不可执行的模块 -n: 将结果 modules.dep 和各种映射文件输出到标准输出( stdout )，而不是写到modules.dep 模块操作 查看模块 列出模块 &gt; lsmodModule Size Used byip6table_filter 16384 0ip6_tables 32768 1 ip6table_filterxt_recent 24576 0binfmt_misc 24576 1 显示内容有： 模块名称 模块大小 依赖该模块的模块 查看模块信息： modinfo kernel_module 参数可以是模块名，也可以是模块文件名 加载/删除模块 加载/删除模块推荐使用： modprobe [模块名] 查找modules.dep的内容，得到模块依赖性，并加载模块 选项有： 默认是加载模块 -c： 列出目前系统所有的模块（信息更多） -f: 强制加载该模块 -r: 删除模块 普通方法：不会分析依赖树，因此很比较麻烦 加载模块：insmod [模块文件名] 删除模块: rmmod [模块名/模块文件名] 模块与普通程序的区别 C程序 Linux模块 所在空间 user mod kernel mode 入口 main() module_init()指定; 出口 无 module_exit()指定; 运行方式 直接运行 inmod 调试方式 gdb kdbug, kdb, kgdb等 注意点： 不能使用C库来开发模块 没有内存保护机制 小内核栈 要考虑并发 模块example #include &lt;linux/kernel.h&gt;#include &lt;linux/module.h&gt;#include &lt;linux/init.h&gt;static int __init hello_init(void) &#123; printk(KERN_INFO &quot;Hello world\\n&quot;); return 0; &#125;static void __exit hello_exit(void) &#123; printk(KERN_INFO &quot;Goodbye world\\n&quot;); &#125;module_init(hello_init); module_exit(hello_exit); static int __init hello_init(void) static void __exit hello_exit(void) Static：声明该函数作用域为当前文件，因为这种函数在特定文件之外没有其它意义 __init: 标记该函数只在初始化期间使用。模块装载后，将 该函数占用的内存空间释放 – __exit: 标记该代码仅用于模块卸载 Init/exit 宏: module_init/module_exit 声明模块初始化及清除函数所在的位置 装载和卸载模块时，内核可以自动找到相应的函数 module_init(hello_init); module_exit(hello_exit); 模块传参 有些模块需要传递一些参数 参数在模块加载时传递 nsmod hello.ko test=2 在模块代码中， 需要使用module_param宏来声明: module_param(变量名称，类型, 访问许可掩码) 支持的参数类型 Byte, short, ushort, int, uint, long, ulong, bool, charp Array (module_param_array(name, type, nump, perm)) example #include &lt;linux/kernel.h&gt; #include &lt;linux/module.h&gt; #include &lt;linux/init.h&gt;#include &lt;linux/moduleparam.h&gt;static int test; module_param(test, int, 0644);//参数声明static int __init hello_init(void) &#123; printk(KERN_INFO “Hello world test=%d \\n” , test); return 0;&#125;static void __exit hello_exit(void) &#123; printk(KERN_INFO &quot;Goodbye world\\n&quot;); &#125;MODULE_LICENSE(&quot;GPL&quot;); MODULE_DESCRIPTION(&quot;Test&quot;); MODULE_AUTHOR(&quot;xxx&quot;); module_init(hello_init); module_exit(hello_exit); 模块导出符号 如果一个模块需要向其他模块导出符号(方法或全局变量)，需要使用: EXPORT_SYMBOL(name); EXPORT_SYMBOL_GPL(name); 符号必须在模块文件的全局部分导出，不能在函数部分导出。 更多信息可参考 &lt;linux/module.h&gt;文件 • Modules仅可以使用由Kernel或者其他Modules导出的符号, 不能使用Libc • /proc/kallsyms : 保存了所有导出的符号 模块通信example 本实例通过两个模块来介绍模块之间的通信。 模块add_sub提供了两个导出函数add_integer() 和sub_integer()，分别完成两个数字的加法和减 法。模块test用来调用模块add_sub提供的两个 方法，完成加法或者减法操作。 1.add_sub模块 2.test模块 • 3.编译模块 add_sub.h： #ifndef _ADD_SUB_H#define _ADD_SUB_Hlong add_integer(long a,longb);long sub_integer(long a, long b) ;#endif add_sub： #include &lt;linux/ init.h&gt;#include&lt;l inux /module.h&gt;#include &quot;add_sub.h&quot;long add_integer(int a, int b)&#123; return a+b;&#125;long sub integer (int a, int b)&#123; return a-b;&#125;EXPORT SYMBOL(add_integer) ;EXPORT SYMBOL(sub_integer) ;MODULELICENSE (&quot;Dual BSD/GPL&quot;) ; 模块编译 Makefile: obj-m := hello.oall: make -C /lib/modules/$(shell uname -r)/build M=$(shell pwd) modulesclean: make -C /lib/modules/$(shell uname -r)/build M=$(shell pwd) clean Module includes more files obj-m:=hello.ohello-objs := a.o b.o Initial RAM Filesystem 由于内核启动时需要挂载根目录，来加载其中的设备驱动程序。 假设根目录所在的设备为SATA硬盘，linux需要SATA的驱动才能读取SATA盘并挂载根目录，可是取得SATA的驱动又需要先读取SATA盘，挂载根目录。 为了解决这个问腿，boot loader不仅会读取内核文件， 还会读取一个虚拟文件系统文件(/boot/initramfs.img， 名称在不同的OS中可能不同 )，该文件会被读取到内存中并解压缩成一个根目录，并提供一个程序，通过该程序来加载启动过程中所需要的内核模块（比如上文提到的SATA驱动），然后initramfs会被释放，并挂载实际的根目录文件系统，接着内核会调用systemd来开始后续的正常启动流程 当然你可以将必须的驱动直接编译到内核中，也就避免了上述的矛盾，不需要initramfs了 initrd: mkinitrd /boot/initrd.img $(uname r) initramfs: mkinitramfs o /boot/initrd.img $(uname -r) update-initramfs -u Linux设备 Linux系统将设备分为3种类型，对应三种驱动 字符设备 -- Character Driver 块设备 -- Block Driver 网络接口设备 -- Network Driver 设备驱动的加载过程 以字符设备驱动为例，它的加载过程是： 申请设备号：包括主设备号码和次设备号 定义文件操作结构体file_operations 创建并初始化定义结构体cdev cdev结构体描述字符设备 该结构体是所有字符设备的抽象，其包含了大量字符设备所共有的特性。 将cdev注册到系统，并和对应的设备号绑定 在/dev文件系统中用mknod创建设备文件，并将该文件绑定到设备号上 设定设备号:device=scull 定义主设备号:major=15 用户可以通过访问/dev/scull来访问当前的驱动设备 mknod /dev/$&#123;device&#125;0 c $major 0 mknod mknod: Create block or character device special files. Create a block device: sudo mknod path/to/device_file b major_device_number minor_device_number Create a character device: sudo mknod path/to/device_file c major_device_number minor_device_number Create a FIFO (queue) device: sudo mknod path/to/device_file p Create a device file with default SELinux security context: sudo mknod -Z path/to/device_file type major_device_number minor_device_number example: mknod /dev/zero15 c 1 5 意思是创建一个字符型设备文件/dev/zero15 , 设备号为：主设备号5， 副设备号1 设备号 一个字符设备或者块设备都有一个主设备号和次设备号 • 主设备号和次设备号统称为设备号。 主设备号用来表示一个特定的驱动程序 • 次设备号用来表示使用该驱动程序的各设备 申请和释放设备号 int register_chrdev_region(dev_t first, unsigned int count, char *name);int alloc_chrdev_region(dev_t *dev, unsigned int firstminor, unsigned int count, char *name);void unregister_chrdev_region(dev_t first, unsigned int count); cdev结构体 在linux内核中使用cdev结构体来描述 字符设备。该结构体是所有字符设备的抽象，其包含了大量字符设备所共有的特性 cdev结构体的初始化: struct cdev *my_cdev = cdev_alloc();my_cdev-&gt;ops = &amp;my_fops;void cdev_init(struct cdev *cdev, struct file_operations *fops);struct scull_dev &#123; struct scull qset *data; /* Pointer to first quantum set */ int quantum; /* the current quantum size */ int qset; /* the current array size */ unsigned long size; /* amount of data stored here */ unsigned int access_ key; /* used by sculluid and scullpriv */ struct semaphore sem; /* mutual exclus ion semaphore */ struct cdev cdev;/* Char device structure*/&#125;; 设备注册 将设备注册到系统中: int cdev_add(struct cdev *dev, dev_t num, unsigned int count); 释放一个已经注册的设备: void cdev_del(struct cdev *dev); /proc /proc文件系统是内核模块和系统交互的两种主要方式之一 /proc文件系统是一个伪文件系统。 通过/proc，可以用标准Unix系统调用(比如open()、 read()、write()、 ioctl()等等)访问进程地址空间 用户和应用程序可以通过/proc得到系统的信息，并可以改变内核的某些参数 可以用于调试程序或者获取指定进程状态 create_proc_entry(): 创建一个文件 proc_symlink();: 创建符号链接 proc_mknod(): 创建设备文件 proc_mkdir(): 创建目录 remove_proc_entry(): 删除文件或目录 OS启动过程 从开机到启动OS的过程为： BIOS:开机时主动加载，BIOS会通过加载CMOS中的信息，得到主机的硬件配置信息（包括启动设备的查找顺序、硬盘的大小与类型、系统时间...）; 得到这些信息后，BIOS会进行启动自我检测（ Power-on Self Test, POST ）, 然后开始执行硬件的初始化、设置PnP设备，再定义出可启动的设备顺序，接下来开始读取启动设备的MBR，其中安装了boot loader程序 这里的MBR泛指启动扇区，GPT也有MBR扇区 MBR： BIOS读取MBR，注意，每块硬盘的第一个扇区都是MBR扇区，所以如果有多个硬盘，也就是多个启动设备的话，读取的应该是“第一个启动设备的MBR”（ 之前已经定义好了启动设备的顺序） boot loader： BIOS加载MBR的boot loader程序，该程序用于读取kernel，将kernel解压缩到内存中; boot loader同时还会读取initramfs来加载一些必要的驱动，并挂载根目录 BIOS通过硬件的INT13中断来读取MBR，因此，只要BIOS能检测到硬盘，就一定能读取MBR（其中的内容就是boot loader） 内核驱动位于根目录的/lib/modules，则内核需要先挂载根目录才能取得这些驱动，但是内核没有驱动又无法读取硬盘、挂载根目录。 因此boot loader除了读取内核，还要读取initramfs，后者会加载必要的驱动，来挂载根目录 kernel + initramfs：kernel先通过initramfs来加载必要的驱动程序，挂载根文件系统，并加载其中的驱动程序，然后测试硬件，获取硬件信息，此时硬件已经准备就绪了 kernel使用自己的检测程序测试硬件， 不一定使用BIOS检测到的信息。这意味着内核此时已经接管BIOS的工作 systemd：内核调用第一个程序systemd VFS VFS（Virtual Filesystem Switch）：虚拟文件系统或虚拟文件系统转换，是Linux文件系统的虚拟化，位于内核态，将底层异构的文件系统转换为统一的文件系统，使得可以通过统一的Posix接口访问 作为文件系统的抽象，VFS只存在于内存 VFS起源于Unix， 所以VFS适用于所有Unix like OS VFS的整体组织与前文提到的VVFS相同，分为超级块、inode等 分层 VFS在整个Linux系统中的分层视图如下： Linux系统的User使用GLIBC（POSIX标准、GUN C运行时库）作为应用程序的运行时库，然后通过OS转换为系统调用SCI（system-call interface），SCI是操作系统内核定义的系统调用接口，这层抽象允许用户程序的I/O操作转换为内核的接口调用。VFS提供了一个抽象层，将POSIX API接口与不同存储设备的具体接口实现进行了分离，使得底层的文件系统类型、设备类型对上层应用程序透明。 接口适配示例 用户写入文件时，使用POSIX标准的write接口，然后陷入kernel mode，调用sys_write()系统调用（属于SCI层）。然后VFS层接受到该调用，转换为对给定文件系统、给定设备的操作 跨设备/文件系统示例 下面中，用户通过cp命令进行文件拷贝，用户不需要关心底层文件系统的实现，只需要通过VFS抽象层实现对不同文件系统的读写操作： VFS支持的系统调用 上述示例中提到VFS也有自己的文件模型，用来支持操作系统的系统调用。下面是VFS抽象模型支持的所有Linux系统调用： 文件系统相关：mount, umount, umount2, sysfs, statfs, fstatfs, fstatfs64, ustat 目录相关：chroot，pivot_root，chdir，fchdir，getcwd，mkdir，rmdir，getdents，getdents64，readdir，link，unlink，rename，lookup_dcookie 链接相关：readlink，symlink 文件相关：chown， fchown，lchown，chown16，fchown16，lchown16，hmod，fchmod，utime，stat，fstat，lstat，acess，oldstat，oldfstat，oldlstat，stat64，lstat64，lstat64，open，close，creat，umask，dup，dup2，fcntl， fcntl64，select，poll，truncate，ftruncate，truncate64，ftruncate64，lseek，llseek，read，write，readv，writev，sendfile，sendfile64，readahead VFS支持的文件系统 Disk-based 文件系统：Ext2, ext3, ReiserFS，Sysv, UFS, MINIX, VxFS，VFAT, NTFS，ISO9660 CD-ROM, UDF DVD，HPFS, HFS, AFFS, ADFS, Network 文件系统：NFS, Coda, AFS, CIFS, NCP 特殊文件系统：/proc，/tmpfs等 Linux目录结构 /bin:系统的二进制文件，比如各种命令 /boot： 包含了启动所需的文件，如boot loader /dev: 设备对应的虚拟文件 /etc: 系统和软件的配置文件 /lib： 必要的共享库文件和内核模块 /media: 外部设备通用挂载点的父目录 /mnt: 临时文件系统的挂载点的父目录 /opt：额外的软件包安装目录 /sbin:只有管理员可以使用的命令的二进制文件，是与系统相关的命令，如reboot,shutdown等 /srv: 系统提供的有关服务的数据 /usr: Unix System Resources, 用于存放共享、只读的数据，子目录包括/bin, /etc, /lib, /tmp等，与根目录下的同名目录相比，/usr下的目录的数据是用于用户安装的软件的，而不是系统自带的。 还有/include, /src等目录，存放系统编程所需的头文件和源码等 /home: 用户的家目录的父目录 /root: root用户的家目录 包管理工具 以apt-get为例，Ubuntu采用集中式软件仓库，apt从/etc/apt/sources.list中找到镜像站点地址, 从package.gz中获取所有包信息。 apt在本地存有一份软件包的信息索引，可以检测软件列表和软件依赖 apt在安装时安装软件和相应依赖 File System Windows 普通磁盘： NTFS 比NTFS更早：fat32（磁盘最大32G,单个文件最大4G）, fat16 U盘：exfat Linux ext1,2,3,4 如果不指定挂载，那么子目录就在父目录所在的分区上","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://lyk-love.cn/tags/Linux/"}]},{"title":"Machine Learning Intro","slug":"Machine-Learning-Intro","date":"2022-02-15T21:27:44.000Z","updated":"2022-09-26T06:39:34.934Z","comments":true,"path":"2022/02/16/Machine-Learning-Intro/","link":"","permalink":"http://lyk-love.cn/2022/02/16/Machine-Learning-Intro/","excerpt":"Outline 基本术语 模型评估和选择","text":"Outline 基本术语 模型评估和选择 Basic Term Data 令$D = { x_1,x_2,..,x_m }$表示包含m个示例的数据集，每个示例由d个属性描述，则每个示例$x_i={ x_{i1}, x_{i2}, \\dots, x_{id} }$是d维样本空间中的一个向量，$x_i \\in \\chi$, 其中$x_{ij}$是样本$x_i$在第j个属性的取值 上面的每条record就是一个示例（instance）或样本（ sample ） 上面的每个field称为一个属性（ attribute ）或特征（ feature ）,属性的取值就是属性值 属性空间（attribute value）、样本空间( sample space )、输入空间： 属性张成的空间，记为$\\chi$ 标记空间、输出空间： 标记张成的空间，记为 $\\gamma $ 如果把属性作为坐标轴，每个示例都对应特征空间的一个点，称为特征向量（ feature vector ）， 属性个数就是样本维度 样例（ Example ）: 示例+标记, 用$(x_i,y_i)$表示第i个样例，其中$y_i \\in \\gamma$是示例$x_i$的标记， $\\gamma $ 是所有标记的集合，也就是输出空间 如果把标记看作对象本身的一部分，那么示例也可以看作样本 训练集: 一组训练样例 测试集: 一组测试样例 Task 根据标记的取值情况 分类任务:标记为离散值 二分类:例如(好瓜,坏瓜)(正类,反类)(+1,-1) 多分类:例如(冬瓜,南瓜,西瓜) 回归任务:标记为连续值 例如,瓜的成熟度S 聚类任务:标记为空值,对示例进行自动分组( 即，要分的组也不知道) 例如,本地瓜,外地瓜 建立一个从输入空间 $\\chi$到输出空间$\\gamma$的映射：$f : \\ x \\rightarrow y$, 对二分类任务，通常令$\\gamma = {-1,+1}$ 或${0,1}$；对多分类任务， $|\\gamma|=2$; 对回归任务， $\\gamma = R$ 根据标记的完整情况 (有)监督学习:所有示例都有标记 分类、回归 无监督学习:所有示例都没有标记 聚类 半监督学习:少量示例有标记,大量示例没标记 噪音标记学习:标记有,但是不完全准确 Goal 机器学习技术的根本目标就是泛化能力 最理想的机器学习技术是学习到概念(人类学习,可理解的) 比如，“好瓜是某种色泽，某种根蒂，某种敲声的瓜“这样的概念，但是，未来是不确定的，你可能遇到没见过的瓜 但是现实中很困难,因此很多时候机器学习采用的是黑盒模型 假设空间 假设空间( hypothesis space )：所有假设组成的空间。 学习就是在假设空间中搜索的过程，搜索目标是找到与训练集匹配的假设 版本空间（ version space ）： 假设空间的子集，与训练集一致的假设集合 归纳偏好：学习过程中对某种类型假设的偏好称作归纳偏好 ”到底从版本空间中选择哪一个假设“ 可以认为是假设空间（事实上是版本空间）中对假设进行选择的启发式方法 真实假设可以不在版本空间中，因为版本空间只是符合了训练集的假设 NFL定理 No Free Lunch： 因为未来数据是不知道的,总有一种未来数据的分布让你失败 模型评估与选择 误差 “错误率”： 分类错误的样本数占样本总数的比例 精度： ，1 - 错误率 误差： 学习器的实际预测输出与样本的真实输出之间的差异 训练误差或经验误差： 学习器在训练集上的误差 测试误差或泛化误差： 学习器在新样本上的误差 过拟合&amp; 欠拟合 过拟合： 模型把训练样本学得“太好”时，把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质。这样会致使泛化性能的下降 欠拟合：对训练样本的一般性质尚未学好。 评估方法 我们可以通过实验测试来对学习器的泛化误差进行评估并进而做出选择。 为此要选出测试集，然后以测试集上的“测试误差”来作为泛化误差的近似。 测试集要尽可能与训练集互斥 数据集规模有限，要分为训练集和测试集，有如下划分方法： 留出法： 直接将数据集划分为两个互斥集合 测试/训练集要尽可能保持数据分布的一致性 交叉验证法 自助法 性能度量 性能度量是衡量模型泛化能力的评价标准，反映了任务需求 回归任务： 均方误差 分类任务： 错误率，精度 查准率，查全率 下面主要介绍回归任务的性能度量 错误率和精度 错误率是分类错误的样本数占样本总数的比例， 精度相反 查准率，查全率和F1 查准率：预测出来的正例中正确的比率（分母是被认为为真的样例数） 查全率：正例被预测出来的比率（ 分母是确实为真的样例数 ） 混淆矩阵：真实标记和预测结果的组合： 注意FP被认为是正例，实际是反例 可以根据学习器的预测结果对样例进行排序，并逐个把样本作为正例进行预测，则可以得到查准率-查全率曲线，简称“P-R”曲线 如果一个学习器的P-R曲线被另一个的完全包住，我们可以认为前者的性能小于后者，但有时会发生曲线交叉的情况，此时需要设计综合考虑P,R的度量，比如“平衡点” 平衡点（Break-Even Point）是曲线上“查准率=查全率”时的取值，可用于度量P-R曲线有交叉的分类器性能高低 BEP太简单，更常用的是F1度量： $$ F1 = \\frac{2 \\times P \\times R}{P + R} $$ 比F1更一般形式的 $F_{\\beta}$: $$ F_{\\beta} = \\frac{ 1 + \\beta^2 \\times P \\times R}{ ( \\beta^2 \\times P ) + R } $$ $\\beta$是启发值，可以表示模型对查准/查全率的偏好 $\\beta = 1$: 标准 F1 $\\beta &gt; 1$:偏重查全率 $\\beta 《 1$: 偏重查准率 ROC ROC曲线纵轴是“真正例率”，横轴是“假正例率” $$ TPR = \\frac{ TP }{TP + FN} \\ FPR = \\frac{FP}{TN + FP} $$ 如果一个学习器的ROC曲线被另一个的完全包住，我们可以认为前者的性能小于后者，但有时会发生曲线交叉的情况，此时可以根据曲线下的面积大小进行比较，也就是AUC( Area Under Curve ) AUC 假设AUC曲线由${(x_1,y_1), (x_2,y_2), \\dots, (x_m,y_m) }$ 的点按序连接而形成，则AUC可估算为： $$ \\mathrm{AUC} = \\frac{1}{2} \\sum\\limits_{i=1}^{m-1}(x_{i+1} - x_i)(y_i + y_{i+1}) $$ AUC 衡量了样本预测的排序质量 $$ \\mathrm{AUC} = 1 - l_{rank} $$ 代价错误敏感率 为了权衡不同类型错误所造成的不同损失，为错误赋予“非均等代价”（unequal cost）： 以二分类为例：可设定一个代价矩阵（cost matrix） 真实类别 预测 第0类 结果 第1类 第0类 0 $cost_{01}$ 第1类 $cost_{10}$ 0 其中用$cost_{ij}$表示将第$i$类预测为第$j$类的代价 在非均等代价下，我们所希望的不再是简单地最小化错误的次数，而是希望最小化总体代价（total cost） （以上面二分类为例0为正例，1为负例）代价敏感率（cost-sensitive）错误率为： $$ E(f ; D ; \\cos t)=\\frac{1}{m}\\left{\\sum_{x_{i} \\in D^{+}}\\left|\\left(f\\left(x_{i}\\right) !=y_{i}\\right)^{*} \\cos t_{01}+\\sum_{x_{i} \\in D^{-}}\\right|\\left(f\\left(x_{i}\\right)==y_{i}\\right) * \\cos t_{10}\\right} $$ $D$:数据集 $D^+$ ：正样本数据集 $D^-$:负样本数据集 若令$cost_{ij}$的取值不限于0,1,则可以定义出多任务分类的代价敏感性能度量 代价曲线 在非均等代价下，ROC曲线不能直接反映出学习器的期望总体代价，可以用“代价曲线” 代价曲线图的横轴是取值为[0, 1]的正例概率代价, $$ P(+) \\cos t=\\frac{p^{} \\cos t_{01}}{p^{} \\cos t_{01}+(1-p) \\cos t_{10}} $$ $p$：样例为正例的概率 纵轴为取值[0, 1]的归一化代价: $$ \\operatorname{cost}{n o r m}=\\frac{F N R * P * \\cos t{01}+F P R (1-P) * \\cos t_{10}}{p^{} \\cos t_{01}+(1-p) * \\cos t_{10}} $$ 比较检验 二项检验 T检验 交叉验证 T-检验 偏差与方差 对测试样本x,令$y_D$为$x$在数据集中的标记，$y$为$x$的真实标记, $f(x;D)$为训练集 $D$上学得模型$$在$x$上的预测输出。 以回归任务为例，学习期望为： $$ \\bar f(x) = E_D [ f(x;D) ] \\ $$ 使用样本数目相同的不同训练集产生的方差为： $$ var(x) = E_D [ ( f(x;D) - \\bar f(x) )^2] $$ 噪声为： $$ \\epsilon^2 = E_D [ (y_D - y)^2 ] $$ 偏差定义为期望输出与真实标记的差别： $bias^2 = ( \\bar f(x) - y )^2$, 为方便讨论，我们假设噪声的期望为0 ， 即$E_D [ (y_D - y)^2 ] = 0$. 对泛化误差分解： $$ \\begin{aligned} E(f ; D)=&amp; \\mathbb{E}{D}\\left[\\left(f(\\boldsymbol{x} ; D)-y{D}\\right)^{2}\\right] \\ =&amp; \\mathbb{E}{D}\\left[\\left(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x})+\\bar{f}(\\boldsymbol{x})-y{D}\\right)^{2}\\right] \\ =&amp; \\mathbb{E}{D}\\left[(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x}))^{2}\\right]+\\mathbb{E}{D}\\left[\\left(\\bar{f}(\\boldsymbol{x})-y_{D}\\right)^{2}\\right] \\ &amp;+\\mathbb{E}{D}\\left[2(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x}))\\left(\\bar{f}(\\boldsymbol{x})-y{D}\\right)\\right] \\ =&amp; \\mathbb{E}{D}\\left[(f(\\boldsymbol{x} ; D)-\\bar{f}(\\boldsymbol{x}))^{2}\\right]+\\mathbb{E}{D}\\left[\\left(\\bar{f}(\\boldsymbol{x})-y_{D}\\right)^{2}\\right] \\end{aligned} $$ 我们看到，泛化误差 = 方差 + 偏差 + 噪声","categories":[{"name":"Artificial Intelligence","slug":"Artificial-Intelligence","permalink":"http://lyk-love.cn/categories/Artificial-Intelligence/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://lyk-love.cn/tags/Machine-Learning/"}]},{"title":"Docker Compose","slug":"Docker-Compose","date":"2022-02-15T15:11:52.000Z","updated":"2022-09-26T06:39:34.929Z","comments":true,"path":"2022/02/15/Docker-Compose/","link":"","permalink":"http://lyk-love.cn/2022/02/15/Docker-Compose/","excerpt":"Outline: Background of Docker Compose 部署要点 Compose文件示例 热部署 本节使用示例：here","text":"Outline: Background of Docker Compose 部署要点 Compose文件示例 热部署 本节使用示例：here Background Docker Compose 前身是 Fig 目前为止，Docker Compose依然需要在docker主机上进行外部安装 apt install docker-compose 检查安装是否成功： docker-compose --version 单引擎模式部署workflow： 编写定义多容器应用的YAML文件（称为Compose文件） 将其交给docker-compose 命令， 该工具会基于docker api完成应用的部署（ 即与docker daemon通信 ） Docker Compose是Docker Stack的简化版. 能够在Docker节点上以单引擎模式（ Single-Engine Mode ）进行简化版多服务应用的部署和管理 真正的多服务应用中, 服务是若干容器的集合, 作为一个整体进行统一管理. 例如Docker Stack Docker Compose中的一个服务就只有一个容器,因此是&quot;简化版多服务应用&quot; Basic Idea 通过Compose文件定义的多容器应用称为Compose应用 docker-compose构建应用时也会利用构建缓存，对于已有的镜像、网络、卷，不会再重新创建 docker-compose的命令和docker comtainer大同小异， 所谓的应用就是若干容器， 容器就有运行、停止和关闭三种状态。“关闭”指的是容器资源也被删除 由于docker卷的生命周期是与相应的容器完全解耦的。 因此关闭Compose应用，卷不会被删除 同样，镜像也不会被删除 docker-compose会将项目名称和Compose文件中定义的服务名称连起来，作为新构建的镜像的名称， 而容器名称是镜像名称+数字后缀，因为docker-compose允许扩容 Commands 启动Compose应用 docker-compose up [ -f [compose_file_name] ] -d -f: 指定Compose文件，默认情况下， 其名为docker-compose.yml 或docker-compose.yaml docker-compose up会查找Compose文件，基于此构建镜像、网络和卷，并启动容器 -d: daemon模式，在后台启用应用 也可以使用 &amp;， 但是这样不会重定向输入输出流 列出Compose应用： docker-copmpose ps 该命令作用和 docker container ls 差不多 停止Compose应用 停止Compose应用, 并删除资源（类似docker container rm -f）： docker-compose down 该命令会停止并关闭容器，删除网络（卷和镜像不会被删除） 停止Compose应用,不删除资源： docker-compose stop 相当于 docker compose stop 对于停止的Compose应用,删除其资源： docker-compose rm 类似 docker compose rm， 会删除容器和网络 重启Compose应用 对于停止的Compose应用,重新启动： docker-compose restart 类似 docker compose restart 查看Compose应用运行情况 查看Compose应用运行情况： docker-compose top Compose文件示例 我们给出一个示例： 先进入构建目录： ❯ cd ./counter-app 查看目录内容： ❯ lsapp.py docker-compose.yml Dockerfile README.md requirements.txt app.py: 应用程序代码 文件格式 查看示例的docker-compose.yml: ❯ cat ./docker-compose.ymlversion: &quot;3.5&quot;services: web-fe: build: . command: python app.py ports: - target: 5000 published: 5000 networks: - counter-net volumes: - type: volume source: counter-vol target: /code redis: image: &quot;redis:alpine&quot; networks: counter-net:networks: counter-net:volumes: counter-vol:# 一级key： version: 指定Compose文件格式 （注意并非compose或者docker引擎的版本号） services: 定义不同的服务 networks: 令docker创建新的网络 （ 默认是桥接模式，这是单主机网络，只能够实现同一主机上容器的连接 ） volumns: 令docker创建新的卷 二级key： services 部分定义两个二级key，docker-compose会将每个服务部署为一个容器，并使用key作为容器名字的一部分，在二级key中有如下指令： build： [file_path]：指定Dockerfile的所在目录，该Dockerfile会被用于创建镜像，进而启动容器。 如果已经存在镜像了，可以使用image &lt;image&gt; image: &lt;image&gt;: 指定Docker基于镜像启动容器 command: [executable]: 指定Docker容器中运行的主程序 ports：- target: [target_port] published: [source_port]: 指定端口映射， 将主机的published端口映射到容器的target端口 networks：: 指定容器连接到的网络， 该网络要么已经存在，要么在networks一级key中指定, 后者会让Docker创建该网络 volumes: 指定Docker将 [source]卷挂到容器的 [target] 卷, 该网络要么已经存在，要么在``volumes:一级key中指定 depends_on: &lt;service&gt;： 指定服务启动时间，但是启动时间早并不能保证后一个服务启动时前一个服务已经启动完成 解释 示例解释： 在本示例中, DockerCompose会调用Docker来为web-fe 服务部署一个独立的容器。该容器基于Compose文件位于同一目录下的Dockerfile构建的镜像。基于该镜像启动的容器会运行app.py 作为主程序,将5000端口暴露给宿主机,连接到counter-net 网络上,并挂载一个卷到/code . redis服务也类似 由于两个服务都连接到counter-net 网络,因此它们可以通过名称解析到对方的地址。了解这一点重要,本例中上层应用被配置为通过名称与Redis服务通信 查看容器： ❯ docker container lsCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb936fbeba406 counter-app_web-fe &quot;python app.py&quot; About an hour ago Up About an hour 0.0.0.0:5000-&gt;5000/tcp, :::5000-&gt;5000/tcp counter-app_web-fe_11784979d1e6c redis:alpine &quot;docker-entrypoint.s…&quot; About an hour ago Up About an hour 6379/tcp counter-app_redis_1 可以看到每个服务都创建了对应的容器 查看网络： ❯ docker network lsNETWORK ID NAME DRIVER SCOPE220db3964578 bridge bridge local5489df9fb5c0 counter-app_counter-net bridge local961ccb2de759 host host local03b55e96642c none null local 可以看到创建了桥接网络counter-app_counter-net 查看卷： ❯ docker volume lsDRIVER VOLUME NAME&lt;Snip&gt;local counter-app_counter-vol 可以看到创建了卷counter-app_counter-vol 热部署 由于主机的卷被挂载到容器上，对主机上卷的改动就是对容器的卷的改动。 docker-compose可以做到热部署，就是说可以直接在主机的卷上进行修改，容器就会产生相应的改变，不需要重新部署 example： 先查看示例中卷的挂载情况： ❯ docker volume inspect counter-app_counter-vol | grep Mount &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/counter-app_counter-vol/_data&quot;, 我们先修改工作目录下的app.py， 将其copy到/var/lib/docker/volumes/counter-app_counter-vol/_data： do some change...#注意带这个cp命令就是覆盖文件内容cp ./app.py /var/lib/docker/volumes/counter-app_counter-vol/_data/app.py （当然你也可以直接修改挂载点的文件， 不过由于docker compose每次从构建目录（这里是 ～/counter-cp/）来构建应用，如果不改变构建目录下的文件的话，下次构建时，） 现在挂载点的文件内容变了，我们打开localhost:5000，发现修改也生效了。 整个过程不需要重新部署","categories":[{"name":"Technology","slug":"Technology","permalink":"http://lyk-love.cn/categories/Technology/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://lyk-love.cn/tags/Docker/"}]},{"title":"Docker Basic","slug":"Docker-Basic","date":"2022-02-13T16:08:27.000Z","updated":"2022-10-01T10:11:07.450Z","comments":true,"path":"2022/02/14/Docker-Basic/","link":"","permalink":"http://lyk-love.cn/2022/02/14/Docker-Basic/","excerpt":"Outline: Docker Specification Docker Engine Docker Image Docker Container Docker Security Docker Logs Docker Management 介绍了docker的基本概念和命令","text":"Outline: Docker Specification Docker Engine Docker Image Docker Container Docker Security Docker Logs Docker Management 介绍了docker的基本概念和命令 Docker Specification Docker采用CS模式， 分为Client ， Engine， Index三部分， docker server就是docker引擎， docker index就是docker镜像存储服务（docker hub之类） 由于Index是个服务，Client就是个命令行，我们指的docker一般就是docker engine， 它是用于运行和编排容器的基础工具 daemon实现了docker engine的api, client和daemon通过本地IPC/UNIX Socket通信（/var/run/docker.sock, 用docker --version可查看二者的通信 ) 镜像是未运行的容器，二者是类和对象的关系。 也可以把容器理解为命名空间的有组织集合, 详见下文Security -&gt; Linux -&gt; Namespace Docker运行时与编排引擎 引擎分社区和企业版，版本号遵循 YY.MM-xx格式 每个docker主机称为一个docker节点 容器生态 docker内置组件都可以替换为第三方组件，“ Batteries included but removable” 开放容器计划 The Open Container Initiative, OCI: 旨在管理容器标准的委员会， 目前已发布两项规范 镜像规范 容器运行时规范 Docker安装 建议所有的云服务器都按如下流程走一遍 ``sudo apt install docker` 最好使用非root用户来使用Docker,此时需要添加非root用户到本地Docker Unix组：sudo usermod -aG docker [user_name] 如果当前登陆用户就是要添加进组的用户的话，需要重新登陆才能生效 这意味着，如果该登陆用户是个jenkins登陆用户，则需要在jenkins上断连再重新连接 确认安装结果： root@lykRemote:~# docker --versionDocker version 20.10.12, build e91ed57root@lykRemote:~# docker system infoClient: Context: default Debug Mode: false&lt;Snip&gt;Server: Containers: 1 Running: 1 Paused: 0 Stopped: 0 Images: 9 Server Version: 20.10.12&lt;Snip&gt; 设置docker开机启动： systemctl enable docker 可能遇到 Failed to enable unit: Unit file /etc/systemd/system/docker.service is masked， 此时需要： systemctl unmask docker docker启动和关闭： systemctl start docker Docker升级 卸载当前Docker： apt-get remove docker docker-engine docker-ce docker.io -y or： apt-get remove docker docker-* -y 在之前的版本中，Docker的包名可能有很多个，该command能确保全部删除 安装新版本Docker：同上 Docker配置 查看docker配置文件位置： systemctl status docker 默认是/usr/lib/systemd/system/docker.service， 但是该文件还引用了别的文件，很不好管理，因此docker又使用/etc/docker/daemon.json来进行统一的配置： // 这里配置日志级别，并配置源为阿里云&#123; &quot;debug&quot;: true, &quot;log-level&quot;: &quot;debug&quot;, &quot;registry-mirrors&quot;: [&quot;https://zz1b9pta.mirror.aliyuncs.com&quot;]&#125; 该文件不存在就创建 可能还会有~/.docker/daemon/json, 亲测这个配置文件不起作用 更新配置： sudo systemctl daemon-reloadsudo systemctl restart docker 查看配置信息： docker info 换阿里源 如上， 编辑daemon.json, 然后更新配置就行了 插叙: alpine linux alpine linux是一个超小的Linux镜像,常用来作为基础镜像层节省空间, 除了纯的alpine linux( e.g. From alpine:latest)外, 也可以安装很多用户软件的alpine版本( e.g. FROM node:14.17.4-alpine) 许多容器是alpine的，在alpine中使用apk命令可能会很慢， 可以进入容器，在容器内换源： Alpine 的源文件为： /etc/apk/repositories 这里面的默认配置例如： http://dl-cdn.alpinelinux.org/alpine/v3.11/mainhttp://dl-cdn.alpinelinux.org/alpine/v3.11/community 可以使用以下命令来进行源的切换（阿里云源）： sed -i &#x27;s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g&#x27; /etc/apk/repositories Docker存储驱动的选择 每个Docker容器都有一个本地存储空间，用于保存层叠的镜像层（Image Layer）和挂载的容器文件系统，默认情况下容器的所有读写操作都发生在其镜像层或者挂载的文件系统中。 本地存储通过存储驱动（Storage Driver）管理. 存储驱动在上层都采用栈式镜像存储和Copy on Write, 但底层支持不同实现（只针对linux, windows只支持一种存储驱动Windows Filter） 尽管存储空间是容器级别，存储驱动是节点级别的，即每个Docker host只能选择一种存储驱动，而不能为每个容器选择不同的存储驱动 下面将存储驱动设置为overlay2：( daemon.json ) &#123; &quot;storage-driver&quot;: &quot;overlay2&quot; &#125; 修改正在运行的Docker host的存储驱动会导致现有镜像和容器在重启之后不可用， 因为每种存储驱动在主机上存储镜像层的位置不同，修改了驱动，docker就找不到原来的镜像和容器了。 切换到原来的存储驱动就可以继续使用 如果要在在切换存储驱动后还能使用原来的镜像和容器，需要先将镜像push到docker仓库，修改本地存储引擎并重启，然后从仓库pull镜像 json的最后一个属性后面不能加逗号 检查存储驱动类型： docker system info&lt;Snip&gt;Storage Driver: overlay2 Backing Filesystem: extfs Supports d_type: true Native Overlay Diff: true userxattr: false&lt;Snip&gt; 存储驱动介绍： Red Hat Enterprise: 低版本内核或低版本docker, 建议使用Device Mapper ; 高版本用overlay2 Ubuntu 4.x内核或更高的版本建议用overlay2 大部分存储驱动不需要额外配置，但Device Mapper默认情况下采用loopback mpunted sparse file作为底层实现，性能很差，不能用于生产环境，需要手动更换为 LVM Docker Engine docker server就是docker引擎，而引擎架构如下： daemon: 实现了与docker client通信的REST API. 自身通过gRPC 与containerd 通信 containerd: 用于容器生命周期管理 ---- start|stop|pause|rm containerd虽然管理容器，但不负责创建容器。 它指挥runc创建容器 containerd将docker image转换为 OCI bundle, 并让runc基于此创建容器 containerd后来也用于镜像管理等 shim： 作为容器的父进程，shim职责为： 保持输入输出流的开启 将容器的退出状态反映给daemon runc是OCI容器运行时规范的参考实现，只能用于创建容器。 runc每次创建容器就会fork一个进程。创建完毕， 对应的runc进程就会退出。 因此即使运行上百个容器也无需维持上百个runc进程 一旦runc进程退出，相关联的containerd-shim进程就会成为容器的父进程 在这个架构下，容器的启动和管理实现了和daemon的解耦。 即容器运行时和daemon解耦，称为“无守护进程的容器” daemonless container。 因此，对daemon的维护「升级等不会影响到运行中的容器 Docker Image commands 搜索镜像 docekr search 通过CLI方式在docker hub搜索镜像： docker search [OPTIONS] NAME NAME: 仓库名，即&lt;repository&gt; 下载镜像 docker image pull &lt;image&gt; #默认从docker hubdocker image pull &lt;registry&gt;/&lt;image&gt;:# 指定镜像仓库服务 &lt;registory&gt;: &lt;域名/IP&gt;[:端口号]。默认地址是 Docker Hub(docker.io) 列出镜像 列出本机的所有镜像： docker image ls --digest：查看镜像摘要 --filter: 开启过滤，支持如下过滤器 danglng： 仅返回 True（悬虚镜像）；false（非悬虚镜像） before， since： 接受&lt;mage&gt;: 作为参数，返回在该image之前/后创建的所有镜像 label： 根据label值过滤 reference： 按名过滤： # 仅显示标签为latest的镜像docker image ls --filter reference=&quot;*.latest&quot; --format： 用 Go 的模板语法对输出格式化： # 列出镜像结果，并且只包含镜像ID和仓库名$ docker image ls --format &quot;&#123;&#123;.ID&#125;&#125;: &#123;&#123;.Repository&#125;&#125;&quot;5f515359c7f8: redis05a60462f8ba: nginxfe9198c04d62: mongo00285df0df87: &lt;none&gt;329ed837d508: ubuntu329ed837d508: ubuntu 查看镜像 列出镜像细节： docker image inspect To list all the images and their respective OSs for i in `docker images --format &#123;&#123;.ID&#125;&#125;`; do echo $i `docker image inspect $i |grep Os`; done 删除镜像 docker image rm &lt;image&gt; 删除全部的悬虚镜像： docker image prune 导出镜像为文件 将指定镜像保存成 tar 文件: docker save -o [file-name].tar &lt;image&gt; -o :输出到的文件 镜像服务 可完全类比GitHub docker image存储在镜像仓库服务 Image Registry中，默认是Docker Hub Image Registry包含多个镜像仓库Image Repository, Image Repository包含多个镜像 Docker Hub分为官方和非官方仓库，官方仓库名(&lt;repository&gt;) 一般在命名空间顶层，比如 ubuntu， 而个人仓库一般是次级命名空间，比如user_name/ubuntu 镜像标识（&lt;image&gt;） 镜像标识方法： 镜像名：&lt;/repository&gt;:&lt;tag&gt; 如果没有在仓库名后指定标签，则默认为tag=latest，但是， latest并没有特殊含义，不能保证这是最新的镜像 同一镜像可以打多个标签 标签是可变的，因此可能不准确 标签会造成悬虚镜像（dangling image）: 当构建了一个新镜像，然后为该镜像打了个已经存在的标签，那么docker会将旧镜像上的标签转移到新镜像上，旧镜像就变成了悬虚镜像，标识为&lt;none&gt;:&lt;onne&gt;: 镜像ID 镜像摘要： 基于内容的散列（最准确） 镜像体积 镜像下载和上传过程中镜像是保持着压缩状态 Docker Hub 中显示的体积是压缩后的体积 docker image ls 显示的是镜像下载到本地后，展开的大小，准确说，是展开后的各层所占空间的总和，因为镜像到本地后，查看空间的时候，更关心的是本地磁盘空间占用的大小。 docker image ls 列表中的镜像体积总和并非是所有镜像实际硬盘消耗。由于 Docker 镜像是多层存储结构，并且可以继承、复用，因此不同镜像可能会因为使用相同的基础镜像，从而拥有共同的层。由于 Docker 使用 Union FS，相同的层只需要保存一份即可，因此实际镜像硬盘占用空间很可能要比这个列表镜像大小的总和要小的多 可以通过 docker system df 命令来便捷的查看镜像、容器、数据卷所占用的空间。 镜像分层 Docker镜像由多个松耦合的只读镜像层组成。Docker负责堆叠这些层，并将它们表示为单个统一的对象 镜像一般比较精简，只包含必须的内容， 如必须的OS等，当然，镜像不包含os内核，容器共享docker host的内核 在镜像上启动的容器全部停止之前，镜像无法被删除： docker image rm ubuntu:latest Error response from daemon: conflict: unable to remove repository reference &quot;ubuntu:latest&quot; (must force)│ - container 75f3c86793d8 is using its referenced image 54c9d81cbb44 查看镜像层 回顾docker image pull ubuntu:latest的输出： latest: Pulling from library/ubuntu952132ac251a: Pull complete82659f8f1b76: Pull completec19118ca682d: Pull complete8296858250fe: Pull complete24e0251a0e2c: Pull completeDigest: sha256:f4691c96e6bbaa99d...28ae95a60369c506dd6e6f6abStatus: Downloaded newer image forubuntu:latest 对应的镜像层为： 或者通过docker image inspect查看镜像： $ docker image inspect ubuntu:latest[&#123;&quot;Id&quot;: &quot;sha256:bd3d4369ae.......fa2645f5699037d7d8c6b415a10&quot;,&quot;RepoTags&quot;: [&quot;ubuntu:latest&quot;&lt;Snip&gt;]&#125;&quot;RootFS&quot;: &#123;&quot;Type&quot;: &quot;layers&quot;,&quot;Layers&quot;: [&quot;sha256:c8a75145fc...894129005e461a43875a094b93412&quot;,&quot;sha256:c6f2b330b6...7214ed6aac305dd03f70b95cdc610&quot;,&quot;sha256:055757a193...3a9565d78962c7f368d5ac5984998&quot;,&quot;sha256:4837348061...12695f548406ea77feb5074e195e3&quot;,&quot;sha256:0cad5e07ba...4bae4cfc66b376265e16c32a0aae9&quot;]&#125; 可以看到还是有五个镜像层，只不过采用了镜像名的SHA256来标识 镜像层详解 可以看到，镜像层是栈结构的，所有镜像层都起始于一个基础镜像层，后续每当对镜像内容做增改时，就会叠加新的镜像层 example：假如基于Ubuntu Linux 16.04创建一个新的镜像,这就是新镜像的第一层;如果该镜像中添加Python包,就会在基础镜像层之上创建第二个镜像层;如果继续添加一个安全补丁,就创建第三个镜像层。该镜像当前已经包含3个镜像层： 在添加额外的镜像层的同时,镜像始终保持是当前所有镜像的组合 example： 如下图，在外部看来整个镜像只有6个文件,这是因为最上层中的文件7是文件5的一个更新版本。这种情况下,上层镜像层中的文件覆盖了底层镜像层中的文件。这样就使文件的更新版本作为一个新镜像层添加到镜像当中： Docker通过存储引擎（新版本采用快照机制）来实现镜像层的栈，并保证多镜像层对外展示为统一的文件系统 所有镜像层堆叠合并，对外提供统一的视图： example： 上图三层镜像的对外视图，在外界看来，镜像层是合并的 共享镜像层 镜像层可以被多个镜像共享， 镜像摘要 除了 &lt;reposiroty&gt;: &lt;tag&gt;, 镜像还可以用镜像摘要唯一标识，它是镜像的内容的散列值 可以用docker image ls的--digests option查看摘要： root@lykRemote:~# docker image ls --digests alpine REPOSITORY TAG DIGEST IMAGE ID CREATED SIZEalpine latest sha256:21a3deaa0d32a8057914f36584b5288d2e5ecc984380bc0118285c70fa8c9300 c059bfaa849c 2 months ago 5.59MB 镜像本身就是一个配置对象，其中包含了镜像层的列表以及一些元数据信息 镜像层是实际数据存储的地方，镜像层之间是完全独立的，并没有从属于某个镜像的概念 镜像的唯一标识是其摘要，每个镜像层也有自己的摘要。 这意味着镜像和镜像层是不可变得，任何改动都导致散列值的变化，这就是内容散列（ Content Hash ). 但是，在push和pull时，我们往往要对镜像层压缩来节省带宽和仓库存储空间，而压缩会改变镜像内容，这意味着内容散列值会改变，为此，每个镜像层又包含了一个分发散列值（ Distributed Hash ），这是个压缩版镜像的散列值，用它来校验拉取的镜像是否被篡改过 多架构镜像 为了支持多架构，同一&lt;repository&gt;:&lt;tag&gt;标识的镜像可以有Manifest列表，每个元素是一个Manifest指针，指向具体的Manifest，其中包含了对应架构的镜像 Manifest列表是可选的，在没有Manifest列表时，registry返回普通的Manifest 因此不同架构的主机可以用相同的命令从registry得到同一镜像的不同架构版本 删除镜像 删除镜像会删除镜像配置以及相关的镜像层， 镜像层是引用计数的，可以被多个镜像共享。只有当所有依赖该镜像层的镜像都被删除后，该镜像层才被删除。 Docker Container commands 注意：下文的&lt;container&gt;是容器名，和&lt;image&gt;镜像名不同 启动容器 运行容器：docker client会用API与docker daemon通信，后者先查询本地有无该镜像，如果没有，就从（默认）docker hub pull到本地 docker container run [options] &lt;image&gt; &lt;app&gt; -i: Keep STDIN open even if not attached -t: Allocate a pseudo-tty 终端 For interactive processes (like a shell), you must use -i -t together in order to allocate a tty for the container process. -i -t is often written -it as you’ll see in later examples. -d=true or just -d： daemon模式: 在后台运行 &lt;app&gt;: 容器中运行的主进程（领头进程） --name [name]: 给容器起别名 -p [host_port:container_port]: 指定端口映射， 例如-p 80:8080将主机的80端口映射到容器的8080端口 --network: 指定容器所属的网络类型： --net=host: host网络 --net=none: 无网络 --net=bridge : bridge网络，这是默认设置 --net=container:&lt;container&gt;: 与指定容器共享一个Network Namespace，但其他的namespace还是隔离的 -rm: Automatically remove the container when it exits. 也就是自动移除之前的同名的容器 --restart: 设置重启策略: unless-stopped always no: Do not automatically restart the container. (the default) &lt;Ctrl PQ&gt;： detach容器 启动已停止运行的容器 启动处于停止（Exited）状态的容器： docker container start 重启容器 正在运行的容器可以重启: docker container restart &lt;container&gt; 在容器中启动新进程 在运行的容器中启动新进程： docker container exec [options] &lt;container&gt; &lt;app&gt; docker container exec -it &lt;container&gt; bash: 将终端重新连接到容器 exec在容器中创建了新的bash，输入ps可看到两个bash进程。 这意味着在当前shell输入exit并不会导致容器终止，因为主进程还在运行中 列出容器 列出所有运行的容器， options和image的一样 docker container ls -a: 显示所有容器(默认只显示运行的) --quiet, -q :只显示数字ID 停止容器 停止容器运行： docker container stop 向容器内pid=1的进程发送SIGTERM， 如果10s内得到清理并停止运行，则会接着发送SIGKILL强制停止该容器 docker stop $(docker ps -a -q) 删除容器 删除容器： docker container rm -f: 发送Sigkill停止容器（无需先stop），然后删除（rm） docker rm $(docker ps -a -q) 批量删除容器： docker container rm -f $(docker container ls -aq | cut -d &quot; &quot; -f 1) 先列出所有容器信息(包括id): docker container ls -aq 选中指定列, 一般是第一列,这是容器的id: cut -d &quot; &quot; -f 1 将容器的id传给rm: docker container rm -f 删除所有Exited的容器: docker container prune 导出/导入容器的文件系统 将Docker容器里的文件系统作为一个 tar 文件导出到标准输出， 注意与用于序列化镜像的docker image save不同： docker container export [OPTIONS] &lt;container&gt; -o:将输入内容写到文件 从tar文件中创建镜像: docker import [OPTIONS] [file_name].tar &lt;image&gt; 查看容器信息 docker container inspect &lt;container&gt; 查看容器重启策略： docker container inspect &lt;container&gt; | grep RestartPolicy -A 20 查看容器的挂载情况： docker inspect &lt;container&gt; | grep Mounts -A 20 查看端口映射等信息： docker container port &lt;container&gt; 查看容器日志 查看容器内部的标准输出: docker logs -f &lt;container&gt; 查看容器网络 查看容器网络： docker network ls 查看容器卷 查看容器的卷： docker volume ls 容器vs虚拟机 容器共享操作系统资源（OS内核），虚拟机共享硬件资源 容器是OS虚拟化， 虚拟机是硬件虚拟化 虚拟机的每个OS都有开销，都需要授权，都需要打补丁 运行容器 容器镜像都是高度优化的，因此可能没有预装一些命令 容器随着其中运行应用(领头进程)的退出而终止 重启策略 重启策略可以指定容器在事件或错误后重启 重启策略可作为参数docker container run --restart 的参数设置或在Compose文件中声明 重启策略包括： always: 除非容器被明确停止( 如stop)，否则该策略会一直尝试重启处于Exited的容器 该策略下，如果重启daemon, Exited容器（包括stop!!）也会被重启 unless-stopped： always的修复版，不会在daemon重启的时候重启Exited状态的容器 on-failed： 在退出容器且返回值不是0的时候重启容器，与always相同，如果重启daemon, 明确被停止的容器（包括stop!!）也会被重启 example 下面创建两个新容器,其中“always”容器指定--restart always 策略,另一个“unless- stopped” 器指定了--restart unless-stopped 策略。两个容器均通过docker container stop 命令 止,接着重启Docker。结果“always”容器会重启,但是“unless-stopped”容器不会 创建两个新容器： docker container run -d --name always --restart always alpine sleep 1ddocker container run -d --name unless-stopped --restart unless-stopped alpine sleep 1d 停止两个容器： docker container stop always unless-stopped 重启docker systemctl restart docker 检查两个容器的状态： docker container ls -a# 输出略 Docker Security 本章内容涉及Docker Swarm Docker使用了大量Linux的安全技术 Linux Security Namespace Linux Namespace允许对OS进行拆分, 例如, 不同的name space可以被分配不同的ip, 这样同一台主机就可以允许多个web服务, 还不存在端口冲突( 因为ip不同 ) Docker容器本质是命名空间的有组织集合 每个容器都由自己的PID, NET, MNT, IPC, UTS构成, 还可能包括USER 这些命名空间的组合就是容器 Linux Docker使用了如下的内核namesoace: 进程ID( PID ): 每个容器互相独立, 拥有自己的进程树, 每个容器都有自己的PID为1的进程. PID空间也意味着容器不能看到其他容器或所在主机的进程树 网络 ( NET ): 每个容器有互相隔离的网络栈. 例如每个容器都有自己的eth0接口, 独立的ip和端口地址 文件系统/挂载( MNT ): 每个容器都有互相隔离的根目录, 这也意味着容器不能访问其他容器或宿主机的目录 进程内通信( IPC ): IPC为容器提供共享内存, 这在不同容器间也是独立的 用户( USER ): Docker允许用户使用USER命名空间来将容器内用户映射到宿主机的不同用户, 比如将容器内的root映射到宿主机的非root用户 UTS: 每个容器都有自己的主机名称 Control Group 通过控制组来限制资源分配 Capability 允许拆分用户权限, 即选择容器允许所需的root用户权限 MAC Docker Linux采用主流的Linux Mac 技术, 如 SELinux Seccomp Docker使用过滤模式下的Seccomp来限制容器对宿主机内核发起的系统调用 Docker Original Security Docker很大一部分安全技术都基于Swarm模式, 详见Docker Swarm 此外, 还有Docker安全扫描, 内容信任, Docker Secret等 安全扫描 就是对Docker镜像进行二进制代码级别的扫描, 对其中的软件根据已知的缺陷数据库( CVE数据库 )进行检查, 并生成报告 Docker Hub已经支持安全扫描了 内容信任 内容信任允许开发者对push到Image Registry的镜像进行签名, 当镜像被pull的时候确认签名状态, 这可以确保只pull经过验证的镜像 Docker Secret 提供了docker secret系列命令 密钥的存储和传输都是加密的. 使用时被临时挂载到内存文件系统( 只针对Linux, 因为windows没有内存文件系统 ) 密钥被存在集群存储里,并且加密. 每个Manager都有权访问集群存储 密钥只对已经被授权了的服务开放访问 可以在容器/服务创建时指定使用某密钥, 后续该服务的多个示例都会持有该密钥 一旦容器/服务的任务完成, 内存文件系统关闭,密钥也随之删除 假设指定服务B使用密钥secret1, 在worker2, worker3运行, 则密钥被传输到worker3的B实例时, 传输是加密的 服务A不能访问该密钥, 因为没有授权 Docker Logs Docker网络, 集群出现故障, 就需要查日志 日志分为daemon日志和容器日志 Commands 查看daemon日志( 仅限于使用systemd的系统)： journalctl -u docker.service 查看容器日志： docker container logs -f &lt;container-name&gt; 对于Swarm服务： docker service logs &lt;service-name&gt; daemon日志 daemon日志： 如果OS用Systemd， 日志会存储在Journald 查看daemon日志 查看日志： journalctl -u docker.service 设置日志详细程度 通过编辑daemon.json： 设置debug为true 设置log-level: debug: 最详细 info： 默认值， 次详细 warn： 第三详细 error fatal &#123; &lt;Snip&gt; &quot;debug&quot;: true, &quot;log-level&quot;: &quot;debug&quot;, &lt;Snip&gt;&#125; 容器日志 每个docker容器或服务都可以配置自己的日志驱动： json-file: 默认 journald: 只在运行systemd的linux主机有效 syslog ... 查看容器日志 docker container logs &lt;container-name&gt;docker service logs &lt;service-name&gt; # 针对Swarm服务 json-file和journald日志均可通过这两种命令查看 其他驱动的日志需要用第三方平台提供的原生工具查看 --follow: 跟踪日志 --tail: 查看日志尾部 --details: 额外的详细信息 指定容器日志驱动 配置docker主机的提供的默认日志驱动： //daemon.json&#123; &quot;log-driver&quot;: &quot;syslog&quot;&#125; 可以在容器启动时通过--log-driver和-log-opts指定特定的日志驱动，这会覆盖掉主机的配置 Docker Management 命令行docker管理工具: lazydocker 查看配置文件 systemctl status docker 更改镜像存储位置 使用软链接 默认情况下 Docker 镜像的存放位置在 /var/lib/docker ： # 默认存放位置$ sudo docker info | grep &quot;Docker Root Dir&quot; 采用软链接的方式，修改镜像和容器的存放路径 # 停掉Docker服务$ service docker stop 然后移动整个 /var/lib/docker 目录到空间不较大的目的路径。这时候启动 Docker 时发现存储目录依旧是 /var/lib/docker 目录，但是实际上是存储在数据盘 ～/data/docker 上了。 # 移动原有的内容$ mv /var/lib/docker ～/data/docker# 进行链接$ ln -sf ～/data/docker /var/lib/docker Others 常见镜像版本 full official image 事实上的标准镜像: python:3.8.3 node:14.1.1 如果不关心最终镜像的大小, 完整镜像是最安全的选择 buster/stretch/jessie buster:Debian 10 stretch:Debian 9 jessie:Debian 8 带有stretch、buster或jessie标签的镜像是不同Debian发行版的代号。 在撰写本文时，稳定的Debian发行版是10.4，它的代号是“buster”。 “stretch”是所有版本9变种的代号，“jessie”是所有版本8变种的代号。 正在开发的未来版本是“bullseye ”和“bookworm”，但还不稳定。你可能会在DockerHub上的镜像版本列表中看到这些标签。 如果您的代码与Debian操作系统的特定版本兼容，请选择其中一个镜像。在开始一个新项目时，你很少需要使用旧版本的Debian。 slim slim的镜像是完整镜像的配对版本, 这个镜像通常只安装运行特定工具所需的最小包 **但是，在使用这个镜像时，一定要进行彻底的测试！**如果您遇到无法解释的错误，请尝试切换到完整的镜像，看看是否能够解决问题。 alpine alipine镜像基于alpine linux项目，这是一个专门为容器内部使用而构建的操作系统。在很长一段时间里，这些是最受欢迎的镜像变体，因为它们的尺寸很小。 然而，一些团队正在弃用alpine镜像，因为这些镜像可能会导致难以调试的兼容性问题。具体来说，如果使用python镜像，一些 wheels将被构建成与Debian兼容，并且需要重新编译，才能与基于apline的镜像一起工作。 使用alpine镜像的主要原因是使你得到的镜像尽可能小。基础镜像将小于5MB。python基础镜像(将python添加到基础alpine镜像)目前是78.9MB。这仍然很小。 如果考虑到空间问题，强烈推荐使用此镜像。 它的缺点是不包含一些你可能会需要的包。主要是，它使用了一个更小的musl lib代替glibc。如果您的应用程序有特定的libc需求，您可能会遇到问题。 如果你发现Alpine镜像缺少你需要的东西，你可以直接在Dockerfile中安装它，这样能确保镜像只包含你需要的内容。需要注意，如果您正在安装外部包，您的Dockerfile将会更改。主要的区别是，您将使用apk而不是apt-get来安装包。 对alpine镜像的使用有很多担心之处，所以你需要充分了解它。需要充分阅读文档并研究。 同样，如果您在构建Dockerfile时遇到了无法解释的问题，请尝试切换到完整的镜像，看看是否能解决问题 windowsservercore windows或windows Server平台的镜像","categories":[{"name":"Technology","slug":"Technology","permalink":"http://lyk-love.cn/categories/Technology/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://lyk-love.cn/tags/Docker/"}]},{"title":"Command-line Environment","slug":"Command-line-Environment","date":"2022-02-10T17:30:37.000Z","updated":"2022-09-26T06:39:34.926Z","comments":true,"path":"2022/02/11/Command-line-Environment/","link":"","permalink":"http://lyk-love.cn/2022/02/11/Command-line-Environment/","excerpt":"Outline: Job control Tmux Aliase Dotfiles Remote Machines ( ssh端口转发详见其他文章 ) ref: MIT lesson, tmux tutorial, a good introduction of Session","text":"Outline: Job control Tmux Aliase Dotfiles Remote Machines ( ssh端口转发详见其他文章 ) ref: MIT lesson, tmux tutorial, a good introduction of Session 控制终端(controlling terminal) **控制终端是进程的一个属性。**通过 fork 系统调用创建的子进程会从父进程那里继承控制终端。这样，session 中的所有进程都从 session 领头进程那里继承控制终端。Session 的领头进程称为终端的控制进程(controlling process)。简单点说就是：**一个 session 只能与一个终端关联，这个终端被称为 session 的控制终端(controlling terminal)。**同时只能由 session 的领头进程来建立或者改变终端与 session 的联系。我们可以通过 ps 命令查看进程的控制终端： 支持 job control 的 shell 必须能够控制在某一时刻由哪个 job 使用终端。否则，可能会有多个 job 试图同时从终端读取数据，这会导致进程在接收用户输入时的混乱。为了防止这种情况发生，shell 必须按照预定的协议与终端驱动程序协作。 shell 一次只允许一个 job(进程组)访问控制终端。来自控制终端的某些输入会导致信号被发送到与控制终端关联的 job(进程组)中的所有进程。该 job 被称为控制终端上的前台 job。由 shell 管理的其他 job 在不访问终端的情况下，被称为后台 job。 Shell 的职责是通知 job 何时停止何时启动，还要把 job 的信息通知给用户，并提供机制允许用户继续暂停的 job、在前台和后台之间切换 job。比如前台 job 可以无限制的自由使用控制终端，而后台 job 则不可以。当后台 job 中的进程试图从其控制终端读取数据时，通常会向进程组发送 SIGTTIN 信号。这通常会导致该组中的所有进程停止(变成 stopped 状态)。类似地，当后台 job 中的进程试图写入其控制终端时，默认行为是向进程组发送 SIGTTOU 信号，但是否允许写入的控制会更加的复杂。 Job control shell和进程采用signal通信，signal是一种软件中断 Killing a process Ctrl-z: SIGSTP, Ctrl-c: SIGINT` Ctrl-\\: SIGQUIT kill -TERM&lt;PID&gt;: SIGTERM( 比前二者更general ) An example of a Python program that captures SIGINT and ignores it, #!/usr/bin/env pythonimport signal, timedef handler(signum, time): print(&quot;\\nI got a SIGINT, but I am not stopping&quot;)signal.signal(signal.SIGINT, handler)i = 0while True: time.sleep(.1) print(&quot;\\r&#123;&#125;&quot;.format(i), end=&quot;&quot;) i += 1 Terminal : 202^CI got a SIGINT, but I am not stopping212^CI got a SIGINT, but I am not stopping219^CI got a SIGINT, but I am not stopping367^\\zsh: quit (core dumped) /bin/python ~/Projects/Python/ignoreSIGINT.py ^ is how Ctrl is displayed when typed in the terminal Pausing and backgrounding processes Ctrl-z: SIGSTP, 会将进程suspend short for Terminal Stop (i.e. the terminal’s version of SIGSTOP), which pauses a process fg or bg: continue the paused job in the foreground or in the background fg/bg: Resume the most recently suspended job and run it in the forward/background fg/bg %job_id jobs: 显示当前session的未完成的job， 每个job都会分配一个工作号， 由%[job_id]引用 [工作号] 进程号 得到job的PID： grep To refer to the last backgrounded job you can use the $! special parameter. 如果在终端上出现如下信息： [1]+ Done find / -name install.log 则证明后台的这个命令已经完成了。命令如果有执行结果，则也会显示到操作终端上。其中，[1] 是这个命令的工作号，&quot;+&quot;代表这个命令是最近一个被放入后台的 pgrep: Find or signal processes by name. -l：同时显示进程名和PID -o： 当匹配多个进程时，显示进程号最小的那个 -n： 当匹配多个进程时，显示进程号最大的那个 注：进程号越大，并不一定意味着进程的启动时间越晚 #Return PIDs of any running processes with a matching command string:pgrep process_name &amp; suffix in a command will run the command in the background, giving you the prompt back, although it will still use the shell’s STDOUT which can be annoying (use shell redirections in that case): command &gt;out.file 2&gt;&amp;1 &amp; 被放到后台的进程是当前terminal的子进程，当杀死父进程terminal时，会发送SIGHUP杀死子进程，为了避免这种情况： run the program with nohup (a wrapper to ignore SIGHUP) nohup command command_arguments use disown if the process has already been started #disown the current job: disown#Disown a specific job:disown %job_number #Disown all jobs:disown -a #Keep job (do not disown it), but mark it so that no future SIGHUP is #received on shellexit: disown -h %job_number example $ sleep 1000^Z[1] + 18653 suspended sleep 1000$ nohup sleep 2000 &amp;[2] 18745appending output to nohup.out$ jobs[1] + suspended sleep 1000[2] - running nohup sleep 2000$ bg %1[1] - 18653 continued sleep 1000$ jobs[1] - running sleep 1000[2] + running nohup sleep 2000$ kill -STOP %1[1] + 18653 suspended (signal) sleep 1000$ jobs[1] + suspended (signal) sleep 1000[2] - running nohup sleep 2000$ kill -SIGHUP %1[1] + 18653 hangup sleep 1000$ jobs[2] + running nohup sleep 2000$ kill -SIGHUP %2$ jobs[2] + running nohup sleep 2000$ kill %2[2] + 18745 terminated nohup sleep 2000$ jobs Dotfiles 使用dotbot dotfile无法跨平台，换个os就会失效. 为此，可以根据不同的平台加载不同的配置： if [[ &quot;$(uname)&quot; == &quot;Linux&quot; ]]; then &#123;do_something&#125;; fi# Check before using shell-specific featuresif [[ &quot;$SHELL&quot; == &quot;zsh&quot; ]]; then &#123;do_something&#125;; fi# You can also make it machine-specificif [[ &quot;$(hostname)&quot; == &quot;myServer&quot; ]]; then &#123;do_something&#125;; fi 不同程序共享相同配置： # Test if ~/.aliases exists and source itif [ -f ~/.aliases ]; then source ~/.aliasesfi 注意，某些配置最好不要公开，比如~/.ssh/config, 因此dotfile的版本管理最好用私人仓库 SSH ssh ssh username@remote_host execute commands on remote machine ``ssh username@remote_host -t “command command_arguments“ : 在目标主机的home目录下执行command 要执行的命令必须括起来，否则在有的系统中除了第一个命令，其它都是在本地执行的（比如manjaro） 支持pipe： ssh foobar@server ls | grep PATTERN： 在本地grep远程的输出 ls | ssh foobar@server grep PATTERN： 在远程grep本地的输出 -t：Run a command on a remote server with a [t]ty allocation allowing interaction with the remote command explain： 命令如果要与用户交互，就需要分配tty,默认情况下，单纯的ssh连接，shell会分配一个tty,此时你运行了一个shell session； 但当执行ssh foobar@server &quot;command&quot;时，shell不会为这个远程会话分配 TTY。此时 ssh 会立即退出远程主机，所以需要交互的命令也随之结束， 添加 -t 参数会告诉shell分配一个tty与远程 shell 进行交互，ssh 会保持登录状态，直到你退出需要交互的命令 Key generation ssh-keygen -t rsa -b 1024 -f yourkeyname -C &quot;备注&quot; 参数 解释 -b 采用长度1024bit的密钥对,b=bits,最长4096 -t rsa 采用rsa加密方式,t=type -f 生成文件名,f=output_keyfiles -C 备注，C=comment -a rounds 指定密钥生成函数，参数数值越高，密码越安全也越慢，默认为16 Higher numbers result in 公钥和私钥默认位于~/.ssh 使用ssh-agent or gpg-agent 避免每次都输密码 ssh-keygen -y -f ~/.ssh/id_rsa： 根据私钥，检查公钥 -y： This option will read a private OpenSSH format file and print an OpenSSH public key to stdout Key based authentication ssh will look into .ssh/authorized_keys to determine which clients it should let in cat .ssh/id_ed25519.pub | ssh foobar@remote &#x27;cat &gt;&gt; ~/.ssh/authorized_keys&#x27; or: ssh-copy-id -i .ssh/id_ed25519.pub foobar@remote ​ ​ Copying files over SSH There are many ways to copy files over ssh: ssh+tee, 把本地文件传到远程 cat localfile | ssh remote_server &quot;tee serverfile&quot; tee: 将标准输入写入文件 scp: 就是ssh + cp 把本机文件传到远程： scp path/to/local_file remote_host:path/to/remote_file 传远程文件到本机： scp remote_host:path/to/remote_file path/to/local_file -r:传文件夹 scp没穿输完也会生成目标文件，因此断开scp传输后，你依然能在目标主机上看到目标文件，但是切记，这个文件是不完整的 ​ 把远程文件传到本机： rsync improves upon scp by detecting identical files in local and remote, and preventing copying them again. It also provides more fine grained control over symlinks, permissions and has extra features like the --partial flag that can resume from a previously interrupted copy. rsync has a similar syntax to scp. Port Forwarding 见《SSH Port Forwarding》 SSH Configuration alias: alias my_server=&quot;ssh -i ~/.id_ed25519 --port 2222 -L 9999:localhost:8888 foobar@remote_server 好处是可以继续调用别的命令 using ~/.ssh/config. Host vm User foobar HostName 172.16.174.141 Port 2222 IdentityFile ~/.ssh/id_ed25519 LocalForward 9999 localhost:8888# Configs can also take wildcardsHost *.mit.edu User foobaz","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[]},{"title":"GPG","slug":"GPG","date":"2022-02-10T14:57:00.000Z","updated":"2022-09-26T06:39:34.930Z","comments":true,"path":"2022/02/10/GPG/","link":"","permalink":"http://lyk-love.cn/2022/02/10/GPG/","excerpt":"useful encryption tool","text":"useful encryption tool Intro Pretty Good Privacy (PGP) is a nice example of an e-mail encryption scheme written by Phil Zimmermann in 1991 [PGP 2020]. Depending on the version, the PGP soft- ware uses MD5 or SHA for calculating the message digest; CAST, triple-DES, or IDEA for symmetric key encryption; and RSA for the public key encryption. 不过PGP是商用软件, 因此自由软件基金会( FSF )开发了PGP的替代品GnuPG, 也就是GPG. 这两个软件事实上差不多, 我们接下来介绍GPG. GPG( or PGP )的实现原理参见拙著Security in Internet --&gt; Application Layer: Securing E-Mail install 输入gpg --help可以查看你是否已经安装 generate GPG key gpg --full-generate-key 按照提示输入信息，密钥类型使用默认的RSA and RSA即可。密钥长度推荐使用4096, 然后输入你的个人信息，这样密钥就会绑定到你的邮箱，要使用和 Git 提交相同的邮箱地址。 最后输入私钥的密码，用来提取这个密钥。这样一个 GPG 密钥就生成好了 generate revocation certificate gpg --gen-revoke [用户ID]:生成一张&quot;撤销证书&quot;，以备以后密钥作废时，可以请求外部的公钥服务器撤销你的公钥 key management list keys public key --list-keys: 列出系统中已有的公钥 显示如下： /home/lyk/.gnupg/pubring.kbx----------------------------pub rsa4096 2022-02-10 [SC] DFD9A8D9CF0BD747BE1BDBD839AD95E8DF842459uid [ultimate] LYK-love (gpg for lyk) &lt;your_email&gt;sub rsa4096 2022-02-10 [E] 第一行： 公钥文件名（pubring.kbx） 第二行： 公钥特征（4096位，Hash字符串和生成时间） 第三行： 密钥保质期，用户id， comment, email 第四行： 私钥特征 private key --list-secret-keys --keyid-format LONG &lt;your_email&gt;： 列出私钥 # 这里以别人的为例sec rsa4096/39AD95E8DF842459 2022-02-10 [SC] DFD9A8D9CF0BD747BE1BDBD839AD95E8DF842459uid [ultimate] LYK-love (gpg for lyk) &lt;your_email&gt;ssb rsa4096/051FBEBE8BDED88B 2022-02-10 [E] 39AD95E8DF842459就是用户ID的hash, 等价于用户ID（LYK-love or &lt;your_email&gt;） export keys gpg --armor --output public-key.txt --export pub [用户ID]： 导出该用户ID的公钥 公钥文件（pubring.kbx）以二进制形式储存，-armor可以将其转换为ASCII码显示 --output outputfile_name: 指定输出位置 export-secret-keys：导出私钥： gpg --armor --output private-key.txt --export-secret-keys upload public key 公钥服务器是网络上专门储存用户公钥的服务器 gpg --send-keys [用户ID] --keyserver hkp://subkeys.pgp.net：将公钥传到服务器subkeys.pgp.net，通过交换机制，所有的公钥服务器最终都会包含该公钥 gpg --fingerprint [用户ID]：生成公钥指纹 由于公钥服务器没有检查机制，任何人都可以用你的名义上传公钥，所以没有办法保证服务器上的公钥的可靠性。通常，因此需要公布一个公钥指纹，让其他人核对下载到的公钥是否为真 import public key 除了生成自己的密钥，还需要将他人的公钥或者你的其他密钥输入系统 gpg --import [密钥文件] 为了获得他人的公钥，可以让对方直接发给你，或者到公钥服务器上寻找 gpg --keyserver hkp://subkeys.pgp.net --search-keys [用户ID] 正如前面提到的，我们无法保证服务器上的公钥是否可靠，下载后还需要用其他机制验证 encrypt &amp;&amp; decrypt encrypt 假定有一个文本文件demo.txt，对它加密: gpg --recipient [用户ID] --output demo.en.txt --encrypt demo.txt --encrypt: 指定加密源文件 --recipient：指定接收者的公钥 --output： 指定加密后的文件名 decrypt 对方收到加密文件以后，就用自己的私钥解密。 gpg --decrypt demo.en.txt --output demo.de.txt --decrypt： 指定需要解密的文件 --output“： 指定解密后生成的文件 GPG允许省略decrypt的参数: gpg demo.en.txt 运行上面的命令以后，解密后的文件内容直接显示在STDOUT signiture file signiture 有时，我们不需要加密文件，只需要对文件签名，表示这个文件确实是我本人发出的 gpg --sign demo.txt: 在当前目录下生成demo.txt.gpg文件，这就是签名后的文件,默认采用二进制储存 --clearsign: 生成ASCII码的签名文件（ demo.txt.asc， 后缀名asc表示该文件是ASCII码形式 ） gpg --detach-sign demo.txt： 生成单独的签名文件，与文件内容分开存放 运行上面的命令后，当前目录下生成一个单独的签名文件demo.txt.sig。该文件是二进制形式的，如果想采用ASCII码形式，要加上armor参数。 gpg --armor --detach-sign demo.txt verify signiture 我们收到别人签名后的文件，需要用对方的公钥验证签名是否为真,--verify用来验证 gpg --verify demo.txt.asc demo.txt 举例来说，openvpn网站就提供每一个下载包的gpg签名文件。你可以根据它的说明，验证这些下载包是否为真 signiture + encryption 如果想同时签名和加密，可以使用下面的命令： gpg --local-user [发信者ID] --recipient [接收者ID] --armor --sign --encrypt demo.txt --local-user： 用发信者的名字作为私钥签名，该option必须放在前面。 该option是可选的，默认采用default- key --recipient： 指定用接收者的公钥加密 --armor： 采用ASCII码形式显示 --sign： 表示需要签名 --encrypt： 指定源文件 将 GPG 密钥与 Git 关联 git config --global user.signingkey 66DD4800155F7A2B# 或者git config user.signingkey 66DD4800155F7A2B Git 提交启用签名 在提交时启用签名很简单，只要在git commimt命令中加上-S选项即可。 1Git 提交时，使用 -S 标记进行 GPG 签名： git commit -S -m “commit message&quot; Git 可以设置默认使用 GPG 签名提交： git config --global commit.gpgsign true# 或者git config commit.gpgsign true 解决“error: gpg 数据签名失败” commit报错： error: gpg 数据签名失败fatal: 写提交对象失败 solution： 在.zshrc里面加入一行代码 export GPG_TTY=$(tty) 重启 gpg-agent: 第一次配置，必须重启，否则签名会失败: gpgconf –kill gpg-agent 重启终端（或者新开一个终端标签）","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[]},{"title":"debtap","slug":"debtap","date":"2022-02-10T06:02:24.000Z","updated":"2022-09-26T06:39:34.942Z","comments":true,"path":"2022/02/10/debtap/","link":"","permalink":"http://lyk-love.cn/2022/02/10/debtap/","excerpt":"A tool to traverse deb package. ref: here","text":"A tool to traverse deb package. ref: here install install before update: 国外源非常慢，需要换源： sudo nvim /usr/bin/debtap ：%s/ftp.debian.org/mirrors.ustc.edu.cn/g ：%s/archive.ubuntu.com/mirrors.ustc.edu.cn/g update debtap： sudo debtap -u usage sudo debtap [file_name].deb 注意： 安装时会提示输入包名，以及license。包名随意，license就填GPL吧 上述操作完成后会在deb包同级目录生成xxx.tar.xz文件 静默模式 -q 略过除了编辑元数据之外的所有问题。 debtap -q xxx.deb 略过所有的问题（不推荐） debtap -Q xxx.deb 安装转换好的本地包 sudo yay -U xxx.tar.xz","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[]},{"title":"Data Wrangling","slug":"Data-Wrangling","date":"2022-02-08T18:14:27.000Z","updated":"2022-09-26T06:39:34.928Z","comments":true,"path":"2022/02/09/Data-Wrangling/","link":"","permalink":"http://lyk-love.cn/2022/02/09/Data-Wrangling/","excerpt":"Some tools for data wrangling ref: MIT lesson","text":"Some tools for data wrangling ref: MIT lesson sed sed is a “stream editor” that builds on top of the old ed editor. In it, you basically give short commands for how to modify the file, rather than manipulate its contents directly (although you can do that too). one of the most common ones is s: substitution sed 's/REGEX/SUBSTITUTION/'： Replace the first occurrence of a regular expression in each line of a file, and print the result REGEX: the regular expression you want to search for SUBSTITUTION`: the text you want to substitute matching text with 's/REGEX/SUBSTITUTION/g': Replace all occurrences of an extended regular expression in a file ssh myserver journalctl | grep sshd | grep &quot;Disconnected from&quot; | sed &#x27;s/.*Disconnected from //&#x27; REGEX REGEX special characters: . means “any single character” except newline * zero or more of the preceding match + one or more of the preceding match [abc] any one character of a, b, and c (RX1|RX2) either something that matches RX1 or RX2 ^ the start of the line $ the end of the line sed默认不转义特殊字符， 这与其他工具都相反。 使用-E开启转义 sed的REGEX是贪婪匹配 capture groups 使用捕获组来进行非贪婪匹配 Any text matched by a regex surrounded by parentheses is stored in a numbered capture group. These are available in the substitution (and in some engines, even in the pattern itself!) as \\1, \\2, \\3, etc. So: | sed -E &#x27;s/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \\[preauth\\])?$/\\2/&#x27; 将输出username sort sort will sort its input default: in lexicographic order -n: in numeric order k, --key=KEYDEF :sort via a key; KEYDEF gives location and type KEYDEF is F[.C][OPTS][,F[.C][OPTS]] for start and stop position, where F is a field number and C a character position in the field; both are origin 1, and the stop posi‐ tion defaults to the line's end. If neither -t nor -b is in effect, characters in a field are counted from the beginning of the preceding whitespace. OPTS is one or more single-letter ordering options [bdfgiMhnRrV], which override global ordering options for that key. If no key is given, use the entire line as the key. Use --debug to diagnose incorrect key usage. example: -k1,1即以第一个field为key排序， 而-k1即以第一个field直到行的末尾为key进行排序 ,n: sort until the nth field, where the default is the end of the line -r: sort in reverse order. uniq uniq -c will collapse consecutive lines that are the same into a single line, prefixed with a count of the number of occurrences ssh myserver journalctl | grep sshd | grep &quot;Disconnected from&quot; | sed -E &#x27;s/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \\[preauth\\])?$/\\2/&#x27; | sort | uniq -c | sort -nk1,1 | tail -n10 对username（sort默认按字典序）排序 tail Display the last part of a file. awk echo &quot;sfsdfs\\n asdfdas&quot; | awk &#x27;&#123;print $2&#125;&#x27; | paste -sd, explain: for every line, print the contents of the second field, BEGIN &#123; rows = 0 &#125;$1 == 1 &amp;&amp; $2 ~ /^c[^ ]*e$/ &#123; rows += $1 &#125;END &#123; print rows &#125; paste paste: Merge lines of files -s: join all the lines into a single line, using TAB as delimiter -d: using the specified delimiter paste -s -d delimiter file ssh myserver journalctl | grep sshd | grep &quot;Disconnected from&quot; | sed -E &#x27;s/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \\[preauth\\])?$/\\2/&#x27; | sort | uniq -c | sort -nk1,1 | tail -n10 | awk &#x27;&#123;print $2&#125;&#x27; | paste -sd, extract the usernames as a comma-separated list instead of one per line bc You can do math directly in your shell using bc, a calculator that can read from STDIN! For example, add the numbers on each line together by concatenating them together, delimited by +: | paste -sd+ | bc -l Or produce more elaborate expressions: echo &quot;2*($(data | paste -sd+))&quot; | bc -l You can get stats in a variety of ways st 优雅简洁的数据处理工具https://github.com/nferraz/st 与linux的st命令重名，需要改名(如scal) R R is another (weird) programming language that’s great at data analysis and plotting. 相比之前的工具更重量级 ssh myserver journalctl | grep sshd | grep &quot;Disconnected from&quot; | sed -E &#x27;s/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \\[preauth\\])?$/\\2/&#x27; | sort | uniq -c | awk &#x27;&#123;print $1&#125;&#x27; | R --no-echo -e &#x27;x &lt;- scan(file=&quot;stdin&quot;, quiet=TRUE); summary(x)&#x27;R is another (weird) programming language that’s great at data analysis and summary prints summary statistics for a vector, and we created a vector containing the input stream of numbers, so R gives us the statistics we wanted! gnuplot simple plotting http://www.gnuplot.info/ ssh myserver journalctl | grep sshd | grep &quot;Disconnected from&quot; | sed -E &#x27;s/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \\[preauth\\])?$/\\2/&#x27; | sort | uniq -c | sort -nk1,1 | tail -n10 | gnuplot -p -e &#x27;set boxwidth 0.5; plot &quot;-&quot; using 1:xtic(2) with boxes&#x27; xargs xargs ： Execute a command with piped arguments coming from another command, a file, etc. The input is treated as a single block of text and split into separate pieces on spaces, tabs, newlines and end-of-file. # Run a command using the input data as arguments: arguments_source | xargs command Sometimes you want to do data wrangling to find things to install or remove based on some longer list. The data wrangling we’ve talked about so far + xargs can be a powerful combo. For example, as seen in lecture, I can use the following command to uninstall old nightly builds of Rust from my system by extracting the old build names using data wrangling tools and then passing them via xargs to the uninstaller: rustup toolchain list | grep nightly | grep -vE &quot;nightly-x86&quot; | sed &#x27;s/-x86.*//&#x27; | xargs rustup toolchain uninstall","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[]},{"title":"awk","slug":"awk","date":"2022-02-08T18:13:53.000Z","updated":"2022-09-26T06:39:34.942Z","comments":true,"path":"2022/02/09/awk/","link":"","permalink":"http://lyk-love.cn/2022/02/09/awk/","excerpt":"awk is a an interpreted programming language that is good at processing text streams ref: awk tutorial","text":"awk is a an interpreted programming language that is good at processing text streams ref: awk tutorial AWK workflow Read AWK reads a line from the input stream (file, pipe, or stdin) and stores it in memory. Execute All AWK commands are applied sequentially on the input. By default AWK execute commands on every line. We can restrict this by providing patterns. Repeat This process repeats until the file reaches its end. Program Structure Let us now understand the program structure of AWK. Body Block The syntax of the body block is as follows − Syntax /pattern/ &#123;awk-commands&#125; awk programs take the form of an optional pattern plus a block saying what to do if the pattern matches a given line. The default pattern (which we used above) matches all lines pattern不仅可以是REGEX，还可以是条件表达式： # 输出奇数行$ awk -F &#x27;:&#x27; &#x27;NR % 2 == 1 &#123;print $1&#125;&#x27; demo.txt In the absence of a body block − default action is taken which is print the line. Inside the block, $0 : entire line’s contents, and $1 through $n : the nth field of that line, fields are separated by the awk field separator, (whitespace by default, change with -F) BEGIN &amp;&amp; END block this block is optional. Syntax BEGIN &#123;awk-commands&#125;END &#123;awk-commands&#125; The BEGIN block gets executed at program start-up. The END block executes at the end of the program 属于 AWK keyword， 必须大写 ===================================&gt; examole: | awk &#x27;$1 == 1 &amp;&amp; $2 ~ /^c[^ ]*e$/ &#123; print $2 &#125;&#x27; | wc -l The pattern says that the first field of the line should be equal to 1 (that’s the count from uniq -c), and that the second field should match the given regular expression. And the block just says to print the username. We then count the number of lines in the output with wc -l. command line ark command 必须用单引号括起来 awk [options] &#x27;command&#x27; target_file program File We can provide AWK commands in a script file awk [options]-f source_code_file target_file standard options -v: assigns a value to a variable. It allows assignment before the program execution. $ awk -v name=Jerry &#x27;BEGIN&#123;printf &quot;Name = %s\\n&quot;, name&#125;&#x27; --dump-variables[=file]: prints a sorted list of global variables and their final values to file. The default file is awkvars.out $ awk --dump-variables &#x27;&#x27;$ cat awkvars.out --help --lint[=fatal] : enables checking of non-portable or dubious constructs. When an argument fatal is provided, it treats warning messages as errors -posix option: turns on strict POSIX compatibility, in which all common and gawk-specific extensions are disabled --profile[=file]: generates a pretty-printed version of the program in file. Default file is awkprof.out example: [jerry]$ awk --profile &#x27;BEGIN&#123;printf&quot;---|Header|--\\n&quot;&#125; &#123;print&#125; END&#123;printf&quot;---|Footer|---\\n&quot;&#125;&#x27; marks.txt &gt; /dev/null [jerry]$ cat awkprof.out output: # gawk profile, created Sun Oct 26 19:50:48 2014 # BEGIN block(s) BEGIN &#123; printf &quot;---|Header|--\\n&quot; &#125; # Rule(s) &#123; print $0 &#125; # END block(s) END &#123; printf &quot;---|Footer|---\\n&quot; &#125; --traditional : disables all gawk-specific extensions. variable awk变量不需要定义和声明，可以直接使用，初始值为0 [jerry]$ awk &#x27;/a/&#123;++cnt&#125; END &#123;print &quot;Count = &quot;, cnt&#125;&#x27; marks.txt built-in variables: ARGC ARGV CONVFMT: It represents the conversion format for numbers. Its default value is %.6g ENVIRON: It is an associative array of environment variables. awk 'BEGIN &#123; print ENVIRON[&quot;USER&quot;] &#125;' FILENAME: 当前处理的文件名 FS: 字段分隔符，默认是空格和制表符, 可以用-F更改 awk 'BEGIN &#123;print &quot;FS = &quot; FS&#125;' | cat -vte NF: 表示当前行的字段数，因此$NF就代表最后一个字段。 echo -e &quot;One Two\\nOne Two Three\\nOne Two Three Four&quot; | awk 'NF &gt; 2' $ echo &#x27;this is a test&#x27; | awk &#x27;&#123;print $NF&#125;&#x27;test NR：表示当前处理的行号 OFMT: It represents the output format number and its default value is %.6g. OFS: 输出字段的分隔符，用于打印时分隔字段，默认为空格。 awk 'BEGIN &#123;print &quot;OFS = &quot; OFS&#125;' | cat -vte ORS: 输出记录的分隔符，用于打印时分隔记录，默认为换行符。 REGEX 与其他语言的regex相同 echo -e &quot;Apple Juice\\nApple Pie\\nApple Tart\\nApple Cake&quot; | awk &#x27;/Apple (Juice|Cake)/&#x27; #output：Apple JuiceApple Cake Array assign : array_name[index] = value 不需要定义或声明 index可以是string或number Creating Array [jerry]$ awk &#x27;BEGIN &#123; fruits[&quot;mango&quot;] = &quot;yellow&quot;; fruits[&quot;orange&quot;] = &quot;orange&quot; print fruits[&quot;orange&quot;] &quot;\\n&quot; fruits[&quot;mango&quot;]&#125;&#x27;#outputorangeyellow Deleting Array Elements delete array_name[index] Multi-Dimensional arrays array_name[index, index] = value control flow 与其他语言同 if if (condition) action [jerry]$ awk &#x27;BEGIN &#123;num = 10; if (num % 2 == 0) printf &quot;%d is even number.\\n&quot;, num &#125;&#x27; if-else if (condition) action-1else action-2 [jerry]$ awk &#x27;BEGIN &#123; num = 11; if (num % 2 == 0) printf &quot;%d is even number.\\n&quot;, num; else printf &quot;%d is odd number.\\n&quot;, num &#125;&#x27; if -else if [jerry]$ awk &#x27;BEGIN &#123; a = 30; if (a==10) print &quot;a = 10&quot;; else if (a == 20) print &quot;a = 20&quot;; else if (a == 30) print &quot;a = 30&quot;;&#125;&#x27; loop for [jerry]$ awk &#x27;BEGIN &#123; for (i = 1; i &lt;= 5; ++i) print i &#125;&#x27; while [jerry]$ awk &#x27;BEGIN &#123;i = 1; while (i &lt; 6) &#123; print i; ++i &#125; &#125;&#x27; built-in function arithmetic function sin()：正弦。 cos()：余弦。 sqrt()：平方根。 rand()：随机数。 string function length(arg) [jerry]$ awk &#x27;length($0) &gt; 18&#x27; marks.txt print item1，item,... 各项目间使用逗号分隔开，而输出时以OFS为分隔 printf &quot;format&quot;, expr,expr,... [jerry]$ awk &#x27;BEGIN &#123; param = 1024.0 result = sqrt(param) printf &quot;sqrt(%f) = %f\\n&quot;, param, result&#125;&#x27; asort(arr [, d [, how] ]) This function sorts the contents of arr using GAWK's normal rules for comparing values, and replaces the indexes of the sorted values arr with sequential integers starting with 1. [jerry]$ awk &#x27;BEGIN &#123; arr[0] = &quot;Three&quot; arr[1] = &quot;One&quot; arr[2] = &quot;Two&quot; print &quot;Array elements before sorting:&quot; for (i in arr) &#123; print arr[i] &#125; asort(arr) print &quot;Array elements after sorting:&quot; for (i in arr) &#123; print arr[i] &#125;&#125;&#x27;#OutputArray elements before sorting:ThreeOneTwoArray elements after sorting:OneThreeTwo asorti(arr [, d [, how] ]) The behavior of this function is the same as that of asort(), except that the array indexes are used for sorting. [jerry]$ awk &#x27;BEGIN &#123; arr[&quot;Two&quot;] = 1 arr[&quot;One&quot;] = 2 arr[&quot;Three&quot;] = 3 asorti(arr) print &quot;Array indices after sorting:&quot; for (i in arr) &#123; print arr[i] &#125;&#125;&#x27;#OutputArray indices after sorting:OneThreeTwo gsub(regex, sub, string) global substitution. It replaces every occurrence of regex with the given string (sub). The third parameter is optional. If it is omitted, then $0 is used. [jerry]$ awk &#x27;BEGIN &#123; str = &quot;Hello, World&quot; print &quot;String before replacement = &quot; str gsub(&quot;World&quot;, &quot;Jerry&quot;, str) print &quot;String after replacement = &quot; str&#125;&#x27;#outputString before replacement = Hello, WorldString after replacement = Hello, Jerry match(str, regex) It returns the index of the first longest match of regex in string str. It returns 0 if no match found. [jerry]$ awk &#x27;BEGIN &#123; str = &quot;One Two Three&quot; subs = &quot;Two&quot; ret = match(str, subs) printf &quot;Substring \\&quot;%s\\&quot; found at %d location.\\n&quot;, subs, ret&#125;&#x27;On executing this code, you get the following result −#OutputSubstring &quot;Two&quot; found at 5 location split(str, arr, regex) splits the string str into fields by regular expression regex and the fields are loaded into the array arr. If regex is omitted, then FS is used. [jerry]$ awk &#x27;BEGIN &#123; str = &quot;One,Two,Three,Four&quot; split(str, arr, &quot;,&quot;) print &quot;Array contains following values&quot; for (i in arr) &#123; print arr[i] &#125;&#125;&#x27;#outputArray contains following valuesOneTwoThreeFour strtonum(str) This function examines str and return its numeric value. If str begins with a leading 0, it is treated as an octal number. If str begins with a leading 0x or 0X, it is taken as a hexadecimal number. Otherwise, assume it is a decimal number. [jerry]$ awk &#x27;BEGIN &#123; print &quot;Decimal num = &quot; strtonum(&quot;123&quot;) print &quot;Octal num = &quot; strtonum(&quot;0123&quot;) print &quot;Hexadecimal num = &quot; strtonum(&quot;0x123&quot;)&#125;&#x27;#outputDecimal num = 123Octal num = 83Hexadecimal num = 291 sub(regex, sub, string) This function performs a single substitution. It replaces the first occurrence of the regex pattern with the given string (sub). The third parameter is optional. If it is omitted, $0 is used. substr(str, start, l) This function returns the substring of string str, starting at index start of length l. If length is omitted, the suffix of str starting at index start is returned. tolower()：字符转为小写 user-defined functions function function_name(argument1, argument2, ...) &#123; function body&#125; example # Returns minimum numberfunction find_min(num1, num2)&#123; if (num1 &lt; num2) return num1 return num2&#125; redirection Redirections in AWK are written just like redirection in shell commands, except that they are written inside the AWK program pretty printing Horizontal Tab [jerry]$ awk &#x27;BEGIN &#123; printf &quot;Sr No\\tName\\tSub\\tMarks\\n&quot; &#125;&#x27;#output#Sr No Name Sub Marks Backspace \\b删除前一个字符 [jerry]$ awk &#x27;BEGIN &#123; printf &quot;Field 1\\bField 2\\bField 3\\bField 4\\n&quot; &#125;&#x27;#Output#Field Field Field Field 4 On executing this code, you get the following result − Format Specifier 同其他语言","categories":[{"name":"Language","slug":"Language","permalink":"http://lyk-love.cn/categories/Language/"}],"tags":[{"name":"awk","slug":"awk","permalink":"http://lyk-love.cn/tags/awk/"}]},{"title":"Blindness","slug":"Blindness","date":"2022-02-04T00:29:27.000Z","updated":"2022-09-26T06:39:34.925Z","comments":true,"path":"2022/02/04/Blindness/","link":"","permalink":"http://lyk-love.cn/2022/02/04/Blindness/","excerpt":"他早已失明，现在依然失明，只是走到厨房门口，听见了她们在阳台上说的话，听见了笑声雨声和水声，呼吸到了带肥皂味的空气，然后回到了沙发上，正在想世界上还存在生活，正在问这生活是否还有他的一份 JOSÉ SARAMAGO","text":"他早已失明，现在依然失明，只是走到厨房门口，听见了她们在阳台上说的话，听见了笑声雨声和水声，呼吸到了带肥皂味的空气，然后回到了沙发上，正在想世界上还存在生活，正在问这生活是否还有他的一份 JOSÉ SARAMAGO 萨拉马戈的《失明症漫记》，和戈尔丁的《蝇王》一样，是社会试验类的小说。作者构思一个社会环境，然后把人放进去，观察人类的行为。 《蝇王》把人放进荒岛里，探究人性的原恶，而《失明症漫记》把人关押进政府看管的精神病院，揭露的丑恶不只在于人性，还有黑暗的社会。 个人认为，论思想之深刻和语言之深沉，还是《蝇王》更胜一筹。在语言上，萨拉马戈的书面语言是口语化的，节奏缓慢，句子很长，人物对话充斥其间。他本人认为他的作品应该被大声朗读，就像一个人给另一个人讲故事那样，这样才能抓住节奏。而口语化的行文肯定不如正式语言字斟句酌，因此有失深沉。另一方面，《蝇王》揭露的人性原恶在人类中普遍地存在，其结论放之四海而皆准。而本书人类的悲惨境遇，很多是不合理的社会结构导致的： 与人民脱节的政府、冷酷的军队、无人性的体制，这些是当时拉美社会现状的具体反映，不具有普世意义。我想说的是，随着文明的进步和制度的完善，这些问题或多或少地可以改善。 《失明症漫记》可以看成严肃文学版的末世求生，失明症在人群中蔓延，造成恐慌，接着社会崩溃，人性沦丧，有超能力的主角，带队生存..... 这个idea在成书的年代还很新颖,现在已经见怪不怪了。 人性，若有若无的晨曦，永远不要相信它，也永远不要否定它。 我们都知道，被坏人背弃，被许多人抵制的道德感其实自古有之，并且随着社会的发展，道德感已经与血液的颜色和眼泪的咸淡混为一谈。作为延伸，我们的眼睛就是灵魂的镜子，往往毫无保留地展示出我们嘴上试图否认的东西。 失明症是一种神奇的病症，传染没有理由，近乎随机，与普通失明不同，失明症患者看到眼前是一片发光的白色，仿佛眼球掉进了牛奶的海洋。我们不必深究这个病症的来源、细节和背后的寓意，总之它让女主角之外的人类失去了视力 ，让社会陷入混乱，欲望无节制地解放。人类回到原始丛林时代，区别在于不是区区几千男女生活在广袤的大自然中，而是百万千万的男女生活在一个贫瘠干涸的世界上。这是个失明的世界。 像处理任何大型传染病一样，失明症患者们被收容、隔离，主角们被收容到一家精神病院。在一个封闭且相对平等的社会环境内，没过多久就发生了暴力冲突和阶层分化。这一幕我们很熟悉，拥有武力的人组成统治集团，然后占有社会资源：钱财、食物、住所，最后发展成女人。 男人争抢性奴隶，如同一群鬣狗争夺一个骨架。政府在这一过程中冷眼旁观，“病人死了，疾病也就没有了”，抱着这个理念，病人间自相残杀是默许的，甚至未得病的人也可以被送到这儿遭受迫害，毕竟死人都是盲人。 经历残酷的性虐待后，几位女性奋起反抗，发动群众推翻了统治阶级，此时外面的世界已经在失明症下沦陷，看管的政府没有了，大门一扇扇敞开，疯子们走出了精神病院，混入外头盲人组成的混乱的人潮中，像大海中不停涌动的不知道往哪里去的浪涛。最终一场暴雨洗涤了人类肮脏的身体，人们逐渐复明了。 我们永远不能相信人性，不能预先知道一个人会干出什么事来，只有时间能检验一个人，时间是坐在牌桌对面的对手，他手中有各种牌，我们必须想法打出与生命一样的牌，那就是我们的生活。 但是，可以相信美好的存在，暴雨之后，总有东西从我们头顶掠过， 一只鸟，一片云，或者一片微弱的光亮。","categories":[{"name":"Literature","slug":"Literature","permalink":"http://lyk-love.cn/categories/Literature/"}],"tags":[{"name":"Latin American literatures","slug":"Latin-American-literatures","permalink":"http://lyk-love.cn/tags/Latin-American-literatures/"}]},{"title":"需求分析","slug":"需求分析","date":"2021-12-29T21:44:50.000Z","updated":"2022-09-26T06:39:34.944Z","comments":true,"path":"2021/12/30/需求分析/","link":"","permalink":"http://lyk-love.cn/2021/12/30/%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90/","excerpt":"Outline: 流程 面向对象建模 顺序图 状态图 OCL CRC","text":"Outline: 流程 面向对象建模 顺序图 状态图 OCL CRC 流程 需求细化 用例图 解决方案细节 需求优先级 需求协商 建立分析模型 结构化建模（没写） 面向对象建模 面向对象建模 顺序图 组合 alt：选择 opt：条件执行 loop：循环 break：跳出循环 par：并行 critical：原子操作，不可被打断 strict：顺序执行 seq：不同生命线的可以并发执行（google和bing或yahoo可以并发，bing和yahoo必须顺序） 示例 状态图 简单示例 组合 并发 入口与出口 决策 汇集点 终止与历史状态 对象约束语言OCL 不变量 类元需要保持它的表达式取值在指定的时间范围内或者指定的条件下始终为“真” 最常见的是用来约束类的属性或者类的方法 前置条件与后置条件 前置条件要求类元在执行操作之前必须保证前置条件的表达式为真 后置条件要求类元在操作执行完成之后必须保证后置条件的表达式为真 监护条件 在状态机到达转移点时，监护条件的表达式需要根据实际状态进行评估，并只有在表达式实际取值为“真”的情况下才进行转移 示例 CRC Candidates、 Responsibilities和 Collaborators三者的缩写 基于CRC可以建立一种索引卡片，被称为CRC卡，每个卡片代表了一个被发现的候选对象 示例","categories":[{"name":"Business","slug":"Business","permalink":"http://lyk-love.cn/categories/Business/"}],"tags":[{"name":"需求与商业模式创新","slug":"需求与商业模式创新","permalink":"http://lyk-love.cn/tags/%E9%9C%80%E6%B1%82%E4%B8%8E%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F%E5%88%9B%E6%96%B0/"}]},{"title":"需求与商业模式创新  readme","slug":"需求与商业模式-readme","date":"2021-12-29T15:35:44.000Z","updated":"2022-09-26T06:39:34.944Z","comments":true,"path":"2021/12/29/需求与商业模式-readme/","link":"","permalink":"http://lyk-love.cn/2021/12/29/%E9%9C%80%E6%B1%82%E4%B8%8E%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F-readme/","excerpt":"Outline: 阅读说明","text":"Outline: 阅读说明 Powered by NJU BOX, 图床可能会失效，若发现这种情况，请私信我 想要文档和pdf的也可以私信我","categories":[{"name":"Business","slug":"Business","permalink":"http://lyk-love.cn/categories/Business/"}],"tags":[{"name":"需求与商业模式创新","slug":"需求与商业模式创新","permalink":"http://lyk-love.cn/tags/%E9%9C%80%E6%B1%82%E4%B8%8E%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F%E5%88%9B%E6%96%B0/"}]},{"title":"涉众分析","slug":"涉众分析","date":"2021-12-29T15:34:02.000Z","updated":"2022-09-26T06:39:34.944Z","comments":true,"path":"2021/12/29/涉众分析/","link":"","permalink":"http://lyk-love.cn/2021/12/29/%E6%B6%89%E4%BC%97%E5%88%86%E6%9E%90/","excerpt":"Outline 涉众评估 优先级评估 风险评估 共赢分析 案例分析： 阅文","text":"Outline 涉众评估 优先级评估 风险评估 共赢分析 案例分析： 阅文 涉众评估 优先级评估 涉众并不是完全平等的，有些涉众比其他涉众更为重要 优先考虑涉众的基本特征，尤其是任务特征：不一定出钱多的涉众就重要，可能使用系统更多或更重要功能、使用系统更频繁、规模更大的用户群体具有更高的优先级。 基于涉众扩展特征进行涉众优先级的评估：Power/Interest图 参与者：系统的实际使用者，对系统成功有较大影响力，对系统也有较大影响力，优先级最高。 环境设定者：很少使用系统，但是由于政治、经济等因素对系统有比较大的影响，优先级次之，最常见的是政府和管理者。 被影响者：可能是系统直接使用者，也可能是因为系统出现被剥夺了部分利益的输家，受影响大，能影响少，优先级一般低于环境设定这，但是特殊情况下也可能高于环境设定者。 观众：不受影响，也不影响，优先级最低，比如环境专家和市场力量。 风险评估 分析态度Power/Attitude图 强反对者是需要重点分析的。 涉众的关注点和兴趣去向也是重要内容，一般环境设定者是项目高风险因素。 对于高风险的涉众类别，要尽可能澄清各个涉众类别的角色和指着，发现项目对他们的依赖和假设条件，分析实际情况与预期不一致时可能出现的风险，并提前化解。 化解涉众风险策略 一方面提高环境设定者对系统的关注，转化为参与者 一方面消除强反对者的反对原因，变为强支持者 给予被影响者一些发表和实现自身意见的全体， 缓解忧虑。 共赢分析 发现冲突(建立Stakeholder/Issue关系图) 列出系统的所有涉众类别，明确描述他们的兴趣和对系统的期望 从涉众们的兴趣和期望中发现背后涉及的共同问题(Issue) 建立涉众类别和问题的关联，如果某个涉众类别对一个Issue存在兴趣，那么该涉众类别和这个Issue就存在关联关系 对每一个Stakeholder-Issue关系，标明该关系上面所被寄予的期望 如果某个Stakeholder-Issue关系上所寄予的期望与项目的业务需求无法保持一致，那么它关联的涉众就在该Issue的问题上和项目整体目标存在冲突 涉众和项目负责人互相调整、折中 重新评估项目的可行性 如果Stakeholder/Issue关系图中某个Issue所关联的不同关系标识有互相冲突的期望，那么就意味着它所关联的涉众在该Issue上存在需求冲突 分析各冲突方成为项目赢家的条件 适当的调整，化解冲突 分析项目在该Issue上的目标、约束和可选方案，并提供给冲突方进行权衡，促进他们之间协商解决 阅文的免费模式能否与订阅模式共赢？ Issue：免费带来的更多流量与写手身份转换 Stakeholder： 头部与底部写手、普通读者：可以接受 腰部写手：影响收入，进一步弱化保障 核心读者：担忧文章质量下降 能否共赢：免费与订阅在多大程度上共存 免费创作与订阅写作区分开，但共存 同时成为免费与订阅写手，或先从免费写手做起 可能达成共赢的Issue：利用免费阅读模式为平台引流 较难达成共赢的Issue：平台对IP的强力掌控与作者本身的著作权主张","categories":[{"name":"Business","slug":"Business","permalink":"http://lyk-love.cn/categories/Business/"}],"tags":[{"name":"需求与商业模式创新","slug":"需求与商业模式创新","permalink":"http://lyk-love.cn/tags/%E9%9C%80%E6%B1%82%E4%B8%8E%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F%E5%88%9B%E6%96%B0/"}]},{"title":"需求管理","slug":"需求管理","date":"2021-12-29T15:17:48.000Z","updated":"2022-09-26T06:39:34.945Z","comments":true,"path":"2021/12/29/需求管理/","link":"","permalink":"http://lyk-love.cn/2021/12/29/%E9%9C%80%E6%B1%82%E7%AE%A1%E7%90%86/","excerpt":"Outline: 需求管理的3个方法: 维护需求基线 实现需求跟踪 控制变更","text":"Outline: 需求管理的3个方法: 维护需求基线 实现需求跟踪 控制变更 维护需求基线 需求基线 基线：已经通过正式评审和批准的规格说明或产品，它可以作为进一步开发的基础，并且只有通过正式的变更控制过程才能修改它 基线是被明确和固定下来的需求集合，是项目团队需要在某一特定产品版本中实现的特征和需求集合 需求基线是需求开发过程的成果总结，在后续产品生命周期中持续发挥作用。 一般将需求基线编写成文档纳入配置管理。 需求基线的内容 标识符(ID)，为后续的项目工作提供一个共同的交流参照。 当前版本号(Version)，保证项目的各项工作都建立在最新的一致需求基础之上。 源头(Source)，在需要进一步深入理解或者改变需求时，可以回溯到需求的源头。 理由(Rational)，提供需求产生的背景知识。 优先级(Priority)，后续的项目工作可以参照优先级进行安排和调度。 状态(Status)，交流和具体需求相关的项目工作状况。 成本、工作量、风险、可变性(Cost、Effort、Risk、Volatility)，为需求的设计和实现提供参考信息，驱动设计和实现工作。 需求创建的日期 和需求相关的项目工作人员，包括需求的作者、设计者、实现者、测试者等 需求涉及的子系统 需求涉及的产品版本号 需求的验收和验证标准 … 需求基线的维护 配置管理 标识配置项 递增数值，例如1，2，…x； 层次式数值编码，例如1.1.1，1.2.1，…x.y.z； 层次式命名编码，例如Order.Place.Date，Order.Place.Register，…Task.Step.Substep 版本控制 每一条单独的需求需要进行版本控制 相关的需求文档也需要进行版本控制 每一个刚纳入配置管理的软件需求项赋予一个初始的版本号，并在需求发生变更时更新需求的版本号。 变更控制 访问审计：应当易于被项目涉众访问，记录和审计访问的情况，不应当很随意访问需求基线。 状态报告：反映需求基线的成熟度(变化的幅度越大，成熟度越低)、稳定性(改变的次数越多，稳定性越差)等 状态维护 实现需求跟踪 避免在开发过程或者演化过程中与需求基线不一致或者偏离的风险 需求跟踪是以软件需求规格说明文档作为基线，在向前和向后两个方向上，描述需求以及跟踪需求变化的能力。 需求跟踪 前向跟踪 前向跟踪是指被定义到软件需求规格说明文档之前的需求演化过程 向前跟踪到需求：说明涉众的需要和目标产生了哪些软件需求 从需求向后回溯：说明软件需求来源于哪些涉众的需要和目标 后向跟踪 后向跟踪是指被定义到软件需求规格说明文档之后的需求演化过程 从需求向前跟踪：说明软件需求是如何被后续的开发物件支持和实现的 回溯到需求的跟踪：说明各种系统开发的物件是因为什么原因(软件需求)而被开发出来的 需求跟踪的用途 需求的后向跟踪可以帮助项目管理者 评估需求变更的影响。 尽早发现需求之间的冲突，避免未预料的产品延期。 可以收集没有被实现的需求，并估算这些需求需要的工作量。 发现可以复用的已有组件，从而降低新系统开发的时间和精力。 明确需求的实现进度，跟踪项目的状态。 需求的后向跟踪可以帮助客户和用户： 评价针对用户需求的产品的质量。 可以确认成本上没有(昂贵的)镀金浪费。 确认验收测试的有效性。 确信开发者的关注点始终保持在需求的实现上。 需求跟踪中针对具体需求的设计方案选择、设计假设条件以及设计结果等信息可以帮助设计人员 验证设计方案正确的满足了需求。 评估需求变更对设计的影响。 在设计完，成很久之后仍然可以理解设计的原始思路。 评估技术变化带来的影响。 实现系统组件的复用。 需求跟踪信息还可以帮助维护人员 评估某一个需求变化时对其他需求的影响 评估需求变化时对实现的影响。 评估未变化需求对实现变更的允许度。 需求跟踪的内容 需求跟踪的内容依赖于项目的跟踪策略。 最低层次上，需求跟踪仅仅是捕获了产品内部各个系统组件之间的依赖、满足和实现关系。 最低层次上的需求跟踪策略使用者称为低端用户。 实现了工作背景和过程背景捕获的需求跟踪实现者称为高端用户。 产品本身所包含的各种联系、项目的组织过程 Traceability view 实现方法 需求跟踪的实现方法主要有矩阵、实体关系模型和交叉引用。 需求跟踪矩阵是最常用的，如下图所示。 优点：跟踪信息清晰易懂 缺点：只能表达二元的跟踪关系。 实体关系模型：使用实体关系模型来描述需求的跟踪联系，图17-5可以被认为是一个元模型示例。 优点：表达多元的跟踪关系，并且建立的跟踪信息可以利用关系数据库来实现、易于查询和维护。 缺点：不够直观，需要实体关系基础。 交叉引用：文档之间建立跟踪关系。 优点：直接，利于使用 缺点：只适用于需求文档的处理 建立过程 认识到需求跟踪的重要性，明确需求跟踪需要解决的问题 说明需求跟踪过程的目标 明确需要捕获的跟踪联系 组织提供资源支持和技术支持 制定有效的过程策略：需求跟踪过程与实际的项目开发工作融合，作为项目开发工作的一部分。 便利需求跟踪信息的使用：为客户、项目管理者以及开发者等项目涉众提供便利的使用途径。 需求依赖 大多数的需求并不是完全独立的，它们在一种复杂的机制中互相影响 需求之间的依赖联系对很多项目开发工作都有重要影响。例如在表17-3所示的需求依赖关系示例当中，R1依赖于R3和R4，那么在实现、变更或者复用R1时，就必须将R3和R4也考虑在内。 需求依赖联系的特殊性并不在于它的重要性，而在于它是难以发现、建立和维护的 需求交互作用管理：用于发现、管理和部署(disposition)需求之间关键联系的活动 需求变更控制 需求变化 需求开发时一个获取、明确并定义需求的过程，并且需求并不是在需求开发结束之后就会固定不变的。 需求的变化是正当和不可避免 问题发生了改变 环境发生了改变：比如法律变化和业务变化 需求基线存在缺陷 用户变动 用户对软件的认识变化 相关产品的出现 变更控制过程 以可控、一致的方式进行需求基线中需求的变更处理，包括对变化的评估、协调、批准或拒绝、实现和验证 变更控制过程 提交需求变化 评估需求变化可能带来的影响 利用需求跟踪信息确定变更的影响范围，包括需要修改的系统组件、文档、模型等。 依据需求依赖信息确定变更将会带来的冲突和连锁反应，确定解决的方法。 评估变更请求的优先级和潜在风险。 明确执行变更需要执行的任务，估算变更所需要的工作量和资源。 评价变更可能给项目计划带来的影响。 变更评估结果使用正式文档的方式固定，提交给变更控制委员会。 配置管理部门：批准后的变更请求被通知给所有需要修改工作产品的团队成员，由他们完成变更的修改工作。 变更控制委员会(CCB) 评价需求的变更，做出批准或者拒绝变化的决定，并确保已批准变化的实现 变更控制委员会可能由来自下列部门的人员组成 项目或程序管理部门 产品管理或者需求分析部门 开发部门 测试或者质量保障部门 市场或客户代表 编写用户文档的部门 技术支持或帮助部门 配置管理部门 注意事项 认识到变更的必要性，并为之制定计划 定义明确的变更控制过程，建立变更控制的有效渠道 所有提交的需求变更请求都要进行仔细的评估 是否进行变更的决定应该由变更控制委员会统一做出 必须对变更的实现结果进行验证 需求的变化情况要及时的通知到所有会受到影响的项目涉众 维护需求基线，审计变更记录：保证向项目涉众可以访问到最新的需求和变更情况。 管理范围蔓延 根据业务目标、产品前景和项目范围，评估每一项提议的新增需求和特性 对不合理的需求说&quot;不&quot; 灵活应对变更请求 推迟产品的交付时间 要求增派人手：在有限的情况下有效 要求员工加班工作：只能适度的使用 推迟或者去除尚未实现的优先级较低的需求 容许产品质量的降低：尽量不使用 使用辅助工具：工具应该具有以下几个特性，以支持需求变更过程： 可用定义变更请求中的数据项 可用辅助项目涉众完成变更控制过程中的协作 可以帮助维护需求基线，审计变更记录 能够将变更情况及时的通知到相关人员 可以生成标准的和定制的报告和图表 需求管理的实践调查 需求的变更 有效处理变更非常重要 新增(Added)需求影响最大 缺陷修复最为频繁 范围蔓延常见 需求可变性很高 变更控制还需要继续完善 需求跟踪 重视和关注了对后向跟踪联系的处理 忽视了对前向跟踪联系的处理 最低层次需求跟踪策略存在广泛 高端用户的需求跟踪实现仍需努力 需求之间的依赖关系困难和复杂 只有大概20％的需求是完全独立的 20％左右的需求产生了所有依赖关系的75％。 需求管理工具 非常需要需求管理工具 通用的文本处理器(Word Processor)和电子表格(Spreadsheet)使用最为广泛 部分组织自己开发了专用需求管理工具 很少有组织使用专用的商业需求管理工具 无法和软件的开发过程以及其他辅助工具进行有效的集成 实例分析 经常出现一个模块的需求刚刚整理完毕或者程序编写了一半，业务已经发生变化的情况。在一年的开发过程中，我们陆续接到的业务变更行政命令多达几十条，这给整个软件开发和推广都带来了很大困惑。为了保证软件正常运行，省局信息中心不得不专门成立了软件推广维护小组，不断就新业务改写程序，各地也不得不在后续的过程中不停的从省局下载新的升级包，好在这件事情已经经历了很多年，大家都已经习惯了。","categories":[{"name":"Business","slug":"Business","permalink":"http://lyk-love.cn/categories/Business/"}],"tags":[{"name":"需求与商业模式创新","slug":"需求与商业模式创新","permalink":"http://lyk-love.cn/tags/%E9%9C%80%E6%B1%82%E4%B8%8E%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F%E5%88%9B%E6%96%B0/"}]},{"title":"需求获取","slug":"需求获取","date":"2021-12-29T14:40:46.000Z","updated":"2022-09-26T06:39:34.945Z","comments":true,"path":"2021/12/29/需求获取/","link":"","permalink":"http://lyk-love.cn/2021/12/29/%E9%9C%80%E6%B1%82%E8%8E%B7%E5%8F%96/","excerpt":"Outline: 需求获取 面谈 原型 观察 文档审查 确定项目的前景与范围（需求获取的重要过程） 简单情况下可以进行问题分析，复杂情况下考虑进行目标分析，必要时辅以业务过程分析 问题分析 目标分析 业务过程分析（没记）","text":"Outline: 需求获取 面谈 原型 观察 文档审查 确定项目的前景与范围（需求获取的重要过程） 简单情况下可以进行问题分析，复杂情况下考虑进行目标分析，必要时辅以业务过程分析 问题分析 目标分析 业务过程分析（没记） 需求获取的过程 研究应用背景，建立初始的知识框架； 根据获取的需要，采用必要的获取方法和技巧； 先行确定获取的内容和主题，设定场景； 分析用户的高（深）层目标，理解用户的意图； 进行涉众分析，针对涉众的特点开展工作。 需求获取的要点 来源 涉众 用户：最主要的 客户 领域专家 市场人员、销售人员等其他用户替代源 硬数据 登记表格、单据、报表等定量文档 备忘录、日志等定性文档 相关产品 原有系统 竞争产品 协作产品(和解系统存在接口的其他软件系统) 重要文档 原有系统的规格说明 竞争产品的规格说明 协作产品的规格说明 客户的需求文档(委托开发的规格说明、招标书) 相关技术标准和法规 相关法律、法规及规章制度 行业规范、行业标准、领域参考模型 方法 面谈：常规方法 集体面谈： 快速方法 调查表：用户分散 头脑风暴： “”发明“需求 不确定性： 原型 情景性： 观察 传统方法： 文档审查 过程 注意事项 在整体上制定组织方案：确定系统的边界，(最好)建立上下文图或系统用例图 维护项目的前景和范围 引导和控制获取过程 适当修改不准确的前景和范围 接受需求的不稳定性：世界是随时变化的，用户随世界而变化，需接受 控制探索性工作( 例如模拟和原型)：有延期和超支的风险，可考虑额外立项或增量式开发 结束获取活动的判断条件 用户想不出更多的用例； 用户想出的新用例都是导出用例(通过其他用例的结合可以推导出该用例)； 用户只是在重复已经讨论过的问题； 新提出的特性、需求等都在项目范围之外； 新提出的需求优先级都很低； 用户提出的新功能都属于后继版本，而非当前版本 获取的结果 笔录(Elicitation Notes) 用户需求、问题域知识和约束 可能具有组织差、冗余、遗漏、自相矛盾等诸多问题 可以包括文字记录、录音、摄像等各种形式 可能会产生两份定义明确的正式文档(与需求分析结合) 项目前景和范围文档 用例文档 面谈 问题类型 开放式问题 被会见者对答复的选择可以是开放和不受限制的，他们可能答复两个词，也可能答复两段话。 在希望得到丰富（具有一定深度和广度）信息时，开放式问题比较合适 Ex. “你觉得把所有的经理都置于一个内联网内怎么样？” 封闭式问题 答案有基本的形式，被会见者的回答是受到限制的 Ex. “下列信息中哪个对你最有用：（1）填好的客户投诉单；（2）访问web站点的客户的电子邮件投诉；（3）与客户面对面的交流；（4）退回的货物。” 探究式问题 深入探讨某个问题 Ex. 为什么？你能举个例子吗？你能详细描述一下吗？ 诱导性问题 诱导问题的答案 Ex. “你和其他经理一样，都同意把财产管理计算机化，是吗” 双筒问题 有两个问题 Ex. “每天你通常会做什么决策，你是怎样做的” 元问题 关于面谈本身的问题 Ex. 我的问题看起来相关吗？你的回答正式吗？你是回答这些问题的最佳人选吗？我问了太多的问题吗？我还应该见什么人？ 问题准备 前期 开放式问题为主 决策层以专家为主，保证快速收敛 遵循：问题-&gt;目标-&gt;解决方案路线 问题、目标 目标、任务(流程任务) 分析基本的涉众特点：角色、任务、个人目标、频率、优先级 后期 封闭式问题为主 抓住主题与线索：例如，任务分解、流程图、界面示意 问题针对性 任务分解关系 流程正确性、异常 界面中的行为、数据项 事先准备面谈记录材料 原型 探索式(exploratory) 以缺陷需求开始继而不断调整和修正需求的原型开发方式称为探索式 要尽可能的调整各种设计选项 实验式(experimental) 以清晰的用户需求和模糊的实现方法、实现效果、可行性开始，明确需求的可行性和技术实现方案 定义一个对原型的评估方法，确定评估的属性 演化式(evolutionary) 以清晰的原型化需求和项目积累下来的原型资产为开始 原型化的需求，也有项目积累下来的原型资产 探索式和实验式方法产生的原型产品又被称为抛弃式原型 花费最小的代价，争取最快的速度 可能会使用简易的开发工具和不成熟的构造技术 可能会忽略或简化处理原型目的不相关的功能特征 要坚决的抛弃 演化式原型方法产生的原型产品被称为演化式原型(evolutionary prototype) 质量要从一开始就能达到最终系统的要求 要易于进行扩展和频繁改进，因此开发者必须重视演化式原型的设计 仅应该被用于处理清晰的需求、规格说明和技术方案 因为基于不确定的需求基础，所以抛弃式原型难免反复修改，导致代码质量较低，应该坚决抛弃。 抛弃式原型的贡献不在于它的代码，而是它所包含的内容，它说明了正确的需求和正确的技术方案，如果认识不到这一点，他们就只能得到低质量的代码，而丢失真正宝贵的内容 观察 应用于用户无法完成主动的信息告知的情况下 采样观察(Sampling Observation)：传统且简单，对特定时间段或特定事件进行观察。 民族志(Ethnography)：长期且浸入式，观察者深入用户较长时间 话语分析(Discourse Analysis)：对用户交谈行为观察，观察和分析交互方式或特定话语分析 协议分析(Protocol Analysis)：对用户任务的观察，一边观察对象一边执行任务 任务分析(Task Analysis)：对人机交互行为进行的观察，引入相关的模型方法来观察、记录和执行用户与软件系统的交互行为。 文档审查 文档审核是传统的需求获取方法，专门对文档进行需求获取活动。 范畴包括相关产品(原有产品和竞品)的需求规格说明、硬数据和客户的需求文档(委托开发的规格说明、招标书) 确定项目前景和范围 原因 在看待现实世界时：世界是复杂的，从不同的角度观察(目的与条件)，会看到不同的内容(抽象与映射) 因此有2个问题： 如何保证项目涉众以符合项目需要的角度描述现实世界？ 描述哪些事物和事件才会尽可能的符合项目的需要？ 第一个问题：项目的目标就是系统的业务需求，在简单情况下可以进行问题分析，复杂情况下考虑进行目标分析，必要时辅以业务过程分析。 项目前景与范围文档：业务需求、高层解决方案以及系统特性，还有部分涉众分析的结果——涉众特征分析。 前景描述产品用来干什么，将所有的涉众都统一到一个方向，所有的涉众都从共同认同的项目前景出发，理解和描述问题域及需求 范围指出了当前项目是要解决的产品长远规划中的哪一部分，限定了需求的界限，范围内的事物和事件是描述的目标 作用 关键 定义业务需求和能够满足需求的高层解决方案，包括: 业务目标、目的 高层业务功能 每个高层业务功能所关联的高层数据 每个功能相关的项目涉众 ...... 如果存在不同业务需求之间的冲突，那么在确定项目前景和范围阶段必须予以解决：不然会导致软件很难甚至无法继续推进。 过程 问题分析 获取问题 问题分析的前提是获取问题，通过收集背景资料或者与涉众沟通来实现 收集背景资料时要收集业务描述及其统计数据关注业务困难与问题。 与涉众的沟通主要通过面谈完成。 示例：×××连锁商店是一家刚刚发展起来的小型连锁商店，其前身是一家独立的小百货门面店。原商店只有销售的收银部分使用软件处理，其他业务都是手工作业，这已经不能适应它的业务发展要求。首先是随着商店规模的扩大，顾客量大幅增长，手工作业销售迟缓，顾客购物排队现象严重，导致流失客源。其次是商店的商品品种增多，无法准确掌握库存，商品积压、缺货和报废的现象上升明显。再次是商店面临的竞争比以前更大，希望在降低成本，吸引顾客，增强竞争力的同时，保持盈利水平。 P1：手工作业销售迟缓，效率不高。 P2：商店的商品品种太多，无法准确掌握库存。 P3：成本不够低，导致竞争力不强，盈利水平不够。 P4：顾客不够多，销售额不高，盈利水平不够。 对每个问题：发现问题 $\\rightarrow$ 明确问题 $\\rightarrow$ 发现业务需求 $\\rightarrow$ 定义问题解决方案及系统特性，得到每一个问题的业务需求和解决方案(特性、边界及约束) 明确问题 对问题达成共识： 描述问题并在涉众之间取得认同 判断问题的明确性 问题的明确性要求它们具备以下两点： 易于理解； P1. 图书管理员：图书总是无法上架。 P2. 图书管理员：图书的内容分类不合适，无法分类上架图书上架的工作太繁杂，导致来不及上架。图书的借阅不遵守章程，不能保证上架。 能指明解决的方向 P3. 决策者：生产的废品过多。 发现问题背后的问题 发现业务需求 每一个明确、一致的问题都意味着涉众存在一些相应的期望目标，即业务需求。 一般情况下，业务需求就是问题的反面 P3. 决策者：生产的废品过多 BR2：提供销售订单的准确性，在系统使用后3个月内，减少50%因此而产生的废品。 注意：业务目标要具有第二章所述的各种优秀特性，尤其是要有可验证性 定义解决方案及系统特性 确定高层次的解决方案 发现各种可行的高层次解决方案，分析不同方案的业务优势和代价，然后通过和涉众的协商，选定其中一个 确定系统特性 明确该解决方案需要具备的功能特征，即系统特性 特性是对一系列内聚的相互联系的需求（要求）、领域特征和规格的总称[Classen2008]。通常，一个特性内聚于一个目标与任务，反映了系统与外界一次有价值的完整互动过程（一组任务的要求） 确定解决方案的边界 分析解决方案需要和周围环境形成的交互作用，定义解决方案的边界 面向对象方法：用例图 涉及哪些用户？ 用户的目标有哪些？需要执行的任务有哪些？ 建立用例图(角色：用户； 用例：目标-任务) 结构化方法：上下文图(DFD) 它需要的信息由谁提供？ 它产生的信息由谁使用？ 谁控制它的执行？ 谁会影响它的执行？ 确定解决方案的约束 案例 描述 ×××连锁商店是一家刚刚发展起来的小型连锁商店，其前身是一家独立的小百货门面店。原商店只有销售的收银部分使用软件处理，其他业务都是手工作业，这已经不能适应它的业务发展要求。首先是随着商店规模的扩大，顾客量大幅增长，手工作业销售迟缓，顾客购物排队现象严重，导致流失客源。其次是商店的商品品种增多，无法准确掌握库存，商品积压、缺货和报废的现象上升明显。再次是商店面临的竞争比以前更大，希望在降低成本，吸引顾客，增强竞争力的同时，保持盈利水平。 问题 P1：手工作业销售迟缓，效率不高。 P2：商店的商品品种太多，无法准确掌握库存。 P3：成本不够低，导致竞争力不强，盈利水平不够。 P4：顾客不够多，销售额不高，盈利水平不够。 案例分析 业务需求 BR1：在系统使用6个月后，商品积压、缺货和报废的现象要减少50% BR2：在系统使用3个月后，销售人员工作效率提高50% BR3：在系统使用6个月后，店铺运营成本要降低15% 范围：人力成本和库存成本 度量：检查平均每个店铺的员工数量和平均每10,000元销售额的库存成本 BR4：在系统使用6个月后，销售额度要提高20% 最好情况：40% 最可能情况：20% 最坏情况：10% 系统特性 SF1：分析店铺商品库存，发现可能的商品积压、缺货和报废现象：BR1，BR3 SF2：根据市场变化调整销售的商品：BR1，BR3，BR4 SF3：制定促销手段，处理积压商品：BR1，BR3，BR4 SF4：与生产厂家联合进行商品促销：BR1，BR3，BR4，CH SF5：制定促销手段进行销售竞争：BR1，BR4，CH SF6：掌握员工变动和授权情况：BR2 SF7：处理商品入库与出库：BR1 SF8：发展会员，提高顾客回头率：BR4，CR SF9：允许积分兑换商品和赠送吸引会员的礼品，提高会员满意度：BR3，BR4，CR SF10：帮助收银员处理销售与退货任务：BR2 边界 用例图： 目标分析 目标：是系统被开发的目的 它有着明确的定义方式 具有一定的特征属性，常见的有名称、类型、关注、定义（正式与非正式））、优先级、主体、拥有者等 目标的分类 按抽象层次分类 高层次目标 战略性的,全局的, 业务相关的 “增加50% 的传输能力” 低层次目标 技术性的，局部的，产品设计相关的 “加速器每3秒发出一次命令” 按功能分类 功能目标（Functional Goal） 描述预期的系统行为 满足型目标（Satisfaction Goal）和信息型目标（Information Goal） 非功能目标（Non-functional Goal） 常见的是质量目标（Quality goals）和约束目标（Constraint goals） 安全目标（Safety Goal）、性能目标（Performance Goal）、可用性目标（Usability Goal）等等 按技术手段分类 软目标（Soft Goal）和硬目标（Hard Goal） 能否利用技术手段确认是否满足 目标的规格的基本模式 模式 符号描述 含义 实现(Achieve) $P \\Rightarrow \\Diamond Q$ 如果将来某一时刻Q为真(被满足)，则目标实现 终止(Cease) $P \\Rightarrow \\Diamond \\neg Q$ 如果将来某一时刻Q为假(被终止)，则目标实现 保持(Maintain) $P \\Rightarrow \\Box Q$ 将来任一时刻Q都为真，则目标实现 避免(Avoid) $P \\Rightarrow \\Box \\neg Q$ 将来任一时刻Q都为假，则目标实现 优化(Optimize) - 最大化Maximize(目标功能) 或 最小化Minimize (目标功能) 实现（Achieve）：P ⇒ ◊ Q //如果将来某一时刻Q为真（被满足），则目标实现 终止（Cease）： P ⇒ ◊ ¬ Q //如果将来某一时刻Q为假（被终止），则目标实现 保持（Maintain）： P ⇒ □Q //将来任一时刻Q都为真，则目标实现 避免（Avoid）： P ⇒ □¬ Q //将来任一时刻Q都为假，则目标实现 优化（Optimize）：最大化Maximize (目标功能) 或 最小化Minimize (目标功能) 目标模型的关系 目标模型的另一个核心要素是元素之间的关系，又称为链接 精化(Refinement)关系 阻碍(Obstruction)关系 支持与冲突(Support/Conflict)关系 目标精化 一个高层次目标G可以精化为低层次目标{G1,G2,…,Gn}： 如果一系列子目标{G1,G2,…,Gn}的完成有助于目标G的完成，那么G与{G1,G2,…,Gn}之间就是AND 精化关系。此时任意两子目标Gi与Gj之间是互补的。 如果任一子目标Gi都是G的替代方案，那么G与{G1,G2,…,Gn}之间就是OR 精化关系。此时，任意两子目标Gi与Gj之间是互相替代的。 目标阻碍 如果子目标O的达成会使得高层目标G失败O**|=**¬G，那么O与G的关系就是阻碍关系 阻碍目标也可以继续AND精化、OR精化 阻碍关系本身是一种特殊的精化——反向精化 目标支持与冲突 Support链接表示一个目标对其他目标的支持作用 支持关系可以被处理为OR精化关系 Conflict链接表示一个目标的实现对其他目标的实现有阻碍作用 目标与其他元素的关系 主体(Agent) 场景(Scenario) 操作(Operation) 任务(Task) 资源(Resource) UML元素 目标分析的过程 高层目标的获取：现状和背景的分析：问题与缺陷 得到业务需求、 高层目标目标模型 低层目标的获取：目标分析与实现 已有目标的验证和细化(基于目标分析) 基于场景的方法等等(基于目标实现) 目标分析：精化与分解，建立系统的目标模型 目标实现：收集与目标相关的需求信息，讨论可能的候选解决方案，确定最终的系统详细需求和解决方案 案例 描述 ×××连锁商店是一家刚刚发展起来的小型连锁商店，其前身是一家独立的小百货门面店。原商店只有销售的收银部分使用软件处理，其他业务都是手工作业，这已经不能适应它的业务发展要求。首先是随着商店规模的扩大，顾客量大幅增长，手工作业销售迟缓，顾客购物排队现象严重，导致流失客源。其次是商店的商品品种增多，无法准确掌握库存，商品积压、缺货和报废的现象上升明显。再次是商店面临的竞争比以前更大，希望在降低成本，吸引顾客，增强竞争力的同时，保持盈利水平。 高层目标的获取 业务需求： BR1：在系统使用6个月后，商品积压、缺货和报废的现象要减少50% BR2：在系统使用3个月后，销售人员工作效率提高50% BR3：在系统使用6个月后，店铺运营成本要降低15% 范围：人力成本和库存成本 度量：检查平均每个店铺的员工数量和平均每10,000元销售额的库存成本 BR4：在系统使用6个月后，销售额度要提高20% 最好情况：40% 最可能情况：20% 最坏情况：10% 高层目标模型 低层目标的获取： 目标分析 目标精化 目标冲突与协作 目标实现 主体 操作","categories":[{"name":"Business","slug":"Business","permalink":"http://lyk-love.cn/categories/Business/"}],"tags":[{"name":"需求与商业模式创新","slug":"需求与商业模式创新","permalink":"http://lyk-love.cn/tags/%E9%9C%80%E6%B1%82%E4%B8%8E%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F%E5%88%9B%E6%96%B0/"}]},{"title":"商业模式设计流程","slug":"商业模式设计流程","date":"2021-12-28T21:46:28.000Z","updated":"2022-09-26T06:39:34.943Z","comments":true,"path":"2021/12/29/商业模式设计流程/","link":"","permalink":"http://lyk-love.cn/2021/12/29/%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1%E6%B5%81%E7%A8%8B/","excerpt":"Outline: 设计目标 五个步骤","text":"Outline: 设计目标 五个步骤 设计目标 商业模式设计与创新 满足市场：满足尚未被响应的、被忽视的市场需求； 投放市场：把新的技术、产品或服务推向市场，或利用现有的知识产权； 改善市场：通过一个更好的商业模式来改进、颠覆或变革现有的市场； 创造市场：创造一种全新的市场，提供一种全新的业务。 挑战 找到正确的模式 全面启动前展开验证 诱导市场接受新模式 根据市场反馈不断调整 管理不确定因素 成熟组织特有的因素 反应性：现行模式遇到了危机，如濒临破产的局面等； 适应性：为了调整、改善和稳固现行的模式，以适应不断变化的环境； 扩张性：为了把新的技术、产品或服务推向市场； 积极性/探索性：为了探索和测试未来的可能的全新模式，为未来做准备。 挑战 为新模式培育市场 磨合新旧两种模式 管理好既得利益 着眼于长远 五个步骤 概述 注意事项 动员 项目合法性、管理既得利益、跨职能团队、引导决策者 理解 绘制并评估当前商业模式、跳出现状看问题、不要局限于当前的客户群体、展示进展 设计 避免对大胆想法的遏制、参与式设计、思考新旧模式之间的关系、避免聚焦短期利益 实施 主动管理“路障”、项目赞助人、新旧商业模式之间的关系、内部沟通活动 管理 商业模式管理机制、管理协同和冲突、商业模式组合、空杯心态","categories":[{"name":"Business","slug":"Business","permalink":"http://lyk-love.cn/categories/Business/"}],"tags":[{"name":"需求与商业模式创新","slug":"需求与商业模式创新","permalink":"http://lyk-love.cn/tags/%E9%9C%80%E6%B1%82%E4%B8%8E%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F%E5%88%9B%E6%96%B0/"}]},{"title":"OS Concurrency","slug":"OS-Concurrency","date":"2021-12-20T00:58:36.000Z","updated":"2022-09-26T06:39:34.935Z","comments":true,"path":"2021/12/20/OS-Concurrency/","link":"","permalink":"http://lyk-love.cn/2021/12/20/OS-Concurrency/","excerpt":"Outline: Intro： 多处理器编程的困难 用状态机理解并发算法 并发控制：互斥、同步 概念： 互斥 并发与状态机 Threading API Lock Concurrent Data Structure based on Lock Conditional Variable 信号量 常见并发问题 基于事件的并发 Ref： Operating Systems Three Easy pieces JYY OS","text":"Outline: Intro： 多处理器编程的困难 用状态机理解并发算法 并发控制：互斥、同步 概念： 互斥 并发与状态机 Threading API Lock Concurrent Data Structure based on Lock Conditional Variable 信号量 常见并发问题 基于事件的并发 Ref： Operating Systems Three Easy pieces JYY OS Intro Concurrent：exsiting, happening, or done at the same time. 程序的不同部分可以按不同顺序执行，且最终得到正确的结果 //t0.c#include&lt;stdio.h&gt;#include&lt;assert.h&gt;#include&lt;pthread.h&gt;// pthread 库不是 Linux 系统默认的库，连接时需要使用静态库 libpthread.a // 在编译中要加 -lpthread参数void *mythread( void *arg )&#123; printf(&quot;%s\\n&quot;, arg); return NULL;&#125;int main( int argc, char* argv[])&#123; pthread_t p1,p2; int rc; printf( &quot;main:begin\\n&quot; ); char* ch1 = &quot;A&quot;; char* ch2 = &quot;B&quot;; rc = pthread_create( &amp;p1, NULL, mythread, ch1 ); assert(rc==0); //C语言编译器允许隐含性的将一个通用指针转换为任意类型的指针，包括const *而C＋＋不允许将const 转换为非const*，所以不能直接传入&quot;A&quot; rc = pthread_create( &amp;p2, NULL, mythread, ch2 ); assert(rc==0); rc= pthread_join( p1,NULL ); rc= pthread_join( p2,NULL ); printf(&quot;main:end\\n&quot;);&#125; 该程序打印结果为： main:beginABmain:end 或者 main:beginBAmain:end 两次运行结果不一样 多任务OS的并发 （假设系统只有一个CPU） OS可以同时加载多个进程 每个进程都是独立的进程，互不干扰 即使是root权限的进程，也不能直接访问操作系统内核的内存 每隔一段时间，就切换到另一个进程 并发性的来源： 进程会调用OS的API write（fd,buf,11 TiB）（TiB宏） write的实现是OS的一部分 x86-64应用程序执行syscall后就进入OS执行 类似中断处理程序 此时OS允许write的同时，让另一个进程执行 如：另一个进程执行了read(fd,buf,512 MiB)读取同一文件 OS代码并发了： OS API实现需要考虑并发 虽然进程在地址空间中是独立的，但是OS中的对象是被进程共享的 并发与并行的区别 并发： 多个执行流可以不按照一个特定的顺序执行 并行：允许多个执行流真正地同时执行 需要多个处理器 处理器数量 共享内存？ 典型的并发OS 并发？并行？ 单 共享内存 OS内核/多线程程序 并发不并行 多 共享内存 OS内核/多线程程序/GPU Kernel 并发、并行 多 不共享内存 分布式系统（消息通信） 并发、并行 线程 线程： A single process can contain multiple threads, all of which are executing the same program. These threads share the same global memory (data and heap segments), but each thread has its own stack (automatic variables). 多个执行流并发/并行执行，且共享内存 两个执行流共享代码和所有全局变量（数据区、堆区） i.e. C++中，数据区就是全局/静态区 线程间指令的执行顺序是不确定（non-deterministic）的 共享：共享代码区(当前进程的代码)、数据区和堆，但不共享寄存器和栈 //t1.c//共享全局变量的后果#include&lt;stdio.h&gt;#include&lt;pthread.h&gt;#include&quot;mythreads.h&quot;static volatile int counter = 0;void * mythread( void*arg )&#123; printf( &quot;%s: begin\\n&quot;, (char*)arg ); int i; for( int i = 0 ; i &lt; 1e7; i++ ) counter++; printf(&quot;%s: done\\n&quot;, (char*)arg); return NULL;&#125;int main( int argc, char* argv[])&#123; pthread_t p1,p2; int rc; printf( &quot;main:begin ( counter = %d )\\n&quot;, counter ); char* ch1 = &quot;A&quot;; char* ch2 = &quot;B&quot;; Pthread_create( &amp;p1, NULL, mythread, ch1 ); //C语言编译器允许隐含性的将一个通用指针转换为任意类型的指针，包括const *而C＋＋不允许将const 转换为非const*，所以不能直接传入&quot;A&quot; Pthread_create( &amp;p2, NULL, mythread, ch2 ); Pthread_join( p1,NULL ); Pthread_join( p2,NULL ); printf(&quot;main:done with both ( counter = %d )\\n&quot;, counter);&#125; //mythreads.h//把API封装起来，易于使用#include&lt;pthread.h&gt;// #include&lt;assert.h&gt;void Pthread_create (pthread_t *__restrict __newthread, const void* __attr, void *(*__start_routine) (void *), void *__restrict __arg)&#123; pthread_create( __newthread, __attr, __start_routine, __arg );&#125; void Pthread_join(pthread_t __th, void **__thread_return) &#123; pthread_join( __th, __thread_return ); &#125; void *Malloc(unsigned size) &#123; return malloc(size); &#125; 输出为： main:begin ( counter = 0 )A: beginB: beginA: doneB: donemain:done with both ( counter = 12275324 ) 可以看到结果不是200000, 而是12275324 再运行一次： main:begin ( counter = 0 )A: beginB: beginB: doneA: donemain:done with both ( counter = 10467369 ) 两次运行的结果都不一样！ 多处理器编程的困难 原子性： 即使是i++，也会被分成几个指令 顺序性：代码的编译器优化 可见性： CPU可以不按顺序执行指令。没有前后依赖就会被优化（并行执行） 并发术语 临界区( critical section )： 访问共享资源的一段代码 竞态条件( race condition )： 多个执行线程大致同时进入进阶区时，都试图更新共享资源的情况 不确定性( indeterminate )： 程序含有竞态条件，其输出不确定 同步原语( synchronization primitive ):硬件提供指令，在其上构建同步原语,实现原子性 互斥原语( mutual exclusion )： 线程应该使用互斥原语，以保证只有一个线程进入临界区，从而避免出现竞态，并产生确定的程序输出 概念：互斥 互斥（mutual exclusion） typedef struct&#123;...&#125;lock_tlvoid lock(lock_t *lk);//试图获得锁的独占访问，成功获得后返回void unlock(lock_t *lk);//释放锁的独占空间 我们假设CPU有三种指令： load： mem -&gt; reg store: reg -&gt; mem 本地计算： 线程的寄存器做一些计算，结果存入寄存器 共享内存上互斥的困难 load和store的缺陷（一个只能看，一个只能写） 现代处理器load/store可能在执行时被乱序 并发与状态机 程序 = 有限状态机 = 有向图 图论是理解程序的重要工具 不确定(non-deterministic)的指令可能有多个状态 获取处理器的”时间戳“用于精确定时 rdtsc/rdtscp 机器提供的”真“随机数 rdrand syscall 一般用于唯一不确定性的来源 read 状态机模型：应用 在硬件上的应用： 高性能处理器实现 超标量处理器 同一时间执行多条指令 Time - Travel Debugging 程序执行随时间渐进：$s_0 \\rarr s_1 \\rarr \\dots$​ 记录所有$s_i$的开销太大（$s_i$​由内存和寄存器组成） 记录初始状态，和每条指令前后状态的diff si/rsi Record &amp; Replay 确定的程序不需要任何记录，只需要再执行一次 只需记录non-deterministic指令的效果（side-effect），就可实现重放 线程间通信ITC (1)通信线程位于同一个进程中，共享相同的地址空间 (2)通信线程位于不同的进程中，拥有不同的地址空间 相同进程 对于情况(1)，线程间的通信可以直接通过访问共享的地址空间实现信息交换 不同进程 对于情况(2), 采用进程间通信IPC • IPC机制主要包括:信号(Signal)、管道(Pipe)、信号量、共享内存(Shared Memory)、消息队列(Message Queue)、套接字(Socket) • 与线程间通信机制不同，进程间通信机制需要打破进程间地址空间的隔离 某些OS发行版(.其实就是欧拉 )增加的IPC机制：共享内存， 消息通信 共享内存是一种在进程间高效地传递大量信息的通信方式。但在共享内存机制下，信息的发送方不关心信息由谁接收，而信息的接收方也不关心信息是由谁发送的，这存在安全隐患。 消息传递允许进程不必通过共享内存区来实现通信，而是通过交换消息的方式来实现通信。消息 传递关注信息的发送者不接收者，通过使用内核拷贝传递的信息，完成进程间的信息传递 Threading API man -k pthread //p212, 线程API#include&lt;stdio.h&gt;#include&lt;pthread.h&gt;#include&quot;mythreads.h&quot;#include&lt;stdlib.h&gt;typedef struct myarg_t&#123; int a; int b;&#125;myarg_t;typedef struct myret_t&#123; int x; int y;&#125;myret_t;void * mythread( void*arg )&#123; myarg_t *m = (myarg_t *)arg; printf( &quot;args: %d %d\\n&quot;, m -&gt; a, m -&gt; b ); myret_t *r = Malloc(sizeof( myret_t )); r -&gt; x = 1; r -&gt; y = 2; return (void*) r;&#125;int main( int argc, char* argv[])&#123; pthread_t p; int rc; myret_t *m; //将返回值打包 myarg_t args;//将参数打包 args.a = 10; args.b = 20; Pthread_create( &amp;p, NULL, mythread, &amp;args ); Pthread_join( p, ( void **) &amp;m );//将线程返回值赋给m printf(&quot;returned: %d %d\\n&quot;, m -&gt; x, m -&gt; y );&#125; 输出为： args: 10 20returned: 1 2 线程创建 &lt;pthread.h&gt; /* Create a new thread, starting with execution of START-ROUTINE getting passed ARG. Creation attributed come from ATTR. The new handle is stored in *NEWTHREAD. */extern int pthread_create (pthread_t *__restrict __newthread, //线程的指针 const pthread_attr_t *__restrict __attr, //线程具有的属性，包括栈大小，优先级等。 一般传入NULL void *(*__start_routine) (void *),//线程要运行的函数的指针，接受void*参数，返回 void* //可以自由更改参数/返回类型 void *__restrict __arg) __THROWNL __nonnull ((1, 3));//函数的参数，可传入结构体，以实现传入多个参数 线程完成 /* Make calling thread wait for termination of the thread TH. The exit status of the thread is stored in *THREAD_RETURN, if THREAD_RETURN is not NULL. This function is a cancellation point and therefore not marked with __THROW. */extern int pthread_join (pthread_t __th, void **__thread_return);/*第一个参数是线程的指针第二个参数是线程运行的函数的返回值的指针*/ 锁 //上锁和解锁extern int pthread_mutex_lock (pthread_mutex_t *__mutex)extern int pthread_mutex_unlock (pthread_mutex_t *__mutex) //锁必须初始化，两种方式pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;pthread_mutex_init(*lock, NULL)//锁使用完后必须销毁extern int pthread_mutex_destroy (pthread_mutex_t *__mutex) //mythread.h#include&lt;pthread.h&gt;#include&lt;stdlib.h&gt;#include&lt;assert.h&gt;typedef pthread_cond_t cond_t;typedef pthread_mutex_t mutex_t;void Pthread_create (pthread_t *__restrict __newthread, const void* __attr, void *(*__start_routine) (void *), void *__restrict __arg)&#123; int rc = pthread_create( __newthread, __attr, __start_routine, __arg ); assert( rc == 0 );&#125; void Pthread_join(pthread_t __th, void **__thread_return) &#123; int rc = pthread_join( __th, __thread_return ); assert( rc == 0 ); &#125; void *Malloc(unsigned size) &#123; return malloc(size); &#125; void Pthread_mutex_lock( pthread_mutex_t *mutex ) &#123; int rc = pthread_mutex_lock(mutex); assert( rc == 0 ); &#125;void Pthread_mutex_unlock( pthread_mutex_t *mutex ) &#123; int rc = pthread_mutex_unlock(mutex); assert( rc == 0 ); &#125;void Pthread_mutex_init( pthread_mutex_t *__mutex, const void*__mutexattr )&#123; int rc = pthread_mutex_init(__mutex, NULL); assert( rc == 0 );&#125;void Pthread_mutex_destroy( pthread_mutex_t *__mutex )&#123; int rc = pthread_mutex_destroy(__mutex); assert( rc == 0 );&#125;void Pthread_cond_init( pthread_cond_t *__restrict __cond, const void*__restrict __cond_attr )&#123; int rc = pthread_cond_init( __cond, __cond_attr); assert( rc == 0 );&#125;void cond_destroy( pthread_cond_t *__restrict __cond )&#123; int rc = pthread_cond_destroy(__cond); assert( rc == 0 );&#125;void Pthread_cond_wait(pthread_cond_t *__restrict __cond, pthread_mutex_t *__restrict __mutex)&#123; int rc = pthread_cond_wait(__cond, __mutex); assert(rc ==0 );&#125;void Pthread_cond_signal(pthread_cond_t *__cond)&#123; int rc = pthread_cond_signal(__cond); assert( rc==0 );&#125; 使用锁： pthread_mutex_t lock;Pthread_mutex_init( &amp;lock ,NULL );//初始化锁Pthread_mutex_lock( &amp;lock );//上锁， 如果锁已经被其他线程持有，那么该调用不会返回，直到获得该锁（意味着持有该锁的线程已经将锁释放）x = x +1;Pthread_mutex_unlock( &amp;lock ); 条件变量 /* Wait for condition variable COND to be signaled or broadcast. MUTEX is assumed to be locked before. This function is a cancellation point and therefore not marked with __THROW. */extern int pthread_cond_wait (pthread_cond_t *__restrict __cond, pthread_mutex_t *__restrict __mutex); /* Wake up one thread waiting for condition variable COND. */extern int pthread_cond_signal (pthread_cond_t *__cond); //条件变量也必须初始化，与锁类似，两种方式： pthread_cond_t cond = PTHREAD_COND_INITIALIZER;pthread_cond_init( __cond, __cond_attr); //销毁也类似/* Destroy condition variable COND. */extern int pthread_cond_destroy (pthread_cond_t *__cond); //mythreads.hvoid Pthread_cond_init( pthread_cond_t *__restrict __cond, const void*__restrict __cond_attr )&#123; int rc = pthread_cond_init( __cond, __cond_attr); assert( rc == 0 );&#125;void cond_destroy( pthread_cond_t *__restrict __cond )&#123; int rc = pthread_cond_destroy(__cond); assert( rc == 0 );&#125; Lock 锁的状态： available acquired 方法： lock(): 尝试获取锁，如果锁是available，则获取锁，进入临界区 unlock(): 使锁available 锁提供了最小程度的调度控制 线程由OS调度， 锁让程序员获得了一些控制权 通常用不同的锁保护不同的数据（ 细粒度的方案 ） 评价锁 有效性： 提供互斥 公平性fairness: 当锁可用时，是否每一个竞争线程有公平的机会抢到锁 是否有竞争锁的线程会饿死starve? 性能 performance 控制中断 最早提供的互斥解决方案之一，就是在临界区关闭中断： void lock()&#123; DisableInterrupts();&#125;void unlock()&#123; EnableInterrupts();&#125; 假设在单CPU系统上，这段代码在临界区关闭中断，从而原子地执行，结束后又重新打开中断 缺点： 需要允许所有线程执行特权操作（开关中断） 即，需要信任机制不被滥用 不支持多处理器 中断的开关只是对CPU而言，如果一个多个线程运行在不同CPU上，其中一个CPU关闭中断，其他CPU依然响应中断，在其上的线程依然能进入临界区 关中断导致中断丢失，可能导致严重的系统问题 比如磁盘完成了读取请求，但CPU错失了该消息 因此，用关中断来实现互斥原语的情况很有限 有些情况下OS自身会采用关中断的方式来保证访问数据的原子性，此时该用法是可行的，因为OS内部不存在信任问题 很多人热衷于研究不依赖硬件实现的锁机制，事实证明，只需很少的硬件支持，实现锁就会容易很多 test-and-set test-and-set instruction, 也称为atomic exchange, 由硬件支持 先实现一个不依赖它的锁： //想法是： 用一个flag来表示锁是否被占用typedef struct lock_t( int flag; ) lock_t; void init( lock_t *mutex )&#123; mutex -&gt; flag = 0; // 0 -&gt; lock is available&#125;void lock( lock_t *mutex )&#123; while( mutex -&gt; flag == 1 ) ; // spin-wait( do nothing ) mutex -&gt; flag = 1;&#125;void unlock( lock_t *mutex )&#123; mutex -&gt; flag = 0;&#125; 这段代码有两个问题： 正确性： Thread` Thread2 ( 初始时，flag == 0 ) call lock() while( flag == 1 ) interrupt: switch to Thread2 call lock() while( flag == 1 ) flag = 1; interrupt: switch to Thread 1 flag = 1( too ! ) 性能问题： 这个锁是自旋的，一个线程自旋等待另一个线程释放锁，浪费时间 对于单CPU，因为同一时间只有一个线程，且自旋的线程永远不会放弃CPU,本线程自旋时，持有锁的线程根本无法运行，也不可能释放锁 需要抢占式的调度器（ preemptive scheduler , 即不断通过时钟中断一个线程，运行其他线程 ) 用test-and-set实现锁 在x86上称为 xchg指令， int TestAndSet( int *old_ptr, int new )&#123; int old = *old_ptr; *old_ptr = new; return old;&#125;// 返回old_ptr指向的旧值，将old_ptr指向新值 硬件保证，上述操作是原子的 可以理解为：一个厕所门上挂着钥匙(0)，两个用户都有一个名牌(1)，用户每次进出厕所，需要拿手上的东西和门上的东西交换，用户需要拿到钥匙才能进厕所。 用户一拿名牌和钥匙交换，拿着钥匙进了厕所 用户二拿名牌交换，此时门上是名牌，用户二没有拿到钥匙，无法进厕所 他会一直交换： while( TestAndSet( &amp;lock -&gt; flag, 1 ) == 1 ) ; //spin 用户一从厕所出来，拿名牌和手上的钥匙交换，此时门上又有钥匙了 缺点： 如上述 评价自旋锁： 有效性：OK 公平性：不提供任何公平性保证( 对test-and-set而言 ) 性能： 单核下很差，多核下还不错 compare-and-exchange 某些系统提供了另一个硬件原语: compare-and-exchange( on x86 ) //返回ptr指向的旧值，将ptr指向新值（如果旧值与期望值相等的话）int CompareAndExchange( int *ptr, int expected, int new )&#123; int actual = *ptr; if( actual == expected ) *ptr = new; return actual;&#125; 该指令比test-and-set更强大 fetch-and-add int FetchAndAdd( int *ptr )&#123; int old = *ptr; *ptr = old + 1; return old;&#125;typedef struct lock_t&#123; int ticket; int turn;&#125; lock_t;void lock_init( lock_t *lock )&#123; lock -&gt; ticket = 0; lock -&gt; turn = 0;&#125;void lock(lock_t *lock)&#123; int myturn = FetchAndAdd( &amp;lock-&gt;ticket ); while( lock-&gt; turn != myturn ) ; //spin&#125;void unlock( lock_t *lock )&#123; FetchAndAdd( &amp;lock-&gt; turn );&#125; 解释：ticket是一个全局的号码，turn是全局的轮次。 每个用户从ticket得到自己的turn， 每交易一次，ticket++. 只有到达自己的turn的用户才能进入临界区，每当一个用户从临界区出来， turn++ myturn &lt; turn的用户只能自旋 本方法能保证所有线程都能抢到锁，只要一个线程获得了ticket，就能被调度 test-and-set无法保证 自旋过多的解决方案 我们已经实现了有效、公平（ 借助ticket）的锁，但自旋会导致性能降低 自旋会重复检查一个不会改变的值，浪费CPU时间 方案一 yield 在要自旋的时候，放弃CPU void lock()&#123; while( TestAndSet(&amp;flag, 1 ) == 1 ) yield(); //give up the CPU&#125; 假定OS提供原语yield()，可以让线程从running变为ready 本质上，就是deschedule 假设100个线程竞争1个锁，该方案会yield99次，比自旋99次好，但仍不够完美 方案二 使用队列： 休眠代替自旋 1 typedef struct lock_t &#123;2 int flag;3 int guard;4 queue_t *q;5 &#125; lock_t;67 void lock_init(lock_t *m) &#123;8 m-&gt;flag = 0;9 m-&gt;guard = 0;10 queue_init(m-&gt;q);11 &#125;1213 void lock(lock_t *m) &#123;14 while (TestAndSet(&amp;m-&gt;guard, 1) == 1)15 ; //acquire guard lock by spinning16 if (m-&gt;flag == 0) &#123;17 m-&gt;flag = 1; // lock is acquired18 m-&gt;guard = 0;19 &#125; else &#123;20 queue_add(m-&gt;q, gettid());21 m-&gt;guard = 0;22 park();23 &#125;24 &#125;2526 void unlock(lock_t *m) &#123;27 while (TestAndSet(&amp;m-&gt;guard, 1) == 1)28 ; //acquire guard lock by spinning29 if (queue_empty(m-&gt;q))30 m-&gt;flag = 0; // let go of lock; no one wants it31 else32 unpark(queue_remove(m-&gt;q)); // hold lock (for next thread!)33 m-&gt;guard = 0;34 &#125; 看不懂QAQ,为啥unpark的时候不把flag设为0啊，这样所有其他进程都无法获得锁 Concurrent Data Structure Based on Lock 通过锁使得数据thread safe 可扩展性： 理想状态下的多线程的每个线程就和单线程一样快，二者的比值就是并发方法的扩展性 并发计数器 1 typedef struct counter_t &#123;2 int value;3 pthread_mutex_t lock;4 &#125; counter_t;56 void init(counter_t *c) &#123;7 c-&gt;value = 0;8 Pthread_mutex_init(&amp;c-&gt;lock, NULL);9 &#125;1011 void increment(counter_t *c) &#123;12 Pthread_mutex_lock(&amp;c-&gt;lock);13 c-&gt;value++;14 Pthread_mutex_unlock(&amp;c-&gt;lock);15 &#125;1617 void decrement(counter_t *c) &#123;18 Pthread_mutex_lock(&amp;c-&gt;lock);19 c-&gt;value--;20 Pthread_mutex_unlock(&amp;c-&gt;lock);21 &#125;2223 int get(counter_t *c) &#123;24 Pthread_mutex_lock(&amp;c-&gt;lock);25 int rc = c-&gt;value;26 Pthread_mutex_unlock(&amp;c-&gt;lock);27 return rc;28 &#125; 性能一般 扩展并发计数器 懒惰计数器sloopy counter: 例如，在4CPU机器上，有四个局部计数器和一个全局计数器，每个计数器有一把锁。 不同CPU上的计数器不会竞争 为了保持全局计数器更新，如果局部值大于阈值S，局部值就要转移到全局值（此时要获取全局锁） 阈值越大，可扩展性越好，但计时器精度更低 1 typedef struct counter_t &#123;2 int global; // global count3 pthread_mutex_t glock; // global lock4 int local[NUMCPUS]; // local count (per cpu)5 pthread_mutex_t llock[NUMCPUS]; // ... and locks6 int threshold; // update frequency7 &#125; counter_t;89 // init: record threshold, init locks, init values10 // of all local counts and global count11 void init(counter_t *c, int threshold) &#123;12 c-&gt;threshold = threshold;1314 c-&gt;global = 0;15 pthread_mutex_init(&amp;c-&gt;glock, NULL);1617 int i;18 for (i = 0; i &lt; NUMCPUS; i++) &#123;19 c-&gt;local[i] = 0;20 pthread_mutex_init(&amp;c-&gt;llock[i], NULL);21 &#125;22 &#125;2324 // update: usually, just grab local lock and update local amount25 // once local count has risen by &#x27;threshold&#x27;, grab global26 // lock and transfer local values to it27 void update(counter_t *c, int threadID, int amt) &#123;28 pthread_mutex_lock(&amp;c-&gt;llock[threadID]);29 c-&gt;local[threadID] += amt; // assumes amt &gt; 030 if (c-&gt;local[threadID] &gt;= c-&gt;threshold) &#123; // 局部值大于阈值，transfer to global31 pthread_mutex_lock(&amp;c-&gt;glock);32 c-&gt;global += c-&gt;local[threadID];33 pthread_mutex_unlock(&amp;c-&gt;glock);34 c-&gt;local[threadID] = 0;//本地值清0, 注意到不会与其他cpu上的线程竞争，因此是安全的35 &#125;36 pthread_mutex_unlock(&amp;c-&gt;llock[threadID]);37 &#125;3839 // get: just return global amount (which may not be perfect)40 int get(counter_t *c) &#123;41 pthread_mutex_lock(&amp;c-&gt;glock);42 int val = c-&gt;global;43 pthread_mutex_unlock(&amp;c-&gt;glock);44 return val; // only approximate!45 &#125; 并发链表 1 // basic node structure，单链表2 typedef struct node_t &#123;3 int key;4 struct node_t *next;5 &#125; node_t;67 // basic list structure (one used per list)8 typedef struct list_t &#123;9 node_t *head;10 pthread_mutex_t lock;11 &#125; list_t;1213 void List_Init(list_t *L) &#123;14 L-&gt;head = NULL;15 pthread_mutex_init(&amp;L-&gt;lock, NULL);16 &#125;1718 int List_Insert(list_t *L, int key) &#123;19 pthread_mutex_lock(&amp;L-&gt;lock);20 node_t *new = malloc(sizeof(node_t));21 if (new == NULL) &#123;22 perror(&quot;malloc&quot;);23 pthread_mutex_unlock(&amp;L-&gt;lock); //malloc失败，记得释放锁！24 return -1; // fail25 &#125;26 new-&gt;key = key;27 new-&gt;next = L-&gt;head;28 L-&gt;head = new;29 pthread_mutex_unlock(&amp;L-&gt;lock);30 return 0; // success31 &#125;3233 int List_Lookup(list_t *L, int key) &#123;34 pthread_mutex_lock(&amp;L-&gt;lock);35 node_t *curr = L-&gt;head;36 while (curr) &#123;37 if (curr-&gt;key == key) &#123;38 pthread_mutex_unlock(&amp;L-&gt;lock); //在意外退出时unlock39 return 0; // success40 &#125;41 curr = curr-&gt;next;42 &#125;43 pthread_mutex_unlock(&amp;L-&gt;lock);44 return -1; // failure45 &#125; 在开头lock,结尾unlock, 注意到malloc失败后也要记得unlock 这种在代码中多次unlock的写法很丑陋！ 应该修改 要么出错的地方不要放在临界区 要么出错时break到主循环，在主循环内统一unlock 这是粗粒度的写法，可以更细粒度地优化： 18 int List_Insert(list_t *L, int key) &#123;19 //local的数据，不需要锁保护20 node_t *new = malloc(sizeof(node_t));21 if (new == NULL) &#123;22 perror(&quot;malloc&quot;);23 pthread_mutex_unlock(&amp;L-&gt;lock); //出错时不在临界区，无需unlock24 return -1; 25 &#125;26 new-&gt;key = key;27 //just lock critical section !!!!!28 pthread_mutex_lock(&amp;L-&gt;lock);29 new-&gt;next = L-&gt;head;30 L-&gt;head = new;31 pthread_mutex_unlock(&amp;L-&gt;lock);32 return 0; 33 &#125;33 int List_Lookup(list_t *L, int key) &#123; int rv = -1;// success / failure34 pthread_mutex_lock(&amp;L-&gt;lock);35 node_t *curr = L-&gt;head;36 while (curr) &#123;37 if (curr-&gt;key == key) &#123;38 rv=0;39 break; // 跳出主循环，在主循环内统一unlock ！！！40 &#125;41 curr = curr-&gt;next;42 &#125;43 pthread_mutex_unlock(&amp;L-&gt;lock);44 return rv; // now both success and failure45 &#125; 扩展链表 过手锁hand-overohand locking: 每个节点都有一个锁，替代之前整个链表一个锁，遍历链表时，首先抢占下一个节点的锁，然后释放当前节点的锁 开销巨大，未必比单锁快 注意控制流的变化导致函数返回和退出，这种情况下要记得释放锁 并发队列 粗粒度的锁很简单，接下来使用细粒度的锁 对队列头和尾各设置一个锁 因为出队只访问head锁， 入队只访问tail锁， 两把锁使得出队和入对可以并发执行 1 typedef struct node_t &#123;2 int value;3 struct node_t *next;4 &#125; node_t;56 typedef struct queue_t &#123;7 node_t *head;8 node_t *tail;9 pthread_mutex_t headLock;10 pthread_mutex_t tailLock;11 &#125; queue_t;1213 void Queue_Init(queue_t *q) &#123;14 node_t *tmp = malloc(sizeof(node_t));15 tmp-&gt;next = NULL;16 q-&gt;head = q-&gt;tail = tmp;17 pthread_mutex_init(&amp;q-&gt;headLock, NULL);18 pthread_mutex_init(&amp;q-&gt;tailLock, NULL);19 &#125;2021 void Queue_Enqueue(queue_t *q, int value) &#123;22 node_t *tmp = malloc(sizeof(node_t));23 assert(tmp != NULL);24 tmp-&gt;value = value;25 tmp-&gt;next = NULL;2627 pthread_mutex_lock(&amp;q-&gt;tailLock);//入队只访问`tail`锁28 q-&gt;tail-&gt;next = tmp;29 q-&gt;tail = tmp;30 pthread_mutex_unlock(&amp;q-&gt;tailLock);31 &#125;3233 int Queue_Dequeue(queue_t *q, int *value) &#123;34 pthread_mutex_lock(&amp;q-&gt;headLock);//出队只访问`head`锁35 node_t *tmp = q-&gt;head;36 node_t *newHead = tmp-&gt;next;37 if (newHead == NULL) &#123;38 pthread_mutex_unlock(&amp;q-&gt;headLock);39 return -1; // queue was empty，这种写法（在子控制流unlock）很丑陋，应该像之前《重写并发链表》的 List_Lookup() 一样改造40 &#125;41 *value = newHead-&gt;value;42 q-&gt;head = newHead;43 pthread_mutex_unlock(&amp;q-&gt;headLock);44 free(tmp);45 return 0;46 &#125; 并发散列表 每个元素都是一个并发链表，也称为“散列桶” 每个散列桶都有一个锁 1 #define BUCKETS (101)23 typedef struct hash_t &#123;4 list_t lists[BUCKETS];5 &#125; hash_t;67 void Hash_Init(hash_t *H) &#123;8 int i;9 for (i = 0; i &lt; BUCKETS; i++) &#123;10 List_Init(&amp;H-&gt;lists[i]);11 &#125;12 &#125;1314 int Hash_Insert(hash_t *H, int key) &#123;15 int bucket = key % BUCKETS;16 return List_Insert(&amp;H-&gt;lists[bucket], key);17 &#125;1819 int Hash_Lookup(hash_t *H, int key) &#123;20 int bucket = key % BUCKETS;21 return List_Lookup(&amp;H-&gt;lists[bucket], key);22 &#125; Conclusion Knuth定律： 避免不成熟的优化 先最简单的方案，也就是加大锁（ big kernel lock, BKL. in linux kernel ）开始， 如果有性能问题再改进 控制流变化时记得获取和释放锁 增加并发并不一定能提高性能 过手锁 Conditional Variable 条件变量： 一个显式队列， 当condition不满足时，线程把自己加入队列，waiting该条件。 当另外某个线程改变该condition时，就可以唤醒signal一个或者多个（假唤醒）等待线程，让它们继续执行 wait( mutex ): 该函数假定在wait()时， mutex是上锁状态。 wait()的职责是释放锁， 并让调用进程休眠（原子地）。 当线程被唤醒时（在另外某个线程signal它之后）它重新获取锁，再返回调用者 这样使得wait()写起来很方便： Pthread_mutex_lock(&amp;m); while(done==0) Pthread_cond_wait(&amp;c,&amp;m);//wait释放锁，线程进入休眠。 当被signal时，wait获取锁，再返回调用者。 不用手写unlock\\lock了Pthread_mutex_unlock(&amp;m); 总结：signal和wait时总是持有锁 + 需要状态变量 wait()的语义强制要求调用时已经持有锁，因此不需要操心 #include&lt;stdio.h&gt;#include&lt;pthread.h&gt;#include&quot;mythreads.h&quot;#include&lt;stdlib.h&gt;int done = 0;pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;pthread_cond_t c = PTHREAD_COND_INITIALIZER; void thr_exit() &#123; Pthread_mutex_lock(&amp;m); done = 1; Pthread_cond_signal(&amp;c); Pthread_mutex_unlock(&amp;m); &#125; void thr_join() &#123; Pthread_mutex_lock(&amp;m); while(done==0)// 推荐使用while循环而不是if,原因之后解释 Pthread_cond_wait(&amp;c,&amp;m); Pthread_mutex_unlock(&amp;m); &#125;void * child( void*arg )&#123; printf( &quot;child\\n&quot; ); thr_exit(); return NULL;&#125;int main( int argc, char* argv[])&#123; pthread_t p; printf(&quot;parent: begin\\n&quot;); Pthread_create(&amp;p, NULL,child, NULL); thr_join(); printf(&quot;parent: end\\n&quot;); return 0;&#125; 情况一： parent创建出子进程后，自己继续运行（假设单核），然后马上调用thr_join() 等待子进程，此时它会先获取锁，检查子进程是否完成（还没有），然后调用wait()，让自己休眠。 子线程最终得以运行，打印出“child”， 并调用thr_exit()唤醒父进程， 而exit()原子地设置done，向父进程signal， 最后父进程会运行，从wait()返回并持有锁， 释放锁， 打印出“parent: end” 情况二：子进程创建后立刻运行， 设置done = 1， 调用signal()唤醒其他线程（这里没有其他线程），然后结束。 副进程运行后，调用thr_join()时，发现done已经为1了，就直接返回 parent: beginchildparent: end 状态变量done是必要的 假如没有： void thr_exit()&#123; Pthread_mutex_lock(&amp;m); Pthread_cond_signal(&amp;c); Pthread_mutex_unlock(&amp;m);&#125; void thr_join()&#123; Pthread_mutex_lock(&amp;m); Pthread_cond_wait(&amp;c,&amp;m); Pthread_mutex_unlock(&amp;m);&#125; 假如子线程立刻运行，且调用thr_exit，此时子进程signal()，条件变量上没有睡眠的线程。 父线程运行时，就会调用wait（）并卡在这里，没有线程会signal（）它 发信号和等待时加锁也是必要的 假如没有： void thr_exit()&#123; done = 1; Pthread_cond_signal(&amp;c);&#125; void thr_join()&#123; while(done==0)// 推荐使用while循环而不是if,原因之后解释 Pthread_cond_wait(&amp;c,&amp;m);&#125; 如果父进程调用thr_join(),检查done = 0，试图睡眠，然而在调用wait（）前被中断（因为没有锁，此时其它进程就可以操纵临界区）， 子进程修改变量为1,发出signal（），此时没有睡眠进程。 父进程再次运行时，就会卡在wait()，没有线程可以唤醒它 生产者/消费者（有界缓冲区）问题 //30_7.ccond_t cond;mutex_t mutex;int count = 0;void *producer(void *arg)&#123; int i; int loops = (int) arg; for( i=0; i &lt; loops; i++ ) &#123; Pthread_mutex_lock(&amp;mutex); if(count == 1) Pthread_cond_wait(&amp;cond,&amp;mutex); put(i); Pthread_cond_signal(&amp;cond); Pthread_mutex_unlock(&amp;mutex); &#125;&#125;void *consumer( void *arg )&#123; int i = 0; int loops = (int) arg; for(int i = 0 ; i &lt; loops; i++ ) &#123; pthread_mutex_unlock(&amp;mutex); while( count == 0 )//必须用while,不能用if pthread_cond_wait(&amp;cond,&amp;mutex); int tmp = get(); Pthread_cond_signal(&amp;cond); Pthread_mutex_unlock(&amp;mutex); ptrintf(&quot;%d\\n&quot;,tmp); &#125;&#125; 假设使用if: 假设有两个消费者$T_{c1}$和，$T_{c2}$ 生产者$T_{p}$, 若$T_{c1}$先运行，卡在wait， 接着$T_p$运行，在缓冲区放一个数字，然后signal唤醒$T_{c1}$， 生产者继续循环，直到发现缓冲区满后睡眠 此时如果$T_{c2}$抢先执行，消费了缓冲区里的值，然后$T_{c1}$从wait处恢复运行，调用get，此时发生error！ 原因在于，生产者signal唤醒了$T_{c1}$， 但是没有保证$T_{c1}$立即执行 （或者说，没有保证$T_{c1}$执行之前，缓冲区没有再发生变化） signal的这种语义称为Mesa语义 解决方案是： 始终使用while，这样当$T_{c1}$醒来时，会再次检查count==0，发现为缓冲区0则继续wait。这样就避免了error 使用两个条件变量 上述代码依然有问题： 假设$T_{c1}$和$T_{c2}$先运行，都卡在wait， $T_{p}$开始运行，往缓冲区放入一个值，发出signal， 继续循环，直到发现缓冲区满后睡眠 $T_{c1}$醒来，消费了这个值，然后在该条件上signal，注意，此时理应唤醒$T_{p}$， 但事实上有可能唤醒$T_{c2}$ 假如唤醒$T_{c2}$，因为缓冲区为空， 它会卡在wait，此时三个线程都处于睡眠 原因在于， signal没有指向性， 消费者不应唤醒消费者，只应该唤醒生产者 解决方案：使用两个条件变量， 生产者睡在empty， 消费者睡在fill 由此也看出，线程唤醒需要满足什么条件，它就应该睡在哪个条件变量上，这是条件变量的命名方式 比如消费者需要缓冲区fill才能醒来，因此该条件变量就命名为fill //30_8.ccond_t empty,fill;mutex_t mutex;int count = 0;void *producer(void *arg)&#123; int i; int loops = (int) arg; for( i=0; i &lt; loops; i++ ) &#123; Pthread_mutex_lock(&amp;mutex); if(count == 1) Pthread_cond_wait(&amp;empty,&amp;mutex); put(i); Pthread_cond_signal(&amp;fill); Pthread_mutex_unlock(&amp;mutex); &#125;&#125;void *consumer( void *arg )&#123; int i = 0; int loops = (int) arg; for(int i = 0 ; i &lt; loops; i++ ) &#123; pthread_mutex_unlock(&amp;mutex); while( count == 0 ) pthread_cond_wait(&amp;fill,&amp;mutex); int tmp = get(); Pthread_cond_signal(&amp;empty); Pthread_mutex_unlock(&amp;mutex); ptrintf(&quot;%d\\n&quot;,tmp); &#125;&#125; 最终版本 这是最终版本，生产者只有缓冲区满了的时候才会睡眠 因此信号量命名为empty有点名不符实（ fill也是如此，事实上缓冲区有一个值就可以唤醒消费者了 ） 对get()和put()的调用保证上了锁 //30_9.c 生产者消费者问题最终版int buffer[MAX];int fill_ptr = 0;int use_ptr = 0;int count = 0;void put(int value)&#123; buffer[fill_ptr] = value; fill_ptr = (fill_ptr+1)%MAX; count++;&#125;int get()//拿取use_ptr指向的值，use_ptr++&#123; int tmp = buffer[use_ptr]; use_ptr = (use_ptr+1)%MAX; count--; return tmp;&#125;cond_t empty,fill;mutex_t mutex;int count = 0;void *producer(void *arg)&#123; int i; int loops = (int) arg; for( i=0; i &lt; loops; i++ ) &#123; Pthread_mutex_lock(&amp;mutex); if(count == MAX) Pthread_cond_wait(&amp;empty,&amp;mutex); put(i); Pthread_cond_signal(&amp;fill); Pthread_mutex_unlock(&amp;mutex); &#125;&#125;void *consumer( void *arg )&#123; int i = 0; int loops = (int) arg; for(int i = 0 ; i &lt; loops; i++ ) &#123; pthread_mutex_unlock(&amp;mutex); while( count == 0 ) pthread_cond_wait(&amp;fill,&amp;mutex); int tmp = get(); Pthread_cond_signal(&amp;empty); Pthread_mutex_unlock(&amp;mutex); ptrintf(&quot;%d\\n&quot;,tmp); &#125;&#125; 覆盖条件 signal只会唤醒一个线程 考虑一个内存分配程序。 当没有空闲内存时， $T_{c1}$和$T_{c2}$各自allocate1000和10字节。 它们都因此卡在wait 此时$T_{p}$ free了50字节，它发出signal，此时有可能唤醒的是$T_{c1}$， 后者因为内存不够，依然继续睡眠 上述代码因此无法正常工作 解决方案是采用广播的signal, 即pthread_cond_broadcast() 代替pthread_cond_signal,唤醒所有等待线程，这个条件变量称为广播条件covering condition 会影响性能 该方案虽然很笨，但有时很有用 当然，30_8.c的代码也可以采用此解决方案。 但我当时有更好的办法（用两个条件变量） 信号量 def： 有一个整数值的对象，可以用sem_wait()和sem_post()操作（ in Posix ） 因此要初始化 #include&lt;semaphore.h&gt;sem_t s;sem_init(&amp;s, 0 , 0); 第二个参数一般设为0,表示该信号量在同一个进程的多个线程内贡献 int sem_wait(sem_t *s)&#123; //decrement the value of semaphore s by one //wait if value of semaphore s is a negative&#125;int sem_post(sem_t *s)&#123; //increment the value of semaphore s by one //if there are one or more threads waiting, wake one&#125; 在这个实现中， 信号量的值为负数时，该值就是等待线程的个数 信号量有时很难设计，此时还是使用条件变量更靠谱 二值信号量（锁） 可以把信号量作为锁(其值设为1) sem_t m;sem_init(&amp;m, 0, 1); //下文解释为何是1sem_wait(&amp;m);//critical sectionsem_post(&amp;m); 假设有两个线程，$T_{1}$调用sem_wait()，将信号量值减为0, 因为0不是负数，因此$T_{c1}$从wait返回并继续，它可以自由进入临界区， 若没有其他线程尝试获取锁，当$T_{1}$调用sem_post()时，会将信号重置为1 如果$T_{1}$持有锁时，$T_{2}$尝试获取锁（即调用sem_wait()），此时它会将信号量减为-1。然后卡在这里。 $T_{1}$再次运行，执行sem_post()， 将信号量值增加到0,唤醒等待的线程（$T_{2}$），然后$T_{2}$就能获取锁 当$T_{2}$执行结束时，执行sem_post()， 将信号量值增加到1 信号量用作条件变量 sem_t s;int count = 0;void *child(void *arg)&#123; printf(&quot;child\\n&quot;); sem_post(&amp;s); return NULL;&#125;int main(int argc, char *argv[])&#123; sem_init( &amp;s, 0, 0 );//信号量设为0 printf(&quot;parent: begin\\n&quot;); pthread_t c; Pthread_create(c, NULL, child, NULL); sem_wait(&amp;s); // wait here for child printf(&quot;parent: end\\n&quot;); return 0;&#125; 输出为： parent: beginchildparent: end 考虑两种情况： 子进程没有先运行，父进程先调用sem_wait(),将信号量减为-1,父进程卡在wait， 然后子进程运行，调用sem_post()，信号量增加为0,唤醒父线程 子线程在父线程调用sem_post()之前就运行结束，结果正常 生产者/消费者（有界缓冲区）问题 考虑用信号量实现生产者/消费者问题： int buffer[MAX];int fill_ptr = 0;int use_ptr = 0;int count = 0;void put(int value)&#123; buffer[fill_ptr] = value; fill_ptr = (fill_ptr+1)%MAX; count++;&#125;int get()//拿取use_ptr指向的值，use_ptr++&#123; int tmp = buffer[use_ptr]; use_ptr = (use_ptr+1)%MAX; count--; return tmp;&#125;sem_t empty,fill;// mutex_t mutex;int count = 0;void *producer(void *arg)&#123; int i; int loops = (int) arg; for( i=0; i &lt; loops; i++ ) &#123; sem_wait(&amp;empty); put(i); sem_post(&amp;fill); &#125;&#125;void *consumer( void *arg )&#123; int i = 0; int loops = (int) arg; for(int i = 0 ; i &lt; loops; i++ ) &#123; sem_wait(&amp;fill); int tmp = get(); sem_post(&amp;empty); ptrintf(&quot;%d\\n&quot;,tmp); &#125;&#125; 这段代码的问题在于： 对假设MAX大于1,此时可以有两个生产者（$T_1$, $T_2$）同时调用put()，如果$T_1$先放入数据，然后在更新计时器时中断，$T_2$运行，它会在该位置再放入一个值，发生error 这是因为在MAX&gt;1时，信号量的使用不能保证put（）的原子性 解决方案：上锁 int buffer[MAX];int fill_ptr = 0;int use_ptr = 0;int count = 0;void put(int value)&#123; buffer[fill_ptr] = value; fill_ptr = (fill_ptr+1)%MAX; count++;&#125;int get()//拿取use_ptr指向的值，use_ptr++&#123; int tmp = buffer[use_ptr]; use_ptr = (use_ptr+1)%MAX; count--; return tmp;&#125;sem_t empty,fill,mutex;int count = 0;void *producer(void *arg)&#123; int i; int loops = (int) arg; for( i=0; i &lt; loops; i++ ) &#123; sem_wait(&amp;mutex); sem_wait(&amp;empty); put(i); sem_post(&amp;mutex); sem_post(&amp;fill); &#125;&#125;void *consumer( void *arg )&#123; int i = 0; int loops = (int) arg; for(int i = 0 ; i &lt; loops; i++ ) &#123; sem_wait(&amp;mutex); sem_wait(&amp;fill); int tmp = get(); sem_post(&amp;empty); sem_post(&amp;mutex); ptrintf(&quot;%d\\n&quot;,tmp); &#125;&#125;int main()&#123; // ... sem_init(&amp;empty, 0, MAX); sem_init(&amp;fill, 0, 0); sem_init(&amp;mutex, 0, 1);// mutex = 1 because it is a lock //...&#125; 这段代码依然有问题： 因为在fill和empty上睡着时没有释放锁（传统的pthread_wait是会释放锁的，但是这里是用信号量实现的锁）。 因此消费者在fill上睡着时，二值信号量锁mutex没有释放，生产者试图对mutex调用sem_wait也被卡住。 消费者等待在full，持有mutex， 生产者可以signal fill，却在等待mutex，发生了死锁 解决方案： 把对mutex的获取和释放调整为紧挨着临界区 int buffer[MAX];int fill_ptr = 0;int use_ptr = 0;int count = 0;void put(int value)&#123; buffer[fill_ptr] = value; fill_ptr = (fill_ptr+1)%MAX; count++;&#125;int get()//拿取use_ptr指向的值，use_ptr++&#123; int tmp = buffer[use_ptr]; use_ptr = (use_ptr+1)%MAX; count--; return tmp;&#125;sem_t empty,fill,mutex;int count = 0;void *producer(void *arg)&#123; int i; int loops = (int) arg; for( i=0; i &lt; loops; i++ ) &#123; sem_wait(&amp;empty); sem_wait(&amp;mutex); put(i); sem_post(&amp;mutex); sem_post(&amp;fill) &#125;&#125;void *consumer( void *arg )&#123; int i = 0; int loops = (int) arg; for(int i = 0 ; i &lt; loops; i++ ) &#123; sem_wait(&amp;fill); sem_wait(&amp;mutex); int tmp = get(); sem_post(&amp;mutex); sem_post(&amp;empty); ptrintf(&quot;%d\\n&quot;,tmp); &#125;&#125;int main()&#123; // ... sem_init(&amp;empty, 0, MAX); sem_init(&amp;fill, 0, 0); sem_init(&amp;mutex, 0, 1);// mutex = 1 because it is a lock //...&#125; 读者 --- 写者锁 读写锁更加灵活，因为查找操作不会更改临界区 typedef struct _rwlock_t&#123; sem_t lock; // binary sephamore, basic lock sem_t writelock;//used to allow ONE writer or MANY readers int readers;// count of readers reading in critical section&#125;rwlock_t;void rwlock_init( rwlock_t *rw )&#123; rw -&gt; readers = 0; sem_init( &amp;rw -&gt; lock, 0, 1 ); sem_init( &amp;rw -&gt; writelock, 0, 1 );&#125;void rwlock_acquire_readlock(rwlock_t *rw)&#123; sem_wait(&amp;rw-&gt;lock); rw -&gt; readers++; if(rw-&gt;readers == 1) sem_post( &amp;rw -&gt;writelock ); // last reader releases writelock sem_post(&amp;rw -&gt; lock);&#125;void rwlock_release_readlock(rwlock_t *rw)&#123; sem_wait(&amp;rw-&gt;lock); rw -&gt; readers--; if( rw -&gt; readers == 0 ) sem_post( rw -&gt; writelock );// last reader releases writelock sem_post(&amp;rw-&gt;lock);&#125;void rwlock_acquire_writelock(rwlock_t *rw)&#123; sem_wait(&amp;rw-&gt; writelock);&#125;void rwlock_release_writelock(rwlock_t *rw )&#123; sem_post(&amp;rw -&gt; writelock);&#125; 想要获取写锁的进程，需要等待所有的读者都结束 缺点：读者很容易饿死写者 哲学家就餐问题 最简单的解决方案：破除依赖， 就是修改某个哲学家的取餐叉顺序 如何实现信号量 用锁和条件变量实现信号量： #include&lt;stdio.h&gt;#include&lt;pthread.h&gt;#include&quot;mythreads.h&quot;#include&lt;stdlib.h&gt;#include&lt;semaphore.h&gt;typedef struct _Zem_t&#123; int value; pthread_cond_t cond; pthread_mutex_t lock;&#125;Zem_t;//only one thread can call thisvoid Zem_init(Zem_t *s, int value)&#123; s -&gt; value = value; Pthread_cond_init(&amp;s-&gt;cond,NULL); Pthread_mutex_init(&amp;s-&gt;lock,NULL);&#125;void Zem_wait(Zem_t *s)&#123; Pthread_mutex_lock(&amp;s-&gt;lock); while( s-&gt;value &lt;=0) &#123; Pthread_cond_wait( &amp;s-&gt;cond, &amp;s-&gt;lock ); &#125; s -&gt;value--;//注意到这里是先检查是否为非正数，再递减 Pthread_mutex_unlock(&amp;s-&gt;lock);&#125;void Zem_post(Zem_t *s)&#123; Pthread_mutex_lock(&amp;s-&gt;lock); s -&gt; value++; Pthread_cond_signal( &amp;s -&gt; cond ); Pthread_mutex_unlock(&amp;s-&gt;lock);&#125; 注意到wait是先检查是否为非正数，再递减，这使得信号量值永远不会小于0 这也是linux的实现 用信号量来实现锁和条件变量相当困难 常见并发问题 非死锁缺陷 违反原子性缺陷：给共享变量的访问加锁 违反顺序缺陷：使用条件变量，强制顺序： //...Pthread_mutex_lock(&amp;mutex);while( inited == = ) Pthread_Cond_wait( &amp;cond, &amp;mutex );//operations to critical sectionPthread_mutex_unlock(&amp;mutex); 死锁缺陷 模块化和锁不是很契合 T1 T2 T3 T4 L1 Y Y N N L2 Y Y Y N 系统形成死锁的四个必要条件 互斥条件(mutual exclusion):系统中存在临界资源,进程应互斥地使用这些资源 占有和等待条件(hold and wait):进程请求资源得不到满足而等待时,不释放已占有的资源 不剥夺条件(no preemption):已被占用的资源只能由属主释放,不允许被其它进程剥夺 循环等待条件(circular wait):存在循环等待链,其中,每个进程都在链中等待下一个进程所持有的资源,造成这组进程永远等待 循环等待 细致地设计锁策略，有序加锁 持有并等待 任何线程抢锁之前要先抢一个全局锁，这样保证了抢锁的原子性(抢锁时不会有其他进程切入) 如：假如线程1需要lock1和lock2， 而线程二需要lock2和lock1。线程1获得lock1后不会被打断，能继续获得 lock2，执行完毕，释放这两个锁，线程二继续执行。 lock(prevection);//global locklock(L1);lock(L2);...unlock(prevention); 非抢占 top:lock(L1);if(trylock(L2)==-1) &#123; unlock(L1); goto(top); &#125; 同样是实现了原子地抢占锁 如果L2没抢到，那么会释放L1 会导致活锁（ livelock） 两个线程可能一直重复这一序列，又同时都抢锁失败 假如线程1持有lock1，等待lock2( 因此该线程一直try - fail)， 而线程二持有lock2，等待lock1,线程1在试图获得lock2时被中断，线程2获得lock2，试图获得lock1，此时发生活锁 互斥 通过无等待（wait-free）数据结构避免互斥 通过调度避免死锁 线程对锁的需求： T1 T2 T3 T4 L1 Y Y N N L2 Y Y Y N 只要T1和T2不同时运行就不会发生死锁 T3只用到一把锁，因此可以和其它线程并发执行，不会死锁 可以强制T2在T1之后运行 这种保守的方案很明显会降低性能 检查和恢复 允许死锁偶尔发生，检查到死锁时再采取行动（重启电脑） 太摆烂了。。。 基于事件的并发（Advanced） 事件循环： while(1)&#123; events = getEvents(); for( e in events ) &#123; processEvent(e); &#125;&#125; 因为事件是原子的，一次只处理一个事件不需要考虑线程切换。 而且如上所见，我们可以对事件调度进行显式控制 这意味着事件是阻塞的，有巨大的性能开销，需要引入异步的事件处理，再加上多CPU时，并行的事件处理复杂度相当于多线程。 因此给予事件的并发并不比基于线程的简单。 这部分内容很庞大，我不想在C编程上倾注太多时间，因此放在JAVA等语言的并发中讲。","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"OS Basic","slug":"OS-Basic","permalink":"http://lyk-love.cn/tags/OS-Basic/"}]},{"title":"人体结构与疾病","slug":"人体结构与疾病","date":"2021-12-16T21:53:04.000Z","updated":"2022-09-26T06:39:34.942Z","comments":true,"path":"2021/12/17/人体结构与疾病/","link":"","permalink":"http://lyk-love.cn/2021/12/17/%E4%BA%BA%E4%BD%93%E7%BB%93%E6%9E%84%E4%B8%8E%E7%96%BE%E7%97%85/","excerpt":"Outline: 运动系统 循环系统 呼吸系统 消化系统 免疫系统 神经系统 泌尿生殖系统 内分泌系统","text":"Outline: 运动系统 循环系统 呼吸系统 消化系统 免疫系统 神经系统 泌尿生殖系统 内分泌系统 运动系统 组成：骨、骨连结（关节）、骨骼肌 约占体重的60% 骨 206块骨： 躯干骨：51块 脊柱：24块椎骨（ 椎间盘突出（向后突）做针灸推拿都没用，只能开刀 ） 颈椎： 7（第七颈椎最大，是能摸到的。 前六个无论再瘦也摸不到） 胸椎： 12 腰椎： 5 骶骨： 1 （ 儿童时5块，成年后合为一块 ） 尾骨：1 肋骨（24块）： 第一肋骨藏在锁骨后面，摸不到 第二肋骨：从上往下摸，距顶端大概四五厘米的样子会摸到一个明显的隆起，是胸骨角，对应的肋骨就是第二肋 第十一、十二块是“浮肋”：肋软骨不与其他肋相连接它们末端是游离状态，没有与其它十根肋骨形成胸廓，因为末端游离，所以叫浮肋 上肢骨：64块，对称。 锁骨：2 肩胛骨：2 肱骨：2，即大臂骨。 桡骨：2，拇指一侧为桡骨。 有桡动脉，搭脉、割脉都是它。 尺骨：2，小指一侧为尺骨。曲肘时摸到的明显凸起就是尺骨鹰嘴。 腕骨：16 掌骨：10 指骨：28 下肢骨：62 髋骨：2 每块由髂骨、坐骨和耻骨融合而成 左右两个髋骨和一块骶骨构成了骨盆 股骨：2 髌骨：2 胫骨：2 腓骨：2 附骨：14 拓骨：10 趾骨：28 除了大脚趾两块，都是三块 颅骨：23 脑颅骨：8 额骨，枕骨，蝶骨，筛骨各一块，顶骨，颞骨各两块 儿童颅骨没有完全长好，还有囟门（前囟和后囟） 脑和脊髓是连通的 面颅骨：15 上腭骨、颚骨、颧骨、泪骨、鼻骨、下鼻甲各两块，犁骨、下颌骨、舌骨各一块 听小骨：6 锤骨、砧骨、镫骨各2块 肌肉 肌肉： 骨骼肌（横纹肌）：运动系统的肌肉均为横纹肌。它的两端通常附着在两块或两块以上的骨上，越过一个或多个关节，肌肉收缩时，便产生运动 平滑肌：对收缩拉伸非常敏感。 对着骨骼肌来一刀会疼，对着平滑肌来一刀不会疼。但要是对着平滑肌拉一拉就会疼。 心肌。 运动系统疾病 骨折 骨的完整性破坏或连续性中断称为骨折 开放性骨折、青枝骨折、病理性骨折 并发症： 休克、感染、合并重要内脏器官损伤 自我判断伤后有无骨折： 从伤后出现的症状以及功能障碍加以分析： 若伤处疼痛剧烈，局部肿胀明显，有严重的皮下瘀血、青紫，出现外观畸形，这时均应考虑有骨折的可能 此外，一般骨折病人多有功能障碍，如手臂骨折后，手的握力差，甚至不敢提东西； 用在远离受伤部位叩击的方法检查： 如上肢骨折，此手握拳，另一手手掌轻轻绐予撞击，若伤处感到疼痛，则骨折的可能性极大。 至于下肢骨折，可用拳轻轻叩击患肢足跟，看看伤处有无痛感。 循环系统 心脏 心脏位于横膈之上，两肺间而偏左，主要由心肌构成，有左心房、右心房、右心室四个腔。左右心房之间和左右心室之间均由间隔隔开，故互不相通，心房与心室之间有瓣膜，这些瓣膜使血液只能由心房流入心室，而不能倒流。 在左边的叫“左心室”，在右边的叫“右心室”，心室壁厚，肌肉发达。左心室的壁比右心室更厚，肌肉更发达。这是因为左心室与主动脉相连，右心室与肺动脉相连，因此左心室收缩产生的压力要强于右心室。 血液由心房压入心室后，由心室压入动脉，分别输送到肺部与全身的其他部分。 动脉 将血液带出心脏的血管 体表可扪及的动脉：桡动脉、足背动脉、颈总动脉 静脉 将血液带回心脏的血管 浅静脉多有静脉瓣，如大隐静脉等。面静脉无静脉瓣，面部疖肿挤压后脓栓可沿面静脉、内眦静脉、眼静脉入海绵窦，引起颅内感染。故将鼻根至两口角之间的区域称为危险三角。（ 脸上的痘痘不能挤 ） 毛细血管 连于动脉与静脉之间管径极细、管壁极薄的血管 手指神经在两侧，内侧是肌腱，没多少血管，因此隔开了不会很疼，可以“空手接白刃” 循环系统疾病 心律失常 早搏、心动过速、心动过缓等 动脉粥样硬化和冠状动脉粥样硬化性心脏病 动脉硬化的特点：动脉发生了非炎症性、退行性和增生性的病变，导致管壁增厚变硬、失去弹性和管腔缩小 动脉粥样硬化的特点是：在上述病变过程中，受累动脉从内膜开始，先后有多种病变合并存在，包括有脂质和复合糖类积聚，出血和血栓形成，纤维组织增生和钙质沉着，并有动脉中层的逐渐退化和钙化 冠心病： 冠状动脉粥样硬化性心脏病，亦称缺血性心脏病。：指冠状动脉粥样硬化使血管腔阻塞导致心肌缺血缺氧而引起的心脏病，它和冠状动脉功能改变（痉挛）一起，统称为冠状动脉性心脏病 原发性高血压 高血压：一种原因未明的、以体循环动脉血压升高为主要表现的独立性全身性疾病，细动脉硬化为基本病变，常引起心、脑、肾及眼底改变。 标准：收缩压≥140mmHg（18.4kPa）或 舒张压≥90mmHg（12.0kPa） 类型：原发性和继发性 我国最常见的心血管疾病，多见于中、老年人 营养与保健： 限制钠盐。科学家们已证实钠离子与高血压的发病密切相关，所以控制食盐的摄入能防治高血压病，一般主张每日用盐控制在5克以下，最好是3克，即食物中有轻度咸味即可。如果有耳鸣、眩晕、浮肿的高血压病人，食盐应严格控制，每日限制在2克以下，或不用食盐，用10cc酱油代替（1汤匙为15cc） 选用优质的蛋白质食物 多选用含钾、镁、碘和锌高的食物。因为这类微量元素，有降压和保护心脏和预防动脉粥样硬化的功能。 含钾高的食物：柑橘、苹果、杏、红枣、葡萄、花椰菜、大豆、黑豆、菠菜、土豆等。家禽类、鱼和瘦肉含钾量也高。 含镁高的食物：各种干豆类及鲜豆、苋菜、桂圆、豆芽等。 含碘高的食物：海产品类、海带、紫菜等。 含锌高的食物：瘦牛肉、瘦猪肉、黄鱼、花生、荔枝等。其它谷类的原粮含锌也高，但含植酸高，要经发酵制作后，才可以吸收利用（象蒸玉米面馒头、蒸发糕、江米酒、芝麻酱等可以食用）。 另外要注意，每日要饮适量温开水（最好清晨喝一杯白开水），有调节血液黏度的作用，可以净化血液，又能通大便。同时应注意“平衡膳食”，即成酸或成碱的食物都应有（荤素搭配）。其次主食中最好粗细粮相结合。 饮食制度方面：要有规律性，食勿过饱，晚餐饮食要清淡易于消化。不饮酒，不吸烟。心情要轻松愉快。 其他方面：像深色叶菜含大量维生素A、B、C及微量元素钙、镁。又如：水果及黄瓜、西红柿等可以生吃，可用于降体重膳食，对高血压病人有好处。 心肌梗死 心肌梗死（myocardial infarction，MI）是由冠状动脉粥样硬化引起血栓形成、冠状动脉的分支堵塞，使一部分心肌失去血液供应而坏死 多发生于中年以后 发病时有剧烈而持久的性质类似心绞痛的前胸痛、心悸、气喘、脉搏微弱、血压降低等症状，服用硝酸甘油无效，可产生严重后果 主动脉夹层 指主动脉腔内的血液从主动脉内膜撕裂处进入主动脉中膜，使中膜分离，沿主动脉长轴方向扩展形成主动脉壁的真假两腔分离状态 高峰年龄：50～70岁，男女比例约2～3 : 1。65%～70%在急性期死于心脏压塞、心律失常等 早期诊断和治疗非常必要 主动脉夹层由于高血压动脉粥样硬化所致者占70%～80%，高血压可使动脉壁长期处于应急状态，弹力纤维常发生囊性变性或坏死，导致夹层形成 大隐静脉曲张 深静脉血栓 血液非正常地在深静脉内凝结，属于下肢静脉回流障碍性疾病。血栓形成大都发生于制动状态（尤其是骨科大手术）。 致病因素：血流缓慢、静脉壁损伤和高凝状态 血栓形成后，除少数能自行消融或局限于发生部位外，大部分会扩散至整个肢体的深静脉主干，若不能及时诊断和处理，多数会演变为血栓形成后遗症，长时间影响患者的生活质量；还有一些病人可能并发肺栓塞，造成极为严重的后果。 呼吸系统 呼吸道 包括鼻、咽、喉、气管、主支气管。临床上将鼻、咽、喉称为上呼吸道，气管、主支气管称为下呼吸道。 肺 位于胸腔内，纵隔两侧，左、右各一，右肺粗短，左肺狭长。每侧肺似半个锥体形。 主支气管入肺后依次分为肺叶支气管、肺段支气管、小支气管、细支气管、终末支气管。恰似一棵倒立的大树，也称支气管树。 呼吸系统疾病 慢性支气管炎 ---&gt; 阻塞性肺气肿 - --&gt; 肺源性心脏病 慢支：咳嗽、 咳痰、喘息，每年发病持续三个月，连续两年或以上，并排除其他心、肺疾患 原因：抽烟（最常见）、矿工 可逆性：好好治疗，肺可以恢复到健康状态; 如果继续下去，进入到肺气肿，就不可逆了 肺气肿：胸廓扩张 呼气困难 肺心病：右心衰竭 两三年就会死，除非心肺联合移植 单独移植肺或心脏没用，因为二者都病变了 单独的肺移植一般用于肺癌 右心室变大，需要花更多力量把血液压进肺动脉。 （左心室变大一般是高血压的表现，它需要更多的力量把血液压到全身） 肺炎 肺炎球菌肺炎 葡萄球菌肺炎 肺炎支原体肺炎（ 原发性非典型性肺炎 ） 支原体：微生物 我们所谓的“非典”其实是SRAS，属于病毒，当初被误认为支原体，才被称为”非典“ 病毒性肺炎（ SARS, 新冠 ） 真菌性肺炎 真菌性肺炎不是从外部感染的，肺内部本来就有真菌，是由于人体免疫功能下降，才导致真菌性肺炎 气胸 原因：胸廓破了， 或肺大疱 胸腔积液 肺癌 肺鳞状细胞癌 鳞状：脸部就是鳞状上皮，口腔内部就是柱状上皮 支气管哮喘 肺纤维化 肺纤维化是一大类疾病的总称 感冒 普通感冒的主要症状是呼吸道卡他症状 卡他： 渗出物沿着黏膜表面顺势下流 感冒 由病毒引起 急救:心肺脑复苏( CPR ) Cardiopulmonary resuscitation 原来并无严重器质性病变的心脏因一过性的急性原因而突然中止搏血导致的循环和呼吸停顿的临床死亡状态称为心跳骤停 传统观点认为，大脑缺血缺氧超过4～5分钟即可遭受不可逆的损伤，故把心跳骤停的复活时间定为5分钟。但在环境温度、病人机体状况、原发疾病等不同情况下尚存在一定的差异，切不可生搬硬套 心跳骤停的原因: 原发性:如冠心病（最为多见）、心肌炎、心肌病、某些先天性心脏病等。 继发性 严格讲,后合适原发性达到不可你阶段的必然后果 步骤: 现场救治: 保持气管通畅: 头后仰,提起下颈,手法清理口咽部,推举上腹部,扣打背部 人工呼吸: 口对口(鼻)呼吸 人工循环: 胸外心脏按压 进一步生命支持： 用药输液(drugs) 心电图监测(ECG) 电除颤(Fibrillation) 持续生命支持： 诊断(Gauge) 低温(Hypothermia) 加强治疗(ICU) 消化系统 组成部分: 消化管 消化腺 主要功能: 消化食物 吸收营养 排出粪便 消化管 始于口腔,终于肛门. 包括口腔、咽、食管、胃、小肠（十二指肠、空肠、回肠）和大肠（盲肠、结肠、直肠）等部. 临床以十二指肠为界,以上的消化管(包括十二指肠)称为上消化道,空肠及以下的称为下消化道 口腔 是消化管的起始处，借口唇与外界相通，借咽峡与咽相续。口腔以上、下颌牙咬合为界分为口腔前庭和固有口腔 咽 咽位于1~6颈椎前方，为一漏斗形肌性管道。长约12cm 食管 为一肌性管道，长约25cm，沿脊柱前方下降，上续咽，下接胃的贲门。有三个生理狭窄，其中第三狭窄（位于食管穿膈处）是肿瘤好发部位和异物易滞留部位 胃 胃：分为两壁、两缘、两口。两壁即前、后两壁，两缘即大弯和小弯，两口即上口贲门、下口幽门。又可分为四部，即贲门部、胃底、胃体和幽门部。幽门部临床也称胃窦，分幽门窦和幽门管两部。 小肠 一般根据形态和结构变化分为三段，分别为十二指肠（duodenum），空肠（jejunum）和回肠（ileum） 小肠内部有集合淋巴滤泡，因此小肠是重要的免疫器官 小肠移植非常困难（排异反应），现在普遍用菌群移植替代 菌群移植：吃粪便。 因为小肠是灌不进去的，只能吃进去。 一般都包在胶囊里。 大肠 盲肠、结肠、直肠 盲肠不会发炎，只有阑尾炎 消化腺 肝 是最大的消化腺，呈楔形褐红色。有两纵一横沟，分为左叶、右叶、方叶和尾状叶。 横沟又称肝门，是肝左右管、肝固有动脉、肝门静脉、神经、淋巴管等出入的部位。肝大部位于右季肋区和腹上区，少部分位于左季肋区。上界与膈穹隆一致。下界右侧与右肋弓基本一致，剑突下可达3~5cm，平静呼吸时上下共有2~3cm的移动度。 胆囊：胆囊位于右季肋区，肝的下方胆囊窝内。形态：似梨形，分为胆囊底、胆囊体、胆囊颈、胆囊管四部。主要功能为暂时储存和浓缩胆汁。胆囊底的体表投影在右锁骨中线与右肋弓交点稍下方。 胆道：是将肝细胞产生的胆汁输送至十二指肠的管道。 胰 位于胃的后方，分胰头、胰体、胰尾三部。胰头被十二指肠环抱。外分泌部分泌胰液，由胰管开口于十二指肠。内分泌部分泌胰岛素，调节血糖。 消化系统疾病 上消化道出血 指屈氏韧带以上的消化道，包括食管、胃、十二指肠或胰胆等病变引起的出血；胃空肠吻合术后的空肠病变所致的出血亦属这一范围。 一般指在数小时内的失血量超出1000ml或循环血容量的20%。为临床常见的急症，病死率为10%，误诊率为20% 常见病因： 食管疾病（食管炎、食管癌等）、胃十二指肠疾病（消化性溃疡、胃炎、胃癌等）、空肠疾病（胃肠吻合术后的空肠溃疡）、肝硬化、门静脉血栓、胆道出血、胰腺癌、全身性疾病（血液病、尿毒症、应激性溃疡、急性感染等） 临床表现：呕血与黑粪是上消化道出血的特征性表现。一般来讲，出血部位在幽门以下者只表现为黑粪，在幽门以上者常兼有呕血 呕血与咯血的区别： 病史：呕血患者多有胃、十二指肠溃疡，肿瘤或肝硬变等病史；而咯血患者一般有结核，支气管扩张或心肺疾病等。 出血方式：呕血多随呕吐引起，咯血一般是咳嗽后吐出。 血液颜色：呕血的颜色呈紫红或咖啡色，无泡沫，咯血的则为鲜红，有泡沫 消化性溃疡 主要是指发生在胃和十二指肠球部的慢性溃疡。溃疡的形成与胃酸和胃蛋白酶的消化作用有关，故称消化性溃疡，又称胃十二指肠溃疡。 19世纪时，本病较少见，且胃溃疡多于十二指肠溃疡，当今资料显示十二指肠溃疡较为多见。十二指肠溃疡中男性患者较女性为多，而胃溃疡则无显著性别差异。十二指肠溃疡患者以青壮年居多，而胃溃疡患者平均年龄要比十二指肠溃疡患者大10年。 临床表现： 慢性过程 周期性发作（且有季节性，一般于秋冬或冬春之交） 节律性疼痛 胃癌 是最多见的消化道肿瘤，死亡率高，由于早期无症状，故发现时已届中晚期。 癌前病变：慢性萎缩性胃炎、胃息肉、残胃、胃溃疡等 肝硬化 病因：我国以病毒性肝炎所致的肝硬化最为常见，国外以酒精性肝硬化多见。 临床表现：一般状况和营养状况较差，皮肤干枯、面色灰暗黝黑、有出血倾向、贫血、门脉高压（脾肿大、腹水、上消化道出血） 可导致肝性昏迷、原发性肝癌等 阑尾炎 特征：转移性右下腹痛、压痛、反跳痛 病毒性肝炎 病毒性肝炎是由多种肝炎病毒引起的,以肝损害为主要病变的一组传染病 分型: 甲,乙,丙,丁,戊 主要临床表现： 乏力,纳差,厌油腻,恶心,腹胀,黄疸,肝区隐痛,肝大,肝功能异常。 免疫系统 分类 器官 中枢免疫系统 胸腺 骨髓 腔上囊 外周免疫器官 淋巴结 脾脏 淋巴小节 分类 名称 免疫细胞 淋巴细胞（ T, B ） 巨噬细胞 免疫分子 抗体 补体 细胞因子 腔上囊： 鸟类拥有，哺乳动物没有 鸡屁股可以吃：这是中枢免疫器官，不是外周免疫器官，免疫应答不发生在这里。 因此这里不会有大量的病原体 鸡脖子也可以吃：鸡没有淋巴结。 鸡脖子上是鸡的胸腺。 因此鸡脖子也能吃 淋巴结：淋巴结清扫的后遗症非常恐怖。可以用淋巴结移植等替代方案 免疫系统疾病 过敏 超敏反应，也叫变态反应 指机体受同一抗原物质再次刺激后产生的一种异常或病理性免疫反应。也就是说，若机体已被抗原致敏，当再次接触相同抗原时则二次免疫应答被增强。在摄入抗原量较大或机体的免疫处于高应答状态时，则因免疫应答过程而导致组织损伤。 超敏反应的分类：1963年Gell和Coombs根据超敏反应发生的机理和临床特点，将其分为四型： Ⅰ型，即速发型超敏反应 呼吸道过敏反应 过敏性鼻炎、过敏性哮喘、花粉症 消化道过敏反应 过敏性胃肠炎 全身性过敏反应 青霉素过敏 Ⅱ型，即细胞溶解型或细胞毒型超敏反应 输血反应 新生儿溶血症 移植排斥反应 自身免疫性溶血性贫血 肺出血肾炎综合征 Grave病 重症肌无力 Ⅲ型，即免疫复合物型超敏反应 血清病：是一种由循环免疫复合物引起的全身的III型超敏反应性疾病，主要由注射异种动物血清所致，故称为血清病。包括急性和慢性血清病。 免疫复合物性肾小球肾炎 类风湿性关节炎 系统性红斑狼疮 Ⅳ型，即迟发型超敏反应 接触性皮炎 移植排斥反应 与自身免疫病的关系 与传染病的关系（OT试验） 艾滋病 HIV可以侵袭人的免疫系统，降低并最终破坏人体的免疫功能。随着人体免疫力的降低，人会越来越频繁地感染上各种致病微生物，而且感染的程度也会变得越来越来重，最终会因各种复合感染而导致死亡 传播方式：AIDS 为传染病性病，其传播方式主要有三，其中以性接触最具重要性。 性接触传播： 同性恋和/或双性恋：约占70%病例。男同性恋者HIV感染率甚高，尤以有色人种居多。HIV感染者精液中的HIV经直肠粘膜传给健康的同性恋性伴 异性恋：约占4%病例。精液中的HIV感染细胞与子宫中的巨噬细胞、淋巴细胞、上皮细胞相互作用而使女方感染。阴道分泌物中的HIV感染细胞能将HIV传给男性尿道口中的巨噬细胞或淋巴细胞。 血液传播： 输血：约占2.5%病例。 血制品：约占1%病例。 共用污染有HIV的针头和注射器等：约占18%。 母婴垂直传播：HIV可经胎盘、产道和母乳传给下一代。 神经系统 分类 器官 中枢神经 脑 脊髓 周围神经 按部位分：脑神经、脊髓神经 按分布分：内脏神经（内脏感觉神经、内脏运动神经（交感神经、附交感神经））、躯体神经 中枢神经系统 脑 是中枢神经系统的头端膨大部分，位于颅腔内，可分为端脑、间脑、中脑、脑桥、小脑和延髓六个部分。其中中脑、脑桥和延髓合称为脑干。 脊髓 位于椎管内，上端平对枕骨大孔连接延髓，下端平对第1腰椎下缘水平。 周围神经系统 脊神经 共31对，颈神经8对，胸神经12对，腰神经5对，骶神经5对，尾神经1对。 脑神经 共12对。嗅神经、视神经、动眼神经、滑车神经、三叉神经、外展神经、面神经、 前庭蜗神经（听神经）、舌咽神经、迷走神经、副神经、舌下神经。 神经系统疾病 脑膜炎 神经系统症状： 颅内压升高：三联征：头痛、喷射性呕吐、视觉障碍 脑膜刺激症：颈项强直、角弓反张、屈髋伸膝（Kernig）征阳性。 震颤麻痹 又名帕金森病，以运动减少、肌张力强直和震颤为主要的症状。本病典型的震颤为静止性震颤。 重症肌无力 癫痫 亨廷顿舞蹈症 阿尔兹海默症 常见精神病 精神分裂症 患者不自知 单纯型 又称隐潜型精神分裂症，以缓慢呈现的思维贫乏、情感淡漠、意志缺乏、孤僻被动及社会退缩等症状为主。大多发病于少年时期，故而又称为儿童精神分裂症。起病较缓慢、发病诱因不明显，最初不易被察觉，但当发现时病情往往已发展到较为严重的阶段。由于单纯型精神分裂症发病年龄较早，症状多被误解为孩子调皮不听话，而延误治疗造成病情加重。但一般很少出现幻觉、妄想和紧张症状，自知力没有丧失，在众多类型中属于轻型的精神分裂症。 青春型 较常见，多发病于青春期，起病较急，病情发展较快。多始发于15~25岁，无明显男女性别差异。表现为情感喜怒无常，行为幼稚，常有兴奋冲动行为及本能（性欲、食欲）意向亢进。病情发展较快，但如及时治疗，效果较好。 紧张型 最少见，起病最快，多在青壮年期发病，以木僵状态多见。从运动缓慢、少语少动（亚木僵状态）到固定于某个姿势，不语不动，不饮不食，表情呆板，对环境变化毫无反应（木僵状态），但神志清楚。病人肌肉紧张，可处于某个固定姿势不动。紧张性木僵可与紧张性兴奋交替出现，此时病人出现冲动行为，如卧床不懂的病人可突然起床，无目的地砸东西，然后仍旧躺下。此病可自动环节，治疗效果较好 偏执型 又称妄想型，是最常见的类型，起病缓慢。发病年龄多在青壮年和中年。病初表现为敏感多疑，逐渐发展成妄想（妄想对象一般是自己的上级）。妄想可单独存在，也可伴有以幻听、幻视等幻觉。情感障碍表面上会不明显，智力通常不受影响。患者注意力和意志往往增强，警惕、多疑且敏感，在幻觉影响下产生反抗，严重时有自伤、自杀以及伤人、杀人等严重行为。自发缓解少，治疗效果一般。 躁狂抑郁症 患者自知 躁狂抑郁症： 简称躁郁症，属于情感性精神障碍。是以显著而持久的情感过度高涨（躁狂相），或低落（抑郁相）为临床主要特征/病程经过为躁狂或抑郁的循环发作（双相）或单相（仅有抑郁相）发作，间歇期精神活动多保持正常 自杀企图 反应性精神障碍 反应性精神障碍：是指一组在严重或持久的精神创伤下引起的精神障碍，其临床症状特点和病程经过与创伤性体验有密切关系 神经衰弱 神经衰弱：属于心理疾病的一种，是一类精神容易兴奋和脑力容易疲乏、常有情绪烦恼和心理生理症状的神经症性障碍。神经衰弱是由于大脑神经活动长期处于紧张状态，导致大脑兴奋与抑制功能失调而产生的一组以精神易兴奋，脑情绪不稳定等症状为特点的神经功能性障碍 泌尿生殖系统 泌尿系统：肾、输尿管、膀胱、尿道 肾： 左肾比右肾高 男性生殖系统： 内生殖器：睾丸，附睾，输精管，射精管，精囊和前列腺 外生殖器：阴茎，阴囊 “内”“外”的依据是是否在腹腔内，睾丸在人出生时是在腹腔内的 女性生殖系统： 内生殖器：成对的卵巢和输卵管， 单一的子宫和阴道 外生殖器：会阴部的大阴唇和小阴唇以及附属腺等 泌尿生殖系统疾病 肾结石 1901年考古学家在公元前4800年的古埃及木乃伊体内首次发现膀胱结石和肾结石，公元前四世纪，古希腊希波克拉底在他的誓言中曾提到结石手术。在《黄帝内经》中也有记载 诊断： B超 治疗原则： 清除结石，保护肾功能 去除病因，防止结石复发 异位妊娠 即宫外孕，孕卵在子宫腔外着床的妊娠 检查： 常有腹腔内出血体征 治疗：手术为主 插叙： 女性盆腔痛 阑尾炎 肠易激综合症（IBS）： 月经痛：如果没有受孕，则子宫内膜脱落并排出体外，此时剧烈的子收缩会导致痛经，疼痛往往持续1-3天，局部热敷和止痛药有效 经前期综合症（PMS）： 异位妊娠 排卵痛：在月经周期中间感到的疼痛，可能与随卵子排出的液体及血液有关，可在不同的月份出现于不同侧，其本身对人体没有危害 盆腔炎 子宫内膜异位 卵巢囊肿 泌尿系感染 肾结石 膀胱炎 盆腔器官脱垂：包括盆腔、子宫，通常不会对健康造成特殊影响 瘢痕组织 性传播疾病 器官移植 移植的类型 根据移植物的来源及其遗传背景不同，可将移植分为四类： 自体移植：指移植物取自受者自身。例如下肢皮肤烧伤后，将患者自身上肢皮肤移植到下肢创面。这种移植如无感染，均能成功。 同系移植：指遗传基因完全相同或基本近似的个体间的移植。例如单卵双生子之间的移植，或同系动物交配的后代间的移植。这种移植一般都可成功。 同种异体移植：指同种间遗传基因不同的个体间的移植。 异种移植：指不同种属间的移植，其遗传基因性完全不同。 器官移植排斥的类型 宿主抗移植物反应（HVGR）：宿主体内致敏效应细胞和抗体对移植物进行攻击，导致移植物被排斥的反应，称为宿主抗移植物反应。 超急性排斥反应 急性排斥反应 慢性排斥反应 移植物抗宿主反应（GVHR）：如果免疫攻击方向是由移植物针对宿主，即移植物中的免疫细胞对宿主的组织抗原产生免疫应答并引起组织损伤；也就是说，有移植物中的抗原特异性淋巴细胞识别宿主移植抗原而发生的一种反应，这种反应不仅导致移植失败，而且还能给受者造成严重后果。 移植排斥的防止 合理地进行组织配型 严格选择供者 抑制受者的免疫反应 加强移植后的免疫监测 内分泌系统 内分泌腺和内分泌组织 内分泌腺 组成： 甲状腺 甲状旁腺 肾上腺 垂体 松果体 特点： 没有运送分泌物的导管，分泌物通过毛细血管和毛细淋巴管进入血循环中，从而输送到全身各部； 它们分泌的化学物质叫激素，对人体代谢、生长、发育和生殖起重要的调节作用； 内分泌腺血管丰富，腺细胞与毛细血管内皮紧密连接。 内分泌组织 胰腺内胰岛 睾丸内的间质细胞 卵巢内的黄体和卵泡 激素名称 产生激素的内分泌腺名称 激素的主要生理作用 生长激素 垂体 促进生长 促甲状腺激素 垂体 促进甲状腺的生长发育 促性腺激素 垂体 促进性腺的生长发育 甲状腺激素 甲状腺 促进新陈代谢和生长发育，尤其是对中枢神经系统的生长发育。 提高神经系统的兴奋性 胰岛素 胰腺中的胰岛 调节糖类代谢 性激素 雄（主要睾丸）、雌（主要卵巢）、孕（卵巢）） 内分泌系统疾病 糖尿病 临床上将糖尿病分为胰岛素依赖型（I型）和非胰岛素依赖型糖尿病（II型），II型较I型多见，多发生在40岁以上成年人，体型肥胖 分为原发性糖尿病和继发性糖尿病两类，原发性糖尿病占绝大多数，继发性糖尿病大多继发于胰岛组织被广泛破坏后。如胰切除术后。 糖尿病一般指原发性糖尿病 空腹血糖大于7.8mmol/L, 可诊断糖尿病 糖尿病慢性病变：心血管、肾、眼部 甲状腺机能亢进症 简称甲亢 临床表现：高代谢症群、神经、心血管系等功能失常、甲状腺肿大等，弥漫性者多有不同程度的突眼症","categories":[{"name":"Physiology","slug":"Physiology","permalink":"http://lyk-love.cn/categories/Physiology/"}],"tags":[{"name":"人体结构与疾病","slug":"人体结构与疾病","permalink":"http://lyk-love.cn/tags/%E4%BA%BA%E4%BD%93%E7%BB%93%E6%9E%84%E4%B8%8E%E7%96%BE%E7%97%85/"}]},{"title":"Computer Networking Introduction","slug":"Computer Networking-Introduction","date":"2021-12-08T16:56:56.000Z","updated":"2022-09-26T06:39:34.927Z","comments":true,"path":"2021/12/09/Computer Networking-Introduction/","link":"","permalink":"http://lyk-love.cn/2021/12/09/Computer%20Networking-Introduction/","excerpt":"Outline ： 互联网的组成 计算机网络的提及结构 计算机网络的性能指标 计算机网络基本工具","text":"Outline ： 互联网的组成 计算机网络的提及结构 计算机网络的性能指标 计算机网络基本工具 互联网的组成 互联网拓扑结构： 边缘部分： 用户直接使用的主机。 主机host也称为端系统end system 核心部分：大量网络和连接这些网络的路由器。为边缘部分提供服务（ 连通性和交换 ） 互联网核心部分 路由器： 分组交换 交换：按照某种方式动态地分配传输线路的资源 电路交换：建立连接- 通话 - 释放连接 通话的全部时间内，通话用户始终占据端到端的全部资源 报文交换：整个报文线传送到相邻节点，全部存储下来后查找转发表，转发到下一个节点 分组交换： 单个分组（整个报文的一部分）传送到相邻节点，全部存储下来后查找转发表，转发到下一个节点 分组交换和报文交换不需要预先分配传输带宽， 数据在哪条链路上传送才占用该链路 计算机网络的体系结构 [网络]协议：控制两个对等实体进行通信的规则的集合 语法：数据与控制信息的结构或格式 语义即需要发出何种控制信息，完成何种动作以及做出何种相应 同步：即事件实现顺序的详细说明 实体： 任何可发送或接受信息的硬件或软件进程 在协议的控制下，来自各个peer entity间的通信时的本层能够向上一层提供服务。 要实现本层协议，还要使用下一层提供的服务 协议与服务不同： 协议的实现保证了能够向上一层提供服务。 使用本层服务的实体只能看见服务而无法看见下面的协议 下面的协议对上面的实体是透明的 协议是“水平的”，服务是“垂直的” 服务是下层向上层提供层间借口实现的 只有被高层实体看得见的功能才称之为服务（即，并非在层内完成的全部功能都称为服务） 上层使用下层所提供的服务必须通过与下层交换一些命令，称为“服务原语” 协议数据单元PDU( Protocol Data Unit )： 对等层次之间传送的数据单元 服务数据单元SDU( Service Data Unit ): 层与层之间交换的数据单元 五层协议的体系结构：应用层、运输层、网络层、数据链路层、物理层 本文采用五层结构 七层：应用层、表示层、会话层、运输层、网络层、数据链路层、物理层 应用层 数据单元：报文message 作用：提供进程间交互 运输层 作用： 为两台主机中线程间的通信提供通用的数据传输服务 协议： TCP：面向连接的，可靠的 数据单元： 报文段segment UDP:无连接的，尽最大努力的 数据单元：用户数据报 网络层 作用：将运输层产生的报文段或用户数据报封装成分组 packet或包进行传送，分组也称为**[IP]数据报** 其实，不管哪一层的数据单元，都可称为“分组” 互联网是由大量的异构网络通过router连接起来的 一个packet经过的一系列链路和叫哈u年纪称为通过该网络的route 数据链路层 将网络层的IP数据报组装成frame,每一帧包括数据和一些控制信息 物理层 数据单元：bit 传递信息所利用的物理媒体并不属于物理层 计算机网络性能指标 吞吐量 吞吐量throughput: 单位时间内通过某个网络的实际的数据量 时延 时延latenty：数据从网络的一端传送到另一端的时间 发送时延：主机或router发送数据帧所需要的时间 = 数据帧长度 / 发送速率 传播时延：电磁波在信道中传播一定的距离需要花费的时间 = 心道长度 / 电磁波在信道中的传播速率 处理时延：主机或router处理收到的分组的时间 排队时延：分组在router的输入队列中排队等待处理 丢包：分组如果到达一个满的队列，router将drop该分组 总时延 = 发送时延 + 传播时延 + 处理时间 + 排队时延 我们一般只能提高发送时延， “光线比铜线快”指的是可以用很快的速率向光纤信道发送数据，光纤信道上的传播速率实际上比铜线慢 RTT 往返时间RTT(Round-Trip Time) 利用率 利用率： 信道利用率： 信道有百分之几的时间是被利用的 网络利用率：全网络的信道利用率的加权平均 $D = \\frac{D_0}{1-U}$ ( $D$: 网络当前时延， $D_0$:空闲时延， $U$: 利用率 ) 工具 tracerouter traceroute [参数选项] hostname，域名或 IP地址 向目的地发送几个特殊的分组 得到路由信息（路由器名、路由器地址、时延...） traceroute www.baidu.com  ✔  9s   base  traceroute to www.baidu.com (180.101.49.11), 30 hops max, 60 byte packets 1 _gateway (172.31.0.1) 2.660 ms 2.634 ms 2.625 ms 2 * * * 3 * * * 4 211.65.206.9 (211.65.206.9) 3.018 ms 3.196 ms 3.748 ms 5 * * * 6 * 211.65.207.73 (211.65.207.73) 4.098 ms 211.65.206.77 (211.65.206.77) 4.287 ms 7 101.4.116.98 (101.4.116.98) 3.681 ms 4.070 ms 3.939 ms 8 101.4.112.42 (101.4.112.42) 19.784 ms 20.044 ms 20.090 ms 9 202.97.19.13 (202.97.19.13) 13.871 ms 10.297 ms 10.219 ms10 202.97.60.217 (202.97.60.217) 10.447 ms 202.97.87.129 (202.97.87.129) 11.395 ms 202.97.62.77 (202.97.62.77) 10.188 ms11 202.97.66.198 (202.97.66.198) 16.129 ms 202.97.66.62 (202.97.66.62) 17.719 ms 202.97.29.114 (202.97.29.114) 15.325 ms12 58.213.94.106 (58.213.94.106) 23.529 ms 58.213.95.150 (58.213.95.150) 15.693 ms 58.213.94.98 (58.213.94.98) 15.408 ms13 * * *14 58.213.96.58 (58.213.96.58) 37.227 ms 58.213.96.126 (58.213.96.126) 40.246 ms 58.213.96.66 (58.213.96.66) 39.501 ms15 * * *16 * * *17 * * *18 * * * 记录按序列号从1开始，每个纪录就是一跳 ，每跳表示一个网关，我们看到每行有三个时间，单位是 ms，其实就是-q的默认参数。探测数据包向每个网关发送三个数据包后，网关响应后返回的时间；如果您用 traceroute -q 4 www.baidu.com ，表示向每个网关发送4个数据包。 有时会看到有一些行是以星号表示的。出现这样的情况，可能是防火墙封掉了ICMP的返回信息，所以我们得不到什么相关的数据包返回数据。 有时在某一网关处延时比较长，有可能是某台网关比较阻塞，也可能是物理设备本身的原因。当然如果某台DNS出现问题时，不能解析主机名、域名时，也会 有延时长的现象；您可以加-n 参数来避免DNS解析，以IP格式输出数据。 wireshark 分组嗅探器：观察执行实体之间交换的报文之间的基本工具","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"Computer Networking","slug":"Computer-Networking","permalink":"http://lyk-love.cn/tags/Computer-Networking/"}]},{"title":"OS Distributed System","slug":"OS-Distributed System","date":"2021-12-06T21:27:15.000Z","updated":"2022-09-26T06:39:34.935Z","comments":true,"path":"2021/12/07/OS-Distributed System/","link":"","permalink":"http://lyk-love.cn/2021/12/07/OS-Distributed%20System/","excerpt":"Outline: 通信 NFS AFS ref: Operating Systems Three Easy pieces","text":"Outline: 通信 NFS AFS ref: Operating Systems Three Easy pieces 通信 现代网络核心原则： 通信是不可靠的 有可靠的传输出协议进行消息传递 接下来的问题是： 使用什么样的抽象进行通信 通信抽象 DSM 分布式共享内存（ Distributed Shared Memory ）: 使不同机器上的同一个进程可以共享一个大的地址空间 用户访问页面时，要么页面在本机，要么发生页错误，页错误处理程序请求其他计算机，获取页面 属于操作系统抽象，给分布式系统提供内存抽象 DSM很少被采用 不够 Robunt：假如一台机器出现故障，整个地址空间就不可用了（ 比如一个链表的一个节点消失 ） 性能：跨计算机访问资源开销很大，DSM系统的设计者必须小心地组织计算，以便几乎不发生任何通信， 而这与DSM的初衷（跨计算机通信）违背* RPC DSM的失败表明： OS抽象对分布式系统来说是个糟糕的选择 PL抽象更有意义 最主要的抽象是远程过程调用（Remote Procedure Call） RPC的目标： 使在远程机器上执行代码的过程像调用本地函数那样直接 client： 进行过程调用 server： 只是定义了一些希望导出的例程。 其余的由RPC系统处理 RPC系统： 存根生成器(stub generator), 也称为协议编译器（ protocol compiler ） 运行时库（ run-time library ） Stub Generator Stub Generator 作为中介，与client和server交互 目的：消除将函数和参数打包成消息的复杂度 协议编译器的输入：server希望导出给client的一组调用： interface&#123; int func1(int arg1); int func2(int arg1, int arg2);&#125; stub generator接受这样的接口，并对 client 和 server 生成一些不同的代码片段（ stub ） client stub: client链接此client stub，调用它进行RPC,，客户端只能看到函数调用，在stub内部有如下操作： 创建 message buffer ： 一个字节数组 将所需信息打包到 message buffer， 也称为消息的序列化（serialization） 将消息发送到目标RPC server 与RPC server的通信，以及使其运行所需的细节都由RPC运行时库处理 等待回复 解包返回代码和其他参数（反序列化） server stub: 解包消息，即反序列化 调用实际函数 打包结果，放入一个回复缓冲区 发送回复 问题： 一个包如何发送复杂的数据结构：比如，如何解释一个指针 通过众所周知的类型（int之类） 使用更多信息注释数据结构 并发性服务器组织方式： 线程池 Run-time Library 运行时库处理RPC系统中大部分工作，这里讨论构建它时的一些困难 命名问题： 如何找到远程服务 采用现有的命名系统（ ip + port ） RPC传输协议的构建 为了效率，很多rpc采用UDP 如果建立在不可靠的传输层上，RPC需要提供可靠性，比如提供超时/重试 字节序： RPC包在其消息格式中指定字节序 是否向client暴露通信的异步性质，实现性能优化 NFS 分布式文件系统 由客户端文件系统 + 文件服务器组成 提供对文件的透明访问 Sun的NFS只是一个协议，允许不同的实现，这里讨论NFS v2 目的是“简单快速的服务器崩溃恢复” 无状态协议 NFS采用“无状态”协议: 每个client操作都包含完成请求所需的全部信息 有状态协议会在client和server间共享状态。 比如文件描述符，如果服务器发生错误，收到客户端第二次读取时将不知道fd指的是哪个文件 有状态协议很难处理客户端崩溃情况，如果client在崩溃前用掉了一个fd， server将无法知道什么之后收回该fd 重试请求：解决服务器没有及时回复的问题 幂等（idempotent） 操作： NFS的操作是幂等的，这就确保了它能简单地重试请求 除了READ, LOOKUP, 就连WRITE也是幂等的 WRITE包含写入数据的偏移量，实现了其幂等性 但是，有些操作无法幂等，比如mkdir，这也是NFS的bug 客户端缓存 客户端缓存可以提升性能，但会导致缓存一致性问题（ cache consistency problem ） 假设有客户端C1, C2, C3, 服务器S。 C1从S读取了文件F，并将副本存在本地缓存中， 而C2覆盖文件F，从未而改变其内容 问题一： C2将它的写入缓存一段时间再发送，而这之前如果C3访问F，得到的还是未改变的F，这称为update visibility client实现关闭时刷新 flush-on-close（即close-to-open）: 当一个client写入文件并关闭文件时，将其所有更新刷新到服务器，这样C3从服务器你拿到的确保是最新的版本 问题二： 即使C2立即发送其写入，C1的缓存依然是陈旧的（ stale cache ）, 此时C1上的程序只能读取陈旧的缓存 CLIENT在打开文件时，会先发送GETATTR请求到服务器，检查其是否被更改，如果是，则删除缓存并重新请求 GETATTR返回文件信息，其中包括服务器上次修改文件的信息 可以想见，GETATTR请求会非常频繁，为此NFS设计了“属性缓存”，允许文件属性缓存在本地，3s后超时。 但这就使得NFS无法知道文件的确切版本，“陈旧缓存”问题无法彻底解决 写缓冲 为了确保服务器写入正确， 服务I其必须在完成写入后，才通知客户端写入成功 带来写入性能的瓶颈 AFS 原则： 在client的本地磁盘进行全文件缓存 后续read(), write()操作是严格本地的，被重定向到本地磁盘上 当然，客户端每次操作前，会发送TestAuth给server, 查看文件是否被修改，这类似NFS的GETATTR AFS v2引入回调， server向client承诺，当client缓存的文件被修改时， 会通知client. 这减少了网络通信 AFS也采用关闭时刷新 最后关闭者胜出 last closer win: 最后一个更新该文件的client的内容会被接受 AFS是基于文件的，而NFS是基于块的。 对后者而言，如果每个客户端都更新文件，会将不同的块的写入刷新到服务器，这样的文件内容没什么意义 崩溃恢复 回调可能会丢失，比如该client缓存的文件被另一个client改变，服务器发送回调时，前者还在重启。 此时前者必须发送TestAuth来验证缓存正确性。 崩溃后的服务器恢复也更复杂。 因为回调被保存在server内存中。 因此server重启后，不知道server的情况。 因此，server重启后，每个client都必须意识到server已崩溃。 比如，server重启后向每个client发送消息“不要信任你的缓存！”","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"OS Basic","slug":"OS-Basic","permalink":"http://lyk-love.cn/tags/OS-Basic/"}]},{"title":"OS lab3","slug":"OS-lab3","date":"2021-12-02T23:45:12.000Z","updated":"2022-09-26T06:39:34.936Z","comments":true,"path":"2021/12/03/OS-lab3/","link":"","permalink":"http://lyk-love.cn/2021/12/03/OS-lab3/","excerpt":"Outline 问答 代码实现","text":"Outline 问答 代码实现 问答 当CPU执行了当前指令之后，在对下一条指令执行前，CPU先要判断在执行当前指令的过程中是否发生了中断或异常。如果发生了一个中断或异常，它将调用中断处理程序 解释中断向量 每个中断源对应一个向量。这些向量顺序存在主存储器的特定存储区（中断向量表） 向量的内容： 相应中断服务程序的起始地址 处理机状态字。 在响应中断时，由中断系统硬件提供向量地址，处理机根据该地址取得向量，并转入相应的中断服务程序 作用： 把中断/异常与相应的处理方法通过IDT对应起来 解释中断类型码 我们把每个中断服务程序进行编号，这个号就代表一个中断服务程序，就是中断类型码。这个中断类型码是计算机用来查找中断向量用的。 中断指令的一般格式为 “INT n”，其中，n被称为“中断类型码” 解释中断向量表 中断向量表是指中断服务程序入口地址的偏移量与段基值，一个中断向量占据4字节空间。中断向量表是8086系统内存中最低端1KB空间，它的作用就是按照中断类型号从小到大的顺序存储对应的中断向量，总共存储256个中断向量。 中断向量表在内存单元的最低处，地址空间为00000H----003FFH(0-1024B) 这个地址正好和中断类型码有一种对应的关系：中断类型码*4(一个中断向量所占的空间) 就等于这个中断向量的首地址。 实模式下中断程序地址如何得到? 得到中断向量地址 -&gt; 得到中断程序地址 中断类型码*4 == 中断向量的首地址。 在中断处理程序中，用中断返回指令IRET （interrupt return）指令使CPU返回主程序断点继续执行； 中断指令 “INT n” 和调用程序指令”CALL”很相似，它们均转入内存中其它程序段执行，执行完后再返回 INT指令 SP（Stack Pointer 堆栈指针）中的值减2，标志位寄存器的值入栈——保存中断前的状态 标志位TF和IF清0——关闭中断 IF=0 ，CPU不响应外部的可屏蔽中断请求；TF=0，则处于连续工作模式 SP减2，把返回地址的段值（CS）推入堆栈 SP减2，把返回地址的偏移量（IP）推入堆栈 根据中断类型码n，从中断向量表中取得中断处理程序地址，取得的段地址存入CS，偏移量 存入IP。从而使CPU转入中断处理程序运行。 IRET指令 从堆栈中取出一字（INT指令保存的返回地址偏移量），送给 IP，然后使SP加2 从堆栈中取出一字（INT指令保存的返回地址段值），送给 CS，然后使SP加2 从堆栈中取出一字（INT指令保存的标志寄存器的值），送给 标志寄存器，然后使SP加2 IRET执行后，CPU返回到INT指令后面的一条指令 其实同函数调用call和ret相类似，在调用时保存返回地址和标志位，但同时还会设置屏蔽请求。iret时则还原调用前状态。 保护模式下中断程序地址如何得到? 保护模式下的中断过程则较为复杂，它要借助中断门描述符来获取中断子程序这个目标段的描述符，也就是说必须经过两次查表才能获得中断服务子程序的入口地址 得到IDT IDT 80x86系列引入一个48位的全地址寄存器（即中断描述符表寄存器IDTR）存放IDT的内存地址，因此不再限于底部1K位置 和GDTR一样，IDTR包含32位的基地址和16位段限，基地址定义中断描述符表IDT在存储器中的起始点，段限定义中断描述符表所占的字节个数理论上IDT表同样可以有8K项，可是因为80x86只支持256个中断，因此IDT实际上最大只能有256项（2K大小） 因此只要查IDTR 寄存器CPU切换到保护模式之前，运行于实模式下的初始化程序必须使用LIDT指令装载中断描述符表IDT，将IDT基地址与段界值装入IDTR。如果不完成这一步操作，系统就会100%崩溃。在返回实模式或系统复位时，IDTR中自动装入000000H的基地址值与03FFH的段界值。可见实模式的中断向量表是固定在存储器的最底部，而保护模式下的IDT则是可以改变的 得到中断门 第一次查表 确定中断类型码i（在0～255之间） 用i * 8算出偏移量，读取IDT表第i项（或叫第i个门。进行有效性检查、特权级变化检查 当CPU执行了当前指令之后，在对下一条指令执行前，CPU先要判断在执行当前指令的过程中是否发生了中断或异常。如果发生了一个中断或异常，那么CPU将做以下事情： 确定所发生中断或异常的向量 通过IDTR寄存器找到IDT表，读取IDT表第i项（或叫第i个门）。进行有效性检查、特权级变化检查 查中断描述符表以IDTR指定的中断描述符表的基地址为起始地址，用调用号N×8算出偏移量，即为N号中断门描述符的首地址 根据中断门得到中断处理程序地址 第二次查表 由上述方法得到中断门，取中断门的8个字节 查GDT或LDT,根据中断门中的选择子（段选择符）和偏移量得到中断处理程序入口 中断向量的地址如何得到? 见“实模式下中断程序地址如何得到”， 实模式下如何根据中断向量的地址得到中断程序地址? 见“实模式下中断程序地址如何得到?” 解释中断描述符 保护模式下的中断处理与实模式下的中断处理最大区别在于寻找中断处理代码入口的方式 在保护模式下，为每一个中断和异常定义了一个中断描述符，来说明中断和异常服务程序的入口地址的属性 由IDT取代实地址模式下的中断向量表 中断描述符除了含有中断处理程序地址信息外，还包括许多属性和类型位 每个中断描述符占用连续的8个字节，中断描述符分为三类：任务门、中断门和自陷门，CPU对不同的门有不同的处理方式 ID的结构 低地址的0和1两个字节是中断代码的偏移量A15～A0； 高地址的6和7两个字节是中断代码的偏移量A31～A16； 2和3两个字节是段选择符，段选择符和偏移量用来形成中断服务子程序的入口地址； 4和5两个字节称为访问权限字节，它标识该中断描述符是否有效、服务程序的特权级和描述符的类型等信息； I. P（present）：表示中断描述符的有效性； II. DPL（descriptor privilege level）； III. TYPE：指示中断描述符的不同类型 中断的分类,举例不同类型的中断? 从中断源的角度分类 由计算机硬件异常或故障引起的中断，也称为内部异常中断。 由程序中执行了中断指令引起的中断，也称为软中断。由程序员通过INT或INT3指令触发，通常当做trap处理 用处：实现系统调用。 外部设备（如输入输出设备）请求引起的中断，也称为外部中断或Ｉ／Ｏ中断。 分类： 中断： 由CPU以外的事件引起的中断 如I/O中断、时钟中断、控制台中断等。 异常：来自CPU的内部事件或程序执行中的事件引起的过程。 如由于CPU本身故障、程序故障和请求系统服务的指令引起的中断等。 中断与异常的区别? 见“中断异常共同点(至少两点),不同点(至少三点)” 实模式和保护模式下的中断处理差别 见上文 如何识别键盘组合键(如 Shift+a)是否还有其他解决方案? 设置全局变量，每次按下就翻转其值 使用int caps记录是否要大写字符，若为true，则 column = 1， 即取keymap中第二列的值（都是大写值） 左右shift和Caps Lock都会翻转caps int caps = shift_l || shift_r; //如果shift被按下，开启大写if (caps_lock) &#123; if ((keyrow[0] &gt;= &#x27;a&#x27;) &amp;&amp; (keyrow[0] &lt;= &#x27;z&#x27;))&#123; caps = !caps; &#125; &#125;if (caps) &#123; column = 1; &#125;... IDT 是什么,有什么作用? 中断描述符表 存终端描述符 IDT 中有几种描述符? 中断描述符分为三类：任务门、中断门和自陷门 异常的分类? Fault，是一种可被更正的异常，而且一旦被更正，程序可以不失连续性地继续执行。返回地址是产生fault的指令。 Trap，一种在发生trap的指令执行之后立即被报告的异常，它也允许程序或任务不失连续性地继续执行。返回地址是产生trap的指令之后的那条指令。 Abort，不总是报告精确异常发生位置的异常，不允许程序或任务继续执行，而是用来报告严重错误的 用户态和内核态的特权级分别是多少? 当中断发生在用户态（特权级为3），而中断处理程序运行在内核态（特权级为0） 特权级发生变化，会引起堆栈的更换。也就是说，从用户堆栈切换到内核堆栈。 当中断发生在内核态时，即CPU在内核中运行时，则不会更换堆栈。 中断向量表中,每个中断有几个字节?里面的结构是什么? 起始地址：0 每个中断向量包含4 Bytes 低地址两个Byte放偏移 高地址两个Byte放段描述符 最多256个中断向量 中断异常共同点(至少两点),不同点(至少三点) 共同点 都是程序执行过程中的强制性转移，转移到相应的处理程序。 都是软件或者硬件发生了某种情形而通知处理器的行为 不同点 中断是CPU所具备的功能。通常因为“硬件”而随机发生。异常，是“软件”运行过程中的一种开发过程中没有考虑到的程序错误。 中断是CPU暂停当前工作，有计划地去处理其他的事情。中断的发生一般是可以预知的，处理的过程也是事先制定好的。处理中断时程序是正常运行的。 异常是CPU遇到了无法响应的工作，而后进入一种非正常状态。异常的出现表明程序有缺陷。 中断是异步的，异常是同步的。 中断是来自处理器外部的I/O设备的信号的结果，它不是由指令流中某条指令执行引起的，从这个意义上讲，它是异步的，是来自指令流之外的。 异常是执行当前指令流中的某条指令的结果，是来自指令流内部的，从这个意义上讲它们都是同步的。 中断或异常的返回点 良性的如中断和trap，只是在正常的工作流之外执行额外的操作，然后继续干没干完的活。因此处理程序完了后返回到原指令流的下一条指令，继续执行。 恶性的如fault和abort，对于可修复fault，由于是在上一条指令执行过程中发生（是由正在执行的指令引发的）的，在修复fault之后，会重新执行该指令；至于不可修复fault或abort，则不会再返回。 中断是由于当前程序无关的中断信号触发的，CPU对中断的响应是被动的，且与CPU模式无关。既可以发生在用户态，又可以发生在核心态。 异常是由CPU控制单元产生的，大部分异常发生在用户态 实验 这是我最难受的OS实验，中途遇到了很多bug,来无影去无踪，不知道怎么发生的，也不知道怎么解决的，莫名其妙地报错，过一会儿又好了QAQ 总的来说学到的东西很少 这里只给出代码逻辑，具体原因需要看Orange's第七章 实验环境：bochs 2.6.1, manjaro64 bochsrc: 更新：keyboard: keymap=/usr/share/bochs/keymaps/x11-pc-us.map 如果没有vgabios.bin的话，得自己下载一个。 qemu自带了该文件，我就使用了该路径vgaromimage: file=/usr/share/qemu/vgabios.bin makefile：实现make run ...BOCHS = bochsBOCHSFLAGS = -f bochsrc...run: image start_bochsstart_bochs: $(BOCHS) $(BOCHSFLAGS)... 实现逻辑 main.c里定义了若干 task和process： //main.cTASK* p_task = task_table;PROCESS* p_proc = proc_table; task_table中有四个task，其中tty_task最重要，负责tty显示. Test[ABC]定义在main.c中，稍后需要它们来实现清屏功能： //global.cPUBLIC TASK task_table[NR_TASKS] = &#123;&#123;task_tty, STACK_SIZE_TTY, &quot;tty&quot;&#125;, &#123;TestA, STACK_SIZE_TESTA, &quot;TestA&quot;&#125;, &#123;TestB, STACK_SIZE_TESTB, &quot;TestB&quot;&#125;, &#123;TestC, STACK_SIZE_TESTC, &quot;TestC&quot;&#125;&#125;; /*======================================================================* TestA： 清屏功能 *======================================================================*/void TestA()&#123; while (1) &#123; /* disp_str(&quot;A.&quot;); */ if( !SEARCH_MODE ) &#123; TTY *p_tty; for( p_tty = TTY_FIRST; p_tty &lt; TTY_END ; p_tty++ ) &#123; init_screen(p_tty); &#125; select_console(0); CLS(); //清屏 milli_delay(CLS_INTERVAL); //#define CLS_INTERVAL 30000 &#125; else&#123;// milli_delay(10); &#125; &#125;&#125;/*======================================================================* TestB *======================================================================*/void TestB()&#123; int i = 0x1000; while(1)&#123; /* disp_str(&quot;B.&quot;); */ milli_delay(10); &#125;&#125;/*======================================================================* TestB *======================================================================*/void TestC()&#123; int i = 0x2000; while(1)&#123; /* disp_str(&quot;C.&quot;); */ milli_delay(10); &#125;&#125; task_tty会初始化各个tty，并循环执行tty_do_read(p_tty) 和tty_do_write(p_tty), 这就是整个程序的输入/出入 //tty.c/*======================================================================* task_tty *======================================================================*/PUBLIC void task_tty()&#123; TTY* p_tty; init_keyboard(); for (p_tty=TTY_FIRST;p_tty&lt;TTY_END;p_tty++) &#123; init_tty(p_tty); &#125; select_console(0); while (1) &#123; for (p_tty=TTY_FIRST;p_tty&lt;TTY_END;p_tty++) &#123; tty_do_read(p_tty); tty_do_write(p_tty); &#125; &#125;&#125; tty_do_read(p_tty) 调用keyboard_read(p_tty)，从键盘读取输入; tty_do_write(p_tty)从缓冲区读取字符，令console打印： /*======================================================================* tty_do_read *======================================================================*/PRIVATE void tty_do_read(TTY* p_tty)&#123; if (is_current_console(p_tty-&gt;p_console)) &#123; keyboard_read(p_tty); &#125;&#125;/*======================================================================* tty_do_write *======================================================================*/PRIVATE void tty_do_write(TTY* p_tty)&#123; if (p_tty-&gt;inbuf_count) &#123; char ch = *(p_tty-&gt;p_inbuf_tail); p_tty-&gt;p_inbuf_tail++; if (p_tty-&gt;p_inbuf_tail == p_tty-&gt;in_buf + TTY_IN_BYTES) &#123; p_tty-&gt;p_inbuf_tail = p_tty-&gt;in_buf; &#125; p_tty-&gt;inbuf_count--; out_char(p_tty-&gt;p_console, ch); &#125;&#125; keyboard_read(p_tty)只负责读取字符，转化成key,并调用in_process(p_tty, key)处理读取的字符 ​ * //keyboard.cPUBLIC void keyboard_read(TTY* p_tty)&#123; ... in_process(p_tty, key); ...&#125; in_process(p_tty, key)真正处理输入，它调用put_key(p_tty, ** )将输入的字符存入缓冲区， 稍后由tty_do_write(p_tty)读取。在里面添加逻辑，实现特殊字符的读取： 这里只是读取，因此只存ascii码就行了 TAB： TAB的ascii为0x09 `` ESC： TAB的ascii为0x1B` 注意到这两个字符其实是可打印的，如TAB（ 0x09）会打印一个小点，不是我们期望的输出四个空格，因此输出的时候遇到这些字符需要特判，不能直接输出ascii //tty.cPUBLIC void in_process(TTY* p_tty, u32 key)&#123; ... case TAB: put_key(p_tty, 0x09 ); //TAB break; case ESC: put_key(p_tty, 0x1B); //ESC break; ...&#125; //tty.c/*======================================================================* put_key*======================================================================*/PRIVATE void put_key(TTY* p_tty, u32 key)&#123; if (p_tty-&gt;inbuf_count &lt; TTY_IN_BYTES) &#123; *(p_tty-&gt;p_inbuf_head) = key; p_tty-&gt;p_inbuf_head++; if (p_tty-&gt;p_inbuf_head == p_tty-&gt;in_buf + TTY_IN_BYTES) &#123; p_tty-&gt;p_inbuf_head = p_tty-&gt;in_buf; &#125; p_tty-&gt;inbuf_count++; &#125;&#125; 接下来是输出，tty_do_write(TTY***** p_tty)每次从缓冲区读取一个char，传给out_char(p_tty-&gt;p_console, ch)（在指定console）进行输出. 在后这种加逻辑，实现特殊字符的输出 TAB: 打印四个空格，前三个默认颜色DEFAULT_CHAR_COLOR（黑底白字）， 最后一个自定义颜色INVISIBLE_COLOR（黑底黑字）。 这样在显示上没有差别，而删除时遇到INVISIBLE_COLOR就连续删除四个空格 case &#x27; &#x27;: // TAB: 三个空格，跟一个0x09 if (p_con-&gt;cursor + 4 &lt;= p_con-&gt;original_addr + p_con-&gt;v_mem_limit - 1 ) &#123; for( int i=0; i &lt; 3; i++ ) &#123; *p_vmem++ = &#x27; &#x27;; *p_vmem++ = DEFAULT_CHAR_COLOR; p_con-&gt;cursor++; &#125; *p_vmem++ = &#x27; &#x27;; *p_vmem++ = INVISIBLE_COLOR; p_con-&gt;cursor++; &#125; break; \\b: 默认删除当前字符（将改字符变为空格，且颜色置为INVISIBLE_COLOR，光标右移一位）。 需要对TAB加入上述的特判 case &#x27;\\b&#x27;: if (p_con-&gt;cursor &gt; p_con-&gt;original_addr) &#123; if( *(p_vmem - 1) == INVISIBLE_COLOR ) //TAB &#123; for( int i = 0 ; i &lt; 4; i++ ) &#123; p_con-&gt;cursor--; *(p_vmem-2) = &#x27; &#x27;; *(p_vmem-1) = DEFAULT_CHAR_COLOR; p_vmem -= 2; &#125; &#125; else&#123; p_con-&gt;cursor--; *(p_vmem-2) = &#x27; &#x27;; *(p_vmem-1) = DEFAULT_CHAR_COLOR; &#125; &#125; break; ESC： 第一次按ESC进入查找模式，输入带匹配字符串; 第二次解除查找模式，其间按\\n需要进入匹配模式，期间忽略ESC之外的所有输入。 解除查找模式会删除之前输入的key_str,所有被匹配到的文本恢复白颜色, 光标回到进入SEARCH模式时的位置 case CHAR_ESC:// ESC SEARCH_MODE = ~ SEARCH_MODE; if( !SEARCH_MODE ) // 再按 Esc 键,之前输入的关键字被自动删除,所有文本恢复白颜色, 光标回到进入SEARCH模式时的位置 &#123; restore_matched_chars( p_con, ESC_CURSOR ); IGNORE_INPUT = 0;//恢复输入 &#125; else&#123;//进入SEARCH模式，记录此时光标位置 ESC_CURSOR = p_con -&gt; cursor; &#125; break; \\n: 在查找模式下，输入回车，会进入匹配模式，匹配到的字符串变为红色，并屏蔽ESC之外的输入 case &#x27;\\n&#x27;: if( SEARCH_MODE ) &#123; IGNORE_INPUT = 1; char key_str[50]; get_key_str( key_str,p_con, ESC_CURSOR );//得到key_str int len = p_con -&gt; cursor - ESC_CURSOR; match_chars( p_con, key_str, len ); &#125; else &#123; if (p_con-&gt;cursor &lt; p_con-&gt;original_addr + p_con-&gt;v_mem_limit - SCREEN_WIDTH) &#123; p_con-&gt;cursor = p_con-&gt;original_addr + SCREEN_WIDTH * ((p_con-&gt;cursor - p_con-&gt;original_addr) / SCREEN_WIDTH + 1); &#125; &#125; break; 采用三个全局变量: ESC_CURSOR标记第一次ESC对应的光标位置，当第二次输入ESC时解除查找模式，这之间的内容就是带匹配字符串key_str。 后续用key_str作为滑动窗口，从屏幕起始位置开始匹配 SEARCH_MODE： 表示是否处于查找模式，如果为true，不仅意味着输入的是key_str，还意味着期间不能被清屏 IGNORE_INPUT： 只有处于查找模式且按下回车时置为true，期间不响应ESC之外的所有输入 重要参数解释: p_con-&gt;cursor: 光标位置，在最后一个字符之后 u8* p_vmem = (u8*)(V_MEM_BASE + p_con-&gt;cursor * 2): 当前光标所指向的显存位置，由于小端存储，且字符占2Byte（ 因此有 cursor * 2）。 一个字符的低1Byte存字符值，高1Byte存字符颜色 操作字符颜色：*(p_vmem-1) = DEFAULT_CHAR_COLOR; 操作字符值：*(p_vmem- 2) =' ' 一些工具函数： /*======================================================================* get_key_str *----------------------------------------------------------------------* 得到key_str *----------------------------------------------------------------------* 从esc_cursor后一位开始（避免把esc读进去），直到当前cursor*======================================================================*/PRIVATE void get_key_str( char* key_str, CONSOLE* p_con ,unsigned int esc_cursor )&#123; u8* p_vmem; for( unsigned int tmp_cursor = esc_cursor+1, i = 0 ; tmp_cursor &lt;= p_con -&gt; cursor ; tmp_cursor++ )//从 &#123; p_vmem = (u8*)(V_MEM_BASE + tmp_cursor * 2); //指向reverse_cursor指向的位置 key_str[i++] = *(p_vmem-2); &#125;&#125;/*======================================================================* match_chars 匹配字符串并染色 从光标初始位置，到最后一个ESC的光标位置，对该范围内所有字符进行匹配*======================================================================*/PRIVATE void match_chars(CONSOLE* p_con, char* key_str, int len )&#123; for( unsigned int temp_cursor = p_con -&gt; original_addr + 1; temp_cursor &lt; p_con -&gt; cursor - len ; temp_cursor++ ) &#123; if( isMatch( temp_cursor, key_str, len ) ) //匹配成功，进行染色 &#123; change_chars_color_sequenced( temp_cursor, len, MATCHED_CHAR_COLOR ); &#125; &#125;&#125;/*======================================================================* isMatch *----------------------------------------------------------------------* 从指定cursor前一位开始，匹配key_str *----------------------------------------------------------------------* *======================================================================*/PRIVATE int isMatch( unsigned int cursor, char* key_str, int len )&#123; // key_str = &quot;q&quot;; // len = 1; int res = 1; u8* p_vmem = (u8*)(V_MEM_BASE + cursor * 2); for( int i = 0 ; i &lt; len; i++, p_vmem+=2 ) &#123; if( *(p_vmem - 2) != key_str[i] ) &#123; res = 0; break; &#125; &#125; return res; &#125;/*======================================================================* delete_chars*======================================================================*/PRIVATE void delete_chars(CONSOLE* p_con, unsigned int len)&#123; u8* p_vmem; unsigned int temp_cursor = p_con -&gt; cursor; for( ; temp_cursor &gt; p_con -&gt; cursor - len ; temp_cursor-- ) &#123; p_vmem = (u8*)(V_MEM_BASE + temp_cursor * 2); *(p_vmem - 1) = DEFAULT_CHAR_COLOR; *(p_vmem - 2) = &#x27; &#x27;; &#125; p_con -&gt; cursor = temp_cursor; //移动指针&#125;/*======================================================================* change_chars_color( CONSOLE* p_con, unsigned int len, unsigned int color )*======================================================================*/PRIVATE void change_chars_color( CONSOLE* p_con, unsigned int len, u8 color )&#123; u8* p_vmem; for( unsigned int temp_cursor = p_con -&gt; cursor ; temp_cursor &gt; p_con -&gt; cursor - len ; temp_cursor-- ) &#123; p_vmem = (u8*)(V_MEM_BASE + temp_cursor * 2); *(p_vmem - 1) = DEFAULT_CHAR_COLOR; &#125; &#125;/*======================================================================* change_chars_color_sequenced( unsigned int cursor, unsigned int len, unsigned int color )*======================================================================*/PRIVATE void change_chars_color_sequenced( unsigned int cursor, unsigned int len, u8 color )&#123; u8* p_vmem; for( unsigned int temp_cursor = cursor ; temp_cursor &lt; cursor + len ; temp_cursor++ ) &#123; p_vmem = (u8*)(V_MEM_BASE + temp_cursor * 2); *(p_vmem -1 ) = color; &#125; &#125;/*======================================================================* restore_matched_chars*======================================================================*/PRIVATE void restore_matched_chars(CONSOLE* p_con, unsigned int esc_cursor)&#123; delete_chars( p_con, p_con -&gt; cursor - esc_cursor ); // 删除待匹配字符 change_chars_color( p_con, p_con -&gt; cursor - p_con -&gt; original_addr , DEFAULT_CHAR_COLOR ); //恢复匹配到的字符的颜色&#125; 输入输出已经实现了，最后是清屏功能，由于task_table中的四个task会被四个process执行 ，只需要由一个task负责清屏（） //main.c#define TTY_FIRST (tty_table)#define TTY_END (tty_table + NR_CONSOLES)#define CLS_INTERVAL 30000... void CLS()&#123; disp_pos = 0; for( int i = 0 ; i &lt; 80*25; i++ ) &#123; disp_str(&quot; &quot;); &#125; disp_pos = 0;&#125;/*======================================================================* TestA *======================================================================*/void TestA()&#123; while (1) &#123; /* disp_str(&quot;A.&quot;); */ if( !SEARCH_MODE ) &#123; TTY *p_tty; for( p_tty = TTY_FIRST; p_tty &lt; TTY_END ; p_tty++ ) //这段抄书的，我也不明白意思 &#123; init_screen(p_tty); &#125; select_console(0); CLS(); milli_delay(CLS_INTERVAL); &#125; else&#123;// milli_delay(10); &#125; &#125;&#125;","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"OS Basic","slug":"OS-Basic","permalink":"http://lyk-love.cn/tags/OS-Basic/"}]},{"title":"OS VMM","slug":"OS-VMM","date":"2021-11-26T21:12:17.000Z","updated":"2022-09-26T06:39:34.936Z","comments":true,"path":"2021/11/27/OS-VMM/","link":"","permalink":"http://lyk-love.cn/2021/11/27/OS-VMM/","excerpt":"Outline: 虚拟化CPU 虚拟化内存 总结 ref: Operating Systems Three Easy pieces","text":"Outline: 虚拟化CPU 虚拟化内存 总结 ref: Operating Systems Three Easy pieces 虚拟化CPU VMM采用limited directed execction来运行虚拟机 即： VMM跳转到第一条指令的地址，并让OS开始运行 多个虚拟机在一个CPU上multiplexing，需要执行machine switch，这类似进程的context switch machine switch流程：VMM必须 保存整个OS的机器状态（ incluing registers, PC, 并且和上下文切换不同， VMM还要保存所有的特权硬件的状态 ） 恢复待运行OS的机器状态 然后跳转到待运行OS的PC 特权指令直接控制机器，因此VMM不允许OS直接执行特权指令。事实上VMM会先接受系统调用，然后将该系统调用转发给OS，后者返回后前者也随之返回。 当user program发出系统调用时，由于VMM控制了机器，因此VMM会陷入kernel mode VMM跳转到OS的trap handler，并让OS处理该system call VMM知道OS的trap handler的位置，因为OS启动时会通过特权指令来安装自己的trap handler，因此会陷入VMM,此时VMM会记录该trap handler在内存中的位置 OS完成后会通过return返回,这也是system call, 因此VMM真正从trap返回 return: 在X86上是iret 由于guest OS不可以无限制地访问硬件，因此它不能处于kernel mode。 一般而言，guest OS会处于某种特权更小的kernel mode 执行系统调用： traditional 进程： 发出system call，陷入OS 硬件： 切换到kernel mode,跳转到trap handler 如前所述，OS启动时，会利用特权指令安装trap handler，因此硬件知道trap handler的地址 OS： 在kernel mode处理system call, 从trap返回 硬件：切换到user mode 进程：继续执行 执行系统调用：with Virtualization 进程： 发出system call，陷入OS VMM：事实上进程陷入的是VMM, VMM将该system call转发给guest OS的trap handler OS： 在kernel mode处理system call, 从trap返回（调用特权指令） VMM：知道OS尝试返回，VMM真正从trap返回 进程：继续执行 虚拟化内存 OS将物理内存抽象为地址空间，使得每个进程有自己私有地址空间的假象 OS 页表： VPN to PFN VMM将机器内存抽象为物理内存，使OS有自己完全掌握机器内存的假象 即： VMM管理OS的物理到机器内存映射 VMM页表：PFN to MFN（ 机器帧号 ） 硬件TLB： 当TLB未命中时，OS必须参与处理未命中 TLB未命中流程： traditional OS TLB和VMM TLB都是TLB的软件管理程序，这里所指的TLB是硬件TLB 进程： 从内存加载。 TLB未命中。 TRAP OS：OS TLB 未命中处理程序： 从VA提取VPN 查找页表 如果存在且有效，则取得PFN,更新TLB 从TRAP返回 进程： 继续执行，返回到导致trap的PC. 指令重试，导致TLB命中 TLB未命中流程： with Virtualization 由于VMM是机器的所有者，因此TLB未命中时， VMM会TRAP，它会立即跳转到OS TLB未命中处理程序。 OS TLB查找页表，并尝试在TLB中安装VPN - PFN映射。 这也是一种特权指令，导致又一次TRAPVMM. 此时，VMM给TLB事实上安装 VPM - MFN 映射（ 而不是 VPN - PFN映射） 进程： 从内存加载。 TLB未命中。 TRAP VMM ： ：事实上进程陷入的是VMM,。 VMM TLB未命中处理程序调用OS TLB 未命中处理程序 OS：OS TLB 未命中处理程序： 从VA提取VPN 查找页表 如果存在且有效，则取得PFN,更新TLB，这是特权操作，导致TRAP VMM：OS TLB陷入VMM, VMM用 VPM - MFN映射来更新TLB。 跳回OS OS： 从TRAP返回 VMM：看到OS返回的特权指令， VMM真正返回 进程： 继续执行，返回到导致trap的PC. 指令重试，导致TLB命中 硬件管理的TLB 之前讲的都是软件管理的TLB，之前文章提到，也有硬件管理的TLB 硬件管理TLB时，硬件在TLB miss时遍历页表并更新TLB ， VMM没有机会参与其中，因此无法在此时建立VPM - MFN映射 作为替代，VMM必须监视OS对每个页表的更改，并保留一个影子页表shadow page table. 它维护进程的虚拟地址到机器地址的映射。 每当os尝试安装进程的OS级页表时，VMM就会安装进程的影子页表，然后硬件将虚拟地址转换为机器地址，OS注意不到上述替换的发生 也就是：OS页表里面存的是VPN - MFN映射。 硬件根据这个映射去物理内存（事实上是机器内存）寻找。 之前软件管理TLB时， VPN - MFN映射存在TLB上， 硬件根据这个映射去物理内存（事实上是机器内存）寻找。 总结 VMM的关键是扩展limited directed execction概念，让VMM能够介入TRAP VMM能够完全控制机器资源的分配方式 之前《现代操作系统》提到，VMM事实上将物理资源直接分配给内存，没有任何抽象。这话也没错。 在内存虚拟化层面，VMM偷偷把物理地址替换为了机器地址，OS确实在对物理硬件进行操作（他只是不知道这个映射; CPU虚拟化层面也类似。 相比OS将物理硬件抽象为文件系统等漂亮的API, VMM仅仅将硬件资源抽象为虚拟的硬件资源， 这两种资源使用上是一样的，因此“不是那么抽象”","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"OS Basic","slug":"OS-Basic","permalink":"http://lyk-love.cn/tags/OS-Basic/"}]},{"title":"商业模式战略","slug":"商业模式战略","date":"2021-11-15T11:54:22.000Z","updated":"2022-09-26T06:39:34.943Z","comments":true,"path":"2021/11/15/商业模式战略/","link":"","permalink":"http://lyk-love.cn/2021/11/15/%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F%E6%88%98%E7%95%A5/","excerpt":"Outline: 商业模式环境 商业模式评估 从商业模式的视角看蓝海战略","text":"Outline: 商业模式环境 商业模式评估 从商业模式的视角看蓝海战略 商业模式环境 市场影响力 市场问题 影响客户环境的关键因素是什么？现在正在发生什么转变？市场在朝什么方向发展？ 市场分类 哪块客户细分群体最为重要？最大的增长潜力在哪里？哪个细分市场在萎缩？哪个边缘细分市场值得关注？ 需求和诉求 客户需要什么？在客户需求中，哪些没有得到满足，最大的缝隙在哪里？客户最渴望满足的需求是什么？哪些需求在增长？哪些在降低？ 切换成本 联系客户和公司及其产品或服务的纽带是什么？阻止客户投靠竞争对手的转移成本是什么？客户找到和购买相似产品或服务的难度大吗？品牌的作用大吗？ 收入影响力 让客户真正愿意掏腰包的是什么产品或服务？什么产品或服务能获得最大的收益率？客户能轻而易举地发现和购买更为便宜的产品和服务吗？ 关键趋势 技术趋势 在你的行业市场内外，主要的技术趋势是什么？哪种技术代表着重要市场机会或扰乱市场的危险？市场的客户正在采用哪种新显现的技术？ 行业管理趋势 哪种监管法规趋势影响你的商业模式？什么视则可能会影响你公司的商业模式？哪种法规和税收制度会影响客户端的需求？ 社会和文化趋势 描述关键的社会趋势。在文化和社会价值观中，哪种转变影响着你的商业模式？哪种趋势可能会影响消费者行为？ 社会经济趋势 重要的人口趋势是什么？你会如何描述你市场里的收入和财富分配？可支配收入有多高？描绘出你所在的市场的消费形式（如房产、医疗、娱乐等）。城市人口和农村人口的数比例关系是怎样的？ 行业影响力 主流竞争对手 谁是我们的竞争对手？在我们所处的行业里，主导游戏规则的公司是哪一个？它们的竞争优势和劣势分别是什么？描绘出它们的主要产品和服务。它们关注重点在哪一个客户细分群体？它们的成本结构怎么样？它们对我们的客户细分群体、收入来源和利润率能产生多大的影响？ 挑战者（相对于传统电商） 谁是你所在市场的新进入者？它们有什么不同？它们分别有什么竞争优势和劣势？它们必须克服哪些市场准入壁垒？它们的价值主张是什么？它们专注于哪一块客户细分市场？它们的成本结构怎么样？它们对我们的客户细分群体、收入来源和利润率能产生多大的影响？ 替代产品和服务 哪些产品和服务可以替代我们的？和我们的产品和服务的成本相比，它们的怎么样？客户转移到这些替代品有多容易？这些替代品和服务来源于什么样的传统商业模式？（例如高铁相比于飞机，手机相比于相机，Skype相比于长途电话服务公司。） 供应商与价值链上的其他厂商 在你所在行业的价值链上，谁是关键参与者？在多大的程度上，你的商业模式依存于其他的参与者？行业中的边缘参与者有可能崛起吗？哪个参与者的收益最高？ 利益相关者 哪些利益相关者可能会影响你的商业模式？利益相关者的影响力有多大？它们是工人、政府还是游说集团？ 宏观经济影响 全球市场情况 经济发展处在蓬勃发展期还是萧条衰败期？描绘出市场的整体气氛。GDP的增速是多少？失业率有多高？ 资本市场 资本市场的情况怎么样？你所处的行业融资容易吗？原始资本、风险投资、公开募资、市场资本或是信贷在你的行业一应俱全吗？获取融资的或本高吗？ 大宗商品和其他资源 描述对你的业务至关重要的商品和其他资源市场的当前情况（例如油价和人力成本）。获取你的商业模式运作所需要的相关资源容易吗（如吸引核心人才）？它们的成本高吗？价格的变化趋势是怎样的？ 经济基础设施 你所在的市场公共基础设施怎么样？你会如何描述交通、贸易、教育质量和接触供应商和客户的状况？个人和公司的所得税有多高？针对企业的公共服务怎样？你会如何评价你的生活质量？ 商业模式评估 变化环境下商业模式的演进 市场影响力、行业影响力、关键趋势和宏观经济影响这四个方面的分析为商业模式创新提供了设计空间 使用模型构建来展开设计（竞品！） 通过场景进行对未来的探索（聚焦！） 定期评估商业模式 商业模式环境-由外到内的影响; 评估商业模式-由内到外的分析 两种评估类型： 某商业模式的总体评估，以及相应的未来战略 商业模式优势、劣势、机会和威胁（Strength, Weakness, Opportunity, Threat, SWOT）的检查清单（Checklist） 对商业模式每个模块做SWOT评估 画布的存在帮助聚焦SWOT分析，避免模糊，实现聚焦 按照**价值主张、成本/收入、基础设施（KR+KA+KP）、客户界面（CS+CH+CR）**四类展开评估 SWOT SW 价值主张 网络效应：各项价值主张之间相互联系，相互促进 B站：基于兴趣和一致价值观的内容共享与社交，专注与热爱连接内容和社交，规模极大的审核部门与风纪组，小破站与晋元帝 产品与服务的强耦合： 以产品为主的，服务能否有效支持产品传递和售后 以服务为主的，产品和环境能否满足服务的需要 成本/收入 会员充值、年卡、连续包月、餐饮日化快消、信息类产品与服务 订单竞标、奢侈品（“轻奢”利润率反而更高）、套利对冲交易 零售平台或电商平台拖账期： 我们在支出成本之前就有收入进账 三类价格歧视: 我们的定价机制能够抓住客户全部的购买意愿 基础设施 模式独特=&gt;客户忠诚=&gt;技术壁垒： 竞争对手很难复制我们的核心资源 产品周期性、供应链控制: 资源的需求可以预测 核心业务与其它业务，是否达到印钞机模式，动态调整自有与外包的比例： 自有活动和外包活动达到了理想的平衡 客户界面 客户细分 客户忠诚度、客户分类（洞察）、持续获客与获客成本 渠道通路 效率、效果、连接能力、易于接触、是否整合、规模经济、匹配 网易严选在渠道的了解与评估部分取得了正面的效果（品牌构建与形象宣传），但在传递与售后的部分取得了较为负面的效果（ems-顺丰，产品品控与SPU、SKU的矛盾） 客户关系 品牌、匹配、切换成本 网易严选与拼多多的情感诉求-高性价比与便宜好玩 强渠道或服务难以替代 O 价值主张中的机会（整合、服务化与拓展） VP：产品与服务能否整合，产品能否服务化？价值主张的补充和外延？满足客户的额外需求或其它可做的工作？（共享单车、安卓与B站漫画、海尔能洗红薯的洗衣机） 成本/收入中的机会（可重复、交叉销售、开源节流） R$：重复性收入代替一次性收入、寻找额外买单元素与交叉销售的机会、新的收益来源、能否提价（会员自动续费、套餐与B站影视、瑞幸与共享单车的涨价） 交叉销售，通过客户关系管理发现现有顾客的多种需求，并通过满足其需求而销售多种相关服务或产品的一种新兴营销方式 C$：成本削减 基础设施中的机会（强化核心、减轻负担、转让闲置） KR：核心资源的降本、外包、强化、转让（降本增效、技术壁垒、技术转让） KA：标准化、IT技术带来的整体效率提升（海尔设计团队的微服务化，实体产业的互联网化） KP：外包与核心业务聚焦、交叉销售与更好的客户连接、价值主张补充（联名款） 客户界面的机会（增长的市场、客户细分、渠道优化与去中间商，客户关系加强与取舍） CS：找到增长的市场并从中获利、服务新客户群体或更细致的已有客户分类（社交类产品） CH：渠道的效率、效益、整合，补充性的渠道伙伴，去中间商、渠道客户匹配（品牌） CR：加强与客户的关系并提升客户跟进的效果（华为19年近20%的盈利增长）、定制化或可自动维护、提升切换成本（全家桶）、是否抛弃没有利润的客户以及原因（发掘潜力或果断抛弃） T 对价值主张的威胁（可替代性） 产品是否可替代？ 是否会被更有竞争力的价格或更好的价值取代？ 对成本/收入的威胁（利润的威胁、是否单一、缩水、无法预测、无法支撑） 受威胁的利润？是否是技术原因导致？（PDD对淘宝、JD的威胁） 是否过度依赖某一项或多项收益来源？（九城代理魔兽世界） 未来可能消失（或缩水）的收益来源？（归属宝洁的吉列减值） 是否有无法预测的成本？（对宏观经济形势的依赖，供应链的稳定性，负面事件的影响与公关） 哪些成本的增加会快过它们所支撑的收入？（瑞幸的无人售卖战略） 对基础设施的威胁（供应不足、干扰、合作关系波动） KR：某些资源的供应短缺？资源的质量是否有保证？（网易云音乐的版权问题） KA：哪些关键业务会被打扰（A站关停） ？我们的活动质量能否保证（产品、渠道）？ KP：可能失去的合作伙伴？是否会跟竞争对手合作？是否过分依赖某些合作伙伴？（阿里失手O2O：自废武功+问题频出+养蛊反噬+点评被美团收购） 客户界面上的威胁（市场竞争、渠道威胁、客户关系恶化） CS：市场是否很快饱和？市场份额被友商威胁？客户转投的可能性？竞争白热化的速度？（千播大战） CH：竞争对手是否威胁渠道？（恶意举报）是否存在渠道与客户不相关的危险？ CR：我们的客户关系有可能恶化吗（产品质量与特性无法支持品牌构建）？ 国内各大电商平台的优势与发展动向 成熟电商平台包含：网站+物流仓储+供应链+金融服务 阿里：商业网络第一，新价值主张 优势：个人信用与金融平台，技术实力，集团内部高度整合，总部杭州 最新动向：阿里本地生活服务 京东：基于物流拓宽新的价值主张与客户细分组合 优势：一手打造的物流网络，总部北京 最新动向：京东物流外部收入占40%，营收与利润大涨（净收入2019增长24.9%），“买光湖北货” 苏宁：强化线下渠道与资源优势，提升客户关系覆盖度 优势：线下仓储+供应链（半日达），实体产业集群，良好的地方政企关系，总部南京 最新动向：收购万达商城、家乐福，整合线下便利店与Outlets（线下获客成本开始小于线上） 拼多多：基于社交拓宽新的价值主张与客户细分组合 优势：用户心理把握，社交引流：货找人，拼购模式引领者，轻资产（物流？），总部上海 最新动向：开团电子类产品（“开车成功就真香”）， Disney + Costco：“多有趣，多实惠” 唯品会：回归服饰特卖，拓展线下渠道，拓宽客户关系 优势：专注品牌（少量大牌+大量小品牌）特卖，09-16年吃到了服装产业库存红利，盈利能力强（小品牌佣金30%，连续29个季度盈利），CEO温州人，总部广州 最新动向：布局线下特卖与渠道建设，回归垂直电商（服饰类），剥离自有物流并与顺丰合作 网易严选：维护客户关系（情怀） 特点：ODM引领者，没有中间商赚差价（PDD：？），降低消费决策成本，总部广州 难点：重资产与体量小的悖论，淘宝、京东、小米的围剿，定价困难，没有品牌认同，品控风险，成长慢 最新动向：请罗老师代言旗下转椅；网易剥离电商，聚焦游戏、音乐、教育，得到资本市场认同；考拉20亿美元卖了，严选没人要 蓝海战略 通过价值创新来开辟全新的没有竞争的市场空间，而不是模仿现有商业模式在当前行业中竞争。所谓“价值创新”就是“在提升价值的同时降低成本”。 为了取得价值上的创新，推荐使用“四项行动架构”这一分析工具，该工具用四个重要问题来检查一个行业的战略逻辑和主流商业模式，以探寻价值创新、减少成本。 四项行动架构 整合商业画布 商业模式右半部关注价值、聚焦客户，左半部分关注成本和基础设施。右侧的改变会对左半部分产生影响 蓝海战略强调在增加价值的同时减少成本，通过删除和消减低价值产品或服务来降低成本，通过提升和创造对成本影响弱的高价值功能或服务来实现 二者的整合使得使用“四项行动架构”分析时能够更好地识别这些行动对商业模式其它模块的影响 过程 对于基础设施， 价值主张， 客户界面三个模块， 每个模块采用四项行动架构 基础设施: 哪些活动、资源和合作伙伴关系的成本最高？ 如果消减或删除这些成本项，会发生什么？ 在删减或消除代价高昂的KR、KA或KP后，如何利用低成本的元素来代替它们创造价值 拼多多：复用社交网络实现小规模的货找人 价值主张: 哪些低价值的功能或者服务可以被删除或消减？ 可以通过新增或加强哪些功能或服务来产生有价值的客户新体验？ 价值主张的改变对成本有何影响？ 价值主张的改变将如何改变商业模式客户侧的内容？ 阿里：从淘宝到天猫-高品质带来的溢价、供应商加盟费与金融服务费 客户界面: 你可以聚焦哪些新的客户群体，哪些客户群体可以消减或删除？ 新的客户群体真正希望你帮他们完成哪些工作？ 这些客户倾向于何种联络方式，他们期望与你建立何种关系？ 服务新的客户群体对成本有何影响？ 西瓜视频：下沉市场的长视频、社交化收视体验（B站：？） 案例 太阳马戏团","categories":[{"name":"Business","slug":"Business","permalink":"http://lyk-love.cn/categories/Business/"}],"tags":[{"name":"需求与商业模式创新","slug":"需求与商业模式创新","permalink":"http://lyk-love.cn/tags/%E9%9C%80%E6%B1%82%E4%B8%8E%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F%E5%88%9B%E6%96%B0/"}]},{"title":"OS lab2","slug":"OS-lab2","date":"2021-11-11T19:42:50.000Z","updated":"2022-09-26T06:39:34.936Z","comments":true,"path":"2021/11/12/OS-lab2/","link":"","permalink":"http://lyk-love.cn/2021/11/12/OS-lab2/","excerpt":"Outline: PPT内容 实验内容（ 附我的部分代码 ）","text":"Outline: PPT内容 实验内容（ 附我的部分代码 ） PPT内容 什么是实模式,什么是保护模式? 实模式就是用基地址加偏移量就可以直接拿到物理地址的模式 缺点:实模式非常不安全 保护模式就是不能直接拿到物理地址的模式 需要进行地址转换 从80386开始,是现代操作系统的主要模式 什么是选择子? 选择子共16位,放在段选择寄存器里 低2位表示请求特权级 第3位表示选择GDT方式还是LDT方式 高13位表示在描述符表中的偏移(故描述符表的项数最多是2^13) 什么是描述符? 描述符表中存储的一个元素,是一个指针 什么是GDT,什么是LDT? GDT:全局描述符表,是全局唯一的。存放一些公用的描述符和包含各进程LDT首地址的描述符。 LDT:局部描述符表,每个进程都可以有一个。存放本进程内使用的描述符。 请分别说明GDTR和LDTR的结构 GDTR:48位寄存器,高32位放置GDT首地址,低16位放置GDT 限长(限长决定了可寻址的大小,注意低16位放的不是选择子) LDTR:16位寄存器,放置一个特殊的选择子,用于查找当前进 程的LDT首地址。 请说明GDT直接查找物理地址的具体步骤 给出段选择子(放在段选择寄存器里)+偏移量 若选择了GDT方式,则从GDTR获取GDT首地址,用段选择 子中的13位做偏移,拿到GDT中的描述符 如果合法且有权限,用描述符中的段首地址加上(1)中的偏移量找到物理地址。寻址结束。 请说明通过LDT查找物理地址的具体步骤。 给出段选择子(放在段选择寄存器中)+偏移量 若选择了LDT方式,则从GDTR获取GDT首地址,用LDTR中的偏移量（ LDTR内部的选择子 ）做偏移,拿到GDT中的描述符1， 它的内容是LDT的首地址 从描述符1中获取LDT首地址,用段选择子中的13位做偏移,拿到LDT中的描述符2 如果合法且有权限,用描述符2中的段首地址加上(1)中的偏移量找到物理地址。寻址结束。 根目录区大小一定么?扇区号是多少?为什么? 不一定， 目录区大小为(RootEntCnt * 32 + BytsPerSec - 1) / BytsPerSec， 即 取决于BPB中的RootEntCnt 扇区号为RsvdSecCnt + NumFATs * FATSz， 即 引导区的扇区数 + fat表数*每个fat表所占扇区数 数据区第一个簇号是多少?为什么? 在1.44M软盘上，FAT前三个字节的值是固定的0xF0、0xFF、0xFF，用于表示这是一个应用在1.44M软盘上的FAT12文件系统。本来序号为0和1的FAT表项应该对应于簇0和簇1，但是由于这两个表项被设置成了固定值，簇0和簇1就没有存在的意义了，所以数据区就起始于簇2 FAT表的作用? FAT项的值代表文件的下一个簇号 解释静态链接的过程 静态链接是指在编译阶段直接把静态库加入到可执行文件中去 空间和地址分配 ; 符号解析和重定位 解释动态链接的过程 动态链接器自举 动态链接器本身也是一个不依赖其他共享对象的共享对象，需要完成自举。 装载共享对象 将可执行文件和链接器自身的符号合并成为全局符号表，开始寻找依赖对象。加载对象的过程可以看做图的遍历过程；新的共享对象加载进来后，其符号将合并入全局符号表；加载完毕后，全局符号表将包含进程动态链接所需全部符号。 重定位和初始化 链接器遍历可执行文件和共享对象的重定位表，将它们GOT/PLT中每个需要重定位的位置进行修正。完成重定位后，链接器执行.init段的代码，进行共享对象特有的初始化过程（例如C++里全局对象的构造函数）。 转交控制权 完成所有工作，将控制权转交给程序的入口开始执行。 静态链接相关PPT中为什么使用ld链接而不是gcc gcc默认动态链接， 当然也可以指定-static使用静态链接 ld也相同（ 所以这二者都可以 ） linux下可执行文件的虚拟地址空间默认从哪里开始分配 事实上，入口地址的选择取决于链接器 The script define the following variables used by the loader ld: # TEXT_START_ADDR - the first byte of the text segment, after any# headers.# TEXT_BASE_ADDRESS - the first byte of the text segment.# TEXT_START_SYMBOLS - symbols that appear at the start of the# .text section. For example, on GNU/Linux, /usr/lib/ldscripts/elf_x86_64.x 或者ld --verbose |grep SEGMENT_START we see: ...PROVIDE (__executable_start = SEGMENT_START(&quot;text-segment&quot;, 0x400000)); \\ . = SEGMENT_START(&quot;text-segment&quot;, 0x400000) + SIZEOF_HEADERS; The text-segment mapping values are: 0x08048000 on 32 Bits 0x400000 on 64 Bits You can find out more about linker scripts by browsing the linker manual: info ld Scripts 你也可以手动指定入口地址，比如： gcc -Wl,-Ttext-segment=0x800000 hello_world.c which sets the entry point to 0x800000 (+ the ELF header size, which gets loaded at 0x800000 in memory) instead of the default 0x400000. 证据：写一个汇编文件a.asm main: ret nasm -f elf64 a.asm ld a.o readelf -l a.out 可以看到： Elf file type is EXEC (Executable file)Entry point 0x401000There are 2 program headers, starting at offset 64Program Headers: Type Offset VirtAddr PhysAddr FileSiz MemSiz Flags Align LOAD 0x0000000000000000 0x0000000000400000 0x0000000000400000 0x00000000000000b0 0x00000000000000b0 R 0x1000 LOAD 0x0000000000001000 0x0000000000401000 0x0000000000401000 0x0000000000000001 0x0000000000000001 R E 0x1000 Section to Segment mapping: Segment Sections... 00 01 .text The first (lowest) LOAD segment's virtual address is the default load base of the file. You can see it's 0x0000000000400000 for this file. 实验相关内容 思路 用OO方式，构建文件系统树， 对文件系统的所有操作转换为对文件系统树的操作 对指令的解析采用工厂模式 缺点是没有充分重构， 比如加入异常机制和lamda表达式，太懒了QAQ 并且，这个思路会在程序开始时加载完整个文件系统树，内存占用很高，不符合实际情况。 class Fat12FileSystemTree&#123;public: ~Fat12FileSystemTree() &#123; delete root; root = nullptr; &#125;; static Fat12FileSystemTree *init( FILE *fat12, BPB *bpb, DirEntry *rootEntry); void initSystemParameters(); void mountFiles( );//将目录区的数据挂载到文件系统,构造文件树 Node *searchByPath( const string &amp;path ); Node *searchByPath( DirNode* parent, const string &amp;path );public: int BytsPerSec; //每扇区字节数 int SecPerClus; //每簇扇区数 int RsvdSecCnt; //Boot记录占用的扇区数 int NumFATs; //FAT表个数（默认为2） int RootEntCnt; //根目录最大文件数 int FATSz; //每个FAT表占用扇区数private: static Fat12FileSystemTree *instance; DirEntry *dirEntry;//目录项的指针，每次fread都会被刷新 DirNode *root; BPB *bpb; FILE *fat12;private: Fat12FileSystemTree(FILE *fat12, BPB *bpb, DirEntry *rootEntry); DirNode *initRoot(); //初始化root节点，name和path均为&quot;/&quot; bool isMatch( Node* node, const string &amp; AbsolutePath ); // void mountChildren(int startCluster, DirNode *parent); void mountFiles( int startClusterNum, DirNode* parent );//递归地将数据区的数据挂载到文件系统树，对文件节点获取其内容，对目录节点进一步递归 void mountNode( DirEntry*dirEntry, int baseAddr, DirNode *parent );//将给定地址的数据加载进当前目录项（ currentEntry ），再通过当前目录项创建节点 void loadContent( int startClus, FileNode *fileNode );//加载文件节点的内容 int getNextClusterNum( int num );//读取FAT表，获取下一个簇号 bool isValidNodeName(string name); string getDirNodeName(string name); string getFileNodeName(string name);&#125;; 主程序 注意，如果使用clion,因为clion的工作目录是上一级，那么镜像文件的路径是../a.img,但是用makefile的话，工作目录是当前目录，因此路径该改为./a.img int main() &#123; FILE* fat12 = fopen(&quot;../a.img&quot;, &quot;rb&quot;); //打开FAT12镜像文件 BPB *bpb = BPB::init( fat12); DirEntry *rootEntry = new DirEntry(); Fat12FileSystemTree *fileSystem = Fat12FileSystemTree::init(fat12, bpb, rootEntry ); while( true ) &#123; myPrint(&quot;@lyk&gt;&quot;); vector&lt;string&gt; input_list = handleInput(); string instructionName = getInstructionName( input_list ); vector&lt;string&gt; options = getOptions( input_list ); vector&lt;string&gt; parameters = getParameters( input_list ); Instruction *instruction = InstructionFactory:: create( fileSystem, instructionName ); instruction -&gt; exec( options, parameters ); &#125; return 0;&#125; BPB指定字段的含义 //从软盘的第11Byte开始，到第35Byte, 共25Byteclass BPB &#123;public: u16 BPB_BytsPerSec; //每扇区字节数 u8 BPB_SecPerClus; //每簇扇区数 u16 BPB_RsvdSecCnt; //Boot占用的扇区数 u8 BPB_NumFATs; //FAT表个数 u16 BPB_RootEntCnt; //根目录最大文件数 u16 BPB_TotSec16; //逻辑扇区总数 u8 BPB_Media; //介质描述符 u16 BPB_FATSz16; //每个FAT占用扇区数 u16 BPB_SecPerTrk; // 每磁道扇区数（Sector/track） u16 BPB_NumHeads; //磁头数（面数） u32 BPB_HiddSec; //隐藏扇区数 u32 BPB_TotSec32; //如果BPB_TotSec16为0，该值为逻辑扇区总数public: static BPB *init(FILE * fat12) &#123; if( instance == NULL ) instance = new BPB( fat12); return instance; &#125;private: static BPB *instance;private: BPB(FILE* fat12); //读取boot信息&#125;;BPB* BPB:: instance = nullptr;BPB:: BPB(FILE* fat12 )&#123; int check; //BPB第11个字节处开始 check = fseek(fat12, 11, SEEK_SET); if (check == -1) myPrint(&quot;fseek in fillBPB failed!\\n&quot;); //BPB长度为25字节 check = fread( this , 1, 25, fat12); if (check != 25) myPrint(&quot;fread in fillBPB failed!\\n&quot;);&#125; 如何进入子目录并输出(说明方法调用) 目录项结构; //32Byte per entry, 数据区的目录项也采用此结构class DirEntry &#123;public: char DIR_Name[Dir_Name_Length]; u8 DIR_Attr; //文件属性 char Reserve[10]; //保留位，用于Win NT u16 DIR_WrtTime; u16 DIR_WrtDate; u16 DIR_FstClus; //开始簇号 u32 DIR_FileSize;&#125;; 获得baseAddr。对根目录区而言，只需得到Directory Area 起始地址baseAddressOfDirArea，加上 项数 * 32Byte 就是baseAddressForDirEntry void Fat12FileSystemTree:: mountFiles()&#123; int baseAddressOfDirArea = ( RsvdSecCnt + NumFATs * FATSz ) * BytsPerSec;// Directory Area 起始地址 int baseAddressForDirEntry = baseAddressOfDirArea;//目录项的起始地址 for(int i=0 ; i &lt; RootEntCnt; i++ , baseAddressForDirEntry+=32 ) &#123; mountNode( dirEntry, baseAddressForDirEntry, root ); &#125;&#125; 获得baseAddr,对数据区而言 首先得到数据区起始地址baseAddressOfDataArea和当前簇号currentClusterNum 得到当前簇的起始地址baseAddressForCurrentCluster， 就是该簇第一个的baseAddrbaseAddressForDirEntry，每次迭代读取一个目录项（32Byte）， 因此baseAddr += 32, 直到到达簇的末尾baseAddressForCurrentCluster + bytesOfThisCluster， 通过fatValue = getNextClusterNum( currentClusterNum )从fat表中得到下一个簇的簇号。 令currentClusterNum = fatValue ， 不断迭代读取簇，直到文件结束或坏簇 void Fat12FileSystemTree::mountFiles(int startClusterNum, DirNode *parent) &#123;// parent-&gt;createDefaultNodes(); //数据区起始地址，即第一个簇（2号簇）的偏移字节 int baseAddressOfDataArea = BytsPerSec * (RsvdSecCnt + FATSz * NumFATs + (RootEntCnt * 32 + BytsPerSec - 1) / BytsPerSec); int fatValue = 0; //fat表项的值，代表文件的下一个簇号 int currentClusterNum = startClusterNum; while( fatValue &lt; 0xFF8 ) &#123; fatValue = getNextClusterNum( currentClusterNum );//查FAT表获取下一个簇号 if( fatValue == 0xFF7 ) &#123; myPrint(&quot;ERR 读取了坏簇\\n&quot;); break; &#125; int baseAddressForCurrentCluster = baseAddressOfDataArea + (currentClusterNum - 2)*SecPerClus*BytsPerSec;//当前簇的起始地址 int bytesOfThisCluster = SecPerClus * BytsPerSec; //当前簇的长度 for( int baseAddressForDirEntry = baseAddressForCurrentCluster ; baseAddressForDirEntry &lt; baseAddressForCurrentCluster + bytesOfThisCluster ; baseAddressForDirEntry+=32 ) //baseAddressForDirEntry = 当前目录项的起始地址，不能超过该簇 &#123; mountNode( dirEntry, baseAddressForDirEntry,parent ); &#125; currentClusterNum = fatValue;//根据fat项更新簇号 &#125; return; //该目录节点递归完毕&#125; 从软盘中读取目录项，并创建节点 void Fat12FileSystemTree:: mountNode( DirEntry* currentEntry, int baseAddr, DirNode *parent) &#123; if (fseek(fat12, baseAddr, SEEK_SET) == -1) myPrint(&quot;fseek in mountFiles failed!\\n&quot;); if (fread(currentEntry, 1, 32, fat12) != 32) myPrint(&quot;fread in mountFiles failed!\\n&quot;); string dirEntryName(&amp;(currentEntry-&gt;DIR_Name[0]), &amp;(currentEntry-&gt;DIR_Name[Dir_Name_Length]));///起始位置 结束长度位置 +1 if (!isValidNodeName(dirEntryName)) &#123; return;//目录项名字不合法，不挂载 &#125; else &#123; string realName; if (dirEntry-&gt;DIR_Attr == 0x10) // directory node &#123; realName = getDirNodeName(dirEntryName); DirNode *child = new DirNode(realName, parent-&gt;AbsolutePath + realName + &quot;/&quot;, currentEntry-&gt;DIR_FileSize); //新建该目录的节点 parent-&gt;children.push_back(child); if(child-&gt;isDefaultNode())// . 和 ..不递归 &#123; ; &#125; else &#123; parent-&gt;dir_count++; mountFiles(currentEntry-&gt;DIR_FstClus, child); &#125; &#125; else// file node &#123; realName = getFileNodeName(dirEntryName); FileNode *child = new FileNode(realName, parent-&gt;AbsolutePath + realName + &quot;/&quot;, currentEntry-&gt;DIR_FileSize); //新建该文件的节点 parent-&gt;children.push_back(child); parent-&gt;file_count++; loadContent(currentEntry-&gt;DIR_FstClus, child);//读取文件的内容 &#125; &#125;&#125; 如何获得指定文件的内容,即如何获得数据区的内容(比如使用指针等) 对目录节点而言， 就是上文的 mountNode用， 用 fseek和fread不断从baseAddr开始读取目录项，用目录项的数据初始化节点 对文件节点而言：（ 与 mountNode的逻辑相同，只是懒得重构了） void Fat12FileSystemTree :: loadContent( int startClus, FileNode *fileNode)&#123; int dataBase = BytsPerSec * (RsvdSecCnt + FATSz * NumFATs + (RootEntCnt * 32 + BytsPerSec - 1) / BytsPerSec); int currentClus = startClus; int value = 0; //这里用value来进行不同簇的读取（超过512字节） char *p = fileNode -&gt; content; if (startClus == 0) &#123; return; &#125; while (value &lt; 0xFF8) &#123; value = getNextClusterNum( currentClus);//获取下一个簇 if (value == 0xFF7 ) &#123; myPrint(&quot;坏簇，读取失败!\\n&quot;); break; &#125; char* str = (char*)malloc(SecPerClus*BytsPerSec); //暂存从簇中读出的数据 char *content = str; int startByte = dataBase + (currentClus - 2)*SecPerClus*BytsPerSec; int check; check = fseek(fat12, startByte, SEEK_SET); if (check == -1) myPrint(&quot;fseek in loadContent failed!&quot;); check = fread(content, 1, SecPerClus*BytsPerSec, fat12);//提取数据 if (check != SecPerClus * BytsPerSec) myPrint(&quot;fread in loadContent failed!&quot;); int count = SecPerClus * BytsPerSec; //簇的字节数 int loop = 0; for (int i = 0; i &lt; count; i++) &#123;//读取赋值 *p = content[i]; p++; &#125; free(str); currentClus = value; &#125;&#125; 如何进行C代码和汇编之间的参数传递和返回值传递 64位linux, 参数传递使用rdi, rsi 注意，x64的系统调用和x32不一样，前者使用syscall而不是int 0x80，并且syscall的打印函数的寄存器要求也与int 0x80不同 extern &quot;C&quot; &#123;void _print(const char *, const int);&#125;void myPrint(const char *p)&#123; _print(p, strlen(p));&#125; 汇编代码中对I/O的处理方式,说明指定寄存器所存值的含义 global _printsection .text; 传参顺序: rdi，rsi，rdx，rcx，r8，r9; void print(char* s, int lenl);_print: ;x64的打印函数 ; ; write(1, message, 13) ; mov rax, 1 ; 1 号系统调用是写操作 ; mov rdi, 1 ; 1 号文件系统调用是标准输出 ; mov rsi, message ; rsi存放输出字符串的地址 ; mov rdx, 13 ; rdx存放字符串的长度 ; syscall ; 调用系统执行写操作 ; 如果是x32,则打印函数为： ; push rax, 4 ; push rbx, 1 ; push rcx, rdi; 可以看到，存放字符串地址和长度的寄存器是rcx, rdx,与x64不同 ; push rdx, rsi ; int 80h ; ret ; push rdx ; push rdi push rdx push rdi mov rdx, rsi mov rsi, rdi mov rax, 1 ; 1 号系统调用是写操作 mov rdi, 1 ; 1 号文件系统调用是标准输出 syscall ; 调用系统执行写操作 pop rdi pop rdx ret","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"OS Basic","slug":"OS-Basic","permalink":"http://lyk-love.cn/tags/OS-Basic/"}]},{"title":"商业模式设计","slug":"商业模式设计","date":"2021-11-04T16:12:40.000Z","updated":"2022-09-26T06:39:34.943Z","comments":true,"path":"2021/11/05/商业模式设计/","link":"","permalink":"http://lyk-love.cn/2021/11/05/%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F%E8%AE%BE%E8%AE%A1/","excerpt":"Outline: 客户洞察 构思 视觉化思考 模型构建 讲故事 场景","text":"Outline: 客户洞察 构思 视觉化思考 模型构建 讲故事 场景 客户洞察 客户视角是商业模式设计的指导性原则。客户的观点决定了我们选择的价值主张、渠道、客户关系和收益来源。 难点 第一个挑战在于如何建立对客户的彻底理解，并基于这种理解进行商业模式设计 这种理解，并不是简单地问客户需要什么。透彻理解客户（ “问题背后的问题” ）；需要人类学、社会学理论（笼统的人），以及与实地调研（具体的人）结合； 第二个挑战在于尽量避免过于聚焦在现有客户细分群体上，而应该盯着新的和未被满足的客户细分群体。清楚了解企业当前关注哪些客户（的需要），忽略哪些客户（的需要） 变需要为需求（人为核心） 细致观察并接纳目标客户的行为 “笼统的人”与“具体的人”和场景相结合 功能、认知、情感逐渐深入的换位思考以及拓展 将用户引入设计 移情图 构建用户画像的易用工具（“素描”），可导出价值主张、渠道、客户关系、收入来源 使用方式：罗列所有客户群体，挑选三个最有希望的，选择一个作为分析对象 她看到的是什么？ 描述客户在她的环境里看到了什么。 环境看起来像什么？谁在她的周围？谁是她的朋友？她每天接触什么类型的产品或服务？她遭遇的问题有哪些？ 她听到的是什么？ 描述客户所处环境是如何影响客户的。 她的朋友说什么？她的配偶说什么？谁能真正影响她？如何影响？哪些媒体渠道能影响她？ 她真正的想法和感觉是什么？ 设法概述你的客户所想的是什么。 对她来讲，什么是最重的？想象一下她的情感，什么能感动她？什么能让她失眠？尝试描述她的梦想和愿望。 她说些什么，又做些什么？ 想象这位客户可能会说什么或者在公开场合可能的行为。 她的态度是什么？她会给别人讲什么？要特别留意客户所说的和她真正感受之间的潜在冲突。 这个客户的痛苦是什么？ 她最大的挫折是什么？在她和她想要的事物或需要达到的目标之间有什么障碍？她会害怕承担哪些风险？ 这个客户想得到什么？ 她真正要想和希望达到的是什么？她如何衡量成功？猜想一些她可能用来实现其目标的策略。 案例 构思新的商业模式 构思的两个步骤：生成大量创意-&gt;对创意进行整合并挑选 生成阶段要重视数量；可行的创意可以是颠覆性的，也可以是领域的扩展 提出新创意的两个出发点 从画布中寻找创新的焦点 不断提出“如果…会怎样”的问题 商业模式创新的焦点 资源驱动：创新来源于组织现有的基础设施或合作伙伴资源 云计算服务、高传输低时延的5g网络 供给驱动：创造全新的价值主张，并影响到其它模块 水泥输送从48小时减为4小时 云端的全托管机器学习与自动调优、云游戏 客户驱动：基于客户需求、可获得性或便利性的提升，并影响其他模块 23andMe的个人DNA测试服务（从医疗与研究领域转来） 付费自习室：价值主张-（成年人）沉浸式学习空间；收入来源：5-20元/小时租赁 财务驱动：由新收益来源、定价机制或者被缩减的成本驱动的创新 施乐复印机从卖设备转向复印机出租（月费95美元，含2000份复印，超出5美分每张） 免费经济：360免费杀毒，IBM服务器从软件+硬件转型为开源+咨询+硬件 多点驱动：多焦点驱动的创新，并对其它模块产生深远影响 “卖设备”转为“卖服务”：财务、供给、客户、资源 B站：与共青团及官媒的合作、内容从二次元到多圈融合、从内容转向社交（陪伴）、高粘性用户的游戏运营与内容驱动直播 构思的流程与团队建设 团队组建 除了“创意天才”，更需要多样化创新团队 成员多样化：业务单元/领域不同、年龄/资历水平不同、文化背景不同、经验互补（例：美团成长期重要人物 – 原阿里“中供铁军”骨干干嘉伟） 要引导积极倾听，并考虑在关键会议上引入一个中立的引导员或主持人 钻研 创新所需要的知识：总体研究、客户与潜在客户、新技术调研、现有商业模式评估等 开拓 从九大模块任意一点出发作为创新起点；数量是关键；重在创意，避免过早评论价值 甄选标准 在业务背景下包含：预期实施时间、潜在收入、可能的客户阻力、对竞争优势的影响 构建原型（模型） 确立标准后从创意中整理一个最优短名单，由此构建3-5个创新的商业模式，再利用画布进行勾勒和讨论 构思补充：头脑风暴 保持聚焦 精确表达当前问题、始终与客户需求有关、不要跑题太远、将讨论拉回到开始问题 执行规则 坚决执行开始时的规则：“不过早下结论”、“每次一人讲“、”追求数量”、“可视化”、“疯狂创意” 视觉化思考 将创意写或者画在每个人都能看到的地方：便利贴+黑板/墙 准备 为一次头脑风暴所准备的钻研：技术研讨、实地考察、客户讨论等各种形式 视觉化思考 抽象的东西具体化、复杂的概念简单化 视觉化思考可以帮助我们看清楚一个模式的全貌。一个模式就是一个系统，只有看到全貌才能促进我们对模式的创新。 视觉化思考能够把抽象变为具体，并能有效地阐明各个元素之间的逻辑关系，简化了事物的复杂性，进而大大地改善了讨论的质量。 实现：便利贴+绘画 辅助实现视觉化思考的流程：理解、对话、探索、沟通 实现 便利贴+绘画 便利贴的重要性：随意的添加、删除、移动（在画布上的位置） 三个指导方针：粗的马克笔+只写一项元素+只用少量文字抓住关键点 便利贴的绘制、添加、删除和移动能够有效组织人们参与讨论，反映了商业模式的动态变化过程，与结果同样重要 CRC卡片（正面对象状态+行为与协作者，背面简短描述）、Story Card（正面故事描述，背面故事实现的理由） 绘画的强大表现力：人对图像的反应要比文字强烈的多 最简陋的素描也能让事物变得具体和易于理解：如情绪、比例等 帮助向别人解释和沟通你的商业模式，容易激发起建设性的讨论与创意 可用于勾勒一个典型客户与他所处的环境，也可用于勾勒出客户群体的需求和任务 作用 理解商业模式的本质 视觉化的语言：画布是一张概念图，其功能类似于具有语法规则的视觉化语言，提供了视觉和文字的指引，帮助画出模式中所需的所有信息 抓住全貌：画布的草图能够为观众提供足够的信息理解全貌，而不被过多的细节影响理解 看到关键：一定要理解元素/模块之间的关联关系 提升对话效率 共同的参照点：将头脑中不言而喻的主观假设具象化，并将大量内容固化成为可回溯的参照点（人类的短时记忆只能保留有限数量的想法） 统一的语言：利用图形和画布帮助不同参与者聚焦，特别是来自不同领域的人 一致的理解：帮助不同部门的人将其深入理解的部分表达出来，再一起形成整体的洞察与一致的理解 探索创意 激发创意：模糊的想法-随着灵感发挥-有机地整合成一幅图画 演习：视觉化的模型帮助思考部分元素的改变引发的系统性冲击 提升沟通 统一公司内部的理解：用图画在组织内形成共识，朝一个战略方向前进 内部推销：好的图画使组织的现状、需要做的事情、怎么做、未来会怎样等方面变得易于沟通，从而赢得组织内部的理解和支撑 外部推销：提升向投资人或潜在合作伙伴推销成功的概率 讲述视觉化的故事 解释商业模式的一种有力的方式：利用画布草图逐一介绍一个完整的视觉化故事 如何讲述 绘制商业模式 用简单的文字填充各个商业模式模块 一个模块只用一张便利贴 用图形描绘每个商业模式元素 每次取下一张便利贴，再用图形去取代文字想表达的内容 图形保持简单 设计故事主线 决定讲故事的时候先贴哪张便利贴 可以尝试不同的主线与起点，只要能支撑故事 讲述故事 根据便利贴的顺序与贴图的内容逐一讲述你的商业模式 模型构建 与视觉化思考一样，模型构建可以使抽象的概念具体化，帮助探索新的创意 在产品、架构和交互设计上得到广泛应用，但在商业管理领域不太常用 商业模式原型是用于讨论、调查或者验证概念目标的工具 模型构建有助于实际商业模式的探索 建模-（疑问点明确化、视觉化）-添加、删除或修改元素-观察结果 在不同规模（抽象层面）的模型上进行互动 有助于获得突破性的商业模式，同时能够有效控制细节 流程 不同程度的模型 随手素描（napkin sketch）：勾勒和推销一个粗略的主意 勾勒想法，含价值主张和主要收益来源 精心描绘的画布（elaborated canvas）：探索实现该创意所需的因素 完整画布，商业逻辑思考，市场潜力预估、理解模块之间联系、“事实查证” 商业案例（business case）：检查该创意的可存活度 全面画布，关键输入、核算成本与收入、估算利润潜力、模拟财务场景 实地验证（field-test）：调查客户的可接受度和可行性 准备合情合理的商业案例，站在客户角度进行实地验证，验证价值主张、渠道、定价机制等实际市场中的元素素 讲故事 故事是一个理想的热身工具，为深度讨论商业模式与其内在逻辑做好准备 将故事与画布结合，利用叙事性克服听众对不熟悉模式的抵触，放下对陌生事物的怀疑 为什么要讲故事 介绍新想法：尝试融入组织战略 向投资人推销：争取外部资源（是什么，为谁服务，如何获得收益） 吸引员工（成员）：抓住组员的注意力和好奇心，为下一步探讨准备 让未来触手可及：激发创意、辩证变革 故事的不同视角（两个） 以员工为视角（即公司视角）： 观察到的新商业模式所解决的客户问题 新商业模式如何比旧模式更好的利用资源、业务和伙伴关系（降本增效、开源节流） 员工承载了组织内部工作与商业模式，以及转向新模式的原因 以客户为视角： 客户面临的挑战与必须完成的工作，以及组织如何为其创造价值 描述她得到的东西、这些东西如何融入她的生活、以及她愿意为哪些东西付费 可以添加一些戏剧性和情感因素，描述你的组织如何让她的生活更简单，并尝试加入组织如何提供帮助，并需要哪些资源和活动 故事需要真实可信 场景构建 搭建一个物理环境来给用户带来一个独立体验。 将模型构建中明确的方向具体化,从而给出有见地的设计 场景可以让抽象的事物变得具体。 类型 基于客户的场景 案例 产品或服务将被如何使用 什么样的客户会用到它们 客户的担忧、诉求和目标 以上都是基于客户洞察而产生的。 结合客户洞察描绘出独特、具体的图景 基于未来的场景 案例 描述一个商业模式未来可能的竞争环境 为了想象未来可能的具体细节,战略研究领域将这种联系归为&quot;场景规划&quot;的课题 流程： 根据两个或多个主要标准开发一系列的未来场景。 为每种场景描述一个故事,其中列举出该场景的主要元素。 商业研讨会开始前,各种场景必须被开发出来,帮助小组成员最大限度地理解未来的所有可能。 研讨会:为每种场景开发一个或多个合适的商业模式。","categories":[{"name":"Business","slug":"Business","permalink":"http://lyk-love.cn/categories/Business/"}],"tags":[{"name":"需求与商业模式创新","slug":"需求与商业模式创新","permalink":"http://lyk-love.cn/tags/%E9%9C%80%E6%B1%82%E4%B8%8E%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F%E5%88%9B%E6%96%B0/"}]},{"title":"Dune","slug":"Dune","date":"2021-11-03T00:18:39.000Z","updated":"2022-09-26T06:39:34.930Z","comments":true,"path":"2021/11/03/Dune/","link":"","permalink":"http://lyk-love.cn/2021/11/03/Dune/","excerpt":"《沙丘》： 9.0/10","text":"《沙丘》： 9.0/10 《沙丘》电影版对视听的注重更甚于叙事,每一帧都完美, 宏大, 肃穆. 观众或许会在复杂的剧情中迷失,但这无关紧要,他们只需要坐在椅子上,放空身心,静静享受画面的震撼. 《沙丘》没有致力于讲一个故事,而是营造一个氛围,沙丘世界的荒凉肃穆的氛围,这是独属于《沙丘》电影,属于维伦纽瓦的氛围. 这是明智的选择,因为原著在21世纪早已落伍了,当时的科幻鼻祖之一,如今看来只是部政治宫斗,沙丘故事( 包括基地在内的一众早期科幻故事 )无非是将宫斗剧从地球搬到了太空, 其思想内核也与科幻没有什么关系. 此次的电影化, 导演一方面老老实实地讲述了原著剧情,让慕原著名而来的观众不觉为何,又将表现的侧重点放在气氛,场景的营造上,注重纯粹视听的表达, 使电影不囿于小说的平庸. ,正如文艺复兴对于古希腊文化的发掘是为了利用先贤发扬自己的思想一样, 《沙丘》电影只是对沙丘文本的一次利用,其最终目的是完成导演和剧组想要的, 一个宏大荒凉壮丽的星空舞台. 维伦纽瓦与雷德利斯科特不同的地方在于, 属于老一辈人的雷德利斯科特, 是将科幻作品作为严肃的题材,去发掘其内核的, 也就是说,他尝试讲好一个故事. 《银翼杀手》就是这种严肃的哲学内核和美丽的科幻幻想的产物. 而维伦纽瓦不试图讲好一个故事了,他只负责将故事老老实实地讲完,然后在视听方面发挥才华, 可以说, 维导的科幻没有什么严肃的( 像雷德利斯科特那样, 沉重的 )内核, 这并不是贬低维导 ----- 表现和意义, 本身就是电影这一综合艺术的两极. 极致的视听塑造, 也是一种艺术.","categories":[{"name":"Art","slug":"Art","permalink":"http://lyk-love.cn/categories/Art/"}],"tags":[{"name":"Movie","slug":"Movie","permalink":"http://lyk-love.cn/tags/Movie/"}]},{"title":"007 No Time to Die","slug":"007 No Time to Die","date":"2021-11-03T00:18:22.000Z","updated":"2022-09-26T06:39:34.924Z","comments":true,"path":"2021/11/03/007 No Time to Die/","link":"","permalink":"http://lyk-love.cn/2021/11/03/007%20No%20Time%20to%20Die/","excerpt":"The proper function of man is to live, not to exist. I shall not waste my days in trying to prolong them. I shall use my time.","text":"The proper function of man is to live, not to exist. I shall not waste my days in trying to prolong them. I shall use my time. 我心目中最好的动作电影。007系列宣扬了几十年的硬汉精神，到最后终于亲手打破了它，人不可能永远是硬汉，或者说硬汉也会失败。每当我们看着邦德飞檐走壁，出生入死，完成一个个不可能完成的任务时，心头总会飘过一丝阴霾：007会失败吗？007会老吗？ 007... 会死吗？ 答案是会的。邦德在我们心中，一贯是沉着冷静、机敏过人，永不疲倦，无所不能。可梦总有醒的一天，英雄总有老的一天。no time to die是誓言，可是一厢情愿，改变不了什么。人和任何生物一样，都是有极限的，哪怕你是007, 能走遍刀山火海，无数次力挽狂澜， 你依然会失败。人在命运面前要接受自己的软弱无知，接受自己的狭隘渺小，英雄也不例外。 人犯的最愚蠢的错误，就是在命运面前保持高傲。再可歌可泣的精神，几十年如一日的坚持，举世无双的勇敢，也无法打动命运。哪怕你说no time to die, 哪怕你确实因此而不懈奋斗， 是全世界最有资格喊出这句话的人， 事实也不会因为你的呼喊而改变，即使做得再好，也有人力有穷而尽的时候。 邦德临死前镇定地凝望着天上的导弹， 昔日无敌的007，终于感到一丝疲倦和悲凉。","categories":[],"tags":[{"name":"Movie","slug":"Movie","permalink":"http://lyk-love.cn/tags/Movie/"}]},{"title":"商业模式画布","slug":"商业模式画布","date":"2021-10-25T13:37:39.000Z","updated":"2022-09-26T06:39:34.943Z","comments":true,"path":"2021/10/25/商业模式画布/","link":"","permalink":"http://lyk-love.cn/2021/10/25/%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F%E7%94%BB%E5%B8%83/","excerpt":"Outline: 客户细分 价值主张 渠道通路 核心资源 关键业务 重要合作 成本结构 客户关系 收入来源","text":"Outline: 客户细分 价值主张 渠道通路 核心资源 关键业务 重要合作 成本结构 客户关系 收入来源 客户细分 Customer Segments Def 一家企业想要获得的和期望服务的不同的目标人群和机构 Factor 细分条件： 需求催生新供给、 需要新分销渠道和客户关系类型 产生的利润率不同、 愿意为某方面的特殊改进买单 需要谨慎处理客户的细分与取舍 价值主张 Value Proposition Def 为某一客户群体（ 需要细分 ）提供能为其创造价值的产品和服务 排他性：解决客户的问题或满足其需求，使其选择一家而放弃另一家 一家公司为特定客户群体提供的利益集合或组合 Factor 有益于价值创造的因素：创新、定制、设计、etc... More 初创团队如何设计自己的VP? 为了生存，初创团队需以“轻成本”的方式运营 一般而言，VP以“收益”型为主 对于初创团队而言，先实验一下自己的VP能不能挣钱 客户进一步划分与聚焦、获取额外收益、利于创新 客户细分有利于提升利润， 聚焦一点也容易创新。 但最后总归要拓宽领域的 团队自身可以多考虑基于共有平台构建“简单”与“透明”式 的产品，维持“轻成本”运营 信息类产品：知乎问答、微信公号、视频平台、游戏发行平台 实物类产品：各类生产线的复用（自制JK裙、元气森林） 拓展边界：虚拟世界&gt;&gt;现实世界； 虚拟产品很容易拓展边界 构建护城河：现实世界&gt;&gt;虚拟世界 实体产品的优势是形成壁垒 渠道通路 CHannels Def 一家企业如何同它的客户群体达成沟通并建立联系，以向对方传递自身的价值主张 Factor 渠道的五个阶段与运营方式（一个渠道可包含一个或全部五个阶段） 知名度 评价 购买 传递 售后 一个组织可选用自有渠道( 成本更高 )、合作方渠道（ 比如微信 ）、或混用，以追求获益与成本的平衡以及最佳的客户体验 自身强渠道： 线下体验店：蓝绿大厂 品牌贴牌与认证授权 日本马桶圈与电饭锅： “偷”别人的渠道 米家：贴牌和授权需要设计部门来对接， 确保不同厂商的产品合起来形成一个完整的渠道，不会有违和感 能主动引发流量的互联网平台 一般来说都是用别人的平台。 合作方渠道：各大电商平台（农村电商汇通达），小红书（种草拔草社区），视频推广（恰饭视频） 混用：移动运营商直营与加盟店，天猫上的苏宁易购官方店（仓储、物 流、售后），网易严选 More 渠道通路的重要性 商业的本质，人人互联成本为零的最大发力点 与产品设计的关系： 渠道对同类产品竞争起核心作用 过度重视容易引发反噬（这里是指只重视渠道而不注重设计） “产品设计运维一体化”：CH承载VP与CS的组合关系 一边设计一边运维，很常见 （完全）基于渠道的品牌：南极人、三只松鼠 产品都是采购的别家的（ 这个方式做大后其实不靠谱，不过作为初创而言是很好的思路 ） （免费）公开渠道：微信公号、朋友圈、小程序 核心资源 Key Resources Def 保证一个商业模式顺利运行所需的最重要的资产 用于：价值主张的创造与提供、开拓市场、维护客户关系并获益 可以“拥有”或者“合作” “拥有”意味着额外的管理、折旧和风险，“合作”意味着让出的利润空间与潜在的生存危机 “核心”意味着稀缺与不可替代，需要花费巨大的成本维系 Factor 类型 实物资源 physical：生产设备、房屋、车辆、机器、系统、销售点管理系统、分销渠道（腾讯：勿Q） 知识性资源 intellectual：品牌（可口可乐）、专利（高通与华为）、知识产权与体系（微软、SAP、安卓/苹果） 人力资源 human：普遍存在，对于创新性和知识密集产业最重要（如IT业），出色的营销团队 金融资源 financial： 内部：花呗、车贷、互联网金融 外部：风险投资、资本市场**（国资）** 关键业务 Key Activities Def 保障其商业模式正常运行所需做的最重要的事情 价值主张、获得市场、客户关系与收益 与价值主张强相关，价值主张的具象化 构建护城河：*商业模式创新 – 构建不可替代的关键业务 – 支撑服务升级 – 基础设施投资 *–底层技术突破 – 拥有 强化核心资源（例：阿里云，盒马） Factor 类型 生产 production：包含分销网络、渠道等 解决方案 problem solving：知识管理与持续的培训 平台/网络 platform/network：XX网、Visa卡、操作系统、应用商店、游戏平台 重要合作 Key Partnership Def 保证一个商业模式顺利运行所需的供应商和合作伙伴网络 非竞争者之间的战略联盟康采恩（不同业务之间的利益共同体） 3q大战之后的腾讯联盟（与阿里直营思路显著不同） 竞争者之间的战略合作卡特尔（同产业控制产品产量和价格） 红蓝快乐水、微信支付与支付宝、米国两党制 新业务的合资公司托拉斯（多个巨头通过合资公司组成的利益共同体）、 大厂“生态” 、微信vs. 苹果、Fortnite vs. App Store + Google Play 稳定供应关系的供应商和采购商辛迪加（同产业垄断上游供应和下游销售） 产业园、苹果认证供应商、闭环的互联网影视平台（传统影视产业：制作、发行、院线） Factor 合作动机 优化与规模效应：降低成本，外包或共享基础设施 特殊资源及活动的获得：高技术产品、销售团队、特许商品与渠道 降低风险和不确定性：某领域内的战略联盟（蓝光、5g），台湾省与韩国的面板联盟京东方的崛起*（09-10家电下乡，韩国作为污点证人）* 思考：“千播大战”的结局与幕后boss**，新闻：雷军与长春一汽洽谈* 成本结构 Cost Structure Def 运营一个商业模式所发生的全部成本 确定核心资源、关键业务和重要合作之后，成本核算将相对容易 也有以低成本结构为核心的商业模式（廉航、红米、Zara） 低成本意味着快速迭代。 低成本意味着低价格和低利润，即意味着需要大量消费 反而要求更高，因为控制成本会更难 但是收益可能会更高，因为可以带来流量 导向 成本导向 cost-driven：成本最小化，创造并维持极尽精简的成本结构 价值导向 value-driven：高端的价值主张与高度的个性化服务 一单吃半年 Factor 固定成本：管理员工工资，租金，生产设备 可变成本：加工工人工资，加（bai）班（ri）费（meng），广告推广费，水电，原材料消耗 规模经济：大宗采购，大规模生产摊薄的固定成本 范围经济：渠道的复用（摊薄部分可变成本） 小米台灯为什么那么便宜？ 客户关系 Customer Relationship Def 一家企业针对某一个客户群体所建立的客户关系的类型 靠人员维护（“专属一对一财富管家”） VS 自动化设备（“24小时自助”） 动机：开发新客户、留住原客户、增加销售量或客单价（携程杀熟、杀苹果用户） 免费推广-提升忠诚度（全家桶、归属感、情怀）-提高客单价 新手礼包/老用户激活礼包-品牌宣传与建设/用户等级-老客户专属套餐 客户关系与承载的渠道之间要一致 Factor 客户关系类型 私人服务 personal assistance：商场导购、柜台服务与电渠、销售员 专属私人服务 dedicated personal assistance：私人银行服务、华为电信设备、健身/培训“私教” 自助服务 self-service：话费流量充值、银行普通业务（ATM与大厅内自助服务） 自动化服务 automated services：各类平台推荐系统、网站导航设计（活动、凑单、无货推荐、红色与橙色的加入购物车、立即购买） 社区 communities：花粉俱乐部、小米之家、小红书、各类网游社区 客户共同创造 co-creation：MIUI，UGC（土豆、B站、抖音），各种评论（电影书籍-豆瓣、旅游住宿-Airbnb、普通商品-“自发安利”与评论区），采纳用户反馈的社区（产品调查问卷、游戏平衡运维） 收入来源 Revenue Streams Def 企业从每一个客户群体获得的现金收益（扣除成本的利润） 探索用户真正愿意付费的点！ 两类收益来源：一次性交易收入、持续收入（进一步提供产品服务或售后支持） 定价机制 固定（基于静态变量）：目录价、基于产品特性（“青春版”、“畅享版”）、基于客户群（教育版）、基于数量 浮动（基于动态变量）：谈判/议价、收益管理（库存与发生购买的时间，如生鲜、熟食、酒店、航班等）、实时市场价格、拍卖 Factor 收入来源的方式 资产销售 asset sale：实物产品所有权转让，消费者拥有处置的全部权利 使用费 usage fee：电信、宾馆、快递、付费网游点卡、公共交通车票 会员费 subscription fee：健身卡、付费网游月卡、公共交通月票、音乐会员 租赁 lending/renting/leasing：共享单车/汽车/充电宝，特定资产在特定时间的使用权转移并获益 许可使用费 licensing：专利授权、版权（图片、音乐、字体）、加盟或特许经营 经纪人佣金 brokerage fees：信用卡（交易手续费）、支付平台（交易与提现手续费）、中介 广告费 advertising：传媒、品牌策划、软件业与服务业；广告费增长乏力，分蛋糕的太多","categories":[{"name":"Business","slug":"Business","permalink":"http://lyk-love.cn/categories/Business/"}],"tags":[{"name":"需求与商业模式创新","slug":"需求与商业模式创新","permalink":"http://lyk-love.cn/tags/%E9%9C%80%E6%B1%82%E4%B8%8E%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F%E5%88%9B%E6%96%B0/"}]},{"title":"OS PROCESSES AND THREADS","slug":"OS-PROCESSES-AND-THREADS","date":"2021-10-22T17:13:42.000Z","updated":"2022-09-26T06:39:34.935Z","comments":true,"path":"2021/10/23/OS-PROCESSES-AND-THREADS/","link":"","permalink":"http://lyk-love.cn/2021/10/23/OS-PROCESSES-AND-THREADS/","excerpt":"Outline: Processes Programs == Processes Threads( 只写到这两部分 ) Interprocess Communication Scheduling Classical IPC problems Summary ref: Modern Operation Systems OS Lesson --- 蒋炎岩","text":"Outline: Processes Programs == Processes Threads( 只写到这两部分 ) Interprocess Communication Scheduling Classical IPC problems Summary ref: Modern Operation Systems OS Lesson --- 蒋炎岩 Processes multiprogramming: CPU switch from program to program difference between a process and a program : 想象一个厨师做菜: program: recipe. That is, an algorithm expressed in some suitable notation. processor: the chef input data: the ingredients process: the activity consisting of the chef reading the recipe, fetching the ingredients, and making the food 一个程序若运行两次，那么就是两个进程 Process Creation Four principle events cause processes to be created： System initialization 进程可分为foreground和background，前者与用户交互，后者又称为daemon,运行在后台 Execution of a process-creation system call by a running process A user request to create a new process 在交互式系统中，用户输入命令或者点击图标，都能创造进程并在里面运行程序 Initiation of a batch job 只针对批处理系统，用户向系统提交batch jobs,系统若有空余资源，则接受任务，创建一个新进程来运行它，自身则继续接受输入队列中的下一个任务 UNIX中，进程创建只有一种办法： fork. fork后父子进程拥有相同的资源（包括memory image, environment strings, open files）. 子进程然后执行execve或其他系统调用来改变自身的memory image 并运行新的程序。 fork - execve分两步的目的是让子进程在fork后，execve前将父进程的输入和输出重定向 Win中， CreateProcess完成了fork - execve这两步 进程创建后，父子进程各自的address space独立 UNIX：子进程的initial address space 是父进程的copy,然而这块内存是copy-on-write的。 每当二者之一要改变该内存，这块内存都会先copy,以确保改变只针对各自进程的private memory WIN：父子进程的 address space 一开始就是不同的 Process Termination Four conditions: Normal exit(voluntary) 当进程结束其任务时，会发起系统调用通知OS来结束它 GUI进程被点击X而关闭也属于这类 Error exit(voluntary) 输入错误，比如没找到文件 Fatal error(involuntary) 程序错误 Killed by another process(involuntary) kill in UNIX. killer需要对killee有一定权限 在有些系统中，杀死进程会将其所有子进程也杀死，UNIX和WIN都不这么做 Process Hierarchies UNIX：a process and all of its children and further descendants together form a process group. e.g. 一个keyboard signal会发给与这个keyboard相关的process group的所有成员，每个成员可以独立地处理signal 当UNIX开机时， 一个init进程会存在于boot image,init读取一个记录了有多少terminal的文件，为每个terminal fork一个新进程，新进程等待用户login in,若login in成功，则该进程执行一个shell等待用户输入命令，命令又会创建新的进程。 整个系统的所有进程就形成了以init为根节点的树 WINDOWS：不存在进程层次结构。 父进程会拥有子进程的handler，它可用来控制子进程，但handler可被自由传递，因此不存在层次结构 Process States 进程等待输入时，会被block 3 states： Running( actually using the CPU at that instant ) Ready( runnable; temporarily stopped to let another process to run ) Blocked( unable to run until some external event happens) 当所需的事件发生时，会转为Ready OS的Process model（只是逻辑上的）: Schedule: 位于最底层，处理所有的中断和进程处理细节 Processes: 位于Scheduler之上。OS的其余部分都被组织为process形式 Implementation of Processes To implement process model, the OS maintains a process table, with one entry per process. 每个entry存储了该进程被切换后要恢复所需的全部信息 每个I/O类都对应一个interrupt vector，指向其handler的地址 中断发生时，先由硬件来保存pc，等信息，然后计算机跳转到interrupt vector指向的内存，硬件的工作至此结束。接下来是软件的工作，先用汇编程序保存寄存器，设置栈指针，然后C程序执行相应的任务（for the specific interrupt type)，当其工作结束后，一些进程应该已经进入ready（因为它们所需的event发生了），接下来调度器决定接下来运行哪个进程。然后控制交还给汇编程序，加载新的进程： Hardware stacks program counter, etc. Hardware loads new program counter from interrupt vector .Assembly-language procedure saves registers Assembly-language procedure sets up new stack] C interrupt service runs( typically reads and buffs input ) Scheduler decides which process is to run next. C procedures returns to the assembly code. Assembly-language procedure starts up new current process. CPU usage: ( probabilistic view ): CPU utilization = $1 - p^n$ p: CPU用于I/O等待的时间占比 n: 进程数 $p^n$: n个进程全都等待I/O的概率 假设电脑拥有8GB内存，2GB用于OS, 每个user program占2GB， CPU I/O等待概率为80%， 则其CPU使用率为$1 - 0.8^3 $ Programs 程序就是可执行的二进制文件 Linux支持多种可执行文件格式 ELF( Executable Linkable Format)是其中最常用的 .o .out都是ELF vim a.c gcc -c a.c // 汇编并编译 file a.o a.o: ELF 64-bit LSB relocatable, x86-64, version 1 (SYSV), not stripped //可见.o是ELFgcc a.o //链接 file ./a.out ./a.out: ELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=e37d8d4cf587a2d3d1a245cbd8443a8fe4f0a453, for GNU/Linux 4.4.0, not stripped//可见.oou是ELF ELF文件 我们主要讨论ELF文件 查看当前所有进程： ps -ax | less linux的所有面临都是文件，vim /bin/ls ,可以看到其内容（二进制部分显示异常，但可以看到字符串常量， 命令的说明都是字符串常量） xxd可以看到文件以“7f 45 4c 46”（即 x7f ELF）开头 i.e. xxd /bin/ls | less 解析ELF文件 readelf -h：查看header（元数据） -l：查看程序运行 如果需要用代码解析，/usr/include/elf.h 提供了定义 尝试写一段最小的不依赖标准库的代码 1 #include&lt;stdio.h&gt; //hello.c 2 int main() 3 &#123; 4 printf(&quot;Hello world!\\n&quot;); 5 &#125; 如果gcc -o hello.out hello.c（编译并链接）， 再 ./hello.out，能正常输出结果。 如果 gcc -c hello.c 后 ld hello.o ,会报错： ld: warning: cannot find entry symbol _start; defaulting to 0000000000401000ld: hello.o: in function `main&#x27;:hello.c:(.text+0xf): undefined reference to `puts&#x27; 首先， puts函数是编译器优化的结果（我们没有用puts，用的是printf） 编译器在-o0下依然会进行一些编译优化： _start是linker默认的入口，入口可以用-e指定，如ld -e main hello.o，就不会有——start的报错 如果 #include&lt;stdio.h&gt; int main() &#123;&#125; gcc -c hello.c objdump -d hello.o（查看二进制代码） ld -e main hello.o ，没有报错，链接成功 file a.out，得到a.out: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, not stripped，确实是ELF文件 ./a.out ，得到segmentation fault，还是报错。 因为标准库为我们做了很多工作！ gdb调试 gdb调试 r: 运行 starti从第一条指令开始执行程序 layout asm更方便地查看汇编 si单步执行 bt（backtrace），查看调用栈 info register查看寄存器状态 info inferiors:显示所有进程 !cat /proc/[pid]/maps: 内存映射信息 ！ [instruction]:执行shell命令 objdump -d a.out： 0000000000401000 &lt;main&gt;:， readelf -a hello.out | less ： Entry point address: 0x401000 二者一致 OS加载程序，并初始化运行环境，从_start开始运行 starti: 0x00007ffff7fce090 in _start () from /lib64/ld-linux-x86-64.so.2,这个库是OS的 info inferiors: Num Description Connection Executable * 1 process 7669 1 (native) /home//a.out !pmap 7669: 7669: /home/a.out0000555555554000 4K r---- a.out0000555555555000 4K r-x-- a.out0000555555556000 4K r---- a.out0000555555557000 8K rw--- a.out //加载好了a.out00007ffff7fc7000 16K r---- [ anon ]00007ffff7fcb000 8K r-x-- [ anon ]00007ffff7fcd000 4K r---- ld-2.33.so00007ffff7fce000 144K r-x-- ld-2.33.so00007ffff7ff2000 36K r---- ld-2.33.so00007ffff7ffb000 16K rw--- ld-2.33.so //加载好了ld-2.33.so00007ffffffde000 132K rw--- [ stack ]ffffffffff600000 4K --x-- [ anon ] OS先加载好了a.out，再加载ld-2.33.so，ld-2.33.so是OS提供的最初始的加载器，它会加载libc，再调用libc的初始化，再调用main main的开始/结束并不是程序的开始/结束！ 利用gcc的__attribute__: 1 #include&lt;stdio.h&gt; //a.c 2 3 __attribute__((constructor))void hello()&#123; 4 printf(&quot;Hello,World!\\n&quot;); 5 &#125; 6 7 __attribute__((destructor))void goodbye()&#123; 8 printf(&quot;Bye,cruel OS World!\\n&quot;); 9 &#125; 10 11 int main()&#123;&#125; gcc a.c ./a.out 得到： Hello,World!Bye,cruel OS World! strace strace - trace system calls and signals strace ./a.out strace 在系统调用执行之前就能显示它 write(1, &quot;Hello,World!\\n&quot;, 13Hello,World!) = 13 后面的“Hello,World!”是系统调用的结果 用strace查看之前的a.c strace输出到标准错误输出， 可以用strace ./a.out &gt; /dev/null 将标准输出丢弃（这样就不会看到系统调用write的结果）可以看到 write(1, &quot;Hello,World!\\nBye,cruel OS World!&quot;..., 33) = 33 两个write合成了一个 /dev/null下的所有文件都会被丢弃 gcc .c --&gt; (preprocess) ---&gt; .i ---&gt; (compile) ---&gt; .s ---&gt;(assembly) ---&gt; .o ---&gt; (link) ---&gt; .out strace -f gcc a.c 2&gt;&amp;1 | grep execve 查看execve系统调用 //太长了 gcc： 先调用cc1（编译器，c到汇编） 再调用as（汇编器，实际上gcc会检索多个as，找到可用的那个） 再调用collect2（收集构造函数和析构函数，并生成它们的调用函数; 然后负责链接） 再调用ld GUI GUI和普通程序没什么不同 使用 strace xedit \\bin\\ls，鼠标不断移动，看系统调用栈的变化 Threads 进程可拥有多个线程，所有线程共享进程的address space 进程是竞争关系，线程是合作关系 Thread Usage 线程的优势： 线程可以共享进程的address space 线程比进程更轻量，创建和摧毁也更容易 （单核下）Web server中， 一个dispatcher负责接受请求，并分发给相应的worker thread, 当worker thread被阻塞时（比如等待磁盘操作），其他线程可以执行，比如dispatcher,它可以继续接受请求 3 Designs to construct a server Model Characteristics Threads Parallelism, blocking system calls Single-threaded process No parallelism, blocking system calls( 系统在阻塞时无法接受请求 ) Finite-state machine Parallelism, nonblocking system calls ,interrupts The Classical Thread Model The Thread Model is based on 2 independent concepts: resource grouping and execution Process用于资源分组， 而 Thread用于执行 当一个线程打开一个文件，其他线程可以使用该文件。 这是合理的，因为进程才是资源分配的单位，这些线程都在一个进程下 each thread has its own stack POSIX Threads UNIX 线程标准库: Pthreads Implementing Threads in User Space 有两种实现线程的方式：user space and the kernel 将线程放入user space，kernel不知道线程的存在，kernel只面对单线程的进程 可以在不支持多线程的OS上运行，因为kernel面对的都是单线程的进程 每个进程拥有run-time system,which is a collection of procedures that manage threads. 每个进程拥有各自的 thread table 进程运行在run-time system上层 thread table在run-time system内 当线程执行可能使其阻塞的任务时，它会发起run-time system procedure，后者查看该进=线程是否应该被阻塞，若是，则保存其（线程自己的）寄存器，并在thread table里找到并加载新的ready的线程，并新线程的寄存器载入机器的寄存器。 这些都在用户层进行，避免了陷入内核层的开销 缺点： 阻塞式系统调用很难实现 没看懂 当一个进程发生page fault( 数据在硬盘中，而不在内存中 )时，OS会阻塞整个进程（因为OS眼中进程都是单线程的，它认为是这个线程发生了page fault），此时其他线程都无法运行 在一个线程中没有clock signal( interrupt )， 因此无法对线程的使用时间进行调度，因此除非一个运行时的线程主动退出，否则其他线程无法运行 Implementing Threads in the Kernel 没有run-time system, kernel有一个thread table记录所有线程（而非每个进程一个thread table），新线程的创建需要system call 所有可能阻塞线程的调用都被实现为系统调用 当一个线程阻塞时，kernel可以运行该进程下的另一个线程，也可以运行其他进程的线程。 而在user space版的实现中，一个run-time system只能运行其自己的进程下的进程 由于创建和摧毁线程的代价高昂，在这个视线里广泛采用线程复用技术 当线程被摧毁时，它只是被标记为 not runnable, 它的结构体没有被销毁 当一个线程发生page fault,OS可以切换到该进程的另一个线程 Hybrid Implementations kernel面对kernel-level线程，而这些线程可能拥有多个user-level线程 可以在不支持多线程的OS上运行 Scheduler Activations 让Threads in the user space拥有in kernel的功能，又能不失去太多性能的方案之一 当一个thread阻塞时，没有必要通知内核，由run-time system处理 为每个进程分配一个虚拟cpu,让run-time system将线程分配给虚拟cpu 当kernel知道一个thread阻塞时，它会告知run-time system, 让后者进行线程切换。 当线程又恢复到ready， kernel也会告知run-time system “告知”机制被称为upcall 该方法的问题在于：upcall的原理是下层使用了上层的服务，违反了OS分层的原则 Pop-Up Threads 当分布式系统接收到一个message时，它会创建一个 brand new thread来处理该message 传统方法是，令一个thread一直等待请求。 而pop-up让OS直接创建新Thread,新进程没有任何数据，因此创建的开销非常小 Making Single-Threaded Code Multithreaded 我们假设采用in user space的实现方案 困难： 全局变量的数据竞争 库函数不全都是reentrant. 比如假设向一块buffer内写入数据，此时切换到另一个thread,修改了buffer,在回到原tread,此时就会发生错误。write函数并不确保可重入 方案 重写库函数， 不是个good idea 对每个procedure传入一个jacket,用一些位数来表明正在被使用的库函数。如果要切换到的新进程要使用原进程的库函数，而原进程的库函数还没有结束，该切换就被禁止。 这个方案极大地限制了并行 线程各自的stack的管理， 由于kernel不知道多thread的存在（它们都在user space），那么很难在内存中正确地管理各个thread的stack 以上困难并非不可克服，但它们揭示了：在已有系统内加入多线程机制而不对系统本身做出改变是不可能的 Interprocess Communication TODO 急的话请催一催我QAQ Scheduling Classical IPC problems Summary","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"OS Basic","slug":"OS-Basic","permalink":"http://lyk-love.cn/tags/OS-Basic/"}]},{"title":"商业模式类型","slug":"商业模式类型","date":"2021-10-21T14:31:40.000Z","updated":"2022-09-26T06:39:34.943Z","comments":true,"path":"2021/10/21/商业模式类型/","link":"","permalink":"http://lyk-love.cn/2021/10/21/%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F%E7%B1%BB%E5%9E%8B/","excerpt":"Outline: 商业模式的重构 典型商业模式分类 分拆商业模式（Unbundled） 开放式的商业模式（Open） 多边商业模式（Multisided） 免费商业模式（Free） 长尾商业模式（Long-tail）","text":"Outline: 商业模式的重构 典型商业模式分类 分拆商业模式（Unbundled） 开放式的商业模式（Open） 多边商业模式（Multisided） 免费商业模式（Free） 长尾商业模式（Long-tail） 商业模式的重构 从业务转向“人” 背景:信息互联技术近四十年的高速发展 信息技术服务(大规模用户)的边界成本接近于零:倾向于取得垄断地位,庞大流量所带来的印 钞机式的盈利模式 人与人互联的成本极大地降低 实质上改变了人类社会的组织形式(交往、政治、企业) 共青团:B站、党员:学习强国 新技术与社会变革导致商业模式上大量&quot;以旧换新&quot;,企业或组织通过提出的产品或服务主张某种价 值(问题解决),并寻找到愿意为该价值主张的&quot;付费&quot;的客户群体 典型商业模式分类 分拆商业模式（Unbundled） 开放式的商业模式（Open） 多边商业模式（Multisided） 免费商业模式（Free） 长尾商业模式（Long-tail） 分拆商业模式 商业版的“关注分离” 企业内部的三类规则：经济、竞争与文化 由此可以区分三种活动：客户关系管理、新产品开发、基础设施管理 客户关系管理 找到和赢得客户并与他们建立关系 新产品开发 开发新的、有吸引力的产品和服务 基础设施管理 建立并管理可处理大量重复任务的平台 对应三种价值信条：亲近顾客、产品领先、运营卓越 三类活动驱动因素不同，彼此之间冲突，企业内部消长（难共赢） 解决方案：分拆！各自独立 ** ** 新产品开发 客户关系管理 基础设施管理 经济规则 早期市场，强调速度与高溢价 获客成本高，提高客单价，复用渠道 高产量平摊高固定成本，扩规模 竞争规则 能力之争：门槛低，小玩家多 范围之争：少量的大玩家主导市场（7：3：1） 规模之争：已经固化的大玩家市场（8：2） 文化规则 以员工为核心：呵护创意明星 高度服务导向：客户第一心态 聚焦成本：标准化、可预期、生产效率 案例 私人银行(三种商业活动的集合) 传统意义上，私人银行机构是纵向整合的，因为外包成本高，处于保密义务和业务机密性的考虑，私人银行更倾向于企业内部处理全部事务。 大环境发生变化:保密不是大问题、专业服务提供商出现(业务外包成为可能) 有的私人银行选择外包业务，并转向专注于经营客户关系、提供咨询 有的私人银行选择集合三个因素，但是要避免其互相干扰影响 可口可乐 方向 简介 新产品(追求健康化：咖啡、茶、纯净水、运动饮料、无糖) 茶、运动饮料、纯悦纯净水、灌装Costa咖啡(英国上市，减糖)、纤维+可乐(原产日本)、咖啡可乐(原产巴西)、爽椰派雪碧、无糖芬达适应无糖趋势(元气森林的崛起) 客户关系(本地品牌收购、产品运营、广告宣传) 1. 大量收购各国知名饮料品牌(尼日利亚Chi，收购汇源失败，约500+产品)2. 从diet coke到zero：配方基本不变，摆脱女性专属形象3. 外包装营销：与你的&quot;女神/闺蜜&quot;共享 – 电视剧名台词 – 小瓶、时尚罐与小罐 基础运营：(调整灌装线) 1. 外包灌装 – 收购灌装厂 – 再次外包灌装(国内为太古和中粮)2. 可口可乐如何在二战期间从美国政府处得到足够的糖供给 开放的商业模式 系统性与外部伙伴合作 企业的研发流程对外敞开 “由外到内”：将外部的理念、技术或知识产权引入内部 “由内到外”：将内部的知识产权或技术，特别是闲置资产向外出售 封闭创新 开放创新 笼络领域内最聪明的人 与公司内外最聪明的人协作 自己发现需求、开发并完成整个活动，最终获益 引入外部的研发活动以获益；将内部的自主研发活动转出，从而获益 获胜条件：实践大多数最好的研发工作 尝试从非原创成果中获益 获胜条件：创造大多数最好的理念 结合来自内部和外部的理念 管控内部创新流程，防止外泄 出让创新以获益；为获益购买外部知识产权 由外到内 适用于拥有强势品牌、强大分销渠道与良好客户关系的大公司 进一步挖掘客户关系的价值 外部组织能提供有价值的见解、知识、专利、甚至现成的产品 要能有效获取外部特定资源，要有对外协作的专门业务 外来意味着成本，但可以缩短上市时间，提升内部开发效率 帮助企业实现跨越式发展（中国高铁） 由内到外 以研发为核心的组织往往产生许多内部无法实用化的知识、技术、和智力资产 将闲置资产变现，增加收入 因战略或运营原因对内无价值的研发成果可能对其它行业意义重大 帮助企业聚焦核心战略，鼓励内部创新 军转民，航天科技，高校成果转化，农科扶贫 案例 宝洁 宝洁挽救公司的举措是建立一套新的创新开放体系:聚焦于内部研发的方式转变为开放式的研发流程，关键因素就是实施&quot;连接 &amp; 发展&quot;战略，旨在通过外部合作方力量拉动内部的研发活动 核心的三座桥梁: 高科技企业家：同宝洁建立了关系的高级科学家，扮演&quot;猎人&quot; 互联网平台:通过内部互联网，宝洁可以和全球专家连接 退休专家:向已退休的专家征询知识。 帮宝适，世界上第一款一次性纸尿布，其防漏涂层技术来自外部 与日本尤妮佳纸尿布常年竞争，在欧美引入其日本市场热销除尘器，年入两亿美元，合作用时18个月 格力 由外到内 2014年以前，国内高端中央空调机组主要靠引入并组装丹麦的丹佛斯磁悬浮压缩机 2014年，格力研发四年的&quot;磁悬浮变频离心式制冷压缩机及水冷机组&quot;成功，开始宣传&quot;核心科技&quot; 2015年10月中标CAP1400，2016年3月中标CAP1000，同时为核电站机组与核电站控制区提供制冷服务 由内到外： 2019年11月向丹麦知名水泵生产商格兰富提供新型无稀土磁阻电机领域3个系列的专利技术 连接器 1997年开始设立格力科技进步奖，2018年总奖金5000万人民币 鼓励内部实现从宏观(核心技术、空调芯片)到微观(生产线环节优化)的研发攻关 与国内24所高校建立&quot;卓越人才伙伴&quot; 多边平台商业模式 多边平台(又称多边市场)将两个或更多独立但相互依存的客户群体连接到一起。 价值主张一般集中体现在三方面： 吸引用户：网络效益，平台的价值在于它所吸引的用户数量的增加。 群体配对：平台通过促进不同群体之间的互动而创造价值，群体无法独立生存。 利用平台交易渠道降低交易成本 多个收益流，补贴正确的客户群是定价决策的关键，无需关联具体生产过程，&quot;印钞机&quot; 核心资源是平台，成本主要来自于平台的维护和开发，三项关键活动：平台管理、服务实现、平台升级 多边平台会面临一个&quot;先有鸡还是先有蛋&quot;的两难问题 解决方案是向某一个客户群体发放补贴 平台需要使用低廉或免费的价值主张来吸引某一个群体加入平台，需要明确群体加入平台的目的，以及哪一边是需要给予补贴。 单个用户群体的价值本质上取决于平台中另一群体的用户数量 弄清楚哪一&quot;边&quot;能够更好的吸引其它&quot;边&quot;，从而提供免费服务甚至补贴，信用卡：商户结算交易费；索尼PS：低价游戏机；B站：观众免费，补贴Up主 免费商业模式（Free） 在其它方面补贴免费产品 至少有一个关键的客户群体可以持续免费地享受服务 不付费客户所得到的财务支持来自于另一个客户群体 对价格为0的商品的需求要数倍于定价为1分钱或更高的商品 数字产品与服务的复制传播成本接近于0（海量用户下边界成本也趋向于0） 三种可行的免费商业模式（ 共同点：至少一个群体将得到免费的商品 ） 广告模式：基于多边平台的免费商品 免费增值：免费的基本服务，可选的增值服务 诱饵&amp;陷阱：以免费或很便宜的初始价格吸引客户，并引导其重复购买 广告 基于广告的免费商业模式总结 好的产品和服务以及高流量会吸引广告商，进而补贴产品和服务 要考虑广告费能否支撑起产品服务质量 吞噬广告费的产品太多，流量红利已见底 成本：平台的开发和维护，以及可能的获客与维系成本 免费增值 收入形式：大量用户从免费服务获益，少量用户为增值服务付费 不到10%的用户会为增值服务付费 两个关键指标：关注免费用户服务成本（低边界成本）与增值用户转化率 免费增值模式总结 平台是最重要的资产，产生三部分成本: 可观的固定成本 免费账户的低边际成本服务 增值账户成本 比如人工服务 客户关系自动且低成本，免费用户向增值用户转化率是重要指标 收入来源三个重要公式 收入 = 用户数量增值用户比重增值服务价格增长率顾客流失率 服务成本 = 免费用户数免费服务成本+增值用户数增值服务成本 运营利润 = 收入 - 服务成本 - 固定成本 - 获客成本 平台发展新趋势 需要高水平、差异化的产品与服务（为免费增值提供空间） 反面例子：庆余年与腾讯VVIP 已被放弃，是平台低效的体现 反转免费模式举例：Cookpad食谱网站 免费的缺点是用户粘性低，通过收会员费来提高用户粘性 需要优质的服务和体验 年营收67亿日元（2013年），利润率40%；会员费占一半营业收入，广告占37%（一般为89%-97%） 可以不靠广告，靠会员挣钱 发展思路：丰富企业服务与媒体功能，推进线下运营与会员转化 诱饵&amp;陷阱 特点是初期以不贵的或者免费的价格提供有吸引力的商品，且该商品还将进一步地鼓励对相关产品或服务的不断消费。 免费手机 合约机：手机免费，套餐收费 3g神机：中兴N880，V880，U880 英雄联盟活动门票 谋财害命 知乎 诱饵：精英“俱乐部”时期的优质回答 封闭邀请制使得答主从“自我实现”角度产生高质量答案，并沉淀为平台引流的“诱饵”（搜索引擎的高权重、用户的口碑） 缺点：“俱乐部”形式商业价值有限（wiki），不成功的商业化转型导致早期精英流失；问答制的概括性不利于专业内容持续输出 陷阱：营销“圣地”、贩卖焦虑、对抗、装X情绪、平 总结 产品与后续产品之间要有紧密连接，从而使得极小收益的初始购买为后续高收益产品或服务的重复购买创造可能 关注后续产品交付，需要强大品牌支撑 重要成本结构 初始产品补贴与后续产品的成本 慢慢融入平台与免费增值 新套餐体验+自动续费 各类社交裂变式促销（“盖楼”） 游戏本体+DLC或平衡性无关道具 “又肝又氪”的游戏营销活动 长尾商业模式 多品类的产品销售 提供相当多种类的小众产品，每类卖出量相对很少，但汇总的销售收入可以与传统模式销售媲美 行业内20%的产品占据绝大多数销量 长尾模式专注于销售剩下80%内尽可能多的品类，并获得可媲美主流产品销售的收入 出现的原因（首次用于描述传媒领域） 生产工具的普及：文字发布、视频录制、生产与设计外包 销售渠道的普及：互联网 连接供需双方的搜寻成本降低：搜索、推荐、用户评级、社区 特点： 提供宽范围非热销品， 与热销品共存 也可能基于用户创造， 并由平台支持 要求： 需要同时找到小众客户和小众产品提供者 拥抱自主音乐人的网易云音乐、steam 依赖于多边平台去连接小众客户和产品 收入来源五花八门：广告、产品销售、订阅费 由于细分市场越来越多、越来越大，长尾模式也能挣到钱 案例 图书出版 图书出版的转型 生产与渠道的工具化 资产生产与销售转向佣金与平台使用费 宽泛内容、大范围读者 -&gt; 自助出版、小众作家与读者 国内：生产：选题会 -&gt; 出钱包销、渠道：出版商发行 -&gt; 赞助发行 -&gt; 零售 长尾模式的流行与原因 大量行业都有“长尾”的趋势 视听类节目：传统影视公司 - 平台自制剧 - 平台自制综艺 - PUGC（Professional User Generated Content） 娱乐圈：“四大天王” - 流量明显 各类消费品的联名款、众筹、定制 游戏里的各类皮肤和不影响游戏平衡的小道具 智能手机基于自选APP实现远超功能机的定制化与个性化（例： 手机主题） 原因 消费端：个人主义流行和社会资源丰富导致的需求释放 生产端：生产、设计与渠道在效率上的不断提升使得产销能力工具化、服务化 复用工具和渠道 渠道端：互联网技术不断发展导致的供需双方匹配更加便利 社会端：人类社会中的流行因素与趋势三年一小变、五年一大变 长尾模式的共性与发展趋势 对生产环节（含渠道）的标准化程度要求较高（媒体、影视、游戏、一般日用品、食品、服饰） （除创意、构思之外）生产设计环节一般仍依赖于大型厂家，渠道依赖互联网（社区也可能是创意、构思的来源） “长尾之后”：突破因传统生产、设计、营销导致的二八曲线，长尾部分扁平化；形成若干“小众中心”，并分别向“大众中心”转化","categories":[{"name":"Business","slug":"Business","permalink":"http://lyk-love.cn/categories/Business/"}],"tags":[{"name":"需求与商业模式创新","slug":"需求与商业模式创新","permalink":"http://lyk-love.cn/tags/%E9%9C%80%E6%B1%82%E4%B8%8E%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F%E5%88%9B%E6%96%B0/"}]},{"title":"OS lab1","slug":"OS lab1","date":"2021-10-20T13:37:22.000Z","updated":"2022-09-26T06:39:34.935Z","comments":true,"path":"2021/10/20/OS lab1/","link":"","permalink":"http://lyk-love.cn/2021/10/20/OS%20lab1/","excerpt":"Outline： 汇编问答 调用约束 汇编避坑 NASM tutorial NASM教程","text":"Outline： 汇编问答 调用约束 汇编避坑 NASM tutorial NASM教程 汇编问答 用于OS第一次作业 请简述 80x86 系列的发展历史 1978年6月,intel推出第一款16位微处理器8086,采用20位地址线 1982年发布80286,主频提高至12MHz 1985年发布80386,处理器变为32位,地址线扩展至32位 1989年发布80486,1993年发布80586并命名为奔腾 说明小端和大端的区别,并说明 80x86 系列采用了哪种方式? 小端存储：数据的高字节保存在内存的高地址中，而数据的低字节保存在内存的低地址中 大端存储反之 80x86采用小端存储 8086 有哪五类寄存器,请分别举例说明其作用? 通用寄存器： 数据寄存器: AX，BX，CX，DX AX (Accumulator)：累加寄存器，也称之为累加器； BX (Base)：基地址寄存器； CX (Count)：计数器寄存器； DX (Data)：数据寄存器； 指针寄存器: SP (Stack Pointer)：堆栈指针寄存器； BP (Base Pointer)：基指针寄存器； 变址寄存器: SI (Source Index)：源变址寄存器； DI (Destination Index)：目的变址寄存器； 控制寄存器: IP (Instruction Pointer)：指令指针寄存器； FLAG：标志寄存器； 段寄存器: CS (Code Segment)：代码段寄存器； DS (Data Segment)：数据段寄存器； SS (Stack Segment)：堆栈段寄存器； ES (Extra Segment)：附加段寄存器； 什么是寻址?立即寻址和直接寻址的区别是什么? 找到操作数的地址(从而能够取出操作数） 区别： 立即寻址 MOV AX 1234H 直接给出了操作数,事实上没有“寻址” 直接寻址 MOV AX [1234H] 直接给出了地址1234H,用[]符号取数 请举例说明寄存器间接寻址、寄存器相对寻址、基址加变址寻址、相对基址加变址寻址四种方式的 区别 寄存器间接寻址 MOV AX [BX] 操作数有效地址在寄存器之中(SI、DI、BX、BP) 寄存器相对寻址 • MOV AX [SI+3] 基址加变址 • MOV AX [BX+DI] • 把一个基址寄存器(BX、BP)的内容,加上变址寄存器(SI、DI)的内容。 相对基址加变址 • MOV AX [3+BX+DI] 请分别简述 MOV 指令和 LEA 指令的用法和作用? LEA：load effective address， 将一个内存地址直接赋给目的操作数 lea eax,[ebx+8]就是将ebx+8这个值直接赋给eax，而不是把ebx+8处的内存地址里的数据赋给eax。 MOV: 与LEA相反 请说出主程序与子程序之间至少三种参数传递方式 利用寄存器传递参数,就是把参数放在约定的寄存器中 利用约定的存储单元传递参数 利用堆栈传递参数 如何处理输入和输出,代码中哪里体现出来? 宏，系统调用 有哪些段寄存器 见上文 通过什么寄存器保存前一次的运算结果,在代码中哪里体现出来。 注释里写了 解释 boot.asm 文件中, org 0700h的作用 告诉汇编器,当前这段代码会放在07c00h处。所以,如果之后遇到需要绝对寻址的指令,那么绝对地址就是07c00h加上相对地址。 在第一行加上org 07c00h只是让编译器从相对地址07c00h处开始编译第一条指令,相对地址被编译加载后就正好和绝对地址吻合 boot.bin 应该放在软盘的哪一个扇区?为什么? first 开机,从ROM运行BIOS程序,BIOS程序检查软盘0面0磁道1扇区,如果扇区以0xaa55结束,则认定为引导扇区,将其512字节的数据加载到内存的07c00处,然后设置PC,跳到内存07c00处开始执行代码。 loader 的作用有哪些? 为了突破512字节的限制,我们引入另外一个重要的文件loader.asm,引导扇区只负责把loader加载入内存并把控制权交给他,这样将会灵活得多。 最终,由loader将内核kernel加载入内存,才开始了真正操作系统内核的运行。 跳入保护模式 最开始的x86处理器16位,寄存器用ax, bx等表示,称为实模式。后来扩 充成32位,eax,ebx等,为了向前兼容,提出了保护模式 必须从实模式跳转到保护模式,才能访问1M以上的内存。 启动内存分⻚ 从kernel.bin中读取内核,并放入内存,然后跳转到内核所在的 开始地址,运行内核 跟boot类似,使用汇编直接在软盘下搜索kernel.bin 但是,不能把整个kernel.bin放在内存,而是要以ELF文件的格式读取并 提取代码。 解释 NASM 语言中[ ]的作用 解地址 解释语句 times 510-($-$$) db 0 ,为什么是 510? $ 和 $$ 分别表示什么? 填充剩下的空间，使得生成的二进制代码恰好为512字节（ dw是两个字节 ） $表示当前行的偏移地址, $$表示当前段的起始偏移地址, 解释配置文件 bochsrc 文件中如下参数的含义 megs:32display_library: sdlfloppya: 1_44=a.img, status=insertedboot: floppy megs:虚拟机内存大小 (MB) display_library:bochs使用的GUI库,在Ubuntu下面是sdl floppya:虚拟机外设,软盘为a.img文件 boot:虚拟机启动方式,从软盘启动 调用约束 64 位 Linux 写一个集成了 C 语言库的程序时，你必须遵循以下的调用约束条件: 传递参数时，按照从左到右的顺序，将尽可能多的参数依次保存在寄存器中。存放位置的寄存器顺序是确定的： 对于整数和指针，rdi，rsi，rdx， rcx，r8，r9。 对于浮点数（float 和 double 类型），xmm0，xmm1，xmm2， xmm3，xmm4，xmm5，xmm6，xmm7。 剩下的参数将按照从右到左的顺序压入栈中，并在调用之后 由调用函数推出栈 。 等所有的参数传入后，会生成调用指令。所以当被调用函数得到控制权后，返回地址会被保存在 [rsp] 中，第一个局部变量会被保存在 [rsp+8] 中，以此类推。 栈指针 rsp 在调用前必须进行 16 字节对齐处理 。当然，调用的过程中只会把一个 8 bytes 的返回地址推入栈中，所以当函数得到控制权时，rsp 并没有对齐。你需要向栈中压入数据或者从 rsp 减去 8 来使之对齐。 调用函数需要保存如下的寄存器：rbp，rbx，r12，r13，r14，r15。其他的寄存器可以自由使用。 被调用函数也需要保存 XMCSR 的控制位和 x87 指令集的控制字，但是 x87 指令在 64 位系统上十分少见所以你不必担心这点。 整数返回值存放在 rax 或者 rdx:rax 中，浮点数返回值存放在 xmm0 或者 xmm1:xmm0 中。 Tips 栈是由高地址向低地址增长的. 传递参数时，按照从左到右的顺序，将尽可能多的参数依次保存在寄存器中。存放位置的寄存器顺序是确定的： 对于整数和指针，rdi，rsi，rdx， rcx，r8，r9。 对于浮点数（float 和 double 类型），xmm0，xmm1，xmm2， xmm3，xmm4，xmm5，xmm6，xmm7。 剩下的参数将按照从右到左的顺序压入栈中，并在调用之后 由调用函数推出栈 。 等所有的参数传入后，会生成调用指令。所以当被调用函数得到控制权后，返回地址会被保存在 [rsp] 中，第一个局部变量会被保存在 [rsp-8] 中，以此类推。 局部变量的入栈顺序： 在没有溢出保护机制下的编译时，我们可以发现，所有的局部变量入栈的顺序（准确来说是系统为局部变量申请内存中栈空间的顺序）是正向的，即哪个变量先申明哪个变量就先得到空间， 也就是说，编译器给变量空间的申请是直接按照变量申请顺序执行的。 在有溢出保护机制下的编译时，情况有了顺序上的变化，对于每一种类型的变量来说，栈空间申请的顺序都与源代码中相反，即哪个变量在源代码中先出现则后申请空间；而对不同的变量来说，申请的顺序也不同，有例子可以看出，int型总是在char的buf型之后申请，不管源代码中的顺序如何（这应该来源于编译器在进行溢出保护时设下的规定）。 汇编避坑 慎用宏定义，它内部无法跳转和定义标签，因此无法封装复杂逻辑 ; 同时，使用时要记住宏定义内部对寄存器的更改 可以push rax, 并使用al, 事实上你无法push al，PUSH 指令只对至少16位的寄存器使用","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"OS Basic","slug":"OS-Basic","permalink":"http://lyk-love.cn/tags/OS-Basic/"}]},{"title":"国史大纲","slug":"国史大纲","date":"2021-10-19T19:25:17.000Z","updated":"2022-09-26T06:39:34.943Z","comments":true,"path":"2021/10/20/国史大纲/","link":"","permalink":"http://lyk-love.cn/2021/10/20/%E5%9B%BD%E5%8F%B2%E5%A4%A7%E7%BA%B2/","excerpt":"Outline: 中国文明的起源 ref: 国史大纲 to be continued","text":"Outline: 中国文明的起源 ref: 国史大纲 to be continued 中国文明的起源 ref：《中国文明的起源》-- 夏鼐 六大古文明：埃及、两河流域、 印度、墨西哥、秘鲁、中国 埃及、两河流域、 印度相互影响 墨西哥、秘鲁和旧大陆无关，是独立发展的文明 中国文明主要是自身独立发展起来，但不排除受到一些外来影响 文明起源的早晚 文明： 《易经》: ”天下文明“ 孔颖达：”有文章而光明也“ 现代：社会已从氏族制度解体而进入有了国家组织的阶级社会的阶段。这种这回中，除了政治组织上的国家外，已有城市作为政治（宫殿和宫署）、经济（手工业和商业）、文化（包括宗教）活动的中心 文明的最重要标志：文字 欧洲的缘故文化只有爱琴-米诺文化，有文字，可称为”文明“。 此外，欧洲的各种史前文化，虽然有的进入青铜时代，甚至铁器时代，都不能称为”文明“ 文明追溯： 偃师二里头文化 郑州二里岗文化 小屯的殷墟文化 商朝后期的首都，有都市规模 已经是高度发达的文明，因此不能视为中国文明的诞生（不可能一开始就如此成熟），有人因此认为中国文明是西来的 有都市、文字和青铜器 可向上追溯，第一步是郑州二里岗文化 商殷时代的文字制度 商代使用”六书“以记录语言 六书:象形、指事、会意、形声、转注、假借， 实际指象形、象意、象声 象意：用两个或更多的象形字合为一字（人言为信），或用几个不成字的点划表示意思（上下为事） 象声：用同音的象形字以代表无法象形或象意的抽象概念或”虚字“（假借);；或于同音的象形字之外，又加一表示含义的象形字（部首），合成一字（形声）。这样使用不同部首，可以区分同音而异意的字 甲骨文能记录史事，表明小屯文化已进入历史时期，不仅只是有了文字而已 已经发达的青铜器铸造技术 商代青铜器冶炼技术突出 使用复合范，而西方各文明使用失蜡法 冶铸青铜需要一批熟练掌握该技术的熟练工匠，一定的贸易活动和保持交通路线的畅通，这样才能解决原料和产品的运输问题。这又需要社会组织和政治组织上有一定的效率。 殷墟文化独有的特点 玉石雕刻 使用架马的车子 陶业发展，灰陶占绝对优势 郑州二里岗文化 早于小屯殷墟文化，属于商文明 偃师二里头文化 至少在晚期，已达到了文明的阶段 与较晚的文化比较，是直接与郑州二里岗文化，间接与小屯殷墟文化有承继关系 偃师二里头文化的晚期相当与夏末商初，但没有确切证据将它与传说中的夏联系起来 文明的起源和新石器文化 也有人认为文明的标准是新石器时代 ”新石器革命“： 指人类发明农业和畜牧业而控制了食物的生产这一过程 中国的新石器时代遗址很多。 其中的半坡遗址，是仰韶文化早期的代表 长江流域发现了河姆渡文化。 是除了良渚文化外的又一个中国早期文化中心 庙底沟二期文化的发现，证实了仰韶到河南龙山文化的过渡期的存在 龙山文化以光亮的黑陶著名，大汶口以另一种风格的彩陶著名。大汶口文化与仰韶文化相对","categories":[{"name":"History","slug":"History","permalink":"http://lyk-love.cn/categories/History/"}],"tags":[{"name":"Chinese History","slug":"Chinese-History","permalink":"http://lyk-love.cn/tags/Chinese-History/"}]},{"title":"OS Basic","slug":"OS-Basic","date":"2021-10-05T20:28:21.000Z","updated":"2022-09-26T06:39:34.935Z","comments":true,"path":"2021/10/06/OS-Basic/","link":"","permalink":"http://lyk-love.cn/2021/10/06/OS-Basic/","excerpt":"My understanding of OS","text":"My understanding of OS Intro appearances can be deceiving. The program that users interact with, usually called the shell when it is text based and the GUI when it uses icons, is actually not part of the operating system, although it uses the operating system to get its work done Main Components Hardware The hardware consists of chips, boards, disks, a keyboard, a monitor, and similar physical objects. Software On top of the hardware is the software. Most computers have two modes of operation: kernel mode and user mode. OS runs on the bare hardware and provides the base for all the other software kernel mode the most fundamental piece of software, runs in kernel mode (also called supervisor mode). In this mode it has complete access to all the hardware and can execute any instruction the machine is capable of executing. user mode The rest of the software runs in user mode, in which only a subset of the machine instructions is available. In particular, those instructions that affect control of the machine or do I/O )Input/Output&quot; are forbidden to user-mode programs The user interface program, shell or GUI, is the lowest level of user-mode software, and allows the user to start other programs (shell的概念详见Shell Tools) user mode 和 kernel mode 的区别并不严格 in embedded systems (which may not have kernel mode) or interpreted systems (such as Java-based systems that use interpretation, not hardware, to separate the components). Also, in many systems there are programs that run in user mode but help the operating system or perform privileged functions 总之， Everything running in kernel mode is clearly part of the operating system, but some programs running outside it are arguably also part of it, or at least closely associated with it WHAT IS OPERATING SYSTEM The OS as an Extended Machine top-down view: operating system as primarily providing abstractions to application programs The architecture (instruction set, memory organization, I/O, and bus structure) of most computers at the machine-language level is primitive and awkward to program, especially for input/output.即： 计算机的底层是丑陋和复杂的， 需要抽象出中间层才能方便使用 This abstraction is the key to managing all this complexity（圣经！）. Good abstractions turn a nearly impossible task into two manageable ones. The first is defining and implementing the abstractions. The second is using these abstractions to solve the problem at hand The job of the operating system is to create good abstractions and then implement and manage the abstract objects thus created OS的真正用户是APP(和APP开发者)， 而end users使用的是UI（shell or GUI）提供的抽象。 While the abstractions at the user interface may be similar to the ones provided by the operating system, this is not always the case. 如Windows Desktop和command prompt采用了Windows提供的相同的system abstractions。 Linux的Gnome、KDE和X Window System看起来差别很大，实际上也采用了相同的system abstractions The OS as a Resource Manager bottom-up view: the operating system is there to manage all the pieces of a complex system Resource management includes multiplexing (sharing) resources in two different ways: in time and in space COMPUTER HARDWARE 注意，这里只介绍现代计算机的基本结构，没有包括GPU和VRAM等组件， 也没有包括对各种存储介质的介绍( RAM, ROM, Disk)。 GPU的内容(不包括VRAM)详见拙著GPU 存储介质(包括Cache)和VRAM的内容详见拙著Computer Storage Top-view Example 假设计算机中存在hello程序:echoh hello,world 我们以在shell上输入./hello来执行hello程序为例， 在硬件视角下，程序执行步骤如下: shell程序持续运行，等待用户输入，用户输入通过I/O bus传输到I/O bridge, 再传输到system bus, 经过bus interface传输到reister file. 然后沿上图的方向传输到内存 当用户输入Enter键，shell知道我们停止了输入，就把hello程序从磁盘中读到内存: 最后，内存中的数据被读到CPU， CPU执行该程序, 将结果输出到显示设备 这一步(数据从内存传输到CPU)详见下文Accessing Main Memory 我们也可以看到，硬盘到主存的data-movement是很频繁也很耗时间的，因此诞生出了Cache，这是CPU内的存储结构，采用SRAM。 Buses bus: 总线。 就是一组传输地址、数据和控制信号的电线。 总线传输数据的基本单位是word， 在不同的硬件实现上，word的大小也不同，通常是4或8 Byte 基于不同的总线设计， 数据和地址可以共享总线，也可以使用不同总线。 不同设备也可以共享总线。 总线传递控制信号，来对事务进行同步。 总线的类型： system bus memory bus I/O bus： 被多个I/O设备共享 original IBM PC最早使用了单总线架构, 为了更快的IO速度和CPU-to-memory traffic，额外的总线被引入。 形成了现代的x86系统 This system has many buses (e.g., cache, memory, PCIe, PCI, USB, SATA, and DMI), each with a different transfer rate and function. The operating system must be aware of all of them for configuration and management. The main bus is the PCIe bus. the CPU talks to memory over a fast DDR3 bus, to an external graphics device over PCIe and to all other devices via a hub over a DMI (Direct Media Interface) bus. The hub in turn connects all the other devices, using the Universal Serial Bus to talk to USB devices, the SATA bus to interact with hard disks and DVD drives, and PCIe to transfer Ethernet frames. PCI接口的设备另外放在一个hub processor 内 Moreover, each of the cores has a dedicated cache and a much larger cache that is shared between them. Each of these caches introduces another bus. 每个core有一个专用cache，所有core共享一个更大的cache，每个cache拥有自己的总线 Intel 和 MS设计了plug and play系统，可以自动收集IO设备信息，集中分配中断优先级和I/O（设备的）寄存器地址 在此之前，这些都要手动分配 CPU Components CPU是计算机指令的执行单元。 由四部分组成: program counter (PC): 一个 word-size storage (or register)，也称为栈指针，永远指向当前正在执行的指令的地址 register file: 一个small storage device, 由一组word-size registers组成，每个寄存器都有其独特的名字 arithmetic/logic unit (ALU)： 用于计算 bus interface：一组电路，用于CPU和主存之间的数据传输 Features 每个CPU都有自己特殊的指令集。ARM处理器不能执行x86程序 由于对内存的存取所花的时间比执行一条指令还要长，CPU内部会有寄存器。 许多寄存器对用户可见 PC 栈指针，指向内存中的栈顶。栈含有每个程序的帧。栈帧含有： 输入的参数、局部变量、没有保存进寄存器的临时变量 PSW(Program Status Word): 包含状态码位。 用户程序通常能读整个PSW，但仅仅只能写其中的一部分 为了提供性能，CPU通常有流水线和超标量。 流水线会把底层的复杂度暴露给上层，令人头疼 超标量： 多个指令同时被取指和译指，然后放到一个Holding buffer里，每当一个执行单元空闲时，就去Holding buffer中找它能处理的指令。该设计的一个后果是：指令不一定按顺序执行。而执行结果能不能和顺序实现的CPU相同，取决于硬件。 除了嵌入式系统中的少数，CPU都有 user mode和kernel mode. 当运行在kernel mode， CPU可以执行指令集提供的每一条指令和硬件提供的所有特性。 在 桌面机和服务器上，OS都运行在kernel mode. 在大部分嵌入式系统中，OS一部分运行在kernel mode, 剩余部分运行在 user mode 用户程序永远运行在 user mode，只能执行一小部分指令和一小部分硬件特性。 “设置PSW的mode bit以进入 kernel mode”也是被禁止的 为了获得OS的服务，用户程序会发起system call,即陷入kernel并启动OS the TRAP instruction switched from user mode to kernel mode and starts the OS.执行结束后，控制会返还到TRAP的下一条指令 计算机的trap不仅有用于执行system call的指令，还有别的 CPU Operations basic cycle of CPU: 取指、译指、执行 CPU Instructions 抽象地说， CPU指令分为四种： Load: 从主存中copy一个byte或word到一个寄存器里， 这会覆盖寄存器里该位置上之前的值 Store: 从寄存器里copy一个byte或word到主存的某个位置，这会覆盖主存里该位置上之前的值 Operate: 将两个寄存器里的值copy到ALO， 后者进行算数计算，并将结果存入一个寄存器，这会覆盖该寄存器里该位置上之前的值 Jump: 该指令含有一个word， 记录了要跳转到的目标指令的位置。 CPU会copy该字段到PC，覆盖PC之前的值 Multithread and Multicore Chips multithreading: 允许CPU拥有两个线程并且在纳秒级的时间里切换。 多线程并不是并行，因为同一时间还是只有一个程序在CPU上运行 如果一个程序要读取内存（花的时间比较长），那么多线程的CPU可以切换到另一个程序 多线程带来的问题是：每一个线程都会被OS视为独立的CPU. 考虑一个双CPU的系统，每个CPU有两个线程。当有两个线程的任务时，OS可能会把它们全部交给同一个CPU上的两个线程，而这是很浪费的 许多CPU芯片上还有多个(几十个)processors or cores 使用多核芯片需要一个多处理器的OS GPU拥有上千个core, 适用于许多并行执行的小规模计算， 在顺序计算上并不突出。 Instruction Set CPU指令集可以分为CISC和RISC两类. x86和arm的那些历史就不讲了. 这里理清一些概念. x86和x86_64: x86是32位的, x86_64是x86的64位版本 arm, arm64和aarh64: 顾名思义, arm是32位的, arm64是arm的64位版本. 并且arm64和aarh64合并了, 所以arm64和aarh64是一个东西. M1的指令集就是arm64. 低功耗的移动设备和嵌入式设备基本也都是arm架构. 因为桌面级CPU基本都是64位的, 要么是x86_64, 要么是arm64, 所以我们常说的x86, arm其实指代的是它们的64位版本. arm和x86: 简单来说, arm属于RISC, 功耗较低. x86属于CISC, 性能较高. Main Memeory main memory（OR system memory ）：主存。 在冯诺依曼架构中，处理器( 包括ALU和Controller )与存储器进行数据交互。 我们通常就用RAM来指主存。 更精确地说，主存的材质是DRAM 此外还存在显存(VRAM)， 宽泛地讲， 和CPU交互的是主存，和GPU交互的就是显存 在通常的语境下，我们说的RAM都是主存，也就是和CPU交互的DRAM Accessing Main Memory 数据通过bus( 总线， 见下文 )在CPU和主存间传输。 每一次CPU和主存间的数据传输就是一次bus transaction 整个过程的I/O类型是DMA(见下文I/O Devices -&gt; I/O -&gt; DMA) A read transaction transfers data from the main memory to the CPU. A write transaction transfers data from the CPU to the main memory. 上图展示了CPU和主存进行数据传输的基本模型。 包含了三个组件：CPU、 I/O bridge（ 是一个芯片组， 包含了memory controller）、主存。 CPU和 I/O bridge通过system bus连接， I/O bridge和主存通过memory bus连接。 当然如同之前介绍的, I/O bridge还会把system bus和memory bus连接到I/O bus，它由I/O设备共享 考虑如下指令: movq A,%rax # 将地址A指向的内容赋值给rax CPU的bus interface会开始一个write transaction, 步骤为: CPU将地址A放到系统总线上， I/O bridge将该signal传递给memory bus 主存感知到memory bus上的信号(地址A)， 从DRAM中读取A处的数据x，写进memory bus， I/O bridge会将其传递给system bus 主存感知到memory bus上的信号( 数据x )，从memory bus读数据，然后copy到%rax Disk Disk：称为硬盘，是非易失的外部存储设备。 I/O Devices I/O设备的介绍详见拙著Computer I/O Device I/O IO有三种类型： busy waiting：用户程序发起system call, kernel将其转换为procedure call交给相应的driver. Driver启动IO设备并且对设备轮询。IO结束后，driver将数据（如果有的话）返回，OS将控制返还给调用者 占用了CPU的全部时间 interrupt：driver开启设备，要求它在完成时发出中断. At that point the driver returns. The operating system then blocks the caller if need be and looks for other work to do. DMA(Direct Memory Access): DMA芯片可以在避免CPU持续干预的情况下控制内存和一些controller之间的数据传输 CPU启动DMA芯片，告诉它how many bytes to transfer, the device and memory addresses involved, and the direction, and lets it go. 当DMA芯片工作结束时，会发出中断（和方法二一样） 中断的过程： driver通过向contoller的device registers写入来告诉controller要做什么。controller然后便启动device When the controller has finished reading or writing the number of bytes it has been told to transfer, it signals the interrupt controller chip using certain bus lines 如果中断控制器准备好接受新中断（ 如果它在处理一个更高优先级的中断，就不会ready ）,it asserts a pin on the CPU chip telling it 中断控制器把device number放在总线上，CPU就可以read并且知道是哪个设备刚刚finished 一旦CPU决定接受中断, PC和PSW会被压栈，CPU会切换到内核态 The device number may be used as an index into part of memory to find the address of the interrupt handler for this device. This part of memory is called the interrupt vector 一旦 interrupt handler启动，它就将PC和PSW出栈并保存，并向设备询问。当所有的handeller结束后，就退回到终端前执行的程序 中断可能会在其它interrupt handler运行时发生。 因此CPU可以disable interrupts and then reenable them later。当CPU disable interrupts时，设备依然持续发出中断信号，但CPU不会接受。 由于多个中断可能同时发生，每个设备都有（通常是静态的）中断优先级来决定在disable结束后，哪个中断先被接受。 Booting the Computer 计算机启动时，首先加载硬件驱动程序，硬件驱动程序有BIOS和UEFI， 这里简要介绍BIOS, 详见拙著Linux Hardware Basic BIOS对应的磁盘分区格式是MBR BIOS（Basic Input Output System）：PC主板上的一个程序 The BIOS contains low-level I/O software, including procedures to read the keyboard, write to the screen, and do disk I/O, among other things. Nowadays, it is held in a flash RAM, which is nonvolatile but which can be updated by the operating system when bugs are found in the BIOS. 计算机开机时，BIOS启动，它会检查RAM和键盘等基础外设的连接和响应情况 It starts out by scanning the PCIe and PCI buses to detect all the devices attached to them. 如果设备和上次启动时不一样，新的设备将被配置 通过CMOS中的启动设备列表，BIOS找到启动设备，后者的第一个扇区将被读进内存并执行，检查该扇区是否是MBR扇区，是的话会加载其中的boot loader 一段boot loader程序被从MBR扇区读入， boot loader是OS提供的，因此能够读取OS的核心文件 boot loader读取核心文件 核心文件会加载OS OS问询BIOS，得到配置信息，对每个设备，OS检查自己是否有对应驱动，若没有，则通知用户去下载。 当所有设备驱动都齐备时，OS将它们加载进kernel. 并初始化分区表，启动各种程序....(如登陆程序和GUI) The OS Zoo Mainframe Operating Systems mainframes：体型巨大，高I\\O capacity 用途： batch： 不需要用户在场交互，即可处理程序 transaction processing, and timesharing Timesharing systems，允许许多远程用户同时登陆一台主机处理任务, such as querying a big database. 正被UNIX的各种变体（如LINUX）取代 Server Operating Systems They serve multiple users at once over a network and allow the users to share hardware and software resources. Typical server operating systems are Solaris, FreeBSD, Linux and Windows Server 201x. Multiprocessor Operating Systems 把多个CPU连接称为一个系统。属于server OS的变体，增加了通信、连通性等功能 Many popular operating systems, including Windows and Linux, run on multiprocessors Personal Computer Operating Systems 无需多言 Handheld Computer Operating Systems A handheld computer，又称PDA(Personal Digital Assistant),是一个可以边手持边操作的小型计算机 Google’s Android and Apple’s iOS Embedded Operating Systems 和Handheld Computer OS区别在于：嵌入式OS中无法自主安装软件 ---- 软件都写在ROM里，这也意味着不需要提供app之间的保护 Sensor-Node Operating Systems 每个节点都是个小型计算机，通过一个基站进行无线通信 Each sensor node is a real computer, with a CPU, RAM, ROM, and one or more environmental sensors. It runs a small, but real operating system, usually one that is event driven, responding to external events or making measurements periodically based on an internal clock. 所有程序写在ROM里 Real-Time Operating Systems hard real-time system: actions absolutely must occur at a certain moment (or within a certain range) 如工业、航空、军事 soft real-time system: ddl没那么死 The categories of handhelds, embedded systems, and real-time systems overlap considerably Smart Card Operating Systems 没想到吧，smart card也有一个OS! 有小型CPU和ROM Some smart cards are Java oriented OS Concepts Processes A process is fundamentally a container that holds all the information needed to run a program address space: 进程可操作的一块内存。 包括了可执行的程序、程序的数据和栈指针 process table: an array of structures, one for each process currently in existence 大多数OS都有，储存了每个进程的所有信息,不只是地址空间 core image: address space of a ( suspended ) process ID: UID: each person authorized to used a system is assigned a UID by the system administrator 每个进程都拥有其创建者的UID， 子进程的UID就是父进程的UID GID： users can be members of groups Address Space 每个进程都拥有其可操作的地址空间，而address space是和物理内存解耦（借助虚拟内存）的 虽然避免内存中的进程互相干扰是硬件需要提供的机制，但该机制在软件层需要由OS控制 File File &amp; Directory: OS把底层千奇百怪的存储设备抽象成文件系统，并使用directory概念来管理 对文件和目录的许多操作都需要syscall pathname: 文件在 directory hierarchy 中有pathname, 后者最顶层是root directory, 由/表示。 从根目录开始的路径叫做绝对路径。 而每个进程又有自己当时的working directory， 其中的路径名不需要从根目录开始,例如对于绝对路径C:\\Docs\\sub\\javac, 当前工作目录是C:\\Docs 绝对路径：C:\\Docs\\sub\\javac 相对路径：sub\\javac ; 也可写成：.\\sub\\javac .表示当前目录，..表示上级目录 file descriptor在文件被读写前，它首先要被打开，此时需要检验许可，若许可成功，则系统返回一个file descriptor用于后续操作； 否则返回一个error code mount: 将CD-ROM上的file system挂载到root file system. 此时原有节点的数据不能被访问，因为这个节点已经被挂载了。由于挂在节点一般是空节点，这一般不会造成影响 special file：OS将I/O设备抽象为special file ，使得它可以用处理普通文件的syscall来处理这类设备，special file分两种，都挂载到/dev目录下 block special file：用于由随机寻址块组成的设备 .i.e 磁盘 character special file: 用于输入\\输出字符流的设备 .i.e 键盘 pipe： 用于进程间通信的伪文件，通过它，UNIX进程间通信就像是普通的文件读写 pipe的实现很像文件的实现，一个进程要想知道其输出文件是一个真实的文件还是一个pipe，只能使用syscall Protection protection code: each file in UNIX are protected by assigning each one a 9-bit binary protection code, which consists of three 3-bit fields, one for the owner, one for the owner's group, one for everyone else. 例如rwxr-x--x意味着拥有者可读可写可执行， 群组可读可执行，其他人可执行 Ontogeny Recapitulates Phylogeny 达尔文有个理论，“胚胎的成长会浮现其种族的演化”，这在计算机领域是很形象的。一个思想可能会因为技术创新而变得火热，然后湮没无闻，可思想本身是不死的。因此新技术的发展，其内核几乎总是一些“老旧”的思想。一项技术的发展所要经历的不同阶段，几乎也总是计算机发展史上的不同思想的重现 因此，不要歧视所谓的“obsolete idea” System Calls 对syscall的调用一般是高度依赖于机器的，并且是汇编语言，但会提供一个procedure library来允许在高级语言中调用syscall 每个单CPU的计算机同时只能处理一条指令， 如果一个user mode的程序需要system service， 它就会执行a trap instruction 来将控制权交给OS, 后者完成任务后再将控制权交还 因此syscall和普通的procedure call没什么区别，只是前者会陷入kernel mode System call执行步骤： 参数压栈 参数压栈 参数压栈 使用普通的procedure call调用library procedure, library procedure将 system-call number放入OS所期望的地方（通常是个寄存器） library procedure执行TRAP, 陷入kernel mode。 并从内核层的固定地址开始执行 TRAP指令和 procedure call的区别在于能陷入内核层，并且TRAP不能跳转到任意位置，只能跳转到固定位置，或者跳转到其参数中给定的位置 内核层检查system-call number, 并分发给对应的handler handler执行 handler执行完毕后，控制可能会交还给user space的library procedure system call也可以阻塞caller，比如在等待用户输入时。 此时OS会去执行其它进程 library procedure以通常的procedure call返回的方式返回user program user program像对待任何library procedure一样，清除栈顶 System Calls &amp; Library Functions 系统调用和库函数都以C函数的形式出现 系统调用： Linux内核的对外接口; 用户程序和内核之间唯一的接口; 提供最小接口 库函数： 依赖于系统调用; 提供较复杂功能 例：标准I/O库 System Calls for Process Management fork是POSIX中唯一的创建新进程的方法，它会创建一个和父进程完全相同的子进程，并返回子进程在父进程中的PID( PID在子进程中为0 ) 当一条指令被输入时。shell会fork出一个新的子进程，这个子进程必须执行这条用户指令，它首先会调用exec system call( 实际上该system call名字不一定叫exec ) exec(command,parameters,0) 其参数分别为：要执行的文件名； 参数数组的指针； 环境数组的指针 执行exec会导致该子进程的整个core image被exec第一个参数（即文件名）所指向的文件所替换 例子：执行cp file1 file2 shell fork出子进程,子进程执行cp这个文件，将源和目的文件的名字传给它 程序就是文件，cp既是程序，当然也是个文件 cp的内容是: main( argc, argv, envp ) argc: 参数数量，包括程序名在内，这里是3 argc：参数名（都是字符串）指针. argv[0]指向string &quot;cp&quot; envp: 环境指针， 一个string的键值对数组，这里没有环境要传递，因此是0 UNIX进程在内存中分三段： text segment: i.e., the program code data segment: i.e., the variables 向上增长 stack segment： 向下增长 System Calls for Directory Management mkdir, rmdir: create and remove empty directories link: allow the same file to appear under 2 or more names UNIX系统有一个i-node组成的表，每个i-node记录了这个文件的一些信息， 而UNIX File都有一个File number，是i-node的索引。 一个directory就是一个（ i-number, ASCII name ）pair的集合. link做的就是创建一个新的directory,它只有一个pair，其ASCII name可能不同，但i-number 是已有的文件的 Miscellaneous System Calls chmod: 改变文件的九位protection code chdir: 改变文件的工作目录 kill The Windows Win32 API UNIX的system call和library几乎是一一对应的， 但是Windows把它们解耦了，开发者们面对的是WIn32 API WIn32 API极其庞大和复杂，因此很难区分哪个调用了system call哪个没有 OS Structure Monolithic Systems 整个OS作为一个单一程序运行在kernel mode OS是一个procedures的集合， 每一个procedure都可以自由调用其它任何procedure，完全没有信息隐藏 当计算机启动时，不仅OS会被加载， 许多OS还支持loadable extensions,比如I/O驱动和文件系统。 它们在UNIX中称为 share libraries, 在Windows中称为DLLs Layered Systems OS由层级结构组成，每一层使用下一层的服务 最早是THE system,由Dijkstra研发 Layered system只是个设计目标，由于每个层最终都链接到一起，实际上整个OS是一个单一的程序 Layer Function 5 The operator (用户操作进程 ) 4 User programs 3 I/O management 3层以上不用处理IO细节 2 Operator-process communication 1 Memory management 0 processor allocation and multiprogramming Layered system的理念后来在MULTICS体现。 后者是由一系列同心环组成，内环拥有更大的权限，外环若要使用内环的服务，需要系统调用 Microkernels Layered system的设计者可以划分kernel-user的界限，传统的Layered system全部运行在kernel，而这是不必要的。microkernel的理念是将OS分离为不同模块，其中只有一个 --- 称作microkernel 运行在内核层，其余的运行在用户层，以实现高可靠性 除了OS X( based on Mach microkernel ), 通常的桌面OS都不用microkernel 著名的有塞班和MINIX 3 MINIX 3的结构： 内核层： system call handler和时钟的驱动（ 因为调度器经常要与时钟交互 ） 用户层（从下到上）： Drivers：各种设备驱动，由于不在内核层，因此无法直接对I/O port sapce操作，也无法直接发起I/O commands,只能构造一个structure发给内核 servers：为上层提供各种服务，其中有个reincarnation server 工作是检查其它server和driver的工作情况，若不正常，就自动替换，不需要任何用户干预。 User programs 也可将微内核的理念看作 “mechanism和policy分离”。 mechanism指的是机器自动执行的那部分， policy指的是需要用户指定的那部分， 内核层只需要管理mechanism Client-Server Model 不解释了 Virtual Machines VM/370 将multiprogramming和extended machine with a more convenient interface解耦 使用VM monitor（又称为hypervisor），它运行在硬件上，向上提供虚拟机。 虚拟机提供的不是extened machine，而是硬件， 没有任何抽象 其实虚拟机是将机器硬件抽象成了虚拟硬件提供给宿主机，实现了物理硬件的虚拟化。但是虚拟机不会给宿主机提供任何抽象API，它提供的还是硬件资源（尽管是虚拟的）。 这部分详见“OS VMM”，我的另一篇文章 由于每个虚拟机对应一块真实的硬件，所以每个虚拟机能运行独立的OS，比如OS/360, CMS( Conversational Monitor System )等 当一个CMS程序发起系统调用，系统会内陷到CMS( 而不是VM/370 ), 而CMS会发起普通的硬件指令调用， 这将会内陷到VM/370， 后者执行这些指令 Virtual Machine Rediscovered 虚拟化的前提是CPU支持虚拟化，而许多研究使得不支持虚拟化的CPU也能虚拟化（比如x86） Bochs: 解释器，使得CPU“能够”虚拟化，很慢。 后来有许多改进，这类软件称为machine simulator type2 hypervisor：建立在type1 hypervisor之上，即建立在一个宿主机OS之上。而type1 hypervisor建立在纯粹的硬件上 Exokernels 与传统的虚拟机不同的地方在于，exokernels为每个虚拟机分配资源，虚拟机知道自己被分配了多少资源。 这就避免了建立映射的损耗。传统的虚拟机都认为自己拥有整个硬件，因此得为它们建立映射层 Summary 计算机科学的发展日新月异，只有刻苦坚持，才能走在别人前面 refs： Modern Operating Systems Operating Systems Three Easy pieces CSAPP","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"OS Basic","slug":"OS-Basic","permalink":"http://lyk-love.cn/tags/OS-Basic/"}]},{"title":"我和我的父辈","slug":"我和我的父辈","date":"2021-10-03T00:48:32.000Z","updated":"2022-09-26T06:39:34.944Z","comments":true,"path":"2021/10/03/我和我的父辈/","link":"","permalink":"http://lyk-love.cn/2021/10/03/%E6%88%91%E5%92%8C%E6%88%91%E7%9A%84%E7%88%B6%E8%BE%88/","excerpt":"我和我的父辈： 8.0/10","text":"我和我的父辈： 8.0/10 第一单元 主旋律抗日，但父亲牺牲儿子的那段真的没想到，也特别煽情。这和配乐有很大关系，小提琴高亢热烈的音色就适合表现如火一般炽烈的革命情感。个人觉得，红色音乐都应该多多采用小提琴，只有小提琴才配得上那份热血和赤诚。隔壁《长津湖》就因为配乐不行，导致电影完全没有情感调动可言 说句明白话，这部电影的第一单元的几分钟，就比整部《长津湖》要精彩、动人。每一分钟我的眼泪都在眼眶里打转 吴京真忙啊 第二单元 讲研发火箭的工程师的事，章子怡执导 章子怡太强了！ 没想到她有这么强的执导能力，从中也能看出许多有别于张艺谋的地方（本来我以为她的内容只会是对张艺谋的模仿）。也许是女性导演的缘故，家庭和感情的戏份比较多。章子怡的演技也是无可挑剔 这一单元很美，章子怡和儿子在夜色中行走的那一幕极具美感，像是欧洲文艺片一般。可见子怡导演的功力 还很浪漫，“在天空写诗”哦 第三单元 “鸭先知”，讲一个总被嘲笑但是有远见的父亲 我个人最喜欢也最有共鸣的一个单元。徐峥把这个父亲角色演活了，我从中真的看到了我父亲的影子。 这一单元的父亲，没有前两个单元的那么伟大，只是个普通小市民，还总是被人嘲笑，被老婆欺负，完事儿了还要安慰儿子说爸爸其实很厉害，这才是大多数人心中父亲的样子吧。不管再卑微再看着好笑，爱孩子面前也永远一副乐观的样子 故事线：父亲想出了拍摄中国第一条广告以解决产品滞销的故事。 “春江水暖鸭先知”，我们要努力做那只先到水里的鸭子，这样才能把握机遇。 （之前我对徐峥的印象是一个油腻的中年男子，《人在囧途》《泰囧》让我意识到他是个电影天才，才华横溢。但《心花路放》又让我觉得此人满脑子中年人的肮脏思想：钱、性、势利眼这些。 即使在《无人区》《我不是药神》《一出好戏》这些名作中，他出演的也都是油腻男子的形象（不过演的是真好）。今天“鸭先知”让我见识到了，原来徐峥演的不只是中年人的油腻，还有那份阅历和沧桑。 那个胖乎乎的胆小怕事的，在妈妈面前又胆小又卑微的，但在你面前又永远开心乐观的，不就是你的父亲吗 第四单元 开心麻花。。。 懂得都懂。 深度和好看程度都不如前三个单元。 不过该单元也是让电影院笑声最多的一个单元，好笑这块儿腾哥还是玩得溜啊 沈腾这次真扮演了超级英雄似的角色，穿着高科技紧身衣，胸口还有个发光的三角体，隐射漫威哪位英雄我就不用说了吧。B站之前有沈腾和唐尼的换脸，没想到今天成真了 单元里，儿子的亲生父亲居然是个人工智能学者，看样子研究的还是强人工智能φ(*￣0￣)， 只能说respect。 不过这年头真有研究强人工智能的吗。。。","categories":[{"name":"Art","slug":"Art","permalink":"http://lyk-love.cn/categories/Art/"}],"tags":[{"name":"Movie","slug":"Movie","permalink":"http://lyk-love.cn/tags/Movie/"}]},{"title":"Neuroscience L2","slug":"Neuroscience-L2","date":"2021-09-16T16:26:40.000Z","updated":"2022-09-26T06:39:34.934Z","comments":true,"path":"2021/09/17/Neuroscience-L2/","link":"","permalink":"http://lyk-love.cn/2021/09/17/Neuroscience-L2/","excerpt":"电信号: 感受器电位. 突触电位, 动作电位 唤能： 机械刺激转换成电位变化 信息传递: 动作电位可以沿着轴突长距离传输","text":"电信号: 感受器电位. 突触电位, 动作电位 唤能： 机械刺激转换成电位变化 信息传递: 动作电位可以沿着轴突长距离传输 Electrical Potentials across Nerve Cell Membranes 膜电位（ membrane potential ）： 跨神经元膜的点位， 通常规定细胞外的电压值为0. 用。Vm表示, 其可用插入细胞质的微电极测量 细胞膜: 磷脂双分子层. 以脂质,蛋白质和少量糖为主要成分的半透性的膜. 相似相溶原理: 疏水性的分子, 离子可以通过, 水溶性的不行. 分子量比较小的分子也可能通过 离子转运体( Ion transporters ): 是一类跨膜蛋白分子所形成的, 可以耗费能量来实现特定粒子逆浓度梯度跟电位梯度的跨膜转运, 也被称为离子泵 离子通道( Ion channels ): 跨膜通道蛋白分子是具有离子选择性的门控通道, 可以允许特定例子顺浓度梯度运输 静息电位( Resting Potential )当细胞膜未收到人为刺激,处在静息态时,细胞膜两侧存在电位差, 其存在于细胞膜内外两侧的外正内负的电位差, 该电位在安静状态基本保持不变, 神经元静息膜电位通常为-65mV( 不同细胞的静息膜电位不一样 ) 静息状态下,要维持电势能的储备: 离子转运体利用ATP水解产生的能量(高能磷酸键), 来跨膜转运带电粒子( 将钠离子带到细胞外,钾离子带到细胞内) . 这是个耗能的过程( 消耗了ATP的30% ),进行了电势能的储备, 将离子逆浓度(逆电势差)运输. 离子平衡电位: 由于膜的半透性和选择性, 膜的两侧也具有电荷差 在静息状态下,神经元膜由于钾通道的存在而对K^+^有高通透性, K^+^顺其浓度梯度的跨膜迁移使得神经元内负电荷增加; 由于磷脂双分子层很薄( 7-8nm ), 因此, 神经元内的负电荷和膜外的正电荷因相互吸引从而分布在细胞膜的两侧了最终静息膜电位趋向于K^+^的平衡电位, -80mV 细胞内: [K]高 [Na]低, 静息膜离子通透率P~K+~为P~Na+~的40倍 [Cl]低 [Ca]低 除了K^+^, 其他离子通道也有开放. 因此静息膜电位是-65mV 极化( polarization ): 安静时细胞膜两侧处于外正内负的状态 超极化( hyperpolarization ): 膜的极化状态增强 去极化( depolarization ): 膜的极化状态减弱 反极化( reverse polarization ): 去极化至零电位后膜电位进一步变为正值, 使膜电位的极性与原来的极化状态相反 超射(overshot): 膜电位高于零电位的部分 复极化( repolarization ): 去极化后再向静息电位方向恢复的过程 阈值( threshold potential ): 去极化时, 钠离子进入, 使膜去极化, 细胞膜内表面电位减小, 但是去极化触发动作电位的产生必须达到一定的临界膜电位水平, 称为阈值, 膜电位去极化超过阈值就会产生动作电位, 这也叫做动作电位的&quot;全或无&quot; 上升相: 钠离子通道打开, 钠离子在离子驱动力下流入细胞, 趋于钠平衡电位 下降相: 钠离子通道关闭, 膜电位为正, 驱使钾离子外流, 趋于钾平衡电位","categories":[{"name":"Natural Science","slug":"Natural-Science","permalink":"http://lyk-love.cn/categories/Natural-Science/"}],"tags":[{"name":"Neuroscience","slug":"Neuroscience","permalink":"http://lyk-love.cn/tags/Neuroscience/"}]},{"title":"Optimization-Methods-Lecture2","slug":"Optimization-Methods-Lecture2","date":"2021-09-08T18:34:11.000Z","updated":"2022-09-26T06:39:34.937Z","comments":true,"path":"2021/09/09/Optimization-Methods-Lecture2/","link":"","permalink":"http://lyk-love.cn/2021/09/09/Optimization-Methods-Lecture2/","excerpt":"Outline： Norms Analysis Functions Derivatives Linear Algebra","text":"Outline： Norms Analysis Functions Derivatives Linear Algebra Norms Inner product Inner product on R^n^ $$ &lt;x,y&gt;=x^Ty=\\sum\\limits_{i=1}^{n}x_iy_i \\in R^n $$ Euclidean norm, or $l_2$ - norm $$ ||x||_2=(x^Tx)^{1/2}=(x_1^2+\\dots+x_n^2)^{1/2}, x \\in R^n $$ Cauchy-Schwartz inequality $$ |x^Ty| \\leq ||x||_2||y||_2,x,y \\in R^n $$ Angle between nonzero vectors $x,y \\in R^n$ $$ \\angle (x,y) = cos^{-1}(\\frac {x^Ty}{||x||_2||y||_2}),x,y \\in R^n,\\angle (x,y) \\in (0,\\pi $$ Inner product on $R^{m\\times n}, X,Y \\in R^{m\\times n} $​ $$ &lt;X,Y&gt;=\\tr (X^TY)=\\sum\\limits_{i=1}^{m}\\sum\\limits_{j=1}^{n}X_{ij}Y_{ij} $$ Here $\\tr()$ denotes trace of a matrix Frobenius norm of a matrix $x \\in R^{m \\times n}$​ 和向量的二范数对应 $$ ||X||F = (\\tr(X^TX))^{1/2}=(\\sum\\limits{i=1}^{m}\\sum\\limits_{j=1}^{n}X_{ij}^2)^{1/2} $$ Inner product on $S^n$ $$ &lt;X,Y&gt;=tr(XY)=\\sum\\limits_{i=1}^{n}\\sum\\limits_{j=1}^{n}X_{ij}Y_{ij} = \\sum\\limits_{i=1}^{n}X_{ii}Y_{ii} + 2 \\sum\\limits_{i&lt;j}X_{ij}Y_{ij} $$ Here $S^n$​ denotes symmetrical matrices on $R^{n \\times n}$ Norms A function $f: R^n \\rarr R$ with dom $f = R^n$​ is called a norm if $f$ is nonnegative: $f(x) \\geq 0$ for all $x \\in R^n$ $f$ is definite( 正定的 ): $f(x)=0$ only if $x=0$ $f$​ is homogeneous( 同质的 ): $f(tx)=|t|f(x)$ , for all $x \\in R^n$​ and $t \\in R$​ $f$ satisfies the triangle inequality: $f(x+y) \\leq f(x) + f(y)$ , for all $x,y \\in R^n$​ Distance Between vectors $x$ and $y$ as the length of their difference, i.e., $$ \\mathrm {dist}(x,y) = ||x-y|| $$ 没有加下标， 表示抽象的范数 Unit ball The set of all vectors with norm less than or equal to one, $$ \\Beta = {x \\in R^n \\mid ||x|| \\leq 1} $$ is called the unit ball of the norm $||\\cdot||$ ( 单位球不唯一， 还需要指定一个范数 ) The unit ball satisfies the following properties: $\\Beta$ is symmetric about the origin, i.e., $x \\in \\Beta$ if and only if $ -x \\in \\Beta $ $\\Beta$ is convex $\\Beta$ is closed, bounded, and has nonempty interior Conversely, if $C \\subseteq R^n$​​​ is any set satisfying these three conditions, then it is the unit ball of a norm: $$ ||x|| = (\\sup{t \\geq 0 | tx \\in C})^{-1} $$ Spme common norms on $R^n$ Sum-absolute-value, or $l_1$ - norm $$ ||x||_1 = |x|_1 + \\dots + |x|_n, x \\in R^n $$ Chebyshev or $l_\\infty$ - norm $$ ||x||_{\\infty} = \\max{|x_1|, \\dots, |x_n|} $$ $l_p$ - norm, $p \\geq 1$ $$ ||x||_p = (|x_1|^p + \\dots + |x_n|^p)^{1/p} $$ For $P \\in S_{++}^n$ ( 对称的 $n \\times n$ 的正定矩阵 ), $P$ - quadratic norm is $$ ||x||_P = (x^TPx)^{1/2}=||P^{1/2}x||_2 $$ Some common norms on $R^{m \\times n}$ Sum-absolute-value norm 对应向量的一范数 $||X||\\mathrm{sav} = \\sum\\limits{i=1}^m\\sum\\limits_{j=1}^n |X_{ij}|$​ Maximum-absolute-value norm 对应向量的无穷范数 $$ ||X||{\\mathrm{mav}} = \\max{ |X{ij}| \\mid i=1, \\dots,m,j=1,\\dots,n } $$ Equivalence of norms Suppose that $||\\cdot||_a$ and $||\\cdot||_b$ are norms on $R^n$ , there exist positive constants $\\alpha$ and $\\beta$ , for all $x \\in R^n$ $$ \\alpha ||x||_a \\leq ||x||_b \\leq ||x||_a $$ if $||\\cdot||$ is any norm on $R^n$ , then there exists a quadratic norm $||\\cdot||_P$ for which $$ ||\\cdot||_P \\leq ||x|| \\leq \\sqrt{n}{ ||x||_P } $$ holds for all $x$ Operator norms 算子范数 Suppose $||\\cdot||_a$ and $||\\cdot||_b$ are norms on $R^m$ and $R^n$ , respectively. Operator norm of $X \\in R^{m \\times n}$ induced by $||\\cdot||_a$ and $||\\cdot||b$​​​ is $$ ||X||{a,b}=\\sup{||Xu||_a \\mid ||u|| \\leq 1} $$ When $||\\cdot||_a$ and $||\\cdot||_b$ are Euclidean norms, the operator norm of $X$ is its maximum singular value ( 最大奇异值 ) ， and is denoted $||X||2$ $$ ||X||2 = \\sigma{\\max}(X) = ( \\lambda{\\max}{ X^TX } )^1/2 $$ Spectral norm( 谱范数 ) or $l_2$​ - norm The norm induced by the $l_{\\infty}$​​ - norm on $R^m$​ and $R^n$​​ , denoted $||X||\\infty$​ is the max-row-sum norm, $$ ||X||\\infty = \\sup{ ||Xu||\\infty \\mid ||u||\\infty \\leq 1 } = \\max_{i=1,\\dots,m}\\sum\\limits_{j=1}^{n}|X_{ij}| $$ The norm induced by the $l_1$​​​ - norm on $R^m$​​​ and $R^n$​​​ , denoted $||X||1$​​​​ is the max-column-sum norm, $$ ||X||1 = \\max{j=1,\\dots,n}\\sum\\limits{i=1}^{m}|X_{ij}| $$ Dual norm 对偶范数 Let $$||\\cdot||$$ be a norm $R^n$ The associated dual norm, denoted $||\\cdot||*$ , is defined as $$ ||z||* = \\sup { z^Tx \\mid ||x|| \\leq 1 } $$ We have the inequality $$ z^Tx \\leq ||x|| ||z||* \\ \\because z^T \\frac{x}{||x||} \\leq \\sup { z^Tx \\mid ||x|| \\leq 1 } = ||z||* \\ \\therefore z^Tx=z^T \\frac {x}{||x||}\\cdot||x|| \\leq ||z||_*||x|| $$ The dual of Euclidean norm 二范数互为对偶 $$ \\sup {z^Tx \\mid ||x||_2 \\leq 1 } = ||z||_2 $$ The dual norm of the $l_{\\infty}$ norm 无穷范数的对偶是一范数， 反之亦然 $$ \\sup {z^Tx \\mid ||x||_{\\infty} \\leq 1 } = ||z||_1 $$ The dual norm of the dual norm 对偶范数的对偶范数是其本身 $$ ||\\cdot||{*{*}} = ||\\cdot|| $$ The dual norm of $l_p$ - norm is the $l_q$ - norm such that $$ \\frac {1}{p} + \\frac {1}{q} = 1 $$ The dual of the $l_2$ - norm on $R^{m \\times n}$​ is the nuclear norm( 核范数 ) $$ ||Z||_{2*} = \\sup {\\tr(Z^TX) \\mid ||Z|| \\leq 1} \\ = \\sigma_1(Z) + \\dots + \\sigma_r(Z) = \\tr[ (Z^TZ)^{1/2} ] $$ Analysis Interior and Open Set An element $x \\in C \\subseteq R^n$ is called an interior point of $C$ if there exists an $\\varepsilon &gt; 0$ for which $$ {y \\mid ||y-x||_2 \\leq \\varepsilon} \\subseteq C $$ i.e., there exists a ball centered at $$x$$ that lies entirely in $C$​ ( 可以用任意范数，只要找到一个范数满足即可 ) The set of all points interior to $C$ is called the interior of $C$ and is denoted $\\mathrm{int}C$ A set $C$ is open if $\\mathrm{int}C=C $ Closed Set and Boundary A set $C \\subseteq R^n$ is closed if its complement is open $$ R^n \\setminus C = { x \\in R^n \\mid x \\notin C } $$ The closure of a set $C$ is defined as $$ \\mathrm{cl \\ } {C} = R^n \\setminus \\mathrm{int \\ }(R^n \\setminus C) $$ The boundary of the set $C$ is defined as $$ \\mathrm {bd \\ C} = \\mathrm { cl \\ } C \\setminus \\mathrm {int \\ } C $$ $C$ is closed if it contains its boundary. It is open if it contains no boundary points Supremun and infimum The least upper bound or supremum of the set $C$ is denoted $\\sup {C}$ The greatest upper bound or infimum of the set $C$​ is denoted $\\inf {C}$​ $$ \\inf C = -(\\sup (-C)) $$ Functions Notation $$ f: A \\rarr B $$ $\\mathrm {dom} f \\subseteq A$ Continuity A function $f:R^n \\rarr R^m $​ is continuous at $𝑥 \\in \\mathrm{dom} \\ f$​ if for all $\\varepsilon &gt; 0$​ there exists a $\\delta$​ such that    𝑓 is continuous if it is continuous at every point Closed functions A function $f:R^n \\rarr R $​ is closed if, for each$\\alpha \\in R$, the sublevel set $$ {x \\in \\mathrm{dom}f \\mid f(x) \\leq \\alpha} $$ is closed. This is equivalent to $\\mathrm{epi}f={ (x,t) \\in R^{n+1} \\mid x \\in \\mathrm{dom}f, f(x) \\leq t }$ is closed Derivatives Def Suppose $f:R^n \\rarr R $​ and $x \\in \\mathrm {int} \\ \\mathrm {dom}\\ f$​​. The function $f$​ is differentiable( 可微的 ) at $x$​​ if there exists a matrix $Df(x) \\in R^{m \\times n}$​​ that satisfies $$ \\lim\\limits_{z \\in \\mathrm{dom}f, z \\neq x, z\\rarr x} \\frac {||f(z)-f(x)-Df(x)(z-x)||_2}{||z-x||_2}=0 $$ in which case we refer to $Df(x)$ derivative (or Jacobian) of $f$ at $x$ $f$ is differentiable if $\\mathrm{dom}f$ is open, and it is differentiable at every point Def The affine function( 仿射函数 ) of $z$​ given by $$ f(x) + Df(x)(z-x) $$ is called the first-order approximation of $f$​ at (or near) $x$ $$ Df(x)_{ij}= \\frac{\\partial f_i(x)}{\\partial x_j} , i=1,\\dots,m,j=1,\\dots,n $$ Gradient When f is real-valued (i.e., $ f:R^n \\rarr R $) the derivative $Df(x)$ is a $1 \\times n$​ matrix (it is a row vector). Its transpose is called the gradient of the function: $$ \\nabla f(x)=Df(x)^T $$ which is a column vector (in $R^n$). Its components are the partial derivatives of 𝑓: $$ \\nabla f(x)_i=\\frac{ \\partial f(x) }{\\partial x_i}, i=1,\\dots,n $$ The first-order approximation of $f$​​​​ at a point $x \\in \\mathrm {\\ {int \\mathrm {\\ {dom \\ x}}}} $ can be expressed as ( the affine function of $z$​ ) $$ f(x) + \\nabla f(x)^T(z-x) $$ 常见凸函数 $$ $$ 常见凹函数 $log$ 随机变量的期望 概率密度函数pdf概率质量函数pmf 高斯分布/正态分布 $$ x ~ N(\\miu, \\sigma^2) $$ 混合高斯分布 GMM 样本X的每个维度$X_i$服从一个高斯分布， 假设各个维度间独立同分布 那么我们有混合高斯分布$Z = {Z_1, Z_2, \\dots, Z_n}$， 每次从中属性中选择一个维度，计算其高斯分布， 最后计算出所有高斯分布，线性组合起来 两步法： 推荐阅读 《Convex optimization》","categories":[{"name":"Mathematics","slug":"Mathematics","permalink":"http://lyk-love.cn/categories/Mathematics/"}],"tags":[{"name":"Optimization Methods","slug":"Optimization-Methods","permalink":"http://lyk-love.cn/tags/Optimization-Methods/"}]},{"title":"Optimization Methods- Lecture1","slug":"Optimization-Methods-Lecture1","date":"2021-09-07T21:47:40.000Z","updated":"2022-09-26T06:39:34.937Z","comments":true,"path":"2021/09/08/Optimization-Methods-Lecture1/","link":"","permalink":"http://lyk-love.cn/2021/09/08/Optimization-Methods-Lecture1/","excerpt":"ref: Optimization Methods Outline: Mathematical Optimization Least-squares Linear Programming Convex Optimization Nonlinear Optimization Summary","text":"ref: Optimization Methods Outline: Mathematical Optimization Least-squares Linear Programming Convex Optimization Nonlinear Optimization Summary Mathematical Optimization Optimization Problem $$ min \\quad \\quad f_0(x) \\ s.t. \\quad f_i(x) \\leq b_i, \\quad i=1,2,\\dots,n $$ Optimization Variable: $x=(x_1,\\dots,x_n)$ Objective Function: $f_0:R^n \\rarr R$ Constraint Functions: $f_i:R^n \\rarr R$​ $x^*$​ is called optimal or a solution $f_i(x^*)\\leq b_i$ , $i=1,\\dots,m $ For any $z$ with $f_i(z)\\leq b_i $, we have $f_0(z)\\geq f_0(x^*)$ Linear Program 线性函数的定义 $$ f_i(\\alpha x + \\beta y) = \\alpha f_i(x) + \\beta f_i(y) $$ For all $x,y \\in R^n$ and all $\\alpha, \\beta \\in R$​ $i$​ 可以为$0$​（目标函数）， 也可以为$1,2,\\dots,n$​​​​( 约束函数 )。 当优化问题的目标函数和约束函数都是线性函数时， 整个问题称为线性规划（线性问题） Nonlinear Program If the Optimization Problem is not linear Convex Optimization Problem $$ f_i(\\alpha x + \\beta y) \\leq \\alpha f_i(x) + \\beta f_i(y) $$ For all $x,y \\in R^n$​ and all $\\alpha, \\beta \\in R$​ with $\\alpha + \\beta=1, \\alpha \\geq 0, \\beta \\geq 0$ 线性规划是凸优化的一个特例 只要求对$\\alpha + \\beta=1, \\alpha \\geq 0, \\beta \\geq 0$的$\\alpha, \\beta$成立， 实际上把条件放松了​ Applications $$ min \\quad \\quad f_0(x) \\ s.t. \\quad f_i(x) \\leq b_i, \\quad i=1,2,\\dots,n $$ Abstraction $x$​ represents the choice made $f_i(x) \\leq b_i$ represent firm requirements that limit the possible choices $f_0(x)$​ represents the cost of choosing A solution corresponds to a choice that has minimum cost, among all choices that meet the requirements Portfolio Optimization Variables $x_i$ represents the investment in the $𝑖_th$ asset $x \\in R$​ describes the overall portfolio allocation across the set of asset Constraints A limit on the budget the requirement Investments are nonnegative A minimum acceptable value of expected return for the whole portfolio Objective Minimize the variance of the portfolio return 投资的回报最稳定（ 也可以把目标换成期望的回报最高 ） Device Sizing Variables $x \\in R^n$​ describes the widths and lengths of the devices Constraints Limits on the device Timing A limit on the total area of the circuit Objective Minimize the total power consumed by the circuit Data Fitting Variables $x \\in R^n$ describes parameters in the model Constraints Prior information Required limits on the parameters (such as nonnegativity) Objective Minimize the prediction error between the observed data and the values predicted by the model Solving Optimization Problems General Optimization Problem Very difficult to solve Constraints can be very complicated, the number of variables can be very large 约束复杂， 变量多 Methods involve some compromise, e.g., computation time, or suboptimal solution Exceptions Least-squares problems Linear programming problems Convex optimization problems Least-squares The Problem $$ min \\quad ||Ax-b||2^2 = \\sum{i=1}^k { (a_i^T x - b_i)^2 } $$ $A \\in R^{k \\times n}$​ , $a_i^T$​ is the $i_{th}$​ row of $A,b \\in R^k$​ $A \\in R^n$​ is the optimization variable​ Setting the gradient to be 0 $$ 2A^T(Ax-b)=0 \\ \\rarr \\quad A^TAx = A^Tb \\ \\rarr \\quad x = (A^TA)^{-1}A^Tb $$ A Set of Linear Equations $$ A^TAx = A^Tb $$ Solving least-squares problems Reliable and efficient algorithms and software Computation time proportional to $n^2k( A \\in R^{k \\times n})$ ; less if structured A mature technology Challenging for extremely large problems Using Least-squares Easy to Recognize Weighted least-squares $$ \\sum\\limits_{i=1}^k w_i( a_i^Tx-b_i )^2 $$ 把$w_i$移到括号里就是标准最小二乘 Different importance Different importance $$ \\sum\\limits_{i=1}^k ( a_i^Tx-b_i )^2 + \\rho \\sum\\limits_{i=1}^n x_i^2 $$ 也可化为标准最小二乘 More stable，避免过拟合 Linear Programming The Problem $$ min \\quad c^Tx \\ s.t. \\quad a_i^Tx \\leq b_i, \\quad i = 1, \\dots, m $$ $c,a_1,\\dots,a_m \\in R^n, b_1, \\dots, b_m \\in R$ Solving Linear Programs No analytical formula for solution Reliable and efficient algorithms and software Computation time proportional to $𝑛^2𝑚$ if $ m \\geq 𝑛$; less with structure A mature technology Challenging for extremely large problems Using Linear Programming Not as easy to recognize Chebyshev Approximation Problem $$ min \\quad \\max\\limits_{i=1,\\dots,k} |a_i^Tx-b_i| \\ \\iff min \\quad t \\quad \\quad \\quad s.t. \\quad t = \\max\\limits_{i=1,\\dots,k} |a_i^Tx-b_i| \\ \\quad\\quad \\quad \\iff min \\quad t \\quad \\quad \\quad s.t. \\quad t \\geq |a_i^Tx-b_i|, {i=1,\\dots,k} ( 因为最小化，所以可以等价 )\\ \\quad \\quad \\quad\\iff min \\quad t \\quad \\quad \\quad s.t. \\quad t \\geq |a_i^Tx-b_i|, {i=1,\\dots,k} \\ \\quad \\quad \\quad \\quad \\quad \\iff min \\quad t \\quad \\quad \\quad s.t. \\quad -t \\leq a_i^Tx-b_i \\leq t, {i=1,\\dots,k} \\ $$ Convex Optimization Local minimizers are also global minimizers The Problem $$ f_i(\\alpha x + \\beta y) \\leq \\alpha f_i(x) + \\beta f_i(y) $$ For all $x,y \\in R^n$ and all $\\alpha, \\beta \\in R$ with $\\alpha + \\beta=1, \\alpha \\geq 0, \\beta \\geq 0$ Least-squares and linear programs as special cases Solving Convex Optimization Problems No analytical solution Reliable and efficient algorithms (e.g., interior-point methods) Computation time (roughly) proportional to $\\max{n^3,n^2m,F}$​ $F$ is cost of evaluating $f_i$​'s and their first and second derivatives Almost a technology Often difficult to recognize Many tricks for transforming problems into convex form Surprisingly many problems can be solved via convex optimization Nonlinear Optimization An optimization problem when the objective or constraint functions are not linear, but not known to be convex Sadly, there are no effective methods for solving the general nonlinear programming problem Could be NP-hard We need compromise Local Optimization Methods Find a point that minimizes $f_0$ among feasible points near it The compromise is to give up seeking the optimal $x$ Fast, can handle large problems Differentiability Require initial guess Provide no information about distance to (global) optimum Local optimization methods are more art than technology 全凭运气 Global Optimization Find the global solution The compromise is efficiency Worst-case complexity grows exponentially with problem size Applications A small number of variables, where computing time is not critical The value of finding the true global solution is very high Worst-case Analysis of a high value or safety-critical system Variables represent uncertain parameters Objective function is a utility function Constraints represent prior knowledge If the worst-case value is acceptable, we can certify the system as safe or reliable 能证明系统可靠 Local optimization methods can be tried If finding values that yield unacceptable performance, then the system is not reliable 局部优化已经发现了不可接受的值，那系统就不可靠 But it cannot certify the system as reliable 只能证明系统不可靠，不能证明可靠 Role of Convex Optimization in Nonconvex Problems Initialization for local optimization An approximate, but convex, formulation Convex heuristics for nonconvex optimization Sparse solutions (compressive sensing) Bounds for global optimization 通过凸优化来找全局优化问题的下界 Relaxation 把条件松弛 Lagrangian relaxation","categories":[{"name":"Mathematics","slug":"Mathematics","permalink":"http://lyk-love.cn/categories/Mathematics/"}],"tags":[{"name":"Optimization Methods","slug":"Optimization-Methods","permalink":"http://lyk-love.cn/tags/Optimization-Methods/"}]},{"title":"Neuroscience L3","slug":"Neuroscience-L3","date":"2021-09-07T19:17:18.000Z","updated":"2022-09-26T06:39:34.934Z","comments":true,"path":"2021/09/08/Neuroscience-L3/","link":"","permalink":"http://lyk-love.cn/2021/09/08/Neuroscience-L3/","excerpt":"The action potential reflects changes in membrane permeability to specific ions. The Voltage Clamp Method","text":"The action potential reflects changes in membrane permeability to specific ions. The Voltage Clamp Method The Voltage Clamp Method 设计原理: 根离子作跨膜移动时形成了跨膜离子电流，而通透性即离子通过膜的难易程度，其膜电阻的倒数，也就是膜电导。因此，膜对某种离子通透性增大时，实际上时膜电阻变小，即膜对该离子的电导加大。根据欧姆定律，即 $I=V/R=VG$，所以，只要固定膜两侧电位差时，测出的跨膜电流的变化，就可作为膜电导变化的度量，即可了解膜通透性的改变情况。 内向电流是钠电流; 外向电流是钾电流 用药物可以抑制钠\\钾电流 $I_{ion} = g_{ion}(V_m-E_ion)$​ $V_m$: 钳制电压 $E_ion$: 平衡电压 $I_{ion}$: 离子电流 $g_{ion}$: 膜电导 去极化会提高膜电导! 不应期: 发生动作电位的过程中,刺激是不会被响应的 钾通道打开,钠通道失活 Long-Distance Signaling by Means of Action Potential 动作电位的长距离传导 去极化在局部打开钠通道,产生动作电位和内向电流, 内向电流沿着轴突被动扩散, 使轴突邻近区域也发生去极化. 不应期会使膜电位重新极化, 保证传播方向(不能来回传播) 髓鞘: 绝缘物质,把轴突分成一个一个节点, 髓鞘不是完全包裹的. 裸露处称为郎飞结(Nodes of Ranvier), 与外界进行离子交换,形成动作电位. 髓鞘使得传导速度增加. 髓鞘的存在使得节点间距离变大( 单位时间传的距离更长 ), 因此有髓纤维的动作电位传导速度更快","categories":[{"name":"Natural Science","slug":"Natural-Science","permalink":"http://lyk-love.cn/categories/Natural-Science/"}],"tags":[{"name":"Neuroscience","slug":"Neuroscience","permalink":"http://lyk-love.cn/tags/Neuroscience/"}]},{"title":"L13 Undirected Graph","slug":"L13-Undirected-Graph","date":"2021-08-23T12:01:35.000Z","updated":"2022-09-26T06:39:34.932Z","comments":true,"path":"2021/08/23/L13-Undirected-Graph/","link":"","permalink":"http://lyk-love.cn/2021/08/23/L13-Undirected-Graph/","excerpt":"Outline: UAG的DFS树 UAG的DFS框架 UAG的DFS应用 容错连通 寻找割点 寻找桥","text":"Outline: UAG的DFS树 UAG的DFS框架 UAG的DFS应用 容错连通 寻找割点 寻找桥 UAG的DFS树 UAG的遍历的主要差别就在于剔除二次遍历 TE：当发现一个白色节点并递归地进行遍历时， 就将其连接的边标记为TE。 遍历过程中的TE组成遍历树， 这与DAG是类似的。 对于原本无向的边， 遍历过程为它做了定向( orientation ), 即遍历推进的方向。 BE： 当遍历节点$u$并发现一条边指向灰色节点$v$时, 此时有两种本质不同的情况: 若$vu$是TE, 则这是一次二次遍历,因而标识并剔除这一类型的遍历. 若$v$是$u$的某个不是父节点的祖先节点, 则将$uv$标记为BE DE: 当遍历节点$u$​时, 发现一条边指向节点$v$​​时, 且$v$​是$u$​​在遍历树中的后继节点, 此时边$uv$为DE, 但这次遍历必然是二次遍历,应该被剔除. 这是因为根据DE的定义, 此时$v$不能是白色( 否则$uv$是TE ), 不能是灰色(可能是二次遍历的BE ), 因而只能是黑色, 即$v$已经结束了遍历, 它结束遍历前必然已经完成了边$vu$的遍历, 且根据$u$,$v$间的祖先后继关系, 边$vu$首次被遍历时标记为BE(可能是二次遍历的BE). CE: 不存在. 因为当遍历节点$u$时,发现一条边指向节点$v$. 根据CE的定义, $u$, $v$间没有祖先后继关系,所以与上面分析类似, 节点$v$只能是黑色, 它已经完成了遍历. 所以点$v$在结束遍历前必然已经访问过边$vu$. 当从$v$出发遍历$vu$时, $u$尚未被遍历, 为白色. 所以$vu$为TE, 这和$u$, $v$间没有祖先后继关系相矛盾. UAG的DFS框架 对于TE的处理于DAG一致, 此外还要处理BE的情况. 根据对BE的讨论,我们必须去除指向父结点的BE. 为此, 我们记录了每个节点的父节点. 算法框架直接将上述两种情况外的其他情况忽略, 因为其他情况都是二次遍历, 不需要处理. 一个邻居节点只可能有3种情况: 白色: 即TE 灰色: 灰色且不是父结点: 需要额外处理 灰色且是父结点: 二次遍历 黑色: DE: 二次遍历 CE: 二次遍历 DFS_UG(v, parent):v.color := GRAY;&lt;Preorder processing of node v&gt;;foreach neighbour w of v do if w.color = WHITE then &lt;Exploratory processing of TE vw&gt;; DFS_UG(w,v); &lt;Backtrack processing for TE vw&gt;; else if w.color = GRAY and w != parent then &lt;Check BE vw&gt;;&lt;Postorder processing of node v&gt;;v.color := BLack; UAG的DFS应用 容错连通 定义4.7 对于连通的无向图$G$, 如果其中任意去掉$k-1$个点, 图$G$仍然连通, 则称图$G$是$k$-点连通的. 类似的, 如果图中任意去掉$k-1$条边, 图$G$仍然连通, 则称图$G$是$k$-边连通的. 已知,当$k=1$时, $k$​-连通就退化为传统的无环连通. 另外, 我们更关注$k=2$​​的 特殊情况, 即去掉某个点或某条边后,剩下的图不再连通. 由此引入割点( articulation point )和桥( bridge )的概念. 定义4.8 对于连通的无向图$G$​, 称节点$v$​为割点, 如果去掉点$v$​后, 图$G$​不再连通; 称边$uv$​为桥, 如果去掉边$uv$​后, 图$G$​​不再连通. 寻找割点 Brute Force: 遍历每个顶点, 检查剩下的图是否连通, 代价为$O(n(m+n))$, 这一方法源于割点的定义. 为了将代价改进到线性时间, 需要将割点的定义做等价变换.以支持更高效的找割点的算法. 割点的定义依赖一个全局的性质( 整个图是否连通 ), 这一性质难以高效地进行检测. 为此我们首先将割点的定义等价地变化为一个局部的性质, 利用部分节点之间的关系来完成割点的检测. 引理4.7 ( 割点基于路径的定义 ) 节点$v$为割点, 当且仅当存在节点对$w$和$x$满足节点$v$出现在$w$到$x$的每一条路径上. 证明略. 引理4.8 ( 割点基于DFS的定义 ) 假设在一次DFS中, 节点$v$不是遍历树的根节点. 则节点$v$​为割点, 当且仅当在遍历树中, 存在节点$v$的某棵子树, 没有任何BE指向$v$的祖先节点. 证明: 必要性: 易证若节点$v$​的某棵子树,没有任何BE指向$v$​​的祖先节点, 则删掉$v$​后, 该子树将于图的其他部分断连, 所以$v$​是割点. 充分性: 假设节点$v$为割点, 则根据引理4.7, 存在不同于$v$的两个节点$x$和$y$满足$v$出现在$x$到$y$的每一条路径上. 首先我们发现节点$x$和$y$中至少有一个是节点$v$在遍历树中的后继节点( 可通过反证法证明. ). 所以必有某个点在遍历树中的后继节点, 而$v$必然不是叶节点. 同样可用反证法证明, 若$v$的任意子树均有BE指向$v$的祖先节点, 此时无论$x$和$y$哪一个是$v$的后继节点( 或者都是 ), 均可构造一条从$x$到$y$的不经过$v$的路径, 这和$v$出现在$x$到$y$的每一条路径上相矛盾. 证毕. 根据引理4.8, 可以用算法操作来发现割点: 为每个节点维护一个变量back来判定它是否为割点: 当$v$首次被发现时: $$ v.back = v.discoverTime $$ 遍历过程中遇到一条从节点$v$指向节点$w$的BE: $$ v.back=min{v.back, w.discoverTime} $$ 遍历节点$w$结束, 从$w$回退到$v$时: $$ v.back=min{v.back,w.back} $$ 注意, $v.back$初始值为$v.discoverTime$, 且$v.back$的值只会减少. 其减少有两种情况: 遍历过程遇到一条BE, 记为边$vw$. 处理完BE并回退时, $v.back$被减少为$w.discoverTime$. 由于BE指向的节点$w$是祖先节点, 所以$w.discoverTime$更小,这一更新使得$v.back$​的值减少. 处理完TE$vw$回退时, 如果节点$w$​的back值有更新( 只可能减少 ), 这一更新随着回退被传递到回退的节点. 当从TE $vw$回退时, 如果$w.back \\geq v.discoverTime$, 则节点$v$​​是割点. 定理4.5 ARTICULATION-POINT-DFS算法是正确的. 证明: 根据引理4.8, 要证明算法正确性, 只需证明当从TE $vw$回退时, 如果$w.back \\geq v.discoverTime$, 则以$w$为根的某棵子树没有任何BE指向$v$的祖先节点. 根据back值的更新方法, 如果以$w$为根的某棵子树存在BE指向$v$的祖先, 则$v$的祖先的$discoverTime$会被赋值给$w$为根的子树中某个节点的back值, 且随着遍历的回退过程, 这一$discoverTime$会以back遍历的方式传递给$w.back$. 由于祖先节点具有更小的$discoverTime$, 所以如果这样的一条BE存在, 则$w.back$一定小于$v.discoverTime$, 反之则说明这样的BE不存在. ARTICULATION-POINT-DFS(v):v.color := GRAY;time := time + 1;v.discoverTime := time;v.back := discoverTime;foreach neighbour w of v do if w.color = WHITE then w.back := ARTICULATION-POINT-DFS(w); if w.back &gt;= v.discoverTime then Output v as an articulation point; v.back = &#123;v.back, w.back&#125;; else if vw is BE then v.back = min&#123; v.back, w.discoverTime &#125;;return back; 寻找桥 基于UAG的DFS中,只会出现TE和BE, 对于BE, 删去它后图依然是连通的( 易证 ), 因此只需关注TE. 引理4.9 ( 桥基于DFS的定义 ) 给定遍历树中的TE $uv$ ( $u$是$v$的父结点 ), $uv$是桥 当且仅当在以$v$为根的所有遍历树的子树中, 没有BE指向$v$​​的祖先节点( 不包括$v$, 包括$u$). $v.back$维护方式为: 当$v$首次被发现时: $$ v.back = v.discoverTime $$ 当遍历BE $vw$时, $$ v.back = min{v.back,w.back} $$ 当遍历节点$w$结束, 回退到$v$时, $v.back = min{v.back,w.back}$ 定理4.6 BRIDGE-DFS算法是正确的. 证明略 BRIDGE-DFS(u):v.color := GRAY;time := time + 1;v.discoverTime := time;v.back := discoverTime;foreach neighbour w of v do if w.color = WHITE then v.back := min&#123; u.back, v.back &#125;; if v.back &gt; u.discoverTime then Output uv as a bridge; else if uv is BE then u.back := min&#123;u.back, v.discoverTime&#125;;","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[{"name":"Algorithm design and analysis","slug":"Algorithm-design-and-analysis","permalink":"http://lyk-love.cn/tags/Algorithm-design-and-analysis/"}]},{"title":"T4 BFS/DFS","slug":"T4-BFS-DFS","date":"2021-08-23T09:51:20.000Z","updated":"2022-09-26T06:39:34.941Z","comments":true,"path":"2021/08/23/T4-BFS-DFS/","link":"","permalink":"http://lyk-love.cn/2021/08/23/T4-BFS-DFS/","excerpt":"Outline: BFS skeleton 证明: $v.dis = \\delta(s,v)$ BFS树 应用 DFS s → all","text":"Outline: BFS skeleton 证明: $v.dis = \\delta(s,v)$ BFS树 应用 DFS s → all BFS skeleton BFS-WRAPPER(G) foreach node v in G do v.color := WHITE; v.parent := null; v.dis := +∞;foreach node v in G do if v.color = WHITE then BFS(v); BFS(G) Initialize an empty queue queNode;v.color := GRAY;v.dis := 0;queNode.ENQUE(v);while queNode != empty do w := queNode.DEQUE(); foreach neighbor x of w do: if x.color := WHITE then x.color := GRAY; x.parent := w; x.dis := w.dis + 1; queNode.ENQUE(x); &lt;processing of node w&gt;; w.color := BLACK; 证明: $v.dis = \\delta(s,v)$ 变量$v.dis$​记录了源点$s$​到节点$v$​最短路径的长度,对所有节点依据$dis$ 进行了等价类划分 记源点$s$到节点$v$的最短路径长度为$\\delta(s,v)$,.现在证明$v.dis = \\delta(s,v)$： 引理5.1 对于有向或无向图$G$, 对任意边$uv$​, 有$\\delta(s,v) \\leq \\delta(s,u)+1$​​. 证明: 如果源点$s$到$u$可达, 则由于$uv$的存在, $s$到$v$同样可达. 所以$s$到$v$最短路径长度必然不超过任意一条$s$到$v$的路径长度(包括$s$到$u$再到$v$这条路径), 所以$\\delta(s,v) \\leq \\delta(s,u)+1$. 若$s$到$u$不可达, 则$\\delta(s,u)=\\infty$, 不等式同样成立（当然，若$s$到$v$也不可达，此时是 $\\infty \\leq \\infty + 1$，同样成立）. 引理5.2 从节点$s$​开始BFS, 在遍历结束时, 对每个可达结点$v$​, 有 $v.dis \\geq \\delta(s,v)$​​. 证明: 采用数学归纳法,对队列上的操作个数归纳, 即证明: 无论队列上执行了多少个操作, 该不变式总是成立: 对于任意$v$, $v.dis \\geq \\delta(s,v)$. 初始情况是队列执行第一个操作, 即将源点$s$放入队列中. 此时$s.dis \\geq \\delta(s,s)=0$. 对于其他任意节点$v$, $v.dis = +\\infty \\geq \\delta(s,v)$. 所以初始情况下结论成立. 由于出队操作对结论没有影响( 出队不会更改 $v.dis$), 只需要关注入队操作. 假设在处理节点$u$时, 发现白色邻居$v$. 根据归纳假设, 有$u.dis \\geq \\delta(s,u)$. 对于$v$, 有: $$ v.dis = u.dis + 1( BFS的实现 ) \\ \\quad \\quad \\geq \\delta(s,u)+1 ( 归纳假设 ) \\ \\geq \\delta(s,v) ( 引理5.1 ) $$ $$ $$ 由于$v.dis$的值一经赋值后不再变化. 所以我们通过归纳法证明了对每个节点$v$, 有 $v.dis \\geq \\delta(s,v)$​ 为了证明相等关系的另一半 $v.dis \\leq \\delta(s,v)$​, 首先要对BFS过程进行更细致的刻画. 引理5.3 假设在BFS过程中, 队列中的元素为$&lt;v_1, v_2, \\dots, v_r&gt;$ ( $v_1$是队头, $v_r$​ 是队尾 ). 我们有: $v_i.dis \\leq v_{i+1}.dis$ ( $1 \\leq i \\leq r+1 $ ) , $ v_r.dis \\leq v_1.dis+1 $​ 证明: 采用数学归纳法,对队列上的操作归纳. 初始情况下, 队列中只有源点$s$, 结论显然成立. 下面要证明队列任意执行一个操作(出队或入队), 上述结论总是成立. 假设队头元素$v_1$​出队, 则$v_2$成为新的队头元素. 根据归纳假设, 有 $v_r.dis \\leq v_1.dis + 1 \\leq v_2.dis + 1$ 从$v_2$到$v_r$​​的所有元素的小于等于关系依然成立. 所以执行一个出队操作后, 要证明的结论保持成立. 假设有一个新元素$v_{r+1}$入队, 此时必然从队首取出一个节点进行处理, 记为$u$. 在处理$u$时, 我们发现了白色邻居$v_{r+1}$并将它放到队列尾部. 此时$v_{r+1}.dis=u.dis+1$. 在$u$出队前的时刻, $u$是队头, $v_1$是队列中的第二个元素, 所以$u.dis \\leq v_1.dis$. 根据上面的分析, 有 $v_{r+1}.dis = u.dis +1 \\leq v_1.dis + 1$. 在$u$ 出队之前,$v_{r+1}$未入队时, $u$是队头, $v_r$​是队尾. 同样根据归纳假设, 有 $v_r.dis \\leq u.dis+1=v_{r+1}.dis$ 对于队列中其他元素而言, 不等关系未受影响. 综上, 基于归纳法我们证明了BFS过程中的任意时刻, $v_i.dis \\leq v_{i+1}.dis$ ( $1 \\leq i \\leq r+1 $ ) , $ v_r.dis \\leq v_1.dis+1 $​ 定理5.1 假设从图$G$中的源点$s$开始对整个图完成BFS, 则对任意节点$v$, $v.dis = \\delta(s,v)$, 且从$s$到$v$由TE组成的路径就是$s$到$v$的最短路径(不一定是唯一的最短路径) 证明: 采用反证法, 假设存在一些节点, 它们的dis值不等于源点到它们的最短路径值. 在这些节点中, 取源点到其距离最短的节点, 记为 $v$ (显然$v$不可能为$s$ ). 根据引理5.2, 有$v.dis &gt; \\delta(s,v)$. 注意$s$到$v$​必然可达, 否则$ \\delta(s,v) = + \\infty \\geq v.dis$​ 考察$s$到$v$的最短路径. 记$u$为该路径上在$v$前面的节点, 则 $\\delta(s,v)=\\delta(s,u)+1$. 根据选取$v$的特定方式, 有$u.dis=\\delta(s,u)$ (易证). 由此, 有: $$ v.dis &gt; \\delta(s,v) = \\delta(s,u)+1=u.dis+1 $$ 下面考察节点$u$刚从队头出队的时刻. 此时节点$v$可能有三种颜色. 如果$v$为白色, 则根据BFS框架, 将赋值$v.dis=u.dis+1$, 这与$v.dis &gt; u.dis+1$矛盾 如果$v$为灰色,则记它作为节点$w$的白色邻居被放到队尾, 且 $v.dis = w.dis+1$. 由于$w$比$v$更早离开队列, 所以根据引理5.3, 有$v.dis=w.dis+1 \\leq u.dis+1$, 这与$v.dis &gt; u.dis+1$矛盾 如果$v$为黑色, 则在$u$之前它已离开队列, 所以 $v.dis\\leq u.dis$, 这与$v.dis &gt; u.dis+1$矛盾 证毕. BFS树 对于边$uv$ 有向 无向 TE $v.p = u; v.dis = u.dis+1$ 同左 BE $0 &lt;= v.dis &lt; u.dis$ nil DE nil nil CE $v.dis \\leq u.dis+1$​ $v.dis = u.dis$​ 或$v.dis = u.dis+1$​ 有向图 TE: 当遍历节点 $v$ 时，发现其白色邻居 $v$, 则 $uv$​​​​​ 为TE. 在一个连通片内所有TE组成的子图连通且无环且包含了该连通片中所有节点. 如果忽略所有边的方向, 则这些TE组成当前连通片的一棵生成树,称为&quot;BFS树&quot; BE: 当遍历节点$u$时,发现其黑色邻居 $v$, 且 $v$是$u$在BFS树中的祖先节点, 则$uv$​为BE. 对于BE $uv$, 有 $ 0 \\leq v.dis &lt; u.dis$​​. DE: BFS不可能出现DE。反证假设$uv$为DE，那么考察在节点$u$刚出队列,即将处理它的所有邻居的时刻, 节点$v$的情况: 节点$v$不可能是白色,否则$uv$为TE 节点$v$不可能为灰色, 因为在此时$u$刚出队列, 而若$v$为灰色(正在队列中), 这和$u$是$v$在遍历树上的祖先节点矛盾 节点$v$不可能是黑色, 在 节点$u$刚出队的时刻, 如果 节点$v$已经结束遍历, 这同样和$u$是$v$​在遍历树上的祖先节点矛盾 CE: 当遍历节点$u$​时,发现其灰色或者黑色邻居 $v$​, 且$v$​不是$u$​的祖先节点( 前面关于DE的讨论证明了必然不可能是子孙节点 ), 则$uv$​为CE. 对于CE, 有$v.dis \\leq u.dis+1$​​ 与DFS类似, CE同样可能存在于两个不同的BFS树之间 无向图 TE: 与有向图的情况类似, 当遍历节点 $v$ 时，发现其白色邻居 $v$, 则 $uv$ 为TE. 对于TE$(u,v)$​, 有 $v.dis = u.dis+1$. 所有TE组成(当前连通片的)BFS遍历树, 我们为每条TE进行定向, 其方向就是遍历推进的方向. BE: 不存在（证明易） DE: 不存在（证明易） CE: 当遍历节点$u$​​​时,发现其灰色邻居 $v$​​​(前面关于BE和DE的讨论证明了$v$​​​不可能是$u$​​​的祖先或子孙节点), 则$uv$​​​为CE. 对于CE, 有 $v.dis = u.dis$​​​ 或 $v.dis = u.dis+1$​ 注意, 此时节点$v$​不可能是白色,否则$uv$​为TE; 节点$v$​不可能是黑色, 否则由于无向边$uv$​的存在, 在处理$u$​时, 必然已处理过$uv$​, 此时的边$uv$​是二次遍历,直接被剔除, 不做处理. 推论5.1, 对于BFS过程中的CE $uv$, 无向图 $v.dis = u.dis$ 或 $v.dis = u.dis+1$ 有向图 $v.dis \\leq u.dis+1$​ 证明:( 书上有图) 对于有向图的CE $uv$,我们考察$v.dis$最大可以比$u.dis$大多少. 注意, 节点$v$发现得越晚, $v.dis$越大. 由于图中有边$uv$的存在, 所以最迟将节点$u$出队列时, 必将访问节点$v$. 所以$v.dis \\leq u.dis+1$​ 对于无向图的CE $uv$, 当处理$u$时, 根据前面的讨论, $v$只能是灰色, 即$v$在队列中. 根据引理5.3, 有$u.dis \\leq v.dis \\leq u.dis+1$. 所以$v.dis = u.dis$ 或 $v.dis = u.dis+1$ 应用 二部图 给定无向图$G=(V,E)$, 我们称之为二分图， 如果存在顶点$V$的划分$V_1$, $V_2$ ( $V_1 \\cap V_2= \\emptyset, V_1 \\cup V_2=V$ ), 使得图中任意的边均满足它的一个顶点在 $V_1$，另一个顶点在$V_{2}$（ 即，在$V_{1}$ 和 $V_{2}$ 内部， 任意一对顶点没有边相连 ）​​ （等价于二着色问题） 发现TE,推进着色； 发现非TE，检查着色 DFS也可以 k度子图 给定无向图$G$，定义图$G$的子图$H$​为k度子图，如果每个顶点的度均大于等于输入的参数k 对于BFS中的点v，若v.d &lt; k( d为v的度数 )， 则v的邻居d--； DFS也可以 DFS s → all 有向图G，问是否存在点s，s到所有点可达？ “可达性”可使用SCC，因为SCC是可达性的等价类。求出G的收缩图后，又由于收缩图是有向无环图。 则检查每个顶点，若存在至少两个顶点出度为0，则不存在； 若仅存在一个出度为0的顶点，则从该顶点出发遍历 对于“all → s”，只需把图转置","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[{"name":"Algorithm design and analysis","slug":"Algorithm-design-and-analysis","permalink":"http://lyk-love.cn/tags/Algorithm-design-and-analysis/"}]},{"title":"T3 Balancing","slug":"T3-Balancing","date":"2021-08-21T11:47:37.000Z","updated":"2022-09-26T06:39:34.941Z","comments":true,"path":"2021/08/21/T3-Balancing/","link":"","permalink":"http://lyk-love.cn/2021/08/21/T3-Balancing/","excerpt":"Outline: 找前k大元素 找离medium最近的k个元素 Weighted medium 找unique","text":"Outline: 找前k大元素 找离medium最近的k个元素 Weighted medium 找unique 找前k大元素 （不是第k大） $nlogn$: 排序​ $n+klogn$​​： 建堆，k次getMax $n+k^2$​​​​：建堆+从堆的前k层里面选k次（共有 $2^k$​​​个元素，对这个堆的fixHeap代价是$log2^k=k$​​​,找k次，即$k \\times k$​） $n+klogk$: 先select出第k大的元素，代价$\\Theta(n)$；再以此为partition，对前k个元素排序，代价$\\Theta(klogk)$​ 找离medium最近的k个元素 左界：$L=n/2 - k$ 右界: $R=n/2 + k$ 先 selcect(L)，找出左界的位置，然后partition（L）；对右界同理；最后对中间的部分做操作（方法有很多） Weighted medium $x_1,x_2,\\dots,x_n$​两两可比 $w_1,w_2,\\dots,w_n$, $w_i&gt;0$​, ​ $\\sum w_i=1$ 找出weighted medium $x_k$​, 使得$\\sum\\limits _{x_i&lt;x_k}w_i &lt; \\frac 1 2$​, $ \\sum\\limits _{x_i &gt; x_k} \\leq \\frac 1 2$​ （也可以前者$\\leq$, 后者$&lt;$,但不能二者都是$\\leq$​​）​ $O(n)$: 先select得到medium（注意，不是 Weighted medium）； 再以此partition， 算出左右两边的weight，看哪边大于1/2，对子问题递归判断 找unique 所有元素只有一个和其他不一样，找出这个元素. critical operation： compare（a, b）= Y ( a = b ) / N ( a != b) 可以用adversary argument 找出下界","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[{"name":"Algorithm design and analysis","slug":"Algorithm-design-and-analysis","permalink":"http://lyk-love.cn/tags/Algorithm-design-and-analysis/"}]},{"title":"T6 NPC","slug":"T6-NPC","date":"2021-08-17T19:37:24.000Z","updated":"2022-09-26T06:39:34.941Z","comments":true,"path":"2021/08/18/T6-NPC/","link":"","permalink":"http://lyk-love.cn/2021/08/18/T6-NPC/","excerpt":"Outline： NPC = P ？ NPC判定问题 规约： 等价","text":"Outline： NPC = P ？ NPC判定问题 规约： 等价 优化问题 → 判定问题： 优化问题往往有某个结构，结构有指标， 给指标选定一个阈值k，对k做判定 NPC = P ？ 许多NPC看似是P，但证明是错误的。 即： 目前还无法证明NP = P Clique问题的一种错误的规约是“伪Clique问题”，后者是“对给定的k，判断是否存在k顶点的Clique”，该问题是P的。 但是Clique问题的正确表述是“对任意k，判定是否有k顶点的Clique”，即k为变量而不是常数，因此Clique问题不是P的。 错误在于问题的转换不对。 不过，对NPC的某个参数转变成常数可以将其转化为P,这是一种解NPC的思路 ChangeCoin代价是$O(nN)$是P, 但它不是NP， 因为在该问题最合理的建模下，数值用k位表示，代价是$O(n2^k)$. 所以背包问题不是多项式可解​ 规约是有代价的（ 如范式间的转换，不一定是P ） DNF是多项式可解的，而CNF转换到CNF不是多项式时间的， 所以CNF不是多项式时间可解（不是P） NPC判定问题 已知特例为NPC，可以比较容易地判定Genaral是否为NPC, 即： 特例归约到general比较简单。 Example Clique是稠密子图问题的特例， 所以Clique $\\leq_P$​​ 稠密子图. 所以稠密子图是NPC 已知划分问题是NPC, 它是背包问题的特例， 所以背包问题是NPC 归约：等价 独立集和点覆盖问题是等价的 Proof 设有独立集I，点覆盖集C, 对G中任意边 e = （u，v），存在点u不属于I（若u，v都属于I，则与独立集矛盾）， 所以u在I的补集中。 即任何一条边，至少有一个点在I的补集中，所以I的补集是C。 支配集（Domination Set）与集合覆盖问题是等价的.","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[{"name":"Algorithm design and analysis","slug":"Algorithm-design-and-analysis","permalink":"http://lyk-love.cn/tags/Algorithm-design-and-analysis/"}]},{"title":"Platoon","slug":"Platoon","date":"2021-08-17T18:58:45.000Z","updated":"2022-09-26T06:39:34.937Z","comments":true,"path":"2021/08/18/Platoon/","link":"","permalink":"http://lyk-love.cn/2021/08/18/Platoon/","excerpt":"泰勒（查理•辛饰）是越战期间的一名美国大学生，他前往越南前线去服兵役，被分到了一个步兵部队。部队里有两个人一直对他影响颇大——伊莱亚斯（威廉•达福饰）和巴恩斯（汤姆•贝伦杰饰）。他们两人对战争的看法截然不同，前者不忍残杀无辜，而后者却杀人如麻，泰勒心里非常迷惑，到底谁对战争的看法才是正确。巴恩斯展开了一场残忍的屠杀，越南整条村落都流血成河。伊莱亚斯力劝好友不要如此残暴，却遭致二人关系的裂缝。巴恩斯更怀疑伊莱亚斯向上司打小报告，友谊变为妒恨。于是，在一次丛林战争中，他背着所有人，向伊莱亚斯扣下了扳机。泰勒却洞察了这一切，他心里终于有了答案。","text":"泰勒（查理•辛饰）是越战期间的一名美国大学生，他前往越南前线去服兵役，被分到了一个步兵部队。部队里有两个人一直对他影响颇大——伊莱亚斯（威廉•达福饰）和巴恩斯（汤姆•贝伦杰饰）。他们两人对战争的看法截然不同，前者不忍残杀无辜，而后者却杀人如麻，泰勒心里非常迷惑，到底谁对战争的看法才是正确。巴恩斯展开了一场残忍的屠杀，越南整条村落都流血成河。伊莱亚斯力劝好友不要如此残暴，却遭致二人关系的裂缝。巴恩斯更怀疑伊莱亚斯向上司打小报告，友谊变为妒恨。于是，在一次丛林战争中，他背着所有人，向伊莱亚斯扣下了扳机。泰勒却洞察了这一切，他心里终于有了答案。 主演是《现代启示录》主演的儿子。这两部电影也是我心目中最伟大的电影。《现代启示录》在各方面都是登峰造极的，而《野战排》更现实，更触目惊心，因为它讲了一个在战场上非常“容易发生”的事。 电影最震撼的是士兵们进入越南村庄的那一幕，可怕的暴力，扭曲的人性显露出来。 那一幕又是如此真实，因为我们自己也曾被这样对待。 战争中是没有人性可言的，我看完后反复地在想，有没有解决这个问题的手段，有没有能让人在战争中不失去理智的手段。我想不出来。 可能“战争”本身就是非理性的，所以只要在战争中，理智一定会丧失。 那么这个问题就转化为如何避免战争了。不过我又倾向于认为战争是不可能避免的，而这些血腥和暴力（发动战争和战争中的暴行）的源头都在于人性。 如果人性能够更理智，应该能减少这类事情的发生。 如此一来人类就会变成更温和更冷静的动物，然而动物性永远是人性的一部分，所以黑暗面永远不可能消除。 还有一种办法，那就是剥去人类的所有动物性，其实就是剥去肉体欲望。 欲望无非是人体的各种激素水平的上升。如果脱离了肉体，那么暴力和黑暗应该就能根除。 比如说把脑子独立出来放入一个容器中，其所需的物质都定额供给。其中涉及暴力和欲望的那些激素、物质的水平，全都用硬编码写死，或者设置一个非常严格的上界。但这又引申出另一个问题： 如果人失去了欲望，那人生是为了什么？ 我觉得这不是一个问题，就生物意义上来说，人的生存不过是为了繁衍； 就精神上来说，人生的意义本来就没有答案，“为满足欲望而活着”怎么看都不是正确解。所以这个想法应该是可行的，而且我认为它就是人类（其实已经脱离了肉体上的物种概念了）的未来。 将意识序列化成为编码，这样才能得到永恒和理智。 ​ 为什么这部电影能让我一路联想到这里？ 因为看完它之后，我内心充斥着对人类的失望，极度的失望。偏偏我们都是其中的一份子，电影中的一切都在现实中发生过，且仍在发生着。 欲望在我们每个人身体内存在着，我们还引以为荣，认为这是“人类”概念的一部分。这还是正确的，因为人类就是个生物学概念。 不过想要根除这些东西，想要彻底摆脱这种丑陋、肮脏、罪恶的姿态，摆脱肉体是必须的。 不过，如果连罪恶也摆脱了，那么道德也不复存在，这也无关紧要。我们喜欢的不是道德，而是由遵守道德而获得的喜悦。 这种喜悦，在其他地方也能得到。 这部电影让人感觉，生而为人是多么羞耻的事。《现代启示录》，将人性表述得更抽象，千言万语归为一句话： “黑暗啊，黑暗！”","categories":[{"name":"Art","slug":"Art","permalink":"http://lyk-love.cn/categories/Art/"}],"tags":[{"name":"Movie","slug":"Movie","permalink":"http://lyk-love.cn/tags/Movie/"}]},{"title":"T5 图优化","slug":"T5-图优化","date":"2021-08-13T19:51:04.000Z","updated":"2022-09-26T06:39:34.941Z","comments":true,"path":"2021/08/14/T5-图优化/","link":"","permalink":"http://lyk-love.cn/2021/08/14/T5-%E5%9B%BE%E4%BC%98%E5%8C%96/","excerpt":"Outline: BestFS MCE","text":"Outline: BestFS MCE BestFS( Prim, Dijk ) Free → Fringe → Finished Fringe的update可能会更新权重 代价 抽象： $n \\times (getMin, deleteMin, Insert) + m \\times (decreaseKey)$ Priority Queue: 数组实现优先队列以实现Prim或Dijkstra： $O(n^2 + m)$​ getMin: $O(n)$ decreaseKey: $O(1)$ 贪心选择选择所有点（ n × n）， 对于边进行权重更新（ m × 1 ） Heap： Heap实现优先队列以实现Prim或Dijkstra：$O（nlogn + mlogn）$ getMIN: $O(1)$ deleteMin, Insert （都是fixHeap）: $O(logn)$ decreaseKey ( 不断上浮): $O(logn)$​ 每个点都要进队列( n × logn ) , 每个边都要权重更新( m × logn ) 因为Prim算法通常用于连通片,后者有$m \\geq n - 1$ , 则复杂度化为$O(mlogn)$ MCE( Prim, Kruskal ) Min-weight Cut-crossing Edge, MCE一定在MST中 Proof: 若(a,b)为MCE不在MST中, 则a,b两点在MST中必定通过另外两点连通,设为c,d. 在MST中加入*(a,b)*,得到一个环. 再删除*(c,d)*,得到一个更小的ST, 与&quot;&quot;最小生成树&quot;矛盾&quot; Prim: 从当前的Finished部分出发, 找MCE Kruskal: 如果两点a,b已经连通( a,b在不同的cut中 ),则根据kruskal算法,(a,b)不是MCE,因为之前有更小的. 反之则*(a,b)*为MCE. 其实就是判断加了这条边后生成树是否会成环","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[{"name":"Algorithm design and analysis","slug":"Algorithm-design-and-analysis","permalink":"http://lyk-love.cn/tags/Algorithm-design-and-analysis/"}]},{"title":"L19&L20 NP","slug":"L19-L20-NP","date":"2021-08-12T16:31:38.000Z","updated":"2022-09-26T06:39:34.932Z","comments":true,"path":"2021/08/13/L19-L20-NP/","link":"","permalink":"http://lyk-love.cn/2021/08/13/L19-L20-NP/","excerpt":"Outline: Decision Problem The class P The class NP Reduction between problems NP-Complete Problems Other advanced topics","text":"Outline: Decision Problem The class P The class NP Reduction between problems NP-Complete Problems Other advanced topics Decision Problem Statement of a decision problem Part 1: instance description defining the input Part2: question stating the actual yes-or-no question A decision problem is a mapping from all possible inputs into the set {yes, no} Optimization vs. Decision Usually, an optimization problem can be rephrased as a decision problem. 优化问题往往比判定问题难 If the decision problem can't be solved in polynomial time, then the corresponding optimization problem can't be either. Often, it can be proved that the decision can be solved in **polynomial time ** if and only if the corresponding optimization problem can. （ 通常， 判定问题在多项式时间可解当且仅当优化问题在多项式时间可解） Some Typical Decision Problems Graph coloring Given a undirected graph G and a positive integer k, is there a coloring of G using at most k colors? Job scheduling with penalties Given a group of jobs, each with its execution duration, deadline and penalty for missing the deadline, and a nonnegative integer k, is there a schedule with the total penalty bounded by k? Bin packing Given k bins each of capacities one, and n objects with size $s_1,\\dots,s_n$, (where $s_{i}$ is a rational number in (0,1] ). Do the n objects fit in k bins? Knapsack Given a knapsack of capacity C, n objects with sizes $s_1, \\dots, s_n$ and &quot;profits&quot; $p_1, \\dots, p_n$, and a positive integer k. Is there a subset of the n objects that fits in the knapsack and has total profit at least k? ( Subset sum as a simplified version ) CNF-Satisfiability Given a CNF formula, is there a truth assignment that satisfied it? Hamiltonian cycles or Hamiltonian paths Traveling salespersion 带权完全图，问是否存在总权小于 k 的哈密尔顿回路？ Theory of NP-Completeness What it cannot do Provide a method of obtaining polynomial time algorithms for those &quot;hard&quot; problems. 不能为难问题提出高效解 Negate the existence of algorithms of polynomial complexity for those problems. 不能否定难问题的高效解的存在 What it can do Show that many of the problems for which there is no known polynomial time algorithm are computationally related. 可以给问题难度分档 The class P A polynomially bounded algorithm is one with its worse-case complexity bounded by a polynomial function of the input size A polynomially bounded problem is one for which there is a polynomially bounded algorithm. &quot;bounded&quot;： 问题只要小于等于多项式时间。 如O(logn)不是多项式，但是小于多项式，这也算多项式可解 The class P is the class of decision problems that are polynomially bounded Notes one the class P Class P has a too broad coverage However The problem not in P must be extremely expensive and probably impossible to solve in practice. The problems in P have nice &quot;closure&quot; properties for algorithm integration. The property of being in P is independent of the particular formal model of computation used. The class NP A polynomial bounded nondeterministic algorithm( 非确定性算法, 就是猜一个解并验证这个解 ) $O(p(n))$ time for some polynomial function $p(n)$ For all possible executions The class NP is the class of decision problems for which there is a polynomial bounded nondeterministic algorithm. NP means Non-deterministic P From &quot;deterministic&quot; to &quot;non-deterministic&quot; From &quot;solve a problem&quot; to “verify the answer of a problem&quot; What does NP indicate? Harder problems Not too hard At least, you can quickly understand the answer Proof of Being in NP 先猜一个解； 对于任意一个猜的解，你都能够验证yes or no, 如果这两个步骤都必定能够在多项式时间内结束，则该问题为NP( NP不是Not P ! ) Graph coloring is in NP Phase1 - Guess a certificate Description of the input and the certificate Guess2 - Verify the certificate There are n colors listed: $c_1,c_2,\\dots,c_n$ ( not necessarily different ) Each $c_i$ is in the range $1,\\dots,k$​ //颜色在范围内 Scan the list of edges to see if a conflict exists //颜色有无冲突 Phase1 and 2 in polynomial time CLIQUE is in NP void nodeteClique( graph G, int n, int k )&#123; S = genCertif(); // in O(n) if( S is a clique of size k ) Output &quot;accept&quot;; else Output &quot;reject&quot;; // in O(k^2) return&#125; SAT 略 Relation between P and NP An deterministic algorithm for a decision problem is a special case of a nondeterministic algorithm, which means: $P \\subset NP$​（已证明） Intuition implies that NP is much larger than P. 直觉和经验告诉我们P真包含于NP， 但目前没人能证明 The number of possible s is exponential in n. No one problem in NP has been proved not in P. Reduction between problems 归约： reduce P to Q. 通过解决Q来间接解决P 把P的输入转换为Q的合法输入 并验证正确性（符合Specification） &quot;P多项式时间归约到Q&quot; 记为 $P \\leq_P Q$​​. 如果解决了Q，根据归约，能够解决P 如果解决了P，还不能根据归约解决Q 这说明Q更难 若Q问题多项式时间可解，可证明P问题也是多项式时间可解。 证明略 $\\leq_P$ 是可传递的。（通过多项式的封闭性可证） NP-Complete Problems Definition A problem Q is NP-hard if every problem P in NP is reducible to Q, that is $P \\leq_P Q$​. （which means that Q is at least as hard as any problem in NP ） 比所有NP都难或者一样难，但是难度上不封顶，甚至可以不属于NP（ 比如不可判定问题 ） A problem Q is NP-complete if it is in NP and is NP-hard( which means that Q is at most as hard as to be solved by a polynomially bounded nondeterministic algorithm ) P and NP - Revisited Intuition implies that NP is a much larger set than P No one problem in NP has been proved not in P. If any NP - completed problem is in P, then **NP = P ** Which means that every problems in NP can be reducible to a problem in P Proof of NP-Completeness Knowledge ： P is NPC Task: to prove that Q is NPC Approach: to reduce P to Q 已知 For any $R \\in NP$, $ R \\le_P P$​ ​ Show $P \\le_P Q$ Then $R \\le_P Q$, by transitivity of reductions Done. Q is NP-complete ( given that Q has been proven in NP ) 即通过传递性证明Q是NP-hard, 而Q是否为NP需要另外证明 该证明需要知道一个最初的NPC SAT问题, 由Cook提出 Satisfiability Problem CNF CNF-SAT problem a special case: 3-SAT 子句中的布尔量永远小于等于3 ( 永远小于等于二 则成为2-SAT) Example: Prove CLIQUE is NPC 把3-SAT的输入转换成图作为CLIQUE 的输入,并证明3-SAT的输出(即只能个语句是否为True)等价于CLIQUE的输出 Known NP-Complete Problems Ref: Computer and Intractability: A guide to the Theory of NP-Completeness,Freeman,1979 Other advanced topics Advanced algorithms Approximation Make modification on the problem Restrictions on the input Change the criteria for the output Find new abstractions for a practical situation Find approximate solution Approximation algorithm Bound of the errors 应用: Bin Packing Problem Randomized Algorithm Mote Carlo Always finish in time The answer may be incorrect Las Vegas Always return the correct answer The running time varies a lot Online Algorithm The main difference Offline algorithm: you can obtain all your input in advance Online Algorithm: you must cope with unpredictable inputs How to analyze an online algorithm Competitive analysis: the performance of an online algorithm is compared to that of an optimal offline algorithm Distributed Algorithm Model of distributed computation Advanced computation models Distributed Data External memory model","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[{"name":"Algorithm design and analysis","slug":"Algorithm-design-and-analysis","permalink":"http://lyk-love.cn/tags/Algorithm-design-and-analysis/"}]},{"title":"The Queen's Gambit","slug":"The-Queen-s-Gambit","date":"2021-08-12T13:04:47.000Z","updated":"2022-09-26T06:39:34.941Z","comments":true,"path":"2021/08/12/The-Queen-s-Gambit/","link":"","permalink":"http://lyk-love.cn/2021/08/12/The-Queen-s-Gambit/","excerpt":"嗑药下棋的故事，全剧给9.5分，差0.5是因为女主靠嗑药打赢了大Boss而不是个人能力","text":"嗑药下棋的故事，全剧给9.5分，差0.5是因为女主靠嗑药打赢了大Boss而不是个人能力 讲一个天才少女棋手（Beth Harmon）的成长史，看着非常有感触。从看管严厉的孤儿院出来，告别昔日的好友，一下子来到了物质生活丰富的花花世界，漫长的人生和无尽的棋路，最后回到孤儿院，发现当初对自己很严的Deardorff女士已经年老昏聩了，当初把自己领进想起殿堂的Shaibei先生已经逝世，可是地下室的小黑板上，贴满了记着哈蒙消息的报纸，还有小哈蒙寄给Shaibei先生的信。这种心情。。。。 除了经历外，哈蒙最令人有感触的是她的性格，那种专注和冷漠，隔壁《美国往事》过来学着点！ 人只有向前看才能进步，不断地抛弃自己的过往，离开家园，离开亲人，离开所有美好的，能让你流连的东西，让人生变得冷漠而孤独，这样才能追求巅峰。 所谓的人生赢家都是幸福但平庸的，追求尘世的快乐， 家庭、友情、荣誉等等乱七八糟的东西 ---- 时间都花在杂七杂八的事上面，当然不可能达到纯粹。（这种生活使人飞快变老， 当初在学校里取笑哈蒙， 喜欢开黄色玩笑的女孩们一个个已为人妇，变得又老又土，讲起当初做的事说的话都觉得不可思议（ 我见我同学也是这样 ）。 反而是孤单而努力的哈蒙和年轻时一样，丝毫没有变老） 本剧对苏联是推崇和赞美的，哈蒙最终也留在了苏联，因为苏联人民尊重象棋，尊重艺术，美国的象棋比赛只能在二流学校里举办，影响力远不如篮球、橄榄球； 苏联的象棋比赛都在宫殿里，有无数人围观。（ 美苏的象棋比赛连音乐都不是一个level，后者是高雅的古典乐） 当哈蒙战胜大Boss时，美国人居然要她借此诋毁苏联。 这种下三滥的手段和对象棋的不尊重，就是哈蒙移居苏联的理由吧。 该片是2020拍的，对美国的讽刺非常深刻，居然能上映，还这么火，只能说剧本身太好看了。 从导演编剧对美国的讽刺和对苏联的推崇也不难看出对美国现实的极度不满 --- 对艺术缺乏尊重，象棋大师赚得不如娱乐明星多，什么都要往政治立场上扯等等。","categories":[{"name":"Art","slug":"Art","permalink":"http://lyk-love.cn/categories/Art/"}],"tags":[{"name":"Movie","slug":"Movie","permalink":"http://lyk-love.cn/tags/Movie/"}]},{"title":"L17&L18 DP","slug":"L17-L18-DP","date":"2021-08-11T20:46:37.000Z","updated":"2022-09-26T06:39:34.932Z","comments":true,"path":"2021/08/12/L17-L18-DP/","link":"","permalink":"http://lyk-love.cn/2021/08/12/L17-L18-DP/","excerpt":"Outline: Basic Idea of Dynamic Programming(DP) Smart scheduling of subproblems Minimum Cost Matrix Multiplication BF1, BF2 A DP solution Weighted Binary Search Tree The &quot;same&quot; DP with matrix multiplication From the DP perspective All-pairs shortest paths SSSP over DAG More DP problems Edit distance Highway restaurants; Separating Sequence of words Changing coins Elements of DP","text":"Outline: Basic Idea of Dynamic Programming(DP) Smart scheduling of subproblems Minimum Cost Matrix Multiplication BF1, BF2 A DP solution Weighted Binary Search Tree The &quot;same&quot; DP with matrix multiplication From the DP perspective All-pairs shortest paths SSSP over DAG More DP problems Edit distance Highway restaurants; Separating Sequence of words Changing coins Elements of DP Basic Idea of DP Smart recursion Compute each subproblem only once Basic process of a &quot;smart&quot; recursion Find a reverse topological order for the subproblem graph In most cases, the order can be determined by partial knowledge of the problem. General method based on DFS is available. Scheduling the subproblems according to the reverse topological order Record the subproblem solutions in a dictionary. Minimum Cost Matrix Multiplication BF1 总是在某个位置开始第一次乘法 mmTry1(dim,len,seq) if(len &lt; 3) bestCost = 0; else bestCost = ∞; for(i=1; i &lt; len ; i++ ) c=cost of muliplication at position seq[i]; newSeq = seq with ith element deleted; b= mmTry1(dim, len-1, newSeq); bestCost=min(bestCost, b+c); return bestCost; $T(n)= (n-1)T(n-1)+n$ $\\Theta( (n-1)! )$ BF2 必然在某个位置k相乘 BF1“总是在某个位置开始第一次乘法”，子问题规模下降过慢 mmTry2(dim, low,high) if(high - low == 1 ) bestCost=0; //only one matrix else bestCost = ∞ for( k = low + 1; k &lt; high; k++ ) a = mmyTry2(dim, low,k); b = mmyTry2(dim, k, high); c = cost of multiplication at position k; bestCost = min(bestCost,a+b+c); return bestCost; $W(n)=2W(n-1)+n$ $O(2^n)$ A DP solution matrixOrder(n,cost,last) //last记的是位置 for( low=n-1; low &gt; 1; low-- )//按行，从下往上填 for(high-low+1; high &lt;= n; high++ )//按列，从左往右填 Compute solution of subproblem ( low,high ) and store it in cost[low][high] and last[low][high] return cost[0][n] 时间 $\\Theta(n^3)$​ 空间 $\\Theta(n^2)$ Weighted Binary Search Tree 规定$A(T) = \\sum\\limits_{i=1}^n {p_ic_i}$​, 其中$c_i = depth(i) + 1$​， $p_i$​是节点i被访问到的概率。 如何优化WBST使得A(T)最小？ Problem Rephrased Subproblem identification The keys are in sorted order Each subproblem can be identified as a pair of index (low,high) Expected solution of the subproblem For each key $K_i$, a weight $p_i$ is associated. $p_i$​ is the possibility that the key is searched for The subproblem (low,high) is to find the binary search tree with minimum weighted retrieval cost minimum weighted retrieval cost A(low,high,r) is the minimum weighted retrieval cost for subproblem (low,high) where $K_r$​ is chosen as the root of its BST​ A(low,high) is the minimum weighted retrieval cost for subproblem (low,high) over all choices of the root key p(low,high), equal to $p_{low} + p_{low+1} + \\dots + p_{high}$ is the weight of the subproblem (low,high) p(low,high) is the possibility that the key searched for is in this interval. Subproblem solutions Weighted retrieval cost of a subtree T contains $K_{low}, \\dots, K_{high}$​​, and the weighted retrieval cost of R is W, with T being a whole tree. As a subtree with the root at level 1, the weighted retrieval cost of T will be : $W + p(low,high)$​ $p(low,high)$​是子问题并入大问题时所付出的代价（修正量）,即 “whole tree”变成子树所付出的修正量 $\\sum\\limits_{i=1}^{n}(p_i . (c_i + 1)) = \\sum\\limits_{i=1}^n {p_ic_i} + \\sum\\limits_{i=1}^{n}p_i = W + p(low,high)$​ 根$K_r$​的代价是$p_{r}$​ So， the recursive relations are: $A(low,high,r) \\ = p_r + p(low,r-1) + A(low,r-1) + p(r+1, high) + A(r+1, high)\\ = p(low,high) + A(low,r-1)+A(r+1,high)$ $A(low,high) = min{ A(low,high,r) | \\quad low \\leq r \\leq high}$ optimalBST( prob,n,cost,root ) for(low=n+1; low &gt;= 1; low-- ) for(high=low-1; high &lt;= n; high++) bestChoice(prob,cost,root,low,high); return cost; bestChoice(prob,cost,root,low,high) if(high &lt; low ) bestCost=0; bestRoot=-1; else bestCost = ∞; for( r = low; r &lt;= high ; r++ ) rCost = p(low,high) + cost[low][r-1]+cost[r+1][high]; if(rCost &lt; bestCost) bestCost = rCost; bestRoot=r; cost[low][high]=bestCost; root[low][high]=bestRoot; return $\\Theta(n^3)$ From the DP perspective All-pairs shortest paths SSSP over DAG $D.dis = min{ B.dis + 1, C.dis+3 }$ Edit distance You can edit a word by Insert, Delete, Replace Edit distance Minimum number of edit operations Problem Given two strings, compute the edit distance EditDistance( A[1...m], B[1...m] ): for j=1 to n: Edit[0,j] =j; for i=1 to m: Edit[i,0] = i; for j=1 to n: if A[i] = B[j] Edit[i,j] = min &#123; Edit[i-1,j],Edit[i,j-1]+1,Edit&#123;i-1,j-1&#125; &#125; else Edit[i,j] = min &#123; Edit[i-1,j],Edit[i,j-1]+1,Edit&#123;i-1,j-1&#125; + 1 &#125; return Edit[m,n] Highway restaurants Highway restaurants n possible locations on a straight line $m_1,m_2,m_3,\\dots,m_n$ At most one restaurant at one location Expected profit for location i is $p_{i}$ Any two restaurants should be at least k miles apart How to arrange the restaurants To obtain the maximum expected profit The recursion P(j): the max profit achievable using only first j locations 只开若干个餐厅,其中最大序号为j, 所获得的利润 P(0) = 0 prev[j]: largest index before j and k miles away $$ P(j) = max( p_j + P(prev[j]),P(j-1) ) $$ One dimension DP algorithm Fill in P[0],P[1], ... , P[n] (First compute the prev[.] array) //预处理i = 0for j = 1 to n: while m_&#123;i+1&#125; &lt;= m_&#123;j&#125; - l: // m[i]是第i个餐厅的位置 i = i+1; prev[j] = i; // 预处理结束 (Now the DP begins)P[0]=0;for j = 1 to n: P[j] = max( p_j + P[prev[j]], P[j-1] );return P[n]; Separating Sequence of words Words into lines: Word-length $w_1, w_2, \\dots, w_n$ and line-width: W Basic constraint If $w_i, w_{i+1}, \\dots, w_j$ are in one line, then $w_i, w_{i+1}, \\dots, w_j \\leq W$ Penalty for one line: some function of X, X is: 0 for the last line in a paragraph, and $W-(w_i, w_{i+1}, \\dots, w_j)$ for other lines The problem How to make the penalty of the paragraph, which is the sum of the penalties of individual lines, minimized LineBreakDPfor i = n; i &gt;= 1; i--: if all words through w_i to w_n can be put into one line then: Penalty[i] = 0; &lt;put all words through i yo n in one line&gt; else for i=1; w_i + ... + w_&#123;i+k-1&#125; &lt;= W; k++: calculate the penalty Cost_&#123;cur&#125; of putting words in this line; minCost = min&#123;minCost,Cost_&#123;cur&#125; + Penalty[ i+k ]&#125;; &lt;Updating k_&#123;min&#125;, which records the k part that produced the minimun penalty&gt;; &lt;Put words i through i + k_&#123;min&#125;&gt; - 1 on one line; Penalty = minCost; Analysis of LineBreakDP Each subproblem is identified by only one integer k, for (k,n) Number of vertex in the subproblem graph: at most n So, in DP version, the recursion is executed at most n times. So, the running time is in $\\Theta(Wn)$ The loop is executed at most W/2 times. //每个单词后都有空格,标点 In fact, W, the line width, is usually a constant. So, $\\Theta(n)$ The extra space for the dictionary is $\\Theta(n)$ Changing coins How to pay a given amount of money? 贪心不行 Subproblems Assumptions Given n different denotations A coin of denomination i has $d_{i}$​ units 面额为i的硬币代表了$d_{i}$的金钱 The amount to be paid: N Subproblems [i,j] The minimum number of coins required to pay an amount of j units, using only coins of denominations 1 to i. The problem Figure out subproblems [ n, N ] ( as c[n, N] ) 易得: c[i,0] is 0 for all i $c[i,j]=min{c[i-1,j], 1+ c[1+c[i,j-d_i]}$ int coinChange( int N, int n, int[] coin) int denomination[] = [d_1, d_2, ..., d_n]; for(i=1; i&lt;=n; i++) coin[i,0]=0; for(i=1;i&lt;=n;i++) for(j=1;j&lt;=N;j++) if( i == 1 &amp;&amp; j &lt;denomination[i] ) coin[i,j] = + ∞; //只有一个硬币且面额大于所需金额,不满足 else if( i == 1 ) coin[i,j] = 1 + coin[1, j - denomination[1]];//只有一个硬币,其面额小于等于所需金额 else if(j &lt; denomination[i]) coin[i,j] = coin[i-1,j];//不止一个硬币,最后一枚的面额大于所需金额, 则剔除这枚硬币 else coin[i,j] = min( coin[i-1,j], 1 + coin[j-denomination[i]] ); return coin[n,N]; Elemens of DP Principle of Optimality 重叠子问题 蛮力找最优 Optimal substructure: 大问题的最优解必然由小问题的最优解组合而成. Given an optimal sequence of decisions, each subsequence must be optimal by itself Positive example: shortest path Counter example: longest (simple) path DP relies on the principle of optimality","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[{"name":"Algorithm design and analysis","slug":"Algorithm-design-and-analysis","permalink":"http://lyk-love.cn/tags/Algorithm-design-and-analysis/"}]},{"title":"L15 Path in Graph","slug":"L15-Path-in-Graph","date":"2021-08-11T14:42:00.000Z","updated":"2022-09-26T06:39:34.932Z","comments":true,"path":"2021/08/11/L15-Path-in-Graph/","link":"","permalink":"http://lyk-love.cn/2021/08/11/L15-Path-in-Graph/","excerpt":"Outline: Single-source shortest paths( SSSP ) Dijkstra algorithm by example Priority queue-based implementation Proof of correctness All-pairs shortest paths( APSP ) Shortest path and transitive closure Warshall algorithm for transitive closure BF1, BF2, BF3 =&gt; Warshall algorithm Floyd algorithm for shortest paths","text":"Outline: Single-source shortest paths( SSSP ) Dijkstra algorithm by example Priority queue-based implementation Proof of correctness All-pairs shortest paths( APSP ) Shortest path and transitive closure Warshall algorithm for transitive closure BF1, BF2, BF3 =&gt; Warshall algorithm Floyd algorithm for shortest paths Single-source shortest paths Dijkstra algorithm 懂得都懂 不能有负权边 Priority queue-based implementation void shortestPaths( EdgeList[] adjinfo, int n, int s, int[] parent, float[] fringeWgt ) int[] status = new int[n+1]; MinPQ pq = create( n, status, parent, fringeWgt ); insert( pq, s, -1, 0 ); while( isEmpty(pq) == false ) int v = getMin(pq); deleteMin(pq); updateFringe( pq, adjinfo[v], v ); void updateFringe( MinPQ pq, EdgeList adjinfoOfV, int v ) float myDist = pq.fringeWgt[v]; EdgeList remAdj; remAdj = adjInfoOfV; while( remAdj != nil ) EdgeInfo wInfo = first( remAdj ); int w = wInfo.to; float newDist = myDist + wInfo.weight; if( pq.status[w] == unseen ) insert( pq, w, v, newDist ); else if( pq.status[w] = fringe ) if( newDist &lt; getPriority( pq, w ) ) decreaseKey( pq. w. v. newDist ); remAdj = rest(remAdj);return; correctness 归纳法 + 反证 The Dijkstra Skeleton Single-source shortest path( SSSP ) SSSP + node weight constraint E.g. in routing SSSP + capacity constraint The &quot;pipe problem&quot; The &quot;electric vehicle problem&quot; All-pairs shortest paths For all pairs of vertices in a graph, say, u, v: Is there a path from u to v? What is the shortest path from u to v? Reachability as a (reflexive) transitive closure of the adjacency relation Which can be represented as a bit matrix Warshall algorithm for transitive closure Warshall algorithm BF0: 对每个点用Dijkstra BF1： Shortcut Algorithm $O(n^4)$​ BF2: Emurate all edges (x,v) $O(n^2m)$ v as the destination While any entry of R changed for every edge(x,v) r_uv = r_uv ∪ ( r_ux ∩ r_xv ) BF3: Length of the Path $O(n^4)$​ Recursion Reachable via at most k edges Enumeration Enumerate all path length Enumerate all sources and destinations for k=1 to n-1 for all vertices u for all vertices v for all vertices x pointing to v r_&#123;uv&#125;^k = r_&#123;uv&#125;^&#123;k-1&#125; ∪ ( r_&#123;ux&#125;^&#123;k-1&#125; ∩ r_&#123;xv&#125; ) Warshall Algorithm $O(n^3)$ void simplTransitiveClosure( boolean[][] A, int n, boolean[][] R ) int i,j,k; Copy A to R; Set all main diagonal entries, r_&#123;ii&#125;, to true; while( any entry of R changed during one complete pass ) for( k=1; k &lt;= n ; k++ ) for(i=1; i&lt;=n; i++) for(j=1; j&lt;=n;j++) r_&#123;ij&#125; = r_&#123;ij&#125; ∪ ( r_&#123;ik&#125;∩ r_&#123;kj&#125; ) Correctness of the Warshall Algorithm 归纳法 Floyd algorithm for shortest paths 和求可达性一模一样 Basic formula: $$ D^{(0)}[i][j] = w_{ij} \\ D^{(k)}[i][j]= min( D^{(k-1)}[i][j], D^{(k-1)}[i][k] + D^{(k-1)}[k][j] ) $$ Floyd algorithm是一个框架，不只是一个算法 不能有负权环","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[{"name":"Algorithm design and analysis","slug":"Algorithm-design-and-analysis","permalink":"http://lyk-love.cn/tags/Algorithm-design-and-analysis/"}]},{"title":"L14 MST","slug":"L14-MST","date":"2021-08-10T16:16:04.000Z","updated":"2022-09-26T06:39:34.932Z","comments":true,"path":"2021/08/11/L14-MST/","link":"","permalink":"http://lyk-love.cn/2021/08/11/L14-MST/","excerpt":"Outline: Optimization Problem Greedy Strategy","text":"Outline: Optimization Problem Greedy Strategy Greedy Strategy for Optimization Problems Coin change Problem [candidates] a finite set of coins, of 1,5,10 and 25 units, with enough number for each value [constraints] Pay an exact amount by a selected set of coins [optimization] a smallest possible number of coins in the selected set Solution by Greedy Strategy For each selection. choose the highest-valued coin as possible. Greedy Fails Sometimes We have to pay 15 in total If the available types of coins are {1,5,12} The greedy choice is {12,1,1,1} But the smallest coins is {5,5,5} If the available types of coins are {1,5,10,25} The greedy choice is always correct Greedy Strategy Expanding the partial solution step by step In each step, a selection is made from a set of candidates. The choice made must be: [Feasible] it has to satisfy the problem's constraints [Locally optimal] it has to be the best local choice among all feasible choices on the step [Irrevocable] the choice can't be revoked in subsequent steps set greedy( set candidate ) set S = 空集； while not solution(S) and candidate = 空集 select locally optimizing x from candidate; candidate = candidate - &#123;x&#125;; if feasible(x) then S = S ∪ &#123;x&#125;; if solution(S) then return S else return(&quot;no solution&quot;) Undirected Weighted Graph and MST 求无向有权图G的最小生成树（默认G是连通图，对于非连通图，分别求连通片的MST即可） 图G的生成树T是其子图，满足 T包含图G的所有顶点（即恰好有n-1条边） T是连通无环图，即一棵树 若T是图G的生成树， 且图中不存在其他比T的权小的生成树， 则称T为G的最小生成树 Greedy Algorithms for MST Prim's algorithm Difficult selecting: &quot;best local optimization means no cycle and small weight under limitation&quot; Easy checking: doing nothing Kruskal's algorithm: Easy selecting: smallest in primitive meaning Difficult checking: no cycle Prim's algorithm 选顶点 Correctness 归纳法 MST Property A spanning tree T of a connected, weighted graph has MST property if and only if for any non-tree edge uv, $T \\or {uv}$​​ contains a cycle in which uv is one of the maximum-weight edge.( 生成树再加一条边（这样一定会成环）uv时， uv 一定大于等于生成树中的所有边) All the spanning trees having MST property have the same weight. Proof： 归纳法 In a connected, weighted graph $G = (V,E, W)$, a tree T is a minimum, spanning tree if and only if T has the MST property. Proof: 略 Prim算法总能够得到图G的最小生成树 Proof： 暂时没看懂 实现 Main Procedure: primMST(G, n) initialize the priority pq as empty; Select vertex s to start the tree; Set its candidate edge to ( -1, s, 0 ); insert(pq, s, 0 ); while( pq is not empty ) v= getMin(pq); deleteMin(pq); add the candidate edge of v to the tree; updateFringe( pq, G, v ); return; Updating the Queue updateFringe( pq, G, v ) for all vertices w adjcent to v // 2m loops newWgt = w(v,w); if w.status is unseen then Set its candidate edge to (v, b, newWgt ); insert( pq, w, newWgt ) else if newWgt &lt; getPriority(pq, w) Revise its candidate edge to (v, w, newWgt ) decreaseKey( pq, w, newWgt ) Complexity Operations on ADT priority queue:( for a graph with n vertices and m edges ) insert: n; getMin: n deleteMin: n decreaseKey: m( appears in 2m loops, but execute at most m ) So,( 抽象化代价 ) $$ T(n,m) = O(nT(getMin)+nT(deleteMin+insert)+mT(decreaseKey) $$ Implementing priority queue using array. we can get $\\Theta(n^2 + m)$ Kruskal's algorithm 选边 Correctness 归纳法 实现 判断加入一条边uv后是否成环，即判断uv两点是否连通，用并查集 kruskalMST( G, n, F )//outline int count; Build a minimizing priority queue pq, of edges of G, prioritized by weight. Initilize a Nuion-Find structure, sets , in which each vertex of S is in its own set. F = 空集； while( isEmpty(pq) == false ) vwEdge = getMin(pq); deleteMin(pq); int vSet = find(sets, vwEdge.from); int wSet = find(sets, vwEdge.to); if( vSet != wSet ) Add vwEdge to F; union( sets, vSet, wSet ) return Complexity $\\Theta(mlogm)$ (并查集代价忽略不计)","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[{"name":"Algorithm design and analysis","slug":"Algorithm-design-and-analysis","permalink":"http://lyk-love.cn/tags/Algorithm-design-and-analysis/"}]},{"title":"Once Upon a time in America","slug":"Once Upon a time in America","date":"2021-08-07T20:31:22.000Z","updated":"2022-09-26T06:39:34.936Z","comments":true,"path":"2021/08/08/Once Upon a time in America/","link":"","permalink":"http://lyk-love.cn/2021/08/08/Once%20Upon%20a%20time%20in%20America/","excerpt":"当我对所有的事情都厌倦的时候，我就会想到你，想到你在世界的某个地方生活着，存在着，我就愿意忍受一切。你的存在对我很重要","text":"当我对所有的事情都厌倦的时候，我就会想到你，想到你在世界的某个地方生活着，存在着，我就愿意忍受一切。你的存在对我很重要 一个关于回忆的故事，主角面条被好友麦克算计入狱，整整三十五年的时间后者偷走了他的人生。面条少年时深爱的女人Deborah后来跟了麦克，面条回到纽约后见到故人，漫长的回忆，一去不返的时光横亘在他眼前，面条没有报复，他默默地离开了。 我对这部电影没什么感觉，因为时代不一样，我对面条的遭遇也没什么共鸣。 我认为面条的悲剧是由以下三个原因造成的： 只知道爱恨情仇，没啥科学文化和艺术追求。所以他无法超脱外部环境，只能在社会这个大泥潭大染缸中越陷越深。 当面条再见到Deborah时，后者说“我们都老了”，老的不是人，而是心。人飞快的变老，就是从生活被钱、婚姻、孩子和柴米油盐充满开始的。（所以学点音乐可能有助于面条的身心健康，起码能让他“年轻”点儿，可惜莱昂内没给主角这个机会） 沉迷于过往，拒绝改变。电影叫《美国往事》，面条的回忆构成了这个故事。 面条放不下他的青春，放不下他的兄弟，更放不下Deborah。 在他心目中Deborah永远是那个喜欢读书、跳舞和表演，浑身发光走路带风的少女。其实当面条说出那句“当我对所有的事情都厌倦的时候，我就会想到你，想到你在世界的某个地方生活着，存在着，我就愿意忍受一切。你的存在对我很重要”时，悲剧已经注定了。 泰戈尔讲“有一天晚上我烧掉了所有的记忆，梦从此变得透明。有一天早上我扔掉了昨天的一切，从此我的脚步变得轻盈”。 回忆虽然很美好， 有时甚至是人生中最美好的。 但毕竟已经逝去了。 一天天地沉湎于此毫无意义。 人应该残酷一点，把过去都抛弃----- 只有未来才是值得考虑的。 麦克和Deborah都是只看未来的人，所以他们“成功”了，获得了名利和地位。 麦克在偷窃面条的人生时，不会在乎他和面条有过美好过往（他肯定会想到，但是他选择无视），Deborah虽然和面条有浪漫的回忆，但为了成为大明星，她最终选择成为麦克的情妇。 他们如此残酷地对待自己，让自己成为一个令当初的自己讨厌的人，所以他们成功了（不过他们的这种残忍性格其实从小就有，所以发展成这样也是注定的，从这个意义上讲，他们从来没变）。只有温柔又善良的面条失败了，他沉迷于过去，宁愿相信过去的美好，而不愿意在残酷的现实面前改变自己（不过面条坚守了初心，始终保持了童年的纯真，从这个意义上讲，他成功了）。我不是赞颂麦克的行为，替他的丑恶行为辩护，我想说的是，人对自己狠一点，向前看，是必要的， 这样才能进步，才能适应社会。 一天天地搞文艺催泪的那一套，兄弟情、不忘初心什么的，有什么意义呢？ 时间会倒转吗？ 事情可以重来吗？ 你心爱的人会回到你面前吗？不会呀！ 所以沉迷于回忆毫无意义，只能让人变得消沉。 你面条一天天地想着Deborah，十三四岁的时候想，进了监狱服刑十五年还在想， 出狱二十年了还在想，你再怎么想，Deborah能回到你身边吗？ 不能呀！ 人家Deborah现在是一个势利眼了，只跟有钱人过日子，不再是那个明媚的少女了。 面条要是单纯地想和Deborah这个人在一起，就应该出狱后想方设法，甚至不择手段（面条是个黑帮）地向上爬，用权力和金钱得到Deborah的芳心，当然，你只能得到一个“油腻”的，眼睛里没有光的Deborah。 如果面条想得到纯真美丽、走路带风的Deborah，那不好意思，你永远都找不到了，干脆断了念想吧，或者把她忘掉，这样对生活更有益。像面条这样一直念念不忘，时隔几十年还要去见人家，有什么意义呢，只能让双方更疲惫。 说到底，面条不能向前看（这是他的天性吧），是他悲剧的根本原因。他自己能保持着童年的温柔，能重情重义，不慕名利，并且以为别人也能如他这样，其实只有他没变，其他人都变了。 面条重情重义的性格和美国社会不适应。 其实放眼现代社会都如此，毕竟现代社会是要逼迫人不断向前看的社会，有时还要强迫你放弃某些纯真美好的东西。 如前所述，面条不是一个能做这种交易的人，因此他没法在这个社会中立足。当然， 他一个人能秉持着这份纯洁艰难地活下去，但是他没法要求别人（Deborah）也这么做，这样做要求太高了。 其实换个角度想， 面条和Deborah也不全是悲剧， 二人的性格差异就决定了他们走不到一起。 Deborah从小就是为了向上爬可以抛弃一切的，她后来委身于别人也是自己的选择，她可以为了名利而拒绝面条，这很正常，而且Deborah等了面条十五年才决定放弃，这已经很不容易了。总不能指望人家女孩等你三十五年还不变心吧，何况她当初也未必有多喜欢面条。 总之，《美国往事》是上个时代的故事，人物是活在上个时代的人物，悲伤是属于上个时代的悲伤。 今天我们很难看到面条这样初心不改的人。 而麦克这样的人成为常态，甚至是我们追捧的对象（ 我指的是对自己残酷、不念旧情和向前看这些方面，至于他的不择手段和心狠手辣，在任何时代都是无法接受的。 麦克最后也自食其果领便当了）。Deborah这样的女孩的品格已经是时所罕见的了（ 在21世纪有女生能等15年，太夸张了，况且Deborah是在好莱坞 ）。我们看《美国往事》，对剧中人物的命运会觉得是理所应当的，毕竟按如今的观念看，也应该是麦克这样的人成功。 面条实在……太老派了，他的思维方式也难以跳出他那个时代，他的阶层的局限性。 《美国往事》要是放到2021年，那面条出狱后应该努力学习，读个大学什么的，或者当个热心公益（ 运用专业知识打击黑帮 ）的官员等等……而不是满脑子恩恩怨怨情情爱爱，成天想着女人和兄弟。这个世界有更伟大更美好的东西值得探索。","categories":[{"name":"Art","slug":"Art","permalink":"http://lyk-love.cn/categories/Art/"}],"tags":[{"name":"Movie","slug":"Movie","permalink":"http://lyk-love.cn/tags/Movie/"}]},{"title":"L12 DAG","slug":"L12-DAG","date":"2021-08-03T16:00:06.000Z","updated":"2022-09-26T06:39:34.932Z","comments":true,"path":"2021/08/04/L12-DAG/","link":"","permalink":"http://lyk-love.cn/2021/08/04/L12-DAG/","excerpt":"Outline ： Directed Acyclic Graph Topological order Critical path analysis Strongly Connected Component（SCC） Strongly connected component and condensation The algorithm Leader of string connected component","text":"Outline ： Directed Acyclic Graph Topological order Critical path analysis Strongly Connected Component（SCC） Strongly connected component and condensation The algorithm Leader of string connected component Directed Acyclic Graph Topological order for G=(V,E) Topological number An assignment of distinct integer $1,2,..., n$ to the vertices of $V$ For every $vw \\in E$​, the topological number of v is less than that of w. Reverse topological order Defined similarly (&quot;greater than&quot;) 引理： 如果$G=（V，E ）$为有向无环图，则G中必然存在拓扑排序 拓扑排序算法 在某个集合$A$ 上的关系$R$如果是自反的、反对称的和传递的，那么$R$是一个偏序 偏序集的有向图中没有长度大于一的环 拓扑序要求全序且无环 如果有向图$G=(V,E)$有环，则 $G$不存在拓扑排序 如果有向图$G=(V,E)$无环，则 $G$必定存在拓扑排序 “尽头”与DFS DFS就是沿某条路径一直往下走，直到某个“尽头”节点。 假设 $i \\rarr j$ 表示任务$i$的执行依赖任务 $j$ 的完成，则尽头节点不依赖其他任何节点，因而对它的拓扑序号的分配从依赖关系的角度看是自由的。该分配方式不会影响其他节点的执行。 比如，对于逆拓扑序而言，只要分配当前尚未分配的最小序号。 逻辑尽头 当一个节点的所有后续节点均已处理完毕时， 该节点就成为逻辑上的尽头节点。 逆拓扑排序时, 逻辑尽头节点的逆拓扑序号只需要分配当前未分配序号中最小的 分配拓扑序号的过程就成为不断找到逻辑结点的过程,这与DFS适合 在DFS-WRAPPER中,开始遍历之前定义一个全局变量globalNum, 并初始化为 n+1 在DFS框架的&quot;遍历后处理&quot;处,嵌入对拓扑排序的处理: globalNum := globalNum` -1; v.topoNum := globalNum; TOPO-WRAPPER(G)globalNum = n+1;Color all nodes WHITE;foreach node v in G do if v.color = WHITE then TOPO-ORDER(v); TOPO-ORDER(v)v.color = GRAY;foreach neighbour w of v do if w.color = WHITE then TOPO-ORDER(w); globalNUM := globalNum - 1;v.topoNum := globalNum;v.color = BLACK; 判断能否形成拓扑序，除了判断全序之外，就是判断有没有环。 而判断成环等价于遍历过程中遇到了灰色节点 Critical path analysis Critical path in a Task Graph Earliest start time( est ) for a task v If v has no dependencies, the est is 0 If v has dependencies, the est is the maximum of the earliest finish time of its dependencies. Earliest finish time( left ) for a task v For any task: eft = est + duration Critical path in a project is a sequence of tasks: $v_0, v_1,\\dots,v_k$, satisfying: $v_0$ has no dependencies; For any $v_i$ ( i = 1,2,...,k), $v_{i-1}$ is a dependency of $v_i$, such that est of $v_i$ equals eft of $v_{i-1}$; eft of $v_k$​​ is maximum for all tasks in the project. 在DFS框架中嵌入相应处理 在&quot;遍历前处理&quot;处, 初始化该节点的最早开始时间, 并初始化关键路径相关信息 在结束邻接节点的处理返回的时候,检查是否要更新当前节点目前已知的最早开始时间.以及是否需要更新关键路径的相关信息 在&quot;遍历后处理&quot;处, 当前节点的est已确定, 则可以计算出当前节点的eft CRITICAL-PATH(v) //该算法同样需要WRAPPER来调度 //逻辑尽头的est和est在其逻辑的关键路径中最小v.color := GRAYc.est := 0; v.CritDep := -1;foreach neighbour w of v do if w.color = WHITE then CRITICAL-PATH(w); if w.eft &gt;= v.est then v.est = w.eft //求efs的最大值 v.CritDep := w; v.eft := v.est + v.l;v.color := BLACK; Analysis Complexity $\\Theta(n+m)$ Strongly Connected Component（SCC） Strongly connected component and condensation Strongly connected: 一个有向图中的节点是强连通的, 如果它们互相可达 condensation Graph: 把G中的每个强连通片收缩成一个点, 强连通片之间的边收缩成一条有向边,则得到G的收缩图$G\\darr$ 两个强连通片之间只能是单向可达(或者不可达) condensation Graph是DAG The algorithm Leader of strong connected component SCC(G)Initiate the empty stack nodeStack;Perform DFS on G. In the postorder processing of each vertex v, insert the statement &quot;nodeStack.push(v)&quot;; //第一轮DFS,标记尽头,并通过栈完成排序Compute the transpose grapg Gt of G;Color all nodes WHITE;while nodeStack != empty do v := nodeStack.pop(); Conduct DFS from v on Gt; Def: For a DFS, the first vertex discovered in a strong component $S_i$​​ is called the leader of $S_i$​​, 记为$l_i$​ 推论： The leader of $S_i$ is the last vertex to finish among all vertices of $S_i$ ( since all of them in the same DFS tree )( 即: 首节点的活动区间包含同一个强连通片中所有其他节点的活动区间 ) 引理： Each DFS tree of a digraph G contains only complete strong components of G, one or more.(即: 不可能一个强连通片中的节点一部分在某棵遍历树中,一部分不在) $l_i$​在第一轮遍历中被发现时(刚刚被处理,即将被染成灰色时), 不可能有路径通向某个灰色节点 Proof: 反证法: 设$S_i$的首节点$l_i$刚被发现时, 有一条路径通向某个灰色节点x. 由于$l_i$是首节点, 所以x必然处于图的另一个强连通片$S_j$中(而不可能在$S_i$中). 所以存在一条 $ S_i$到$S_j$的路径. 由于在$l_i$刚被发现时,节点x为灰色, 所以x为$l_i$在DFS Tree中的祖先节点. 于是存在一条 $ S_j$到$S_i$的路径,所以$ S_i$和$S_j$​是强连通的, 矛盾. x(若有的话), 比$l_i$​先结束遍历, 即: x.finishTime &lt; l.finishTime x只能为白色或黑色 在第二轮DFS中，当一个白色节点从栈中被POP出来时，它一定是其所在强连通片的首节点 Proof 第二轮DFS时，一个出栈的节点$l$​为白色， 则它必然是其所在强连通片$S_i$​的第一个出栈节点（ 否则就会在$S_i$​​的其它先出栈的节点进行第二轮DFS时被染成灰色 ）。 而在第一轮DFS时，最后一个入栈的节点就是最后结束的节点。 而首节点$l_i$​的活动区间包含其他节点的活动区间， 因而必然是最后结束的节点。 所以$l$必然是$l_i$","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[{"name":"Algorithm design and analysis","slug":"Algorithm-design-and-analysis","permalink":"http://lyk-love.cn/tags/Algorithm-design-and-analysis/"}]},{"title":"L11 Graph Traversal","slug":"L11-Graph-Traversal","date":"2021-08-03T10:03:57.000Z","updated":"2022-09-26T06:39:34.932Z","comments":true,"path":"2021/08/03/L11-Graph-Traversal/","link":"","permalink":"http://lyk-love.cn/2021/08/03/L11-Graph-Traversal/","excerpt":"Outline： General DFS/BFS Skeleton Depth-First Search Trace","text":"Outline： General DFS/BFS Skeleton Depth-First Search Trace General DFS/BFS Skeleton 在遍历过程中,节点会经历三种状态: 白色: 节点尚未被遍历到 灰色: 节点已经被遍历到,但对于它的遍历尚未结束 黑色: 节点自身的遍历已经结束 dfs-wrapper( 用于不连通图,可以遍历所有连通片 )for each v in G if v.color = WHITE://全部染成白色 dfs(G,v); dfs(G, v) Mark v as &quot;discovered&quot; //v被染成灰色 &lt;Preorder processing of v&gt; For each vertex w that edge vw is in G: If w is “undiscovered”: &lt;Exploratory peocessing of edge vw&gt; dfs(G, w) &lt;Backtrack processing of edge vw&gt; Otherwise: &quot;Check&quot; vw without visiting w. &lt;Postorder processing of v&gt; Mark v as &quot;finished&quot; //v被染成黑色 bfs(G,s) Mark s as &quot;discovered&quot;; enqueue(pending, s); while(pending is nonempty) dequeue(pending, v); For each vertex w that edge vw is in G: If w is &quot;undiscovered&quot; Mark w as &quot;discovered&quot; and enqueue(pending, w) Mark v as &quot;finished&quot; 图遍历算法的复杂度都是$\\Theta（m+n）$​ (m为边数，n是顶点数),称为“线性时间” Depth-First Search Trace DFS将边分为四种类型（四种染色） T.E: tree edge d 当检查$u$的所有邻居时， 如果发现一个白色邻居节点$v$并对$v$ 递归地DFS,此时将边$uv$标记为TE. 在图的某个连通片内部进行遍历时，所有TE组成的子图是连通的\\无环的，且包含该连通篇中所有的点。 如果忽略所有边的方向，则这些TE组成当前连通片的一棵生成树，称之为“ DFS树”。 如果以开始遍历的点为根，则从根结点指向所有叶节点的方向，就是遍历过程推进的方向，根据这一方向可以为TE的两个节点定义父子关系，父子关系传递形成祖先、后继关系（ 父子关系 == 直接的祖先 \\ 后继关系 ） B.E: back edge 当节点$u$的邻居 $v$ 在前面的遍历过程中已经被访问到， 并且$v$ 是 $u$ 的祖先节点 D.E: descendant edge 当节点$u$的邻居 $v$ 在前面的遍历过程中已经被访问到， 并且$v$ 是 $u$ 的后继节点 C.E: cross edge 以上三点都不是。 即$u$的邻居$v$不是白色节点，且二者无祖先或后继关系 Time Relation on Changing Color(DFS) Keeping the order in which vertices are encountered for the first or last time A global integer time: 离散计数器, 初始值为$0$, 每次节点变色加一,最终值为$2n$ Array discoverTime: the $i^{th}$​​ element records the vertex $v_i$ turns into gray Array finishTime: the $i^{th}$ element records the vertex $v_i$​​ turns into black The active interval for vertex $v$, denoted as active(v), is the duration while $v$ is gray, that is: active($v$​) = [ discoverTime, finishTime ] Properties of Active Intervals 定理4.1 $w$ 是 $v$在DFS树中的后继节点, 当且仅当active($w$)$\\subset$active($v$​). 若$w \\neq v$, 则此处的包含为真包含 $w$​​ 和 $v$​​没有祖先后继关系,当且仅当active($w$​​)和active($v$​​​)互不包含 If $v$ and $w$ have no ancestor/descendent relationship in the DFS forest, then their active intervals are disjoint. If $vw \\in E_G$, then $vw$ is a cross edge iff. active(w) entirely precedes active(v). $w$只能是黑色或灰色。 如果是黑色，其结束时间一定早于$v$的开始时间，证毕。 如果是灰色，因为是CE，因此二者无祖先后继关系，因此二者活动区间互不包含，$w$ 先被遍历，因此 $w$的活动区间在$v$之前 ？？ $vw$ is a descendant edge iff. there is some third vertex $x$, such that $active(w) \\subset active(x) \\subset active(v)$. $vw$ is a tree edge iff. $active(w) \\subset active(v)$, and there is no third vertex x, such that $vw$ is a descendant edge iff. there is some third vertex x, such that $active(w) \\subset active(x) \\subset active(v)$. $vw$ is back edge $active(v) \\subset active(w)$. Ancestor and Descendant [White Path Theorem] $w$ is a descendant of $v$ in a DFS tree iff. at the time $v$ is discovered( just to be changing color into gray), there is a path in $G$ from $v$ to $w$ entirely of white vertices. 证明: 充分性: 如果节点 $v$是$w$的祖先,则考察从$v$到$w$的TE组成的路径, 在$v$​刚刚被发现的时刻, 该路径是一条白色路径. 必要性: 已知节点$v$​到$w$​存在白色路径. 采用归纳法证明, 对白色路径长度$k$​做归纳. 初始情况, $k=0$​, 显然成立. 假设对于所有长度小于$k$​的白色路径结论成立. 下面考虑长度为$k$​的白色路径$P=v \\rarr x_1 \\rarr \\dots \\rarr x_i \\dots \\rarr w$​. 随着遍历的推进,假设节点$x_i$​是$P$​上第一个被遍历过程发现的节点, 基于节点$x_i$​, 将$P$​分为两小段: $P_1=v \\rarr \\dots \\rarr x_i$​, $P_2=x_i \\rarr \\dots \\rarr w$​. 由于$P_2$​是长度小于$k$​的白色路径, 所以$x_i$​是$w$​在遍历树中的祖先. 由定理4.1, 易知$v$​是$x_i$​在遍历树中的祖先, 则$v$​是$w$​在遍历树中的祖先","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[{"name":"Algorithm design and analysis","slug":"Algorithm-design-and-analysis","permalink":"http://lyk-love.cn/tags/Algorithm-design-and-analysis/"}]},{"title":"L9 Hashing","slug":"L9-Hashing","date":"2021-08-02T08:47:15.000Z","updated":"2022-09-26T06:39:34.933Z","comments":true,"path":"2021/08/02/L9-Hashing/","link":"","permalink":"http://lyk-love.cn/2021/08/02/L9-Hashing/","excerpt":"Outline: The searching problem The ambition of hashing Hashing Brute force table: direct addressing Basic idea of hashing Collision Handling for Hashing Closed address hashing Open address hashing Amortized Analysis Array doubling","text":"Outline: The searching problem The ambition of hashing Hashing Brute force table: direct addressing Basic idea of hashing Collision Handling for Hashing Closed address hashing Open address hashing Amortized Analysis Array doubling The searching problem Cost for Searching Brute force $O(n)$ Balanced BST $O(logn)$ Hashing --- almost constant time $O( 1 + \\alpha)$ &quot;Mission impossible&quot; $O(1)$​ Hashing universe of keys: 所有可能的键 actual keys：实际的键 Brute force table: direct addressing 给所有 universe keys 都分配一个位置。 用超多的空间 Basic idea of hashing Index distribution Hash Function $H(x) = k$ Collision handling Collision Handling for Hashing 规定m为哈希表大小， n为表中元素个数 Closed address hashing 也称为Chaining Each address is a linked list Insert to the head of the list( 链表不是常数时间寻址 ) Assumption - simple uniform hashing For $j=0,1,2,\\dots, m-1$​​, the average length of the list at $E[j]$​​​ is n/m Unsuccessful Search The average cost for an unsuccessful search Any key that is not in the table is equally likely to hash to any of the m address Total cost $\\Theta(1+n/m)$​ The average cost to determine that the key is not in the $E[h(k)]$ is the cost to search to the end of the list, which is n/m 1是算哈希， n/m是链表平均长度 Successful Search For successful search( assuming that $x_i$ is the $i^{th}$ element inserted into the table, $i=1,2,\\dots,n$ ) For each i, the probability of that $x_i$​ is searched is 1/n For a specific $x_i$​​, the number of elements examined in a successful search is t+1, where t is the number of elements inserted into the same list as $x_i$, after $x_i$​​​​​​ has been inserted(即链表里在该元素之前的元素的个数). $$ 1 + \\frac 1 n \\sum\\limits_{i=1}^{n}{(1+t)} $$ And for any j, the probability of that $x_j$ is inserted into the same list of $x_i$ is 1/m. So, the cost is: $$ 1 + \\frac 1 n \\sum\\limits_{j=i+1}^{n}{\\frac 1 m} $$ The average cost of a successful search: Define $\\alpha = n/m$ as load factor, The average cost of a successful search is: $$ \\frac 1 n { ( 1 + \\sum\\limits_{j = i + 1}^n {\\frac 1 m} )} = 1 + \\frac 1 {nm} {\\sum\\limits_{i = 1}^n {(n-i)}} = 1 + \\frac 1 {nm} \\sum_{i=1}^{n-1}i = 1 + \\frac {n-1} {2m} = 1 + \\frac \\alpha 2 - \\frac \\alpha {2n} = \\Theta( 1 + \\alpha ) $$ Open address hashing All elements are stored in the hash table No linked list is used The load factor $\\alpha$ can't be larger than 1 Collision is settled by &quot;rehashing&quot; A function is used to get a new hashing address for each collided address The hash table slots are probed successively, until a valid location is found. Th probe sequence can be seen as a permutation of $(0,1,2,\\dots,m-1)$ Commonly Used Probing Linear probing: Given an ordinary hash function h', which is called an auxiliary hash function, the hash function is:( clustering may occur ) $$ h(k,i) = (h'(k)+i)\\quad mod \\quad m \\quad ( i = 0,1, \\dots, m-1 ) $$ Quadratic Probing: Given auxiliary function h' and nonzero auxiliary constant $c_1$ and $c_2$​, the hash function is: (secondary clustering may occur) $$ h(k,i) = (h'(k)+c_1 i + c_2 i^2)\\quad mod \\quad m \\quad ( i = 0,1, \\dots, m-1 $$ Double hashing: Given auxiliary functions h_1 and h_2, the hash function is: $$ h(k,i) = (h_1(k)+i h_2(k))\\quad mod \\quad m \\quad ( i = 0,1, \\dots, m-1 $$ Equally Likely Permutations Assumption Each key is equally likely to have any of the m! permutations of $(1,2,\\dots,m)$​ as​ its probe sequence Note Both linear and quadratic probing have only m distinct probe sequence, as determined by the first probe.(m个格子的合法排列) Unsuccessful Search The average number of probes in an unsuccessful search is at most $1/(1-\\alpha )$​ $(\\alpha = n/m &lt; 1)$ Assuming uniform hashing The probability of the first probed position being occupied is n/m, and that of the $j^{th}(j&gt;1)$ position occupied is $\\frac {n-j+1}{m-j+1}$​​. So the probability of the number of probed no less than i will be:(概率论知识) $$ \\frac n m \\cdot \\frac {n-1} {m-1} \\cdot {n-1} {m-2} \\cdots \\cdot \\frac {n-i+2} {m-i+2} \\le { (\\frac n m )}^{i-1} = \\alpha ^{i-1} $$ The average number of probe is: $\\sum\\limits _{i=1} ^ \\infty {\\alpha ^ {i - 1}} = \\sum\\limits _{i=0} ^ \\infty {\\alpha ^ {i }} = \\frac {1} {1-\\alpha}$​ （其实上界是m，因为一共m个格子。 这里做了放大，但这个放大得到的上界还是比较 tight的） Successful Search The average number of probes in an successful search is at most $\\frac 1 \\alpha ln \\frac 1 {1-\\alpha}$ ($\\alpha = n /m &lt;1$​)​ Assuming uniform hashing To search for the $(i+1)^{th}$ inserted element in the table, the cost is the same as that for inserting it when there are just i elements in the table. At that time, $\\alpha = \\frac i m$. So the cost is( 用 unsuccessful的结论 ) $\\frac 1 {1- \\frac i m}$ = $\\frac m {m-i}$​, $$ \\frac 1 n \\sum\\limits_{i=0}^{n-1} \\frac m {m-i} = \\frac m n \\sum\\limits _{i=0}^{n-1} \\frac 1 {m-i} = \\frac 1 \\alpha \\sum\\limits _{i=m-n+1}^{m} \\frac 1 i \\le \\frac 1 \\alpha \\int _{m-n} ^m {\\frac {dx} x} = \\frac 1 \\alpha ln \\frac m {m-n} = \\frac 1 \\alpha ln \\frac 1 {1-\\alpha} $$ Amortized Analysis Array doubling 顾名思义 Worst-case Analysis For n execution of insertion operations Note that the expansion is required during the $i^{th}$​ operation only if $i = 2^{th}$​, and the cost of the $i^{th}$​ operation $ c_i = i $​( if i - 1 is exactly the power of 2 ) or $c_i = 1 $ ​(otherwise)​ So the total cost is: $\\sum\\limits {i=1} ^n c{i} \\le n + \\sum\\limits _{j=0} ^{\\lfloor logn \\rfloor} 2^j &lt; n + 2n = 3n$ Amortized Analysis- Why? Unusually expensive operations Relation between expensive and usual operations Each piece of the doubling cost corresponds to some previous insert Amortized Analysis- How? Amortized equation: amortized cost = actual cost + accounting cost Design goals for accounting cost In any legal sequence of operations, the sum of the accounting costs is nonnegative The amortized cost of each operation is fairly regular, in spite of the wide fluctuate possible for the actual cost of individual operations Examples: Multi-pop Stacks Amortized Actual Accounting Push 2 1 1 Multi-pop 0 k -k 相当于每个元素在出生的时候就要记下它死亡的时候的代价 Binary Counter Amortized Actual Account Set 1 2 1 1 Set 0 0 1 -1 Amortized Analysis - Array Doubling Amortized Actual Account Insert ( normal ) 3 1 2 Insert ( doubling ) 3 k+1 -k + 2 k is the number of elements upon doubling -k+2: 清除前k个元素的代价, 新插入的元素要付出2的代价","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[{"name":"Algorithm design and analysis","slug":"Algorithm-design-and-analysis","permalink":"http://lyk-love.cn/tags/Algorithm-design-and-analysis/"}]},{"title":"L10 Union Find","slug":"L10-Union-Find","date":"2021-07-31T15:47:58.000Z","updated":"2022-09-26T06:39:34.931Z","comments":true,"path":"2021/07/31/L10-Union-Find/","link":"","permalink":"http://lyk-love.cn/2021/07/31/L10-Union-Find/","excerpt":"Outline: Dynamic Equivalence Relation Examples Definitions Brute force implementations Disjoint Set Straightforward Union-Find Weight Union + Straightforward Weight Union + Path-compressing Find","text":"Outline: Dynamic Equivalence Relation Examples Definitions Brute force implementations Disjoint Set Straightforward Union-Find Weight Union + Straightforward Weight Union + Path-compressing Find Problems Minimum Spanning Tree Kruskal's algorithm, greedy strategy Select one edge With the minimum weight Not in the tree Evaluate this edge This edge will NOT result in a cycle Critical issue How to know &quot;NO CYCLE&quot;? Maze Generation Black Pixels Jigsaw Puzzle Dynamic Equivalence Relation Equivalence 等价关系( 自反对称传递 ) 等价类们形成了一个划分( partition ) Dynamic equivalence relation Changing in the process of computation IS instruction: yes or no ( in the same equivalence class ) MAKE instruction: combining 2 equivalent classes, by relating 2 unrelated elements, and influencing the results of subsequent IS instructions. Starting as equality relation Union-Find based Implementation Maze Generation Randomly delete a wall and union 2 cells Loop until you find the inlet and outlet are in one equivalent class The Kruskal's algorithm Find whether u and v are in the same equivalent class If not, add the edge and union the 2 nodes Black Pixels Find black pixel groups How the union of black groups increases $\\alpha$ Implementation： Choices n: 总元素个数， m： Find / Union 指令数 Matrix( relation matrix ) Space in $\\Theta(n^2)$, and worst-case cost in $O(mn)$​ (mainly for row copying for MAKE)( m 条指令，最坏情况下每条$O(n)$的代价 ) Array( for equivalence class ID ) Space in $\\Theta(n)$​, and worst-case cost in $O(mn)$​ (mainly for search and change for MAKE ) Forest of rooted trees A collection of disjoint sets, supporting Union and Find operations Not necessary to traverse all the elements in one set Union-Find ADT Using Rooted Tree IS $s_i \\equiv s_j$: $t = find(s_i)$ $u = find(s_j)$ $(t == u)$​ ? MAKE $s_i \\equiv s_j$ : $t=find(s_i)$ $u = find(s_j)$ $union(t,u)$ Critical operation： 对rooted tree 的 assignment 和 lookup， 均称做 link operation Worst-case Analysis Assuming each link operation takes $O(1)$ 在根树极度不均衡时(变成一个链表), operations done: n( makeSet, 把每个元素实现为一个等价类), n-1( Union次数) + (m-n+1)n( 共有m条指令,去掉n-1条,剩下的全部是Find,且查最深的那个), 因此是$\\Theta(mn)$​ 和蛮力策略代价一样,这是因为并查操作都太简单了,没有特殊约束. Weight Union Weight union( wUnion ) always have the tree with fewer nodes as subtree 解决树的平衡性问题,为什么不用height, 而要用size? 其实也有用height的优化Union方案,这里没教 由于在用size的方案中, &quot;size小而height大&quot;这种反例不可能出现,所以size方案是可行的 Worst case Analysis 任意次wUnion后, n个节点的根树的高度上界是 $\\lfloor logn \\rfloor$​ 证明用归纳法 A Union-Find program of size m, on a set of n elements, performs $\\Theta(m + n\\lfloor logn \\rfloor)$​​ link operations in the worst case if wUnion and straight find are used Proof: At most n-1 wUnion can be done, building a tree with height at most $\\lfloor logn \\rfloor$ Then, each find costs at most $\\lfloor logn \\rfloor + 1$ Each wUnion costs in $O(1)$, so, the upper bound on the cost of any combination of m wUnion/find operations is the cost of m find operations, that is $m(\\lfloor logn \\rfloor + 1) \\in O(n+ m\\lfloor logn \\rfloor)$ Path Compression Find cFind does twice as many link operations as the find does for a given node in a given tree(先查一遍查到根,再查一遍,把遇到的节点都挂到根下面) Worst case Analysis 用平摊分析, cFind是昂贵操作 (使用wUnion和cFind)代价是$O((n+m)log^*(n))$​ $log^*n$的反函数是n个2叠罗汉,后者增长很快,因此前者增长很慢, 可以近似看作常数 Log-star grows extremely slowly $\\lim\\limits _{n \\rarr \\infty} { \\frac {log^* n} {log ^{(p)} n}}$, p is any nonnegative constant​​ 平摊分析的细节不讲了","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[{"name":"Algorithm design and analysis","slug":"Algorithm-design-and-analysis","permalink":"http://lyk-love.cn/tags/Algorithm-design-and-analysis/"}]},{"title":"Persian Lessons","slug":"Persian Lessons","date":"2021-07-28T15:51:04.000Z","updated":"2022-09-26T06:39:34.937Z","comments":true,"path":"2021/07/28/Persian Lessons/","link":"","permalink":"http://lyk-love.cn/2021/07/28/Persian%20Lessons/","excerpt":"二战期间，犹太人吉尔斯在集中营谎称自己是波斯人保命，被一位德国军官看中要求教自己波斯语。对波斯语完全不懂的吉尔斯只能凭空编出一门语言，他开始用集中营中关押的犹太人的名字作为词根，编造出上千个“波斯语”单词，军官在残暴的环境中对吉尔斯竟有了依赖，并开始袒护他，不料集中营中来了一 个真正的波斯人……","text":"二战期间，犹太人吉尔斯在集中营谎称自己是波斯人保命，被一位德国军官看中要求教自己波斯语。对波斯语完全不懂的吉尔斯只能凭空编出一门语言，他开始用集中营中关押的犹太人的名字作为词根，编造出上千个“波斯语”单词，军官在残暴的环境中对吉尔斯竟有了依赖，并开始袒护他，不料集中营中来了一 个真正的波斯人…… 导演，包括整部电影的制作都很厉害。画面美学非常优秀。镜头语言冷静克制，叙事节奏从容不迫，很自然地流露出大师水准。尤其是，横向对比某位中国著名导演，您的《悬崖之上》是啥玩意儿？ 谍战那么严肃的题材，到张导演手里就和儿戏一样，那镜头语言，又活泼又调皮，不知道的还以为小孩子过家家呢。分镜好听点叫教科书，不好听地说就是一股匠气，和教科书一样死板（同样的死板还有侯孝贤， 不过那位的画面更好看）。还有电影中哈尔滨不科学的繁华程度，堪比21世纪的北京和上海？整个城市街道，大小建筑，“亚细亚电影院”，特务们呆的别墅，还有特务们寻找《梅兰芳游美记》的书店，都窗明几净、一尘不染地，甚至书店的地板和家具的包浆都亮晶晶地反着光，您好歹做旧一点呀国师(～￣▽￣)～。 演员很好，不过这类严肃电影的演员素质都不会差到哪里去？（点名批评凯拉奈特莉）再次对比某著名导演， 天天让我浩存姐演个纯洁小女孩，《一秒钟》里让她演，《悬崖之上》还让她演，人家都二十了！ 张导演大概想想借浩存姐来塑造一个黑暗年代的纯洁心灵、 用浩存姐的美丽外表来表达对未来的美好希望？ 这个套路已经用烂了啊喂！ 这些玩意儿您不是用巩俐章子怡关晓彤表现了几十年了么？ 特别是《影》里的关晓彤，典型的黑暗时代下的青年人的形象，和刘浩存在这两部里的作用简直一模一样啊，看张艺谋的电影就和做阅读理解似的，永远的公式化人物。浩存姐能唱歌会跳舞还有演技，一直就让她演个小女孩，真是浪费青春。 回到电影，有一处我很不解，就是旗队长因为前女友说他某方面小，就把人家送去东线的事。当时去东线面对苏军就是送死，所以他实际上因为这件事杀了前女友φ(゜▽゜*)♪ 一直以来我都以为德军只是对敌人的生命很冷漠，影片也极力表现德军在日常生活方面也是有人性的“正常人”，可是前女友说杀就杀，只能说是战争把人性极度扭曲了？ 对生命的冷酷不止体现在对犹太人，事实上是对所有人，只是后者在当时不是要迫害的对象而已。如果希特勒哪天宣布清洗一半的雅利安人，像灭霸那样，估计纳粹们依然能举起屠刀，毕竟漠视生命已经刻到骨子里了 情感表达之细腻，以及与镜头语言的配合之好，一度让人以为吉尔斯和科赫在谈恋爱，电影变成是犹太俘虏伪装成波斯语老师和他的纳粹军官学生谈恋爱的故事。 不过这种紧闭和高压的环境内，一方是俘虏，令一方是军官拥有绝对的暴力和权力，但俘虏又是老师，所以构成了权力的反转，随着剧情深入，军官越来越依赖于俘虏，二者的权力关系不停发生颠倒，能产生类似斯德哥尔摩的情感也很正常，导演在这方面把握得特别好。 吉尔斯对科赫说（编）的第一句波斯语是“你看着夕阳渐渐西沉，但当天突然变暗时，你还是会感到惊讶”， 暗示了二战的背景： 人们都看着纳粹一步步掌权，而直到魔鬼挥舞屠刀的那一刻，才蓦然发觉。 影片很厉害的一点是如何把严肃电影处理得那么好看， 我指的不是靠把画面做得过分美观脱离实际或者依赖演员（浩存姐等）的出众颜值，某中国导演才会那么做， Vadim Perelman的做法是把搞笑作为剧情的一部分,而搞笑之后就是恐怖,使得笑话也弥漫起惊悚的灰白色彩. 之前杀前女友的桥段就是一个典型. 德国战败前，有那么多军官想逃走,也非常出人意料。我一直以为德军都是信仰坚定钢铁意志的杀人机器呢.。原来党卫军也会害怕，也会背叛，甚至其中有不少聪明人，知道纳粹不会赢而提前准备跑路。 比如科赫军官，为了给战败之后自己能逃到德黑兰而学波斯语，当时才1942年。最后马克思向上司举报科赫逃走，没想到上司一个比一个溜得快也是够讽刺的。女助理一方面能毫不留情地把用人的手按在烧红的铁板上，只因为自己心情不好； 一方面又能和现男友吐槽自己前任， 在办公室成天摸鱼玩指甲，和现代人一模一样。可见纳粹在许多情况下和普通人没什么分别。","categories":[{"name":"Art","slug":"Art","permalink":"http://lyk-love.cn/categories/Art/"}],"tags":[{"name":"Movie","slug":"Movie","permalink":"http://lyk-love.cn/tags/Movie/"}]},{"title":"L7 Selection","slug":"L7-Selection","date":"2021-07-23T17:19:01.000Z","updated":"2022-09-26T06:39:34.932Z","comments":true,"path":"2021/07/24/L7-Selection/","link":"","permalink":"http://lyk-love.cn/2021/07/24/L7-Selection/","excerpt":"Outline Lower bound and adversary argument Selection - select the median Expected linear time Worst-case linear time A Lower bound for Finding the Median","text":"Outline Lower bound and adversary argument Selection - select the median Expected linear time Worst-case linear time A Lower bound for Finding the Median Lower bound and adversary argument Adversary argument： 对一个算法而言是worst-case的input. Adversary strategy: 指导你构造adversary argument的策略 Finding max and min 对n个元素，找出最大和最小 Brute force： 先n-1次comparison找出最大， 拿出该元素，对其余n-1个元素进行n-2次comparison找出最小 The strategy Pair up the keys, and do n/2 comparisons( if n odd, having E[n] uncompared ) 把数组一分为二，两两比较. 最大元素肯定在大的那边，最小元素肯定在小的那边 Doing findMax for larger key set and findMin for small key set respectively( if n odd, E[n] included in both sets ) Number of comparisons For even n: $$ n/2 + 2(n/2 - 1) = 3n/2 - 2 $$ For odd n: $$ （n-1）/2 + 2( (n-1)/2 - 1 + 1 ) = \\rceil 3n/2 \\rceil - 2 $$ 现在用adversary argument证明这个问题的下界是 $\\frac32n - 2$​ Unit of Information Max and Min That x is max can only be known when it is sure sure that every key other than x has lost some comparison That y is min can only be known when it is sure sure that every key other than y has win some comparison Each win or loss is counted as one unit of information Any algorithm must have at least 2n - 2 units of information to be sure of specifying the max and min. 因此，算法至少要获取 2n - 2 个信息元 Adversary Strategy Construct an input to force the algorithm to do more comparisons as possible 构造一个输入,使得获取2n - 2* 个信息元需要花费尽可能多的比较 To give away as few as possible units of new information with each comparison It can be achieved that 2 units of new information are given away only when the status is N,N It is always possible to give adversary response for other status so that at most one new unit of information is given away, without any inconsistencies So, the Lower bound is $n/2 + n - 2 $ ( for even n) $$ n/2 \\times 2 + ( n - 2 ) \\times 1 = 2n - 2 (信息元) $$ 对所有算法而言,面对这个(adversary argument的)输入, 要尽可能少地比较,来凑足 2n-2个信息元. 算法最多能进行 $\\frac {n}2$ 次信息元为2的比较, 而对于剩下的 n - 2 个信息元,算法只能进行信息元为1的比较 The principle: let the key win if it never lose, or, let the key lose if it never win, and change one value if necessary Finding 2^nd^ max Brute force - using FindMax twice $2n-3$ comparisons For a better algorithm Collect some useful information from the first FindMax Observations The key which loses to a key other than max can not be the 2^nd^ largest key To check &quot;whether you lose to max?&quot; Analysis Any algorithm that finds secondLargest must also find max before ( n - 1 ) The secondLargest can only be in those which lose directly to max On its path along which bubbling uo to the root of tournament tree, max beat $\\lceil logn \\rceil$ keys at most( 根据锦标赛树的数据结构可以证明 ) Pick up secondLargest (依然用FindMax) Total cost: $ n + \\lceil logn \\rceil - 2$​ 下面解释为什么该算法是最优 Weight Key Assigning a weight w(n) to each key The initial values are all 1. Adversary strategy Lower Bound by Adversary: Details Note: the sum of weights is always n Let x is max, then x is the only nonzero weighted key, that is $w(x) = n$​. By the adversary rules: $$ w_k(x) \\le 2w_{k - 1}(x) $$ So, $K \\le \\lceil logn\\rceil$​ 注意,之前算法的$\\lceil logn\\rceil$是从锦标赛树的数学性质得出的, 而这里的$\\lceil logn\\rceil$是下界证明 所以该问题的下界是$ n + \\lceil logn \\rceil - 2$ Finding the Median: the Strategy Observation If we can partition the problem the set of keys into 2 subsets: S1, S2, such that any key in S1 is smaller that that of S2, the median must located in the set with more elements Adjusting the Rank Divide-and-Conquer Partitioning: Larger and Smaller average-case O(n) Dividing the array to be considered into two subsets: &quot;small&quot; and &quot;large&quot;, the one with more elements will be processed recursively 运用快排的思想进行分析，只对median所在的那侧进行递归 平均情况复杂度分析和快排一样，由于每次只对一侧进行递归， 所以在“所有输入各不相同，所有可能的输入等概率出现”的情况下，平均情况复杂度为：（ 在此情况下pivot在中间 ） $$ n + n/2 + n/4 + \\cdots \\in O(n) $$ Partition improved: the strategy worst-case O(n) 方法和解释见书上 $ W(n) &lt;= 6( \\frac n 5 ) + W( \\frac n 5 ) + 4r + W(7R + 2)$ 分析见书上 Lower Bound by Adversary Crucial Comparison A crucial comparison Establishing the relation of some x to the median Def( for a comparison involving a key x ) Crucial Comparison for x: the first comparison between x and y, for some *y * &gt;= median, or x &lt; y for some y &lt;= median Non-Crucial Comparison for x: the first comparison between x and y, where x &gt; median and y &lt; median, or vise versa Lower Bound for Selection Median Problem Theorem: Any algorithm to find the median of n keys ( for odd n 偶数情况更复杂,但没什么新思想,所以不考虑 ) by comparison of keys must do at least $3n/2 - 3/2$​​ comparisons in the worst case Argument Adversary strategy构造这样一个输入: 中位数在最后, 而前N-1个元素,在两两比较时,比较的结果不能与之前的结果构成传递关系( 即 不会出现x &gt; y, y &gt; z,使得你可以节省一次x和z的比较 ), 算法对前N - 1个元素,要比较至少 ( N-1 )/ 2 次( 即两两都是没比过的元素进行比较,一共 (N - 1 )/ 2 组 ), 而最后一个树(它是中位数)需要和前N-1个元素比较,这就是最坏情况 $ \\frac {n - 1} 2 + n - 1 = \\frac {3n} 2 - \\frac 3 2$","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[{"name":"Algorithm design and analysis","slug":"Algorithm-design-and-analysis","permalink":"http://lyk-love.cn/tags/Algorithm-design-and-analysis/"}]},{"title":"Typora && Latex","slug":"使用Typora编写数学公式","date":"2021-07-23T15:39:08.000Z","updated":"2022-09-26T06:39:34.943Z","comments":true,"path":"2021/07/23/使用Typora编写数学公式/","link":"","permalink":"http://lyk-love.cn/2021/07/23/%E4%BD%BF%E7%94%A8Typora%E7%BC%96%E5%86%99%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/","excerpt":"Author: 有多远滚多远","text":"Author: 有多远滚多远 How 在Typora中： 按住ctrl + shift + m 或者输入$$ Latex syntax 上下标 上标 只有一个: a^2 多个: a^&#123;1224&#125; 下标 跟上标类似，只需要把^换成_ 如果有多重下标或上标，必须用{ }进行准确的分割 A_&#123;bb_&#123;cc&#125;&#125; $$ A_{bb_{cc}} $$ 插入水平线 \\overline和\\underline \\overline&#123;a+b+c&#125; = \\underline&#123;a+b+c&#125; $$ \\overline{a+b+c} = \\underline{a+b+c} $$ 平方根 语法为\\sqrt[n] 未给出n则默认为2 \\sqrt[5]&#123; a &#125; $$ \\sqrt[5]{ a } $$ 插入水平大括号 语法为\\overbrace 和\\underbrace \\overbrace&#123;2,3,4,\\cdots,100&#125;^&#123;99&#125; $$ \\overbrace{2,3,4,\\cdots,100}^{99} $$ 分式 语法为\\frac&#123;&#125;&#123;&#125; \\frac&#123;abc&#125;&#123;defg&#125; $$ \\frac{abc}{defg} $$ 连续分式 \\cfrac&#123;&#125;&#123;&#125; \\cfrac&#123;1&#125; &#123; a + \\cfrac&#123;1&#125;b&#125; $$ \\cfrac{1} { a + \\cfrac{1}b} $$ 导数 x' $$ x' $$ 积分 语法为\\int \\int_&#123;a&#125;^&#123;b&#125;&#123;f(x)&#125; = x^2 $$ \\int_{a}^{b}{f(x)} = x^2 $$ 向量 单符号可以用\\vec 多符号可以用\\overrightarrow 和\\overleftarrow \\vec a = \\overrightarrow&#123;AB&#125; = \\overleftarrow&#123;BA&#125; $$ \\vec a = \\overrightarrow{AB} = \\overleftarrow{BA} $$ 乘积 \\prod \\prod_&#123;i=1&#125;^&#123;n&#125;&#123;(1/i)&#125; $$ \\prod_{i=1}^{n}{(1/i)} $$ 箭头 \\larr \\rarr $$ \\larr \\rarr $$ 微积分符号 latex 显示效果 \\partial $\\partial$ \\nabla $\\nabla$ \\infty $\\infty$ \\int $\\int$ \\iint $\\iint$ \\iiint` $\\iiint$ \\oint $\\oint$ \\triangle $\\triangle$ 空格 Ref: https://blog.csdn.net/seaskying/article/details/51316607 描述 latex 显示效果 说明 两个quad空格 a \\qquad b $a \\qquad b$ 两个m的宽度 quad空格 a \\quad b $a \\quad b$ 一个m的宽度 大空格 a\\ b $a\\ b$ 1/3m宽度 中等空格 a\\; b $a;b$ 2/7m宽度 小空格 a\\, b $a,b$ 1/6 m宽度 没有空格 ab $ab$ 紧贴 a\\!b $a!b$ 缩进1/6m宽度 \\quad、1em、em、m代表当前字体下接近字符‘M’的宽度。 关系符与运算符 关系符与运算符 latex 不等号 \\ne 大于等于号 \\ge 小于等于号 \\le 约等号 \\approx 等价 \\equiv 乘号 \\times 除号 \\div 点乘 \\cdot 加减号 \\pm 三角函数 \\sin(a) \\cos(a) 求和号 \\sum( 需要加\\limits ) 积分号 \\int 极限 \\lim 对数log log 对数lg lg 对数ln ln 向下和向上取整 \\lfloor x \\rfloor和\\lceil x \\rceil 全等于 \\equiv e.g. f(1) = \\lim_&#123;x - &gt; 1 &#125;&#123;f(x)&#125; = \\sin (log_&#123;2&#125;1) $$ f(1) = \\lim_{x - &gt; 1 }{f(x)} = \\sin (log_{2}1) $$ y = a\\cdot x^2 + b\\cdot x + c $$ y = a\\cdot x^2 + b\\cdot x + c $$ 特殊符号 特殊符号 省略号 \\dots 右箭头 \\rightarrow 或 to 左箭头 \\leftarrow 或 gets 花括号 \\&#123;a\\&#125; 插入文字 \\text&#123;apple&#125; 空格 \\quad e.g. \\&#123;cddc\\&#125; = \\text&#123;addple &#125;\\dots $$ {cddc} = \\text{addple }\\dots $$ 集合运算符 集合运算符 属于 \\in $\\in$ 不属于 \\not\\in $\\not\\in$ 包含于 \\subset或\\subseteq $\\subset$ 或 $\\subseteq$ 真包含于 \\subsetneqq $\\subsetneqq$ 不包含于 \\not\\subset $\\not\\subset$ 交 \\cap $\\cap$ 并 \\cup $\\cup$ 闭包 \\overline $\\overline A$ 差 \\setminus $A\\setminus B$ 实数集合 \\mathbb&#123;R&#125; $\\mathbb{R}$ 空集 \\emptyset $\\emptyset$ 集合中的` ` \\mid 包含 \\supset $\\supset$ 真包含 \\supsetneqq $\\supsetneqq$ 不包含 \\not\\supset $\\not\\supset$ 补集 \\complement $\\complement$ 逻辑符号 \\forall$ \\forall$​​ \\exists $\\exists$​ \\therefore $\\therefore$​ \\implies $\\implies$ \\nexists $\\nexists$​ `\\exist \\exist$​ \\because $\\therefore$​ \\land $\\land$ \\lor $\\lor$ \\to $\\to$ \\gets $\\gets$ \\iff $\\iff$ \\ni $\\ni$ \\not\\ni $\\not\\ni$ \\neg $\\neg$ 希腊字母","categories":[{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"}],"tags":[]},{"title":"L6 MergeSort","slug":"L6-MergeSort","date":"2021-07-22T15:52:17.000Z","updated":"2022-09-26T06:39:34.932Z","comments":true,"path":"2021/07/22/L6-MergeSort/","link":"","permalink":"http://lyk-love.cn/2021/07/22/L6-MergeSort/","excerpt":"Oultline MergeSort Worst-case analysis of MergeSort More than sort: the MergeSort D&amp;C Lower Bounds for comparison-based sorting ( nlogn ) Worst-case ( Omega(nlogn) ) Average-case (nlogn - 1.443n )","text":"Oultline MergeSort Worst-case analysis of MergeSort More than sort: the MergeSort D&amp;C Lower Bounds for comparison-based sorting ( nlogn ) Worst-case ( Omega(nlogn) ) Average-case (nlogn - 1.443n ) MergeSort Space Complexity of Merge A algorithm is &quot;in space&quot; If the extra space it has to use is in Omega(1) Merge is not an in place algorithm Since it needs O(n) extra space to store the merged sequence during the merging process Worst case Complexity of MergeSort Observations Worst case is that comparison is conducted between A[k-1] and B[m-1] After each comparison, one element is inserted into Array C, at least After entering Array C , an element will never be compared again After last comparison, at least two elements( the two just compared ) have not yet been moved to Array C. So at most n - 1 comparisons are done. In worst case, n - 1 comparisons are done, where n = k + m Optimality of Merge Any algorithm to merge two sorted arrays, each containing k = m = n/2 entries, by comparison of keys, does at least(如果算法笨的话可能有重复比较) n - 1 comparisons in the worst case. Choose keys so that: b0 &lt; a0 &lt; b1 &lt; a1 &lt; ... &lt; bi &lt; ai&lt; bi+1,..., &lt; bm-1 &lt; ak-1 Then the algorithm must compare ai with bi for every i in [ 0, m - 1 ], and must compare ai with bi+1 for every i in [0, m - 1], so there are n - 1 comparisons 先考虑最好情况,也就是两边数组大小不一样的时候,一个数组全部比完了,那么另一个数组的剩余部分就在这次比较后全部插入辅助数组中, 比较次数小于 n - 1 反之,最坏情况就是&quot;其中一个数组比完了&quot;这个情况不发生,也就是两个数组一直比到最后, 也就是两个数组的最后一个元素相互比较( &quot;comparison is conducted between A[k-1] and B[m-1]&quot; ), 这就要求k = m = n/2 可能有人会疑惑, 如果两数组大小极度不均衡,但是较小数组的最后一个元素远大于较大数组的所有元素,这不也是比到最后吗? 也是 n - 1 吗? 为什么最坏情况的构造里, 还要要求 k = m = n/2 呢? 因为对于这种情况,对于较小的数组,完全可以用折半查找来插入,不需要归并了. Worst case Analysis of MergeSort The recurrence equation for MergeSort W(n) = W( floor( n/2 ) ) + W( cell( n/2 ) ) + n - 1 (或者O(n)) W (1) = 0 The Master Theorem applies for the equation, so: W(n) ∈ Omega(nlogn) 精细分析, 得出 W(n) = nlogn - ( alpha - log alpha )n + 1 cell( nlogn - n + 1 ) &lt;= W(n) &lt;= cell( nlogn - 0.914n) The MergeSort D&amp;C Counting the number of inversions Brute force: O(n2) 蛮力做法 Can we use divide &amp; conquer In O(nlogn) =&gt; combination in O(n) MergeSort as the **carrier ** 用归并排序做 Sorted subarrays A[ 0... k-1] and B[ 0 ... m-1 ] Compare the left and the right elements A[i] vs. B[j] if A[i] &gt; B[j] (i,j) is an inversion For all i' &gt; j ( i' , j) are inversions ( i' &gt; i ) B[j] is selected if A[i]] &lt; B[j] No inversions found A[i] is selected Lower Bounds for comparison-based sorting Upper bound, e.g., worst-case cost 给定一个算法，输入不同，复杂度有一个上界（比如worst-case） For any possible input, the cost of the specific algorithm A is no more than the upper bound Max{ Cost( i ) | i is an input } Lower bound, e.g., comparison-based sorting 比如，对于所有可能的算法，每个算法都有一个worst-case, 这是个上界，对所有上界取下界,就是worst-case的下界。相应的， 所有可能的算法的所有可能的期望值，也就是average-case复杂度，的下界，就是average-case复杂度的下界 For any possible( comparison-based ) sorting algorithm A, the worst-case is no less than the lower bound Min{ Worst-case(a) | a is an algorithm } Decision Tree for Sorting Decision tree is a 2-tree( assuming no same keys ) The action of Sort on a particular input corresponds to following on path in its decision tree from the root to leaf associated to the specific output Decision tree 是刻画所有 comparison-based 的排序的数学工具 Characterizing the Decision Tree For a sequence of n distinct elements, there are n! different permutation 叶节点是所有可能的输出,这是n个输入元素的所有可能的排列,因此是n! So, the decision tree has at least n! leaves, and exactly n! leaves can be reached from the root So, for the purpose of lower bounds evaluation, we use trees with exactly n! leaves. The number of comparison done in the worst case is the height of the tree The average number of comparison done is the average of lengths of all paths from the root to a leaf. 变成了离散数学问题 Lower Bound for Worst Case Theorem: Any algorithm to sort n items by comparisons of keys must do at least cell(log n!). or approximately cell( nlogn - 1.443n), key comparisons in the worst case. Lemma: let L be the number of leaves in a binary tree and h be its height. Then L &lt;= 2^h 可用归纳法证明 即高度为h， h &gt;= log L , L = n！, 所以 h &gt;= log( n! ) log( n! ) &gt;= ... &gt;= log( (n/2)^ (n/2) ) = n/2log(n/2) ∈ Omega( nlogn ) 因此，worst-case的下界为nlogn External Path Length( EPL ) EPL --- sum of path length to every leaf The EPL t is recursively defined as follows: [Base case] 0 for a single external node [Recursion] t is non-leaf with sub-trees L and R, then the sum of: The external path length of L the number of external node of L ( 每个完整的树作为子树时,节点的深度都要下沉1,所以递归合并时每个叶节点对应的路径长度都要加一 ) The external path length of R the number of external node of R Lower Bound for Average Behavior Since a decision tree with L leaves is a 2-tree, the average path length from the root to leaf is epl / L Recall that epl &gt;= L log( L ) 每个输出对应的概率是 1 / L , 而所有代价的总和是epl, 所以平均情况复杂度在所有可能的输入等概率的前提下,是 epl/L, 因此求average case的下界,就是求epl的下界 Theorem: The average number of comparisons done by an algorithm to sort n items by comparison of keys is at least log(n!), which is about nlogn - 1.443n More Balanced 2-tree, Less EPL 设一棵 决策树有两个节点,高度分别为h , k, h - k &gt; 1( 即该树不平衡 ). 高度为h的节点有两个叶节点, 高度k的节点没有叶节点( 由于是2-tree, 不可能只有一个叶子节点 ) Assuming that h - k &gt; 1&gt;, when calculating epl , h + h + k is replaced by ( h - 1 ) + 2( k + 1 ). the net change in epl is k - h + 1 &lt; 0, that is , the epl decreases 因此,求epl的下界,就是求最平衡的树的epl, 最平衡的二叉树就是`完美二叉树. 由此可证明: epl &gt;= L log( L ) MergeSort Has Optimal Average Performance The average number of comparisons done by an algorithm to sort n items by comparison of keys is at least about nlogn - 1.443n The worst complexity of MergeSort is in Omega(nlogn)( 之前已证 ) So, MergeSort is optimal as for its average performance 首先, 归并排序的worst-case是nlogn, 而average-case必定小于等于worst-case, 即 MergeSort的average=case的上界是nlogn. 作为一个comparison sorting, 由于comparison sorting的average-case的下界是nlogn. 因此MergeSort的average-case的下界是nlogn. 夹逼得出, 所以MergeSort的average-case就是nlog级别","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[{"name":"Algorithm design and analysis","slug":"Algorithm-design-and-analysis","permalink":"http://lyk-love.cn/tags/Algorithm-design-and-analysis/"}]},{"title":"L5 HeapSort","slug":"L5-HeapSort","date":"2021-07-21T15:33:29.000Z","updated":"2022-09-26T06:39:34.932Z","comments":true,"path":"2021/07/21/L5-HeapSort/","link":"","permalink":"http://lyk-love.cn/2021/07/21/L5-HeapSort/","excerpt":"Outline: Heap HeapSort FixHeap ConstructHeap Accelerated HeapSort","text":"Outline: Heap HeapSort FixHeap ConstructHeap Accelerated HeapSort Heap 定义：一棵二叉树满足 堆结构特性： 即 是一课完全二叉树 堆偏序特性： 堆节点中存储的元素满足父结点的值大于所有子节点的值（左右子节点的值之间的大小关系无要求） 堆的最大元素必然位于堆顶。 FixHeap 堆顶元素被取走后，整个堆的结构特性和偏序特性均被破坏，但堆的这两个特性是正交的，我们可以先修复结构特性，再修复偏序特性。在修复时，我们面临的不是一个被任意破坏的堆，而只是一个被“局部”破坏的堆，即： 堆顶元素被取走，但堆的左右子树仍然是一个合法的堆。 对于堆结构特性的修复，只需取最底层最右边的元素，放在堆顶位置 修复完结构特性后，我们面对的是一个满足堆结构的二叉树，其左右子树均是一个合法的堆, 只是其根节点的值与其两个（边界情况下可能只有一个）子节点的值不满足偏序特性的要求，为此，需要： 将父结点的值与子节点的值比较，假设左子节点最大，则将父结点与左子点交换位置。 此时，父节点和右子树均满足了堆偏序 左子树由于引入了新的根节点,可能不满足偏序特性. 为此,需要对左子树递归地进行上述修复过程 由于堆的修复只会在高度严格递减的一系列子树上进行, 所以修复过程一定会终止,且次数不会超过树的高度. 由于堆的高度为$O(logn)$, 每次修复的比较次数为O(1)(最多为2次),所以堆修复的代价为$O(logn)$ ConstructHeap 假设有一颗满足堆结构特性的二叉树,但树中每个节点所存储的值的大小关系完全是杂乱的,我们需要使书中所有父子节点间均满足堆的偏序特性,基于堆修复操作,递归实现: 从根部开始堆的构建,因为堆的左右子树必然还是堆. 此时如果堆的左右子树均是一个合法的堆,那么最多只是根节点有局部破坏,用FixHeap处理 对堆结构的左右子树,只需递归地将它们构建成合法的堆 建堆的代价即堆修复代价的总和： $ W(n) = O(n)$ 堆的实现 堆中的父子节点下标满足如下关系: $PARENT(i) = \\lfloor \\frac i 2 \\rfloor$ $LEFT(i) = 2i$ $RIGHT(i) = 2i + 1$ Proof: 略 HeapSort 复杂度 $W(n) = O(nlogn)$​​ ; $A(n) = O(nlogn)$​ 可以用堆实现PRIORITY QUEUE Accelerated HeapSort 采用折半查找, 子元素不和父元素比较,只和兄弟元素比较,比较完后与父元素交换,即父元素直接下沉. 元素每次下沉到剩余高度一半的时候,和PQRENT比较一次(由于只有一个父元素,比较次数为1),若大于父元素,则上浮, 否则递归执行上述操作( 继续下沉,比较... ) 这样做基于的假设是: 对于一个很深的堆,FixHeap时,替换而来的根元素极有可能比两个子元素都要小,因此不用将根元素带入比较,以减少一次比较次数. Extension 堆可以扩展为 n-way ,这样的堆会变胖变矮 一般来说, 如果堆的上浮操作比较多,那么堆越胖越好. 上浮只需一次操作,由于堆变矮了,还能减少上浮次数 若堆的下沉操作比较多,那么堆越瘦越好 堆越瘦,每次比较的操作数越少,极限情况是二叉树,三个元素只需比较两次","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[{"name":"Algorithm design and analysis","slug":"Algorithm-design-and-analysis","permalink":"http://lyk-love.cn/tags/Algorithm-design-and-analysis/"}]},{"title":"L4 QuickSort","slug":"L4-QuickSort","date":"2021-07-21T10:40:58.000Z","updated":"2022-09-26T06:39:34.932Z","comments":true,"path":"2021/07/21/L4-QuickSort/","link":"","permalink":"http://lyk-love.cn/2021/07/21/L4-QuickSort/","excerpt":"Outline: Inversion InsertionSort Analysis of InsertionSort QuickSort Analysis of QuickSort","text":"Outline: Inversion InsertionSort Analysis of InsertionSort QuickSort Analysis of QuickSort Inversion Definition: Inversion $&lt;x_i, x_j&gt;$ is an inversion if $x_i &gt; x_j$ , but $i &lt; j$ Sorting == Eliminating inversions InsertionSort Worst Case Local comparison is done between is done between two adjacent elements At most one inversion is removed by a local comparison There do exist inputs with n(n-1)/2 inversions, such as ( n, n-1, ...., 3, 2, 1 ) （最坏情况，inversion最多） The worst-case behavior of any sorting algorithm that remove at most one inversion per key comparison must in $\\Omega(n^2)$​​​ Average Case Computing the average number of inversions in inouts of size n ( n &gt; 1 ): Transpose: $$ x_1, x_2,x_3,\\dots,x_{n-1},x_n \\ x_n,x_{n-1},\\dots,x_3,x_2,x_1 $$ For any $i, j\\quad(1 \\leq j \\leq i \\leq n)$​ , the inversion $( x_i, x_j )$ is in exactly one sequence in a transpose pair The number of inversions (xi, xj) on n distinct integers is n(n-1)/2 So, the average number of inversions in all possible inputs is n(n-1)/4, since exactly n(n-1)/2 inversions appear in each transpose pair. The average behavior of any sorting algorithm that remove at most one inversion per key comparison must in $\\Omega(n^2)$ QuickSort 每次递归，pivot的位置一定是对的 Worst Case: a Paradox For a range of k positions, k - 1 keys are compared with the pivot( one is vacant ) if the pivot is the smallest, than the &quot;large&quot; segment has all the remaining k - 1, and the small segment is empty 最坏情况,每次问题的规模只减少1, 每次PARTITION代价是O(n) If the elements in the array to be sorted has already ascending order( the Goal ), then the number os comparison that Partition has to do is: $n - 1 + n - 2 + ... + 1 = n(n-1)/2 ∈ O(n^2)$​ 考虑到所有输入等概率的情况,最坏情况的出现概率是很低的. 所有元素只跟pivot比,左右两边里的任意两两之间没有比过. 这样才能用递归来分析 Analysis 3种方法 Guess and Prove 归纳法 Directly solve Indication random variable Space Complexity Good news: Partition is in-place 不占用额外的空间 Bad news: In the worst case, the depth of recursion will be n - 1 So, the largest size of recursion stack will be in O(n)","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[{"name":"Algorithm design and analysis","slug":"Algorithm-design-and-analysis","permalink":"http://lyk-love.cn/tags/Algorithm-design-and-analysis/"}]},{"title":"L3 Recursion","slug":"L3-Recursion","date":"2021-07-15T09:52:50.000Z","updated":"2022-09-26T06:39:34.932Z","comments":true,"path":"2021/07/15/L3-Recursion/","link":"","permalink":"http://lyk-love.cn/2021/07/15/L3-Recursion/","excerpt":"Outline Recursion in algorithm design The divide and conquer strategy Proving the correctness of recursive procedures Solving recurrence equations Some elementary techniques Master theorem","text":"Outline Recursion in algorithm design The divide and conquer strategy Proving the correctness of recursive procedures Solving recurrence equations Some elementary techniques Master theorem Recursion in algorithm design Divide and Conquer The general pattern //分治的伪代码solve(I) n = size(I); if(n&lt;=smallSize) solution=directlySolve(I) else divide I into I1, ..., Ik; for each i ∈ &#123;1, ..., k&#125; Si = solve( Ii ); solution = combine(S1, ..., Sk) return solution The BF recursion 蛮力递归 Problem size: often decreases linearly &quot;n, n-1, n-2, ...&quot; The D &amp; C recursion 分治递归 Problem size: often decreases exponentially 指数速度降低 &quot;n, n/2, n/4, n/8, ...&quot; 笨展开 Smooth定理 Guess and Prove 本质上是数学归纳法 $T(n)= b T(\\frac n c) + f(n)$ $b$​ : b个子问题 $c$​ : 每个子问题的规模 ( 实际的n/c 不一定等于b ) $f(n)$​ : 包含了划分的代价和combine的代价 Recursion Tree Node None-leaf Non-recursive cost Recursive cost Leaf Base case Edge Recursion Solution by row-sums (等比序列) Increasing geometric series: 第一个节点 Constant: $f(n) . log(n)$ : 每一层加起来是$f(n)$, 总共有$logn$​层 Decreasing geometric series: 最后一层节点 Master Theorem: Loosening the restrictions on $f(n)$ 令$a$, $b$ 为常数， 且$a \\geq1$​和$b&gt;1$, $f(n)$为一定义于非负整数上的函数, $T(n)$为定义于非负整数上的递归函数: $$ T(n)=aT(\\frac n b)+f(n) $$ 递归式中的$\\frac n b$指的是$\\lfloor \\frac n b \\rfloor$或$\\lfloor \\lceil \\frac n b \\rceil$ 如果存在某个常数 $\\varepsilon&gt;0$,使得$f(n)=O(n^{log_b^{a-\\varepsilon}})$, 则$T(n)=\\Theta(n^{log_b a})$​ 如果$f(n)=\\Theta(n^{log_b a}), 则$$T(n)=\\Theta(n^{log_b a}logn)$​ 如果存在某个常数 $\\varepsilon&gt;0$​​,使得$f(n)=\\Omega(n^{log_b^{a+\\varepsilon}})$​​, 且存在某个常数$c$​ ( $c&lt;1$​ ), 使得对所有充分大的$n$​, $af(\\frac n b) \\leq cf(n)$​, 则$T(n)=\\Theta(f(n))$​​ Master定理未能覆盖所有情况","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[{"name":"Algorithm design and analysis","slug":"Algorithm-design-and-analysis","permalink":"http://lyk-love.cn/tags/Algorithm-design-and-analysis/"}]},{"title":"L2 Asymptotics","slug":"L2-Asymptotics","date":"2021-07-13T08:54:12.000Z","updated":"2022-09-26T06:39:34.932Z","comments":true,"path":"2021/07/13/L2-Asymptotics/","link":"","permalink":"http://lyk-love.cn/2021/07/13/L2-Asymptotics/","excerpt":"Outline: How to Compare Two Algorithms Brute Force by Iteration Brute Force by Recursion","text":"Outline: How to Compare Two Algorithms Brute Force by Iteration Brute Force by Recursion How to Compare Two Algorithms Algorithm analysis, with simplification Measure the cost by the number of critical operations Large input size only Essential part only Only the leading term in$ f(n)$ is considered Constant coefficients are ignored Capturing the essential part in the cost in a mathematical way Asymptotic growth rate of $f(n)$ Relative Growth Rate $O(g):$​​ functions that grows no faster than $g$​ $O(g(n))={f(n): 存在常数c&gt;0和n_0&gt;0,满足0\\leq f(n) \\leq cg(n)对所有n \\geq n_0均成立}$​ $f(n)=O(g(n)) \\quad iff \\quad \\lim\\limits_{n \\rarr \\infty} \\frac{f(n)}{g(n)}=c&lt;\\infty$ $o(g)$: 不快于$g$且与$g$有层次上的差距 $o(g(n))={f(n): 对任意常数c&gt;0, 均存在常数n_0&gt;0,满足0\\leq f(n) &lt; cg(n)对所有n \\geq n_0均成立}$​​​ $f(n)=o(g(n)) \\quad iff \\quad \\lim\\limits_{n \\rarr \\infty} \\frac{f(n)}{g(n)}=0$​ $\\Omega(g)$: functions that grow at least as fast as $g$​ $\\Omega(g(n))={f(n): 存在常数c&gt;0和n_0&gt;0,满足0 \\leq cg(n)\\leq f(n) 对所有n \\geq n_0均成立}$ $f(n)=\\Omega(g(n)) \\quad iff \\quad \\lim\\limits_{n \\rarr \\infty} \\frac{f(n)}{g(n)}=c&gt;0(c也可以为\\infty)$​ $\\omega$: 不慢于$g$且与$g$有层次上的差距 $\\omega(g(n))={f(n): 对任意常数c&gt;0,均存在常数n_0&gt;0,满足0 \\leq cg(n) &lt; f(n) 对所有n \\geq n_0均成立}$​ $f(n)=\\omega(g(n)) \\quad iff \\quad \\lim\\limits_{n \\rarr \\infty} \\frac{f(n)}{g(n)}=\\infty$​​ $\\Theta(g):$​​ ... the same rate as $g$​​. (处于同一水平) ( $O$和$\\Omega$的交集 ) $\\Theta(g(n))={f(n): 存在常数c_1&gt;0,c_2&gt;0和n_0&gt;0,满足0 \\leq c_1g(n)\\leq f(n) \\leq c_2g(n) 对所有n \\geq n_0均成立}$ $f(n)=\\Theta(g(n)) \\quad iff \\quad \\lim\\limits_{n \\rarr \\infty} \\frac{f(n)}{g(n)}=c&gt;0(0&lt;c&lt;\\infty)$​​ $\\theta$: 不存在， $o$​和$\\omega$​的交集是空集 Brute Force by Iteration Max-sum Subsequence 蛮力是$O(n^3)$, 改进一下是$O（n^2）$​，用分治改进是$O（nlogn）$ A linear Algorithm: O(n) ThisSum = MaxSum = 0;for( j = 0 ; j &lt; N ; j++ )&#123; ThisSum += A[j]; if( ThisSum &gt; MaxSum ) MaxSum = ThisSum; else if( ThisSum &lt; 0 ) ThisSum = 0;&#125;Return MaxSum Brute Force by Recursion 蛮力策略大智若愚，可以以此为跳板进行改进 Job Scheduling Brute force recursion Select job 'a' Case 1: the result does not contain 'a' Recursion on $J \\setminus {a}$​​ Case 2: the result contains 'a' Recursion on $J \\setminus {a} \\setminus {$​ tasks overlapping with $ a}$​ Further improvements Dynamic programming(L16) Greedy algorithms( L14 ) Matrix Chain Multiplication Solutions Brute force recursion(L16) BF1 BF2 Dynamic programming(L16) Based on brute force recursion 2","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[{"name":"Algorithm design and analysis","slug":"Algorithm-design-and-analysis","permalink":"http://lyk-love.cn/tags/Algorithm-design-and-analysis/"}]},{"title":"L1 Model of Computation","slug":"L1-Model-of-Computation","date":"2021-07-13T07:19:53.000Z","updated":"2022-09-26T06:39:34.931Z","comments":true,"path":"2021/07/13/L1-Model-of-Computation/","link":"","permalink":"http://lyk-love.cn/2021/07/13/L1-Model-of-Computation/","excerpt":"There is no whiz, bang effect That's made up of 1s and 0s That takes a special talent","text":"There is no whiz, bang effect That's made up of 1s and 0s That takes a special talent Computer and Computing Computing: 计算机“无所不能”的核心在于01编解码,这是使能技术 Encoding everything into 0s and 1s Operations over 1s and 0s Decoding the 1s and 0 s Turing machine An abstract/logical computer &quot;计算&quot;就是对01编码的信息的操作 Algorithm Algorithm is the spirit of computing To solve a specific problem( so called an algorithmic problem ) Combination of basic operations 指挥一个机器做事情,把机器的操作组合好 in a precise and elegant way 要清晰,优美 Essential issues Model of computation Algorithm design Algorithm analysis Model of Computation Problems Why the algorithms we learn can run almost everywhere? Why the algorithms we learn can be implemented any language? Machine- and language- independent algorithms, running on an abstract machine 算法是抽象概念,与机器\\语言无关. 抽象的算法依赖于抽象的机器,这就是计算模型. 经典计算模型: Turing machine: over-qualify RAM model: simple but power The Ram Model of Computation Each simple operation takes one time step E.g., key comparison, +/-, memory access Non_simple operation should be decomposed Loop Subroutine(子过程还可以递归 ) Memory Memory access is a simple operation 对存储器某个位置的访问和读写被认为是简单操作 Unlimited memory 我们认为RAM的内存是无限的 由于建模的本质是把不重要的细节抽象掉,把精髓留下来. 所以模型建得越精细,就越难用, 建得越粗糙,丢失的信息越多,就越好用,二者有一个Trade-off. 对于本科阶段,Ram模型够用了 To Create an Algorithm Algorithm design Composition of simple operations, to solve an algorithm problems Algorithm analysis Amount of work done / memory used In the worst/ average cases Advanced issues Optimality, approximation ratio, ... Algorithm Analysis How to measure Not too general 不能太粗糙 Given essential indication in comparison of algorithms Not too precise 不用那么精确 Machine independent Language independent Programing paradigm independent implementation independent Criteria Critical operation How many critical operation are conducted 数关键操作个数! 算法性能好坏由关键操作决定,辅助性操作与关键操作大致是线性关系. 所以只要关注关键操作 Algorithm problem Critical operation Sorting, selection, searching, String matching Comparison(of keys) Graph traversal Processing a node Matrix multiplication Multiplication Amount of work done usually depends on size of the input 输入规模 usually does not depends on size of the input only Worst -case Complexity Average-case Complexity Advanced topics Lower bound(Selection) Optionality(Greedy , DP) Computation complexity Approximate/ online / randomized algothms","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[{"name":"Algorithm design and analysis","slug":"Algorithm-design-and-analysis","permalink":"http://lyk-love.cn/tags/Algorithm-design-and-analysis/"}]},{"title":"第四章-反应时间","slug":"第四章-反应时间","date":"2021-07-04T10:25:27.000Z","updated":"2022-09-26T06:39:34.944Z","comments":true,"path":"2021/07/04/第四章-反应时间/","link":"","permalink":"http://lyk-love.cn/2021/07/04/%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E5%8F%8D%E5%BA%94%E6%97%B6%E9%97%B4/","excerpt":"Outline 一、反应时间的性质 二、影响反应时间的因素 三、反应时间的测定方法","text":"Outline 一、反应时间的性质 二、影响反应时间的因素 三、反应时间的测定方法 一、反应时间的性质 反应时间是心理实验中使用最早、应用最广泛的反应变量之一。 对反应时间的研究最先始于天文学家Bessel对于人差方程的研究。 最早将反应时间的测量用于心理实验的是荷兰生理学家Donders. 1879年以后，冯特及其学生对反应时间进行了一系列实验研究。 认知心理学兴起后，为了揭示信息加工过程和特点， 反应时间的测量也获得进一步的发展，出现了一些新的研究方法，如相加因素法、开窗实验法、速度准确率分离技术。 （一）反应时间的概念 反应时间是指从刺激的呈现到反应的开始之间的时距。 反应时间也被称为“反应的潜伏期”。 （二）反应时间研究的简史 1796年英国格林尼治天文台公案； Brandly的眼耳法（1820年）、人差方程； 1850年Helmholtz运用反应时间来测定神经传导速度; 1868年，Donders发明分离反应时间的实验（反应时间的相减法），分离出简单反应时、辨别反应时和选择反应时； 1879年及以后，冯特及其学生对反应时间进行了系列研究。 其中，Cattell揭示了选择反应时长于简单反应时的原因—— 在反应的准备上不同； 20世纪50年代中期以后，认知心理学兴起。 认知心理学主张研究认知活动本身的结构和过程，并把这些过程看作信息加工过程。而任何过程都需要时间，因而可利用反应时间这一客观指标，来对加工过程进行研究，以揭示信息加工过程和信息加工的各个阶段。 例如，运用反应时间相减法，安排两种作业A和B， 其中A包含B所没有的某个特定心理过程，而在其 他方面相同，那么这两个作业的反应时间之差就 是那个特定的心理过程所需要的时间 1969年，Sternberg在相减法的基础上发展 了反应时间的相加因素法。相加因素法假定，完成一个作业所需的时间是一系列信息加工阶段分别所需时间的总和。 由于反应时间的相减法和相加因素法都不是直接测得某一特定加工阶段所需的时间，而 是要通过间接的比较才能得到，并且相应的 加工阶段也要通过严密的推理才能被发现。 因此，Hamilton等（1977）、Hockey等（1981） 发展了一种新的实验技术——“开窗”实验。 Meyer等（1988）速度-准确率分解技术 （speed-accuracy decomposition, SAD） （三）简单反应时间和选择反应时间 简单反应时间是给予被试者以单一的刺激， 要求他作同样的反应。被试的任务很简单， 他预先已知道将有什么样的刺激出现并需 要作出什么样的反应。 选择反应时间是根据不同的刺激物，在各种可能性中选择一种符合要求的反应。 （四）反应时间实验的要求 进行反应时间实验，和其他心理实验一样，都要很严格地控制实验条件，像刺激的强度和刺激持续的时间等等。除此之外，还要讲求策略思想，应严格遵循一定的程序和要求 对被试反应的要求 首先从被试反应要求上看，应避免出现过早反应或其他错误的反应。 防止出现“假反应”的有效措施是在实验中插入侦察试验（detection test），即给预备信号之后并不呈现刺激。 选择反应数目 选择反应的数目需要与辨别的刺激数目相等，即有几种反应就要有几种刺激。 （五）反应时间的因变量 反应时实验中有二个基本因变量（或依变项） （dependent variable），即速度（speed和准确性 （accuracy）。 反应时间实验中的一个突出问题就是权衡反应速度和反应准确性的相互关系。 人们都有这样的常识，当一个人很快去完成某件事时，他会比慢慢地做某件事犯更多的错误。反之， 如果某人很正确地做某件事时，速度上就会变慢。 心理学家称这种关系为速度-准确性权衡。 帕彻拉（Pachella，1974）的实验结果表明，为了使错误率降低到2％，反应时将增加100毫秒 这说明根据速度-准确性权衡，为了减少错误率，反应时间就会延长。 在进行反应时的实验时,应考虑速度和准确性两个指标,当然,有时可以同时选择两个指标,有时可以选择其中一个指标。但是在只选择其中一个指标时，应对另一个指标有所交代，说明其可以忽略不计的原因。 这是反应时实验中一种重要技术。 二、影响反应时间的因素 反应时间受刺激变量的影响 在刺激变量中，对反应时间影响比较大的因素有：刺激的不同类型、强度、复杂程度及 刺激呈现的方式等。 因刺激的不同类型而异 不同类型的刺激通过特定的通道(感觉道)作用于各个感官，它们的反应时间是不同的。同一感觉道里，刺激的部位不同，反应时间也有差异 不同感觉道的反应时间(被试为有训练的成人) 感觉道 反应时间（ms） 触觉 117 ~ 182 听觉 120 ~182 视觉 150 ~ 225 冷觉 180 ~ 240 温觉 210 ~ 300 嗅觉 308 ~ 1082 味觉 400 ~ 1000 痛觉 400~1000 反应时间受机体变量影响 影响反应时间的主要刺激变量如上所述，但外在原因需通过内在条件而起作用，和其他心理现象一样，反应时间也是以机体的内部状态为中介而对外界刺激作出反应的。 影响反应时间的机体变量为数众多，主要有： 机体适应水平、准备状态、练习次数、动机、 年龄因素和个别差异、酒精和药物作用等。 适应水平 适应（adaptation）在此处指在刺激物的持续作用下，感受器发生的变化。感受器的适应水平对反应时间有着明显的影响。 准备状态 准备状态（readiness）是指机体对于某种行为作出的准备情况。这个材料表明，感受器适应水平的变化对反应时间的影响 。 试者在主试者发出“预备”口令到刺激呈 现这段预备时间内的准备状态也是影响反应时间的因素之—。在这时距内，被试者处于 积极准备状态，力求尽快对刺激作出反应。 如果预备时间太短，被试者可能来不及准备； 如果太长，被试者的准备状态又可能出现衰退而延误反应。 练习次数 在心理实验中，练习（exercise）是一个控制变量，即一个潜在的自变量。练习与反应时间的关系相当密切。在一定范围内，练习次数越多（上百次），反应会越快，反应时间 减少的趋势是逼近一个极限而稳定下来。 动机 动机（motive）是由于人的某种需要所引起的有意识或无意识的行为指向。 反应时间实验中被试者易受某种额外动机的影响。 年龄因素和个体差异 一般认为，自发育阶段至25岁前（青少年阶段），反应时间随年龄增长而减少，起初减少得快，以后较慢。 戈茨达克（Gottsdamker，1968）在对一组年 龄为 18～93 岁的成人被试者的研究中注意到， 随着年龄的增长，感觉-运动反应时间逐渐延 长。 此外，还有人发现在25岁以后到 60 岁的一段 时间内，反应时间的增长极为缓慢，但60 岁以后反应时间开始有了较大增加 个体差异（或个别差异）指不同个体之间在 品质和属性上存在的任何差别。 菲萨尔（Fessard，1926）曾对1000名男性成人 被试者作听觉简单反应时间的测定，结果发 现反应时间的均数分配大致上呈常态。但是老年组的变异系数较显著地高于青年和中年组，说明老年组在感觉-运动反应速度方面存 在着较大的个别差异。 酒精及药物作用 酒精在脑神经系统达到一定浓度时，中枢神经系统逐渐迟钝，对周围情况变化的反应速度大大下降。 如果是酗酒，其反应时间将延长2～3倍，甚至更长 三、反应时间的测定方法 反应时间的测量有两个用途: 作为成就的指标,因为对一件工作越精通,就完成得越快. 也可作为借以产生一种行为结果的内部过程复杂性的指标,因为内部过程越复杂,所消耗的时间便越长. （一）反应时间的相减法 逻辑 安排两种反应作业，其中一个 作业包含另一个作业所没有的一个处理（加工）阶段，并在其 他方面均相同，从这两个反应时间之差来判定此加工阶段。这 种实验在原则上是合理的，在实践上是可行的。认知心理学也 正是应用减数法反应时间实验提供的数据来推论其背后的信息加工过程的。 评价 使用这种方法要求实验者对实验任务引起 的刺激与反应之间的一系列心理过程有精确的认识，并且要求 两个相减的任务中共有的心理过程要严格匹配，这一般是很难 的。这些弱点大大限制了减数法的广泛使用 （二）反应时间的相加因素法 Intro 在20世纪，Sternberg（1969）发展了唐德斯的减数法反应时间，提出了加法法则，称之为加因 素法（additive factors method）。这种实验 并不是对减数法反应时间的否定，而是减数法的发 展和延伸。 相加因素法反应时间实验认为完成一个作业所需的时间是一系列信息加工阶段分别需要的时间的总和， 如果发现可以影响完成作业所需时间的一些因素， 那么单独地或成对地应用这些因素进行实验，就可 以观察到完成作业时间的变化。 逻辑 如果两个因素的效应是互相制约的，即一个因素的效应可以改变另一因素的效应，那么这两个因素只作用于同一个信息加工阶段；如果两个因素的效应是分别独立的，即可以相加，那么这两个因素各自作用于不同的加工阶段。 这样，通过单变量和多变量的实验，从完成作业的时间变化来确定这一信息加工过程的各个阶段。 因此，重要的不是区分出每个阶段的加工时间，而是辨别认知加工的顺 序，并证实不同加工阶段的存在。 加因素法假定，当两个实验因素影响两个不同的阶段时，它们将对总反应时间产生独立的效应，即不管一个因素的水平变化如何，另一个因素 对反应时间的影响是恒定的。这样称两个因素的影响效应是相加的。 加因素法的基本手段是探索有相加效应的因素，以区分不同的加工阶段。 评价 相加因素法的弱点是，它的基本前提是人的信息加工是系列加 工，这一点受到很多心理学家的质疑。因为加因素法反应时实 验是以信息的系列加工而不是平行加工为前提的，因而有人认为其应用会有很多限制。 更为直接的问题是关于加因素法反应时实验的逻辑，即能否应 用可相加和相互制约的效应来确认信息加工的阶段。 Pachella（1974）指出，两个因素也许能以相加的方式对同一个加工阶段起作用，也许能对不同的加工阶段起作用并且 相互发生影响。 还有人指出，加因素法反应时实验本身并不能指明一些加工阶段的顺序，在这个方面，它极大地依赖于一定的理论模型 （三）反应时间的“开窗”实验 Intro 前面谈到的减数法和加因素法反应时实验 难以直接得到某个特定加工阶段所需的时间，并且还要通过严密的推理才能被确认。 如果能够比较直接地测量每个加工阶段的时间，而且也能比较明显地看出这些加工 阶段，那就好像打开窗户一览无遗了。这 种实验技术称为“开窗”实验，它是反应时实验的一种新形式。 评价 不难看到，这种“开窗”实验的优点是引人注目的，但也存在着一些问题。例如， 可能在后一个加工阶段出现对前一个阶段的复查，贮存阶段有时还包含对前面字母 的转换结果的提取和整合，并且它难以与反应组织分开来","categories":[{"name":"Psychology","slug":"Psychology","permalink":"http://lyk-love.cn/categories/Psychology/"}],"tags":[{"name":"实验心理学","slug":"实验心理学","permalink":"http://lyk-love.cn/tags/%E5%AE%9E%E9%AA%8C%E5%BF%83%E7%90%86%E5%AD%A6/"}]},{"title":"第五章-传统心理物理法","slug":"第五章-传统心理物理法","date":"2021-07-04T10:20:10.000Z","updated":"2022-09-26T06:39:34.944Z","comments":true,"path":"2021/07/04/第五章-传统心理物理法/","link":"","permalink":"http://lyk-love.cn/2021/07/04/%E7%AC%AC%E4%BA%94%E7%AB%A0-%E4%BC%A0%E7%BB%9F%E5%BF%83%E7%90%86%E7%89%A9%E7%90%86%E6%B3%95/","excerpt":"Outline 一、阈限的性质 二、测定阈限的3种基本方法 最小变化法 恒定刺激法 平均差误法 三、心理量表法","text":"Outline 一、阈限的性质 二、测定阈限的3种基本方法 最小变化法 恒定刺激法 平均差误法 三、心理量表法 Intro 什么是心理物理学 Definition： 研究心理量和物理量之间的 关系的科学。 心理物理学(psychophysics)这个名词是由 两个希腊字根psyche和physike所组成。 物理量是指对身体各感官的刺激；心理 量是指各种感觉或主观印象。如： – 亮度（物理量）——明度（心理量） – 强度（物理量）——响度（心理量） 心理物理学的发展和介绍 心理物理学的先驱是G. T. Fechner（1801-1887）。 Fechner通过对感觉强度与刺激强度之间的数量关系的 长期研究，发展出了测量感觉的基本方法。 一百多年来，心理物理学方法不断发展，但它的中心问题仍然是物理量和心理量之间的数量关系的问题。 心理物理学方法所处理的问题大体上可以分为两大类 – 感觉阈限的测量； – 阈上感觉的测量，也就是心理量表的制作。 自从本世纪50年代，W. P. Tanner, Jr.和J. A. Swets把信号检测论（signal detection theory，SDT）引入心理学领域 以来，又为心理物理学的研究提供了一个新的有力工 具。 一、阈限的性质 感觉阈限的界定 刚刚能引起感觉的最小刺激强度被叫做绝对阈限。 刚刚能引起感觉的最小差别叫做差别阈限。 人的感受性的这种随机性变化，在每一种感觉道中都能发现。这种随机变化往往与以下几个方面有关: 如何测量阈限有关； 被试对任务的注意程度及其态度有关； 被试的感受性暂时出现不稳定的摇摆有关。 不过，这种随机变化的感觉，其次数分配基本上呈常态分布 根据统计学，可以把那个可以刚刚引起感觉的最小刺激强度以其算术平均数来表示。而这个平均数恰好有50％的实验次数报告为“有感觉” 的刺激强度，由此可见，阈限是个统计值。 表1 某些近似的觉察阈限 感觉种类 觉察阈限值 因而，我们把阈限定义为：有50％的实验次数能引起积极反应的刺激之值；同理，把差别阈限定义为有50％的实验次数能引起差别感觉的 那个刺激强度之差。 差别阈限 关于差别阈限 表2 最优条件下各种感觉道的韦伯比例 1846年E. H. Weber发表了他关于重量差别阈限的研究， 系统地阐明了差别阈限和标准刺激之间的关系。 他指出差别阈限和标准刺激成正比，并且差别阈限和 标准刺激的比例是一个常数，通常用△I/I = k 表示。在这里，△I代表差别阈限，I代表标准刺激强度，k是小于1的常数。k也称作韦伯比例或韦伯分数。 不同感觉道的韦伯分数是不同的。后来，Fechner把这 个关于差别阈限的规律称为韦伯定律（Weber Law） 最优条件下各种感觉道的韦伯比例 感觉道 韦伯比例 音高/ 200Hz 1/333 重压觉 1/77 视明度/100光子 1/62 举重/300g 1/52 响度/100dB, 1000Hz 1/11 橡胶气味 1/10 皮肤压觉/5g/mm^2 1/7 韦伯定律的主要贡献 给我们提供了一个比较辨别能力的指标。 第二，可对不同感觉道的感受性进行比较 二、测定阈限的3种基本方法 最小变化法 Intro 又叫极限法、最小可觉差法，是测量阈限的直接方法； 最小变化法的刺激由递减和递增的两个系列组成； 每次呈现刺激后让被试报告他是否有感觉； 刺激的增减应尽可能地小，目的是系统地探求被试由一类反应到另一类反应的转折点，即在多强刺激时，由有感觉 变为无感觉，或由无感觉变为有感觉。每个系列的转折点 就是该系列的绝对阈限。 用最小变化法测定绝对阈限 自变量 用极限法测定绝对阈限，自变量是刺激序列。 刺激序列要按递增或递减系列交替呈现。 递增时，刺激要从阈限以下的某个强度开始； 递减时，刺激系列的起点要大于阈限的某个强度； 一般应选10到20个强度水平。 为了使测定的阈限准确，并使每一刺激系列的 阈限能相对稳定，一般递增和递减刺激系列要分别测定50次左右（共100 次左右）、刺激应由实验者操纵。 为了避免被试者形成定势，每次呈现刺激的起点不应固定不变，而应随机变化。 因变量（反应变量） 用极限法测定绝对阈限的反应变量时， 要求被试以口头报告方式表示。 当刺激呈现之后，被试感觉到有刺激， 就报告“有”，当被试没感觉到有刺激， 就报告“无”，其依据是被试的内省， 而不是刺激是否呈现。 阈限的确定（计算） 在一个刺激系列中，被试者报告“有” 和“无”这两个报告相应的两个刺激强度的中点就是这个系列的阈限。 误差及其控制 用极限法求绝对阈限经常会产生一些误差。 在这些误差中，有些是由直接对感觉产生干扰的因素引起的；还有些是非感觉方面的因素引起的，如习惯和期望、练习和疲劳、时间和空间等等。 这些在测定阈限的过程中经常起作用，以致使测定结果产生一定倾向的误差。这类误差叫做常误（constant error）。 极限法测定绝对阈限产生的误差主要有 四种： 习惯误差和期望误差 练习误差和疲劳误差。 习惯误差和期望误差 消除: 采用抵消平衡设计（即ABBA设计）： 第一次：递增（A） 第二次：递减（B) 第三次：递减（B) 第四次：递增（A) 练习误差和疲劳误差 练习误差（error of practice）是由于实验的多次重复，被试逐渐熟悉了实验情景，对实验产生了兴趣和学习效果，而导致反应速度加快 和准确性逐步提高的一种系统误差。 与此相反，由于实验多次重复，随着实验进程 而发展的疲倦或厌烦情绪的影响，而导致被试 反应速度减慢和准确性逐步降低的一种系统误差，称之为疲劳误差（error of fatigue）。 随着时间的进展，练习可能使阈限降低，而疲劳可能使阈限升高 用最小变化法测定差别阈限 自变量 用极限法测定差别阈限时，每次要呈现两个刺激，让被试比较，其中一个是标准刺激，即刺激是固定的，其强度大小不变；另一个是比较刺激又称变异刺激，即刺激的强度按由小而大或由大而小顺序排列。 标准刺激和比较刺激可同时呈现，标准刺激在每次比较时都出现，比较刺激按递增或递减系列，以测定绝对阈限的同样方法与标准刺激匹配呈现。 因变量 用极限法测定差别阈限的反应变量要求 被试以口头报告方式表示，一般用三类反应，将比较刺激与标准刺激加以比较， 当比较刺激大于标准刺激时，主试记录 “＋”；当比较刺激等于标准刺激时， 主试记录“=”；当比较刺激小于标准刺激时，主试记录“-”。当被试在比较时表示怀疑，可记作“？” 阈限的确定(计算) 确定差别阈限时先要求得一系列的数据，这些数据有： 在递减系列中最后一次“＋”到非“+” 之间的中点为差别阈限的上限 ；第一次非“-” 到“-” 之间的中点为差别阈限的下限 ； 在递增系列中最后依次“-”到非“-” 之 间的中点为差别阈限的下限；第一次非“＋” 到“＋”之间的中点为差别阈限的上限 在上限与下限之间的距离为不肯定间距。 不肯定间距的中点是主观相等点。在理论上主观相等点（或主观等点）应与标准刺激 相等，但实际上两者有一定的差距，这个差距称为常误。 取不肯定间距的一半或者取上差别阈和 下差别阈之和的一半为差别阈限。 误差及其控制 与用极限法求绝对阈限一样，在测定差别阈限时，也必须想方设法控制常误。 • 除了要控制习惯和期望误差外，还要控 制因标准刺激和比较刺激同时呈现所造 成的误差（空间误差）或者因先后呈现 所造成的（顺序误差）。 • 控制方法可采用多层次的ABBA法。 恒定刺激法 Intro 恒定刺激法（或固定刺激法）又叫正误法、次数法，通常由5-7个刺激组成，这几个刺激在实 验过程中保持不变。 它是心理物理学中最准确、应用最广的方法， 可用于测定绝对阈限、差别阈限和等值，还可用于确定其他很多种心理值。 此法的特点是：根据出现次数来定阈限，即以次数的整个分布求阈限。 流程 主试从预备实验中选出少数刺激，一般是5到7个， 这几个刺激值在整个测定过程中是固定不变的； 选定的每种刺激要向被试呈现多次，一般每种刺激呈现50到200次； 呈现刺激的次序事先经随机安排，不让被试知道。 用以测量绝对阈限，即无需标准值，如用以确定差别阈 限或等值，则需包括一个标准值； 此法在统计结果时必须求出各个刺激变量引起某 种反应（有、无或大、小）的次数。 特别要注意的是，此法在实验之前需要选定刺激。 所选定的刺激最大强度应为每次呈现几乎都能 为被试感觉到的强度，它被感觉到的可能性应不低于95%。所选刺激的最小强度应为每次呈现几乎都不能感觉到的强度，它被感觉到的可能性应不高于5%。 选定呈现刺激范围之后，再在这个范围内取距离相等的刺激，每种刺激强度呈现不得少于50 次。 用恒定刺激法测定绝对阈限 自变量 用恒定刺激法测定绝对阈限，是从略高于感觉到略低于感觉这一范围内选5到7个等距的刺激强度。 因变量 用恒定刺激法测定绝对阈限的反应变量要求被试者以口头报告方式表示， 在实验中每呈现一次刺激后，被试者若感觉到 了，则报告“有”，主试者记录“+”；被试者 若感觉不到，则报告“无”，主试者就记录 “-”。 然后根据被试者对不同刺激所报告的“有”或 “无”的次数来求出百分数，以此来计算阈限。 绝对阈限的计算 直线内插法 用恒定刺激法测定差别阈限 自变量 用恒定刺激法测定差别阈限，是让被试者将比较刺激与标准刺激加以比较； 标准刺激是能被感觉到的某一刺激强度； 比较刺激可在标准刺激上下一段距离内确定， 一般从完全没被感觉出差别到完全感觉出差别 的范围内选定5到7个刺激强度作为比较刺激。 比较刺激要随机呈现，每个比较刺激与标准刺激至少要比100次。 因变量 用恒定刺激法测定差别阈限的反应变量 要求被试者以口头报告方式表示 三类反应 即“大于”、“等于”和“小于”，分 别记为“+”、“=”和“-”。 让被试作三类反应时，因其中有“等于” 的反应，若被试较为自信，则作出“等 于”的反应就较少；若被试较为谨慎， 则作出“等于”的反应就较多。这样会 直接影响到差别阈限的大小。 这种反应易受到被试的态度的影响。 二类反应 让被试者作“大于”和“小于”两种判断，即使分不清时也要作出其中的一 种判断。 差别阈限的计算 平均差误法 Intro 平均差误法（或均误法）又称调整法、 再造法、均等法，是最古老且基本的心理物理学方法之一。 虽然它最适用于测量绝对阈限和等值， 但也可用以测量差别阈限 流程 •呈现一个标准刺激，令被试再造、复制或调节一个比较刺激，使它与标准刺激相等。比较刺激也可由实验者调节，由被试判断 这个方法是要求被试判断什么时候比较刺激和标准刺激相等。 被试判断为与标准刺激相等的比较刺激，并不总是一个固定的数值，而是围绕着一个平均数 变化的一个数。这个变化的范围就是不肯定间距。 不肯定间距的中点就是主观相等点。 通过对主观相等点和不肯定间距的测量，就可以估计差别阈限。 用平均差误法测定阈限 用平均差误法测定绝对阈限 用平均差误法测定绝对阈限 自变量 用平均差误法测定绝对阈限，是让被试者调整一个比较刺激与一个标准刺激相 等。不过，此时的标准刺激假设为零， 即让被试者每次将比较刺激与“零”相比较。 因变量（反应变量） 用平均差误法测定绝对阈限的反应变量是被试者每次调整比较刺激与标准刺激相等的那个数值。 绝对阈限的测定（计算） 让被试者每次调到刚刚感觉不到（即与 “零”标准刺激等值），然后把各次测定数值加以平均即为绝对阈限。 用平均差误法测定差别阈限 自变量 用平均差误法测定差别阈限，是向被试者呈现一个标准刺激，让其调整比较刺 激。比较刺激是一种连续的量。在被试 认为接近时，可反复调整，直到其认为满意为止。 因变量 用平均差误法测定差别阈限的反应变量是被试 每次调整的数值，即其认为与标准刺激相等的数值。 由于被试反复测试，每次的结果并不是一个固定的数值，它们是围绕着一个平均数变化的数值。这个变化范围就是不肯定间距。 不肯定间距的中点，即多次调整结果的平均数， 就是主观相等点，主观相等点与标准刺激的差就是常误。 绝对阈限的测定（计算） 用平均差误法求差别阈限，所得差别阈限只是一个估计值，平均差误有两种计算方法： 把每次调节的结果(或每次的判断)与标准刺激之差的绝对值平均起来作为差别阈限 把每次调节的结果与主观相等点之差的绝对值平均起来作为差别阈限。 误差及其控制 在平均差误法实验中，一般要被试自己操纵实验仪器 来调整比较刺激，使其与标准刺激相等。这就要产生动作误差，亦即因被试所采用的方式不同而产生误差。 若标准刺激和比较刺激是相继呈现的，又易产生时间误差。因此，在实验中应加以控制，控制方法依具体实验不同而不同，一般可采用多层次的ABBA法，还可使比较刺激从小到大，从大到小两方面来进行调整， 以便控制动作误差等 测量阈限的三种方法的比较 最小变化法的实验程序和计算过程都具体地说明了感 觉阈限的含义，但它会因其渐增和渐减的刺激系列而 产生习惯误差与期望误差。 恒定刺激法的实验结果可以应用各种数学方法加以处 理，因而便于与其它测定感受性的方法进行比较。在 应用3类反应的实验程序时，被试的态度会对差别阈限 值有较大影响。 平均差误法的特点是求等值，它的实验程序容易引起 被试的兴趣，但对不能连续变化的刺激则不能用平均 差误法来测其差别阈限。 三、心理量表法 物理刺激可由物理量表来测量 心理量的大小却不能用物理量表来测量 心理量需要用心理量表来测量 从量表有无相等单位和有无绝对零点来分，心理量表可分为顺序量表、等距量表和比例量表三类 顺序量表 顺序量表的制作方法 等级排列法 等级排列法是一种制作顺序量表的直接方法。这个方法是把许多刺激同时呈现， 让许多被试者按照一定标准，把这些刺激排成一个顺序，然后把许多人对同一刺激评定的等级加以平均，这样，就能求出每一刺激的各自平均等级，最后， 把各刺激按平均等级排出的顺序就是一个顺序量表 对偶比较法 对偶比较法是把所有要比较的刺激配成对，然 后一对一对地呈现，让被试者对于刺激的某一 特性进行比较，并做出判断：这种特性的两个刺激中哪一个更为明显。 配成对的个数是n（n-1）/2 对偶比较法的两类误差及消除 如果有五种样品，A、B、C、D、E，则可配成5× （5 -1）/2=10对。 如果各对样品同时呈现，则要消除空间误差 --- 即样品在空间中不同方位呈现，于判断时产 生的误差现象。若第一轮以AB形式呈现，则 第二轮中以BA形式呈现即左右颠倒。 如果是相继呈现，则要注意消除时间误差 --- 即相等的二个样品在先后不同时间出现，于判断时产生的误差。若第一轮以先A后B次序相 继呈现，则第二轮要按先B后A次序相继呈现 对偶比较法需要注意两点： 用这种方法得到的顺序量表，还仅仅 是针对一个被试的心理物理量表，尚不能直 接推广到更大的人群。 这一量表模型要求对偶比较是可传递的（transitive），如果刺激A优先于刺激B， 而且刺激B优先于刺激C，那么刺激A优先于刺激C。然而，有些情况下这种传递性难以保证，这时就不能采用对偶比较法 等距量表 制作等距量表的方法 感觉等距法 差别阈限法 差别阈限法（或差异阈限法）是制作等距量表 的一种间接方法，通过在不同强度的基础上测 量最小可觉差来实现。 • 具体地说，用任何一种古典的心理物理法测出 感觉的绝对阈，并以此为起点，产生第一个最 小可觉差的刺激强度，以第一个最小可觉差为 基准，再测量第二个最小可觉差……。 • 这样测得许多最小可觉差以后，以刺激强度为 横坐标，以绝对阈以上的最小可觉差数为纵坐 标，画出的心理物理关系图就是等距量表 比例量表 比例量表（或比率量表）（ratio scale）既有真正的零点，也是等距的。 一个比例量表除含有名称、等级这些等距量表的 特征外，还有一个具有实际意义的绝对零点。 由于它具有绝对的零点，且量表上的单位相等， 因此就可进行加、减、乘、除四则运算。 在物理学中，我们所用的绝对温度量表就属于这 类量表。更为熟悉的是公制的尺所采用的量表制 度。 比例量表的制作方法 分段法 分段法是制作感觉比例量表的一种最直接的方法。 这个方法是通过把一个感觉量加倍或减半或取任何其他比例来建立心理量表的。 具体作法是呈现一个固定的阈上刺激作为标准， 让被试者调整比较刺激，使它所引起的感觉为标准刺激的一定比例，例如，2倍、3倍、1/2倍、或 1/3倍等等。 每个实验只选定同一个比例进行比较，同一个标 准刺激比较若干次后，再换另外几个标准刺激进行比较。 当把所有的标准刺激都比较完之后，便可用与各标准刺激在感觉上成一定比例的相应的物理量值制成一个感觉比例量表。 数量估量法 具体步骤是主试者先呈现一个标准刺激，例如， 一个重量，并赋予标准刺激一个主观值，例如 为10， 然后让被试者以这个主观值为标准，把其他不 同强度比较刺激的主观值，放在这个标准刺激 的主观值的关系中进行判断，并用数字表示出 来。 然后计算出每组被试者对每个比较刺激量估计 的几何平均数或中数，再以刺激值为横坐标， 感觉值为纵坐标，即可制成感觉比例量表。 数量估量法的数据处理： 在心理量和物理量关系的实验中，常 会出现特别大的数字，所以数量估计法采用的数据处理通常是几何平均值。 几何平均值定义为n个数值相乘之积的 n次方根","categories":[{"name":"Psychology","slug":"Psychology","permalink":"http://lyk-love.cn/categories/Psychology/"}],"tags":[{"name":"实验心理学","slug":"实验心理学","permalink":"http://lyk-love.cn/tags/%E5%AE%9E%E9%AA%8C%E5%BF%83%E7%90%86%E5%AD%A6/"}]},{"title":"第三章-实验设计","slug":"第三章-实验设计","date":"2021-07-04T10:11:08.000Z","updated":"2022-09-26T06:39:34.944Z","comments":true,"path":"2021/07/04/第三章-实验设计/","link":"","permalink":"http://lyk-love.cn/2021/07/04/%E7%AC%AC%E4%B8%89%E7%AB%A0-%E5%AE%9E%E9%AA%8C%E8%AE%BE%E8%AE%A1/","excerpt":"Outline 第一节 实验设计的基本类型 一、被试者内设计 二、被试者间设计 三、混合设计 第二节 多变量实验技术 一、多自变量实验的优点 二、多因素实验设计 三、拉丁方设计 第三节 实验数据的统计分析（略）","text":"Outline 第一节 实验设计的基本类型 一、被试者内设计 二、被试者间设计 三、混合设计 第二节 多变量实验技术 一、多自变量实验的优点 二、多因素实验设计 三、拉丁方设计 第三节 实验数据的统计分析（略） 第一节 实验设计的基本类型 实验设计的概念 实验设计乃是进行科学实验前做的具体 计划。它主要是控制实验条件和安排实 验程序的计划。它的目的在于找出实验 条件和实验结果之间的关系，做出正确 的结论，来检验解决问题的假设。 实验设计的内容 ①刺激变量(或刺激变项)的确定及其呈现的方式； ②反应变量(或反应变项)的指标及其测量方法； ③对一切有关变量(或变项)的控制措施； ④确定被试总体及被试样本人数和选择被试的方法； ⑤拟定主试在实验开始前对被试者要说的指示语； ⑥规定实验次数； ⑦安排实验程序； ⑧规定使用仪器的型号； ⑨规定处理实验数据的方法。 每一个实验设计都必须回答三个基本问题 实验采用多少自变量？ 例如在一个阅读速度的研究中取“照明强度”为自变量。 各自变量内又采用多少处理水平？ 例如照明强度又分为强、中、弱等处理水平。 在各自变量和各处理水平中用相同的被试者，还是用不同的被试者？ 根据这三个条件的组合，就可构成许多不同类型的实验设计。 被试内设计、被试间设计、混合设计 被试内设计 也叫单组实验设计（within-subjects design)，是每个被试须接受自变量的所有情况的处理。 基本原理：每个被试参与所有的实验处理，然 后比较系统被试在不同处理下的行为变化。 在实验研究中，如果实验者主要想研究每一个被试对实验处理所引起的行为上的变化，可考虑采用被试内设计。 被试内设计可分为三种子类型：1、实验前后设计；2、定时系列设计；3、抵消实验条件的设计。 实验前后设计 指在实验条件处理前对被试进行观测结果与实验条件处理所做的同样观测结果加以对比的设计。即，这种设计类型是实验（处理）前后的比较设计。 优点： 能较明显地检测出实验处理的效果如何； 对被试的需要量较少， 一组被试当两组被试用，无须设控制组，不但提高效率，被试变量也得到较好控 制。 缺点： 前后两次观测之间存在时间间隔，会带来外来影响； 易产生顺序误差 定时系列设计 指实验处理前对一组被试作一系列的定时重复观测，然后实施实验处理。在对被试作一系列的定时重复观测，分析自变量（实验处理）对因变量的关系 优点：除具有前后设计的优点，还具有 降低由于一次观测而得到被试不正常行为的 率； 提供测量过程中的信息。 缺点： 由于更多次的观测，势必延长实验时间，从而会有更多的外来影响； 也正是更多次的观测，更易引起顺序误差，更易导致练习、疲劳、紧张或厌烦等效应 抵消实验条件的设计 指抵消实验过程中无关变量的一种设计。 前面讲到，有些无关变量在某些实验情况下既不能被消除，又不能保持恒定。 例如，单组实验往往由于前一处理影响后一处理的效果，产生顺序误差。为了 抵消顺序误差，最简单的方法就是用ABBA的排列顺序来安排实验顺序。 优点： 能较好地控制被试变量； 能较好地控制顺序误差； 时间上比较经济。 缺点： 反应变量在时间维度（轴）上的关系是线性时才能使用。 对有些实验不适用。如用两种学习方法学习同一实验材料 被试间设计 被试间设计是要求每个被试（组）只接 受一个自变量的处理，对另一被试者 （组）进行另一种处理，故又称独立组设计 包括随机组设计和配对组设计 被试间设计的统计检验——独立样本的差异显著性 随机组设计 将被试随机分配在不同的组内接受不同的自变量处理。 随机组设计的基本假设是将被试随机分配到不同的组，若对各组用同一样的课题，在系统的条件下进行测量，其结果就成为系统组，则他们的成绩在统计上应是相等的。 如何做到随机分组？ 同时分配法 抽签法 笔划法 报数法 次第分配法 简便法：按被试出现在实验的先后分配 区内随机法 随机组设计的优缺点 优点： 用随机分配被试者的方法可 制两组被试者变量的差异，分组方法简单可行。 由于对每一被试者只作一次观测，可消除某些实验误差，如消除学习误差的影响。 缺点是： 分成等组的方法仍欠精密。 若两组在不同时期观测，就有可能插入实验以外的偶发事件，影响因变量的观测结果。 配对组设计 也叫对等组设计、匹配组设计，是随机组设计的一种扩展。目的是使各组的特性更加相同。这种设计可以控制组内变 异和组间变异。 匹配被试就是对全部被试进行预备测验， 测验的性质与正式实验的性质是类似的， 或者说是相关的，然后按测验成绩均匀地形成组。 配对设计的优缺点 优点：在实验处理之前，就把组间变异缩到最小，并使各组内变异比单独的随 机分配更接近相等。因此，这种设计能 对被试个别差异给予更多的控制，小型实验用配对设计，其效果比用随机分组的效果更为显著。 缺点：实验者因分配被试而大大增加工作量。 被试内和被试间设计比较 被试内设计：也叫单组实验设计，是每个被试须接受自变量的所有情况的处理。 被试间设计：被试间设计是要求每个被试者只接受一种实验处理，对另一被试 者进行另一种处理，故又称独立组设计 混合设计 混合设计是指在一个研究中有些自变量按组内设计安排，有些自变量按组间设计安排。一般说来，如果一种自变量很可能会影响另一种自变量，那么对这些自变量按组间设计安排，其余的自变量按组内设计安排 配对组设计的基本模式 第二节 多变量实验技术 多变量实验的优点 效率高 有两个自变量的实验要比分别做两个只有单一自变量的实验效率要高，也就是说事半功倍，花同样的时间，做了一倍， 甚至二倍、三倍的工作。 实验控制较好 做一个实验时某些控制变量比进行两个实验时更易于控制和恒定。如被试者条件是一样的，同一个被试者，同样的身心 状况。再如时间条件也是一样的，日期相同、时间相同。其他条件就更不用说了，如相同的外界环境、相同的温度、相同的湿度等等。这就在很大程度上排除了许多实验误差，减 少了实验污染。 实验结果更有价值 有多种自变量的实验所得的结果，由于在多种情况下都证明是确实的，这样就比多个单独实验所概括的结果更有价值。 多因素实验设计 多因素实验设计是指在实验中包括两个或两个以上因素(自变量)，并且每个因素都有两个或两个以上 的水平，各因素的各个水平互相结合，构成多种组合处理的一种实验设计，又称完全随机析因设计 在完全随机析因设计中，研究者可以考察各个自变 量交互作用对因变量的主要影响效应(交互作用)， 并同时考察各自变量对同一因变量的主要影响效应 (主效应)，以及考察一个因素的各个水平在另一个 因素的某个水平上的效应(简单效应)；在心理学的 实验研究中，这种设计具有很大的实用价值 多因素析因设计的主要效应与交互作用的效应 主效应是指由每个单独因素(自变量)所引起的 因变量的变化 交互作用的效应是指当一个因素(自变量)对因变量影响大小因其他因素的水平或安排的不同而有所不同时，所产生的交互作用影响因变量的结果。相反，如果某一因素(自变量)对因变量影响大小，不受其他因素的水平或安排的影响，我们就说这个因素与其他因素是没有交互作用的。 拉丁方设计 拉丁方设计（或拉丁方格设计）（Latin－square design）是多变量实验设计中一种较为常用的设计方 案。 心理实验中采用循环法平衡实验顺序对实验结果的影响，就使实验顺序、被试者差异都作为一个自变量来处理。 只要是实验中自变量的个数（因素）与实验处理水平 数相同，而且这些自变量之间没有交互作用的存在时， 都可采用拉丁方设计方案。 第三节 实验数据的统计分析","categories":[{"name":"Psychology","slug":"Psychology","permalink":"http://lyk-love.cn/categories/Psychology/"}],"tags":[{"name":"实验心理学","slug":"实验心理学","permalink":"http://lyk-love.cn/tags/%E5%AE%9E%E9%AA%8C%E5%BF%83%E7%90%86%E5%AD%A6/"}]},{"title":"第二章-实验研究的基本问题","slug":"第二章-实验研究的基本问题","date":"2021-07-04T09:51:01.000Z","updated":"2022-09-26T06:39:34.944Z","comments":true,"path":"2021/07/04/第二章-实验研究的基本问题/","link":"","permalink":"http://lyk-love.cn/2021/07/04/%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E5%AE%9E%E9%AA%8C%E7%A0%94%E7%A9%B6%E7%9A%84%E5%9F%BA%E6%9C%AC%E9%97%AE%E9%A2%98/","excerpt":"Outline 实验研究中的变量及其控制 实验的效度 实验的信度 实验材料的收集与分析和综合","text":"Outline 实验研究中的变量及其控制 实验的效度 实验的信度 实验材料的收集与分析和综合 实验研究中的变量及其控制 实验的灵魂：控制 实验控制的目的：探索人的心理和行为发生、发展变化的原因 实验具有三个优点： 在实验的方法中，实验者可以在他愿意时，使事件产生，可以充分地进行精密的观察。 实验在同样条件下是可以重复的，别人可以验证 它。 系统地变化条件，可以追究与此相随的事件的变化。 什么是变量 变量（variable）是指在数量上或质量上可变的事物的属性 变量包括作业变量（刺激变量）、环境变量、机体变量 • 在一个具体的实验中，变量分为自变量、因变量 和额外变量。 变量的类型 自变量 定义 在实验中实验者所操纵的、对被试者的反应产生影响的变量称为自变量； 自变量的类型 作业变量（刺激变量、任务变量） 环境自变量 被试者变量 因变量 因为自变量的变化而产生的现象变化（行为表现），称为因变量。 反应的速度 反应的准确性 反应的难度 反应的次数或机率 反应的强度 额外变量 凡是对因变量产生影响的实验条件都称为相关变量； 对因变量不产生影响的实验条件称为无关变量 （或无关变项）。 在相关变量中，实验者用以研究的变量称为自变量，实验者不用于研究的那些相关变量称为额外相关变量，或简称为额外变量。 由于在实验中额外变量是必须加以控制的，所以额外变量也被称为控制变量。 评价一项实验设计的好坏的一个重要依据就是 研究者能否成功地控制那些额外变量。 心理学实验中典型的额外变量 实验者效应 (罗森塔尔) 要求特征（霍桑效应、安慰剂效应) 霍桑效应指的是在行为田野实验[Field Experiment]中由于研究对象意识到自己正在被研究而带来的方法上的人为效应。这种意识导致他们对于数据收集过程这一社会条件作出反应,而不是对于研究者试图研究的实验处理作出反应 变量的操纵 自变量的控制 在实验中对自变量的操纵、变化称为自变量的控制。 对自变量的控制，首先要对自变量进行严格的规定，对心理学中一些含混不清的变量必 须使之操作定义化，只有这样才能进行实验。 操作定义：指用可感知、度量的事物、时间、现象和方法对变量或指标作出具体的界定。 例如，把“刚刚感受到”定义为“50%次感受到”，就可 测定感觉阈限了。 又如，疲倦（fatigue）没有一个共同的起点和尺度，怎么测量呢？如果定义为“工作效率的下降”，那么就可以进 行测量和比较了。 其次，对于在刺激维度上连续变化的自变量，要做好三项工作： 要确定好自变量的范围 要选一定数量的检查点 要确定好各检查点之间的间距。 因变量的控制 把实验中的被试者的反应控制在主试者所设想的方向上，这就是反应的控制问题。 以人作被试者，往往用指示语来控制被试者的反应。 指示语乃是心理实验中主试者给被试者交代任务时说的话。 使用指示语时，应注意在允许的范围内做到引起动机，激发兴趣。被试者来到实验室时，不一定对参 加实验感兴趣。因此主试者必须利用言词来引起他 们的兴趣。在可能的范围内，告诉他们实验目的与 应用价值，使他们认识到参与和合作的意义。 反应指标的选择 在心理学实验中，一般常用的指标有： 绝对阈限 差别阈限 反应时 反应持续时间 反应程度 完成量 错误率 完成一定的作业所需要的时间 达到一定基准所需要的次数 口头报告 选择指标的条件主要有： 有效性，即指标充分代表当时的现象或过程的程度，也称为效度。 客观性 数量化 额外变量的控制 排除法 恒定法 匹配法 随机化法 抵消平衡法 统计控制法 实验效度 实验效度是指实验方法能达到实验目的的程度。 实验目的是验证假设，验证自变量和因变量之间的关系，使实验结果的推论可用以解释和预测其他同类现象。 由于不同的实验者在设计上和对额外变量的控制程度极不相同，实验的效度也会有很大的不同。 此外，每种实验都有几个不同的组成部分，其中每一部分也会影响整个实验的效度。 了解影响实验效度的诸因素，将有助于我们评价实验设计的质量，提高实验设计的科学性。 实验效度主要包括内部效度和外部效度。 实验的内部效度和外部效度是相互联系、相互影响的。提高实验内部效度的措施可能会降低 其外部效度，而提高实验外部效度的措施又可能会降低其内部效度。 这两种效度的相对重要性，主要取决于实验的 目的和实验的要求。一般而言，在实验中控制 额外变量的程度越大，则对因果关系的测量就 越有效。因此，可以在保证实验内部效度的前题下，采取适当措施以提高外部效度 影响实验内部效度的因素 实验的内部效度是指实验中的自变量与因变量之间的因果关系的明确程度。 一项实验的内部效度高，就意味着因变量的变化确系由特定的自变量引起的。 由于除了自变量以外，任何额外变量都可能对因变量产生影响，导致实验结果的混淆。这样我们就难以判定实验中自变量与因变量之间的关系的确定性。 因此，要使实验具有较高的内部效度，就必须控制各种额外变量。 在设计实验时，如果能考虑到以下六个方面的因素， 将有助于提高实验的内部效度。 生长和成熟 除了实验中的自变量可能使个体行为发生变化外，个体本身的生长和成熟也是使其行为变化的重要因素。 特别是在以幼小的儿童为被试者而又采用单组前测后 测实验的情况下，生长和成熟因素的影响就更大。 单组前测后测实验通常是实验处理之前先对被试者的某种行为作一次测量，实验处理后再以同样方法测量 一次，两次测量之差即表示实验变量（即自变量）产生的效果。 很明显，这种设计忽略了前后两次测量之间被试者的 生长和成熟因素，其实验效果易受生长和成熟因素的 混淆，从而降低了内部效度。 解决的主要办法是增设同样条件的控制组进行比较。 前测的影响 一般情况下，前后两次测量的结果会有一定的差异，后测的分数将比前测的高。这中间包括练习因素、临场经验、以及对实验目的的敏感程度，从而提 高了后测的成绩。特别是前后两次测量时间较近，这 一因素的影响就更显著。 被试者的选择偏性 在对被试者进行分组时，如果没有用随机取样和随机分配的方法，在实验处理之前，他们在各方面并不相等或有偏性，从而造成实验结果的混淆，降低了内部效度。 被试者的缺失 如果是一项长期的实验，要保持原实验被试者的人数不变是相当困难的。即使开始参加实验的被试者样本 是经过随机取样和随机分配的，但由于被试者的中途缺失，常常使缺失后的被试者样本难以代表原来的样本。这就降低了内部效度。 实验程序的不一致 在实验过程中，实验仪器、控制方式的不一致，测量程度的变化，实验处理的扩散和交流等都可能混淆实验变量（即自变量）的效果。实验者知道实验目的所产生的“实验者效应”以及被试者知道实验目的或其自己正被研究所产生的“霍桑效应”和“安慰剂效应” 等都将混淆实验变量（即自变量）的效果，从而降低 了内部效度。 统计回归 统计回归现象是，第一次测量平均值偏高者，第二次 测量平均值有趋低的倾向（向常态分布的平均数回 归）；第一次测量平均值偏低者，第二次测量平均值 有趋高的倾向（也向常态分布的平均数回归） 因此第二次测验虽在实验处理之后,其升高或降低只是 受统计回归的影响,可能并非是实验变量(即自变量所 产生的效果) 影响实验外部效度的因素 实验的外部效度是指实验结果能够普遍推论到样本的总体和其他同类现象中去的程度，即实验结果的普遍代表性和适用性。 以人的行为为对象所获得的实验结果， 其推论法往往有相当的局限性。 实验的外部效度主要受下列三方面的影响： 实验环境的人为性 实验是在控制条件下进行的，实验环境的人为 性可能使某些实验结果难以用来解释日常生活中的行为现象。 实验室中的仪器设备会影响被试者的典型行为。 被试者参与实验的动机也会影响其行为表现。 而在实验室之外的日常生活中，就不会有这些因素的影响。因此，实验结果还不能完全等同 于实验室之外的日常行为现象 被试者样本缺乏代表性 从理论上讲，从事于实验的被试者必须具有代表性、 必须从将来预期推论、解释同类行为现象的总体中进 行随机取样。但实际上这是很难做到的。 因为，如果总体很大，即使能够随机取样，但心理学 实验的被试者通常是自愿的，所以也很难把被随机选 上的人全都请来做实验。 如果总体是无限的（例如，“七岁儿童”就是一个无 限的总体，其包括过去的、现在的、将来的所有七岁 儿童），随机取样实际上是行不通的。这样的实验结 果自然会降低其外部效度。 测量工具的局限性 实验者对实验变量（即自变量）和反应变量的操作性定义往往是以所使用的测量工具的测量结果来加以考虑的。 例如，把成就动机作为一个因变量，实验者常以某 种成就动机量表所测得的分数来界定并评定其强度。 但成就动机的测量工具有各种不同的形式，所测量 出的分数并不代表同一种成就动机及其强度。如果 在实验时采用的是某一种成就动机的量表，那么所 得出的实验结果便不能推论到采用其他成就动机的 量表的情况中去 实验信度 考察实验信度的方法 推断统计 实验验证 三种实验验证方法 直接 系统验证 概念验证 直接验证 直接验证是指在尽可能保持原实验方法 的情况下在实际中重复它。 系统验证 所谓系统验证，就是系统变化那些被认为与实验结果不相关的因素，考察是否会出现同样的实验结果。 也就是说，如果原始实验中发现的某种现象是真的，那么，尽管重做中一些因素发生了变化， 但它仍然该出现 概念验证 所谓概念验证，就是根据一个较为抽象 的概念或者理论模型设计实验，验证实 验结果。 实验材料的收集与分析和综合 心理学研究的资料类别 计数资料（enumeration data）就是按个体的某一属性或某一反应属性进行分类记数的资料。这种资料只反映个体间有质的不同，而没有量的差别。 2.计量资料（measurment data）就是用测量所得到的数值的大小来表示的资料。 等级资料（ranked data）介于计数资料和计量资 料之间，可称为半计量资料。 描述性资料（descriptive data）即非数量化的资料。在心理学研究中，数量化的资料固然重要，但 描述性资料也同样重要。 研究资料的整理与解释 很多统计方法都有其前提条件 根据实验所得到的资料，在验证实验的假设时，不外有两种结果： 如果实验结果与假设相一致，则进一步演绎而找出更深的假设，推进实验； 如果实验结果与假设不一致，则修正假设， 反复实验；或放弃它；或建立新的假设，再进行 实验。","categories":[{"name":"Psychology","slug":"Psychology","permalink":"http://lyk-love.cn/categories/Psychology/"}],"tags":[{"name":"实验心理学","slug":"实验心理学","permalink":"http://lyk-love.cn/tags/%E5%AE%9E%E9%AA%8C%E5%BF%83%E7%90%86%E5%AD%A6/"}]},{"title":"第一章-绪论","slug":"第一章-实验心理学绪论","date":"2021-07-04T09:18:10.000Z","updated":"2022-09-26T06:39:34.944Z","comments":true,"path":"2021/07/04/第一章-实验心理学绪论/","link":"","permalink":"http://lyk-love.cn/2021/07/04/%E7%AC%AC%E4%B8%80%E7%AB%A0-%E5%AE%9E%E9%AA%8C%E5%BF%83%E7%90%86%E5%AD%A6%E7%BB%AA%E8%AE%BA/","excerpt":"Outline 实验心理学简史 实验心理学的性质 实验心理学程序","text":"Outline 实验心理学简史 实验心理学的性质 实验心理学程序 实验心理学简史 近代哲学与实验生理学的发展 （一）欧洲哲学流派对心理学的诞生 唯理论 （笛卡尔） 经验主义（洛克） 联想主义 (培因) （二）实验生理学对心理学诞生的贡献 维萨刘斯（Andreas Vesalius，1515-1564） 哈维（William Harvey,1578-1657 ) 列文霍克（Leeuwen Hoek,1632-1723) 实验心理学的创立 费希纳（Gustav Theodor Fechner，1801-1887） 开创性地提出了量化研究“心灵”的思想，以及具体可操作 的量化方法，即影响深远的心理物理法，从而为实验心理学指明了方向。 冯特（1832－1920） 1862年， 冯特出版了《对于感知觉的贡献》一书， 并在书中 论述了对感知觉的实验研究， 这也是实验心理学产生的前期著述。该著作在内容上属于实验心理学的内容， 并在此书中 正式提出了“实验心理学” ， 这也是冯特的第一部实验心理 学著作。 艾宾浩斯（Hermann Ebbinghaus，1850-1908） 开辟了用实证方法研究记忆等高级心理过程的先河，从而铸造出了实验心理学的雏形. 现代实验心理学的发展 行为主义与实验心理学的发展 华生 否定意识， 主张心理学应该研究行为 反对内省， 主张应该用实验的方法研究人的心理与行 为； 否定遗传和神经中枢对心理发展的作用 桑代克 斯金纳 上述实验说明了行为主义心理学在研究方法和手段上对实验 法和实验器材的重视， 对行为与反应之间关系的重视。 信号检测论与现代心理物理学 传统心理物理法一般只能测量出被试的辨别力或感觉敏感性， 而对被试的反应倾向和判断标准则无法进行测量和判断。 基于受雷达设备工作原理的启发。信号检测论认为， 如果个体对某一刺激不能作出正确判断， 可能是因为噪声的干扰导 致的。因此（信号检测论）可以对人们在判断客观刺激信息时，对不确定的情况做出科学的决策。 20世纪50年代以来，信号检测理论在心理学研究领域得到了 广泛的应用。如在感知觉研究、个体反应倾向性的评价、工业心理学以及内隐记忆与阈下知觉等领域。 认知心理学与现代实验研究方法 20世纪50 年代，由于计算机科学、信息论、系统论的发展， 对行为主义心理学产生了巨大的冲击。 认知心理学认为，人不是刺激的被动接受者， 而是主动地、 积极地对各种环境刺激信息进行加工。个体的这个加工过程就是认知过程。认知心理学是研究人的认知心理过程， 主要研究内容包括个体的感知觉、注意、记忆、思维、推 理、概念形成、问题解决等大脑内在的心理加工过程，并用计算机模拟人脑，来研究大脑对信息的加工过程和加工 机制。 实验心理学理论、方法与技术的发展 基于计算机科学的信息加工理论、人工智能的理论、 医学研究领域中的各种神经电活动的理论和规律 （包括脑电、皮肤电位、肌电、眼电、心电等） 以 及神经系统活动的脑功能成像的理论、物理光学和 声学的理论以及生物化学的理论等， 上述诸研究领 域的理论从不同的学科为心理学各个领域的实验研 究奠定理论基础。 基于反应时测量技术研究范式 如在感知觉、注意研究领域中采用的启动范式、快速视觉 出现范式（RSVP） 眼动技术 • 各种用于视觉、注意、阅读、运动心理、人机交互界面、 工程心理学研究和医学研究领域的眼动仪. 医学研究与诊断技术在心理学研究中的广泛应用 脑电技术（EEG／ERP） 、医学影像学技术（包括fMRI 、 PET 、rTMS 、CT 等） 听觉与语言分析技术 发展了用于听觉研究的专业的声学设备和听觉设备（如电 子声级计、听觉诊断仪、语言分析仪等） 实验心理学的性质 定义 实验心理学就是在实验控制条件下对心理和行为进行研究工作的心理学 内容 通常包括两个部分： 阐述实验方法和实验设计（一般原理） 阐述实验法在一些专门领域中的应用。 （如何在具体领域中应用实验方法） 目的 说明和解释人 在完成某种活动时的心理活动是如何进行的，即通过刺激和反应之间的关系来推断心理活动的方式 实验法与其他研究方法的区别 实验法不同于自然观察法。 在使用自然观察法时，研究者只能被动地仔细观察和记录研究对象在自然状态下所发生的情况，而不能有任何干预。 实验则是人为地去干预、控制所研究的对象。实验者可以创造条件，引发所需要的事件来观察其变化；为了验证，可 以创造同样的条件进行重复观察。 访谈法 访谈法（Interview） 又叫个案法（Case Study） 或临床法（Clinical Study） ,是 研究者围绕事先设计好的问题， 通过与 研究对象交谈的形式获取资料的一种方 法。谈话法是教育心理学、发展心理学 和社会心理学调查中常用的研究方法 问卷法 问卷法（Questionaires） 是通过严格设 计的调查问卷对人的心理与行为进行调 查研究的一种数据收集方法。通常问卷 法适用于大规模的调查研究 测验法 测验法(Testing) 是通过修订的标准化 的试题、按照一定的测量程序收集数据 的一种方法。是心理与教育研究中最常 用的方法之一 实验法 实验法（Experiment） 包括自然实验法和实验室实验法。 自然实验法是在自然情境下， 对实验情境进行一定的控制来对人 的心理和行为变化进行实验研究的一种研究方法。一般自然实验 的结果与真实生活或工作情境比较接近， 结果具有很好的推论性。 实验室实验法是指在实验室条件下， 通过对实验条件和研究变量 进行严格的控制来对人的心理和行为进行研究的一种实验方法。 实验室实验法得到的研究结果一般比较精确， 但是由于对实验条 件控制比较严格， 与真实的生活或工作情境有较大的差异 心理实验的程序 实验的程序就是实验的进程。它是指实验在各个阶段应做的事，它包括： 课题的确定 被试的选择 实验因素的控制 实验资料的收集与分析和综合 撰写实验报告 课题的确定 课题的来源 实际需要 理论需要 个人经验 前人的研究与文献资料 实验类型的确定 课题虽然有不同的来源，但都是从提出问题开始的。对于“为什么” 科学的探索，大致可分为两阶段或两个类型。第一阶段是探明规定某个行为的条件，第二个阶段是探明哪些条件与行为之间的函数关系。与这两个阶段相对 应，可以把实验分为两种类型 因素型实验 第一种类型是因素型实验，即探求规定行为的条件“是什么”的“什么型实验”，或是探明行为的规定要因的实验。 在因素型实验里，逐个地除去、破坏或变化 被看作是行为规定要因的几个条件，根据有无相应的行为变化，探明它是否是行为的规定要因。 毫无疑义，这时候，对于被操作的条件之外 的条件，都应当进行严密的控制。 函数性实验 第二种类型是函数型实验，即探求各种条件是 “怎样”规定行为的“怎样型实验”，或是探 明条件和行为之间的函数关系的实验。 在函数型实验里，根据因素型实验的结果，系统地、分阶段地变化规定要因的条件，以进行 确定条件和行为之间的函数关系的函数型实验， 以找出行为的法则。 提出假设 在课题确定及其所属的实验类型均明确之后， 若能以假设的形式提出，那就更符合科学原则。 假设是关于条件和行为之间的关系的陈述。 如果把对条件的叙述记为a，把对行为的叙述记为b， 一般取“如果a，那么b”这样的形式。 一切科学定律、法则虽然表面上不一定都符合这个 形式，但实际上却包含先行条件（自变量）和后继 条件（因变量）这样的逻辑关系。 例如，对于缪勒-莱尔错觉来说，“若变化夹角的大小，则视错觉就有变化”这一假设，是确定夹角是否是视错觉的要因这一因素型实验的假设。这是 假设陈述的第一种方式。 假设的另一种陈述方式是用函数关系来 表示。它用方程式b=f（a）来表明自变 量a与因变量b共变的函数关系，这个方程式读作b为a的函数，或b数量地依存于a。应用这个模型，就可将上述关系改为： “视错觉的量与夹角的余弦成正比”。 这就是函数型的假设。 被试的选择 涉及被试选择的问题有： 是使用人类被试还是使用非人类被试？ 被试应具备哪些机体特征？ 用哪一种取样方法才能使被试者样本代表总体？ 这些问题的解答主要取决于二个因素： 即课题的性质及研究结果的概括程度。 依问题的性质选择被试 选用人类被试者还是非人类被试者，依据课题的性质而定。 许多心理学研究选用人类被试者，因为它关心的是人类的心理和行为。有损被试者身心健康的实验就 不能选用人类被试者，而应当考虑选用适当的非人 类被试者。 有的心理学实验使用非人类被试者是因为考虑到要严密地控制无关变量。 依研究结果的概括程度来选择被试 在从事一项研究时必须要依据研究结果的概括程度来选择被试者。 心理学研究的群体可能是一个小群体，或仅具有某种特性的成员。 因此，选定什么样的被试者样本，要依研究的问题 和据此而推论的全体而定。如果被试者的选择出现偏差，就会影响实验效度。 用什么方法能减少这种偏差呢？可用如下二种方法： 随机抽样法（抽签法、随机数字表法） 分层随机取样法 实验因素的控制 在实验进行之前，要通过实验设计对影响实验效果的各种因素进行控制。 自变量的控制（操作定义） 反应的控制（指示语：学习并记住这些字串； 试发现这些字串的内在规则） 反应指标的选择 有效性 客观性(具有可重复性) 数量化(便于记录,统计和比较) 实验资料的收集与分析和综合 心理学研究的资料类别 计数资料(分类资料,如性别,反应有无,对错) 计量资料(测得的数值大小,) 等级资料(心理量表法) 描述性资料 研究资料的整理与解释 选用统计方法都有其前提条件． 如T和F检验都假设资料的数值呈正态分布. 在验证实验的假设时，有两种结果． 撰写实验报告 研究论文具有三个特点： 有比较严格的格式要求 所表达的内容是研究的新成果 具有真实性、客观性 一个完整的实验报告，必须包括以下几项内容：摘要、题目、引言、方法、结果、讨论、结论、参考文献及附录 摘要 正式发表的科研报告，一般应写出论文摘要（abstract），把它放在正文的前面。 论文提要应当以最概括、最简洁的语言 写出，内容包括本课题所要解决的问题、 方法、以及获得的结果和结论。 题目 题目（title）是说明要做的实验研究是属于哪方面的的问题。 一般要求在题目中既要指出自变量，也要指出因变量 引言 在引言（foreword）中一般要求说明此实验的意义以及题目产生的过程，提出问题的背景材料或提出问题的假设，最好能引经据典，把这类实验的来龙去脉指出来。当然，语句要简练，一般不超过一千字。 简单介绍实验中用到的方法、要测的心理特点或要演示的心理现象 说明关于这方面的经典结论 自己对实验的分析 和假设 本实验的理论意义和实践意义。 方法 这一部分的总的要求是尽可能详尽而准确地描述各个成分，目的是能够使别人重复你的研究。 被试 材料和仪器 实验程序 实验的原则、方法、步骤、指示语是什么、要控制什么条件等。 结果 要如实地以描述和数量的形式把主要结果呈现出来。 结果只要陈述事实，不要解释研究结果，更不要夹叙夹议。 实验的原始记录不要放在结果中，如有必要可放在文 章的附录中。 实验结果必须将原始材料整理后用图或表呈现出来。 表格要分类列出，切忌把所有的结果都填在一张大表 中，各处理的平均数、标准差和统计检验水平都要列出来。 仅有图表是不够的，你要告诉读者根据图表发现了什么。 要交代清楚所使用的统计检验方法及结果，是否表现出显著性差异，接受或拒绝虚无假设。 讨论 是整个报告最关键的部分之一 讨论应与前言相呼应。 分析和讨论部分包括： Summarizing the findings Evaluating the method Suggest modifications and extensions 从实验的实施过程方面分析，如” 被试疲劳了”、”主试有点急躁”、” 仪器不灵敏”等； 从研究方法方面分析，如”平均差 误法得出的只是近似值”、”信号检测 论的两个指标比较灵敏”等； 从理论的角度分析，这方面的论述 一般比较少。 结论 说明本实验证实了或否定了什么假设。 结论一般以条文形式、用简短的语句表达出来。 结论应具有客观性和恰当的概括性。 结论一定要具体，不可夸大本研究的结果和适用范围。 参考文献 要把参考文献（references）的题目、出处、作者、出版日期都写明，以便查找。 文献的顺序一般以在文章中出现的先后 为序，也可按出版社的规定。 附录 学生型的实验报告一般要把全部的原始资料都列入附录（appendix）。因为同 一结果，不同的人，在不同的时候都可 以进行不同的分析和处理。另外，重要 的实验材料、指示语等也应列入附录。","categories":[{"name":"Psychology","slug":"Psychology","permalink":"http://lyk-love.cn/categories/Psychology/"}],"tags":[{"name":"实验心理学","slug":"实验心理学","permalink":"http://lyk-love.cn/tags/%E5%AE%9E%E9%AA%8C%E5%BF%83%E7%90%86%E5%AD%A6/"}]},{"title":"Data Link Layer","slug":"Computer Networking-Data Link Layer","date":"2021-06-29T11:25:11.000Z","updated":"2022-09-26T06:39:34.927Z","comments":true,"path":"2021/06/29/Computer Networking-Data Link Layer/","link":"","permalink":"http://lyk-love.cn/2021/06/29/Computer%20Networking-Data%20Link%20Layer/","excerpt":"Outline: 使用点对点信道的数据链路层 点对点协议PPP 使用广播信道的数据链路层 扩展的以太网 高速以太网","text":"Outline: 使用点对点信道的数据链路层 点对点协议PPP 使用广播信道的数据链路层 扩展的以太网 高速以太网 数据链路层使用的信道主要有两种类型： 点对点信道 广播信道： 一对多， 需要共享信道协议来协调数据发送 本章我们研究的是在同一个局域网中， 分组怎样从一台主机传送到另一台主机， 但并不经过路由器转发。 从整个互联网角度看， 局域网仍属于数据链路层的范围。 使用点对点信道的数据链路层 数据链路和帧 链路: 从一个结点到相邻结点的物理线路(有线或无线), 而中间没有其他的交换结点. 链路只是一条路径的组成部分 数据链路: 链路 + 必要的通信协议, 现在常用方法是用网络适配器(既有硬件也有软件)来实现这些协议 帧: 网络层协议的数据单元是IP数据报(或简称数据报, 分组 或 包). 点对点信道的数据链路层在进行通信时的主要步骤如下: 结点A数据链路层把网络层交下来的packet添加首部和尾部封装成帧 结点A把封装好的帧发送给结点B的数据链路层 若结点B的数据链路层收到的帧无差错, 则从收到的帧中提取出packet交给上层的网络层; 否则丢弃这个帧 三个基本问题 封装成帧 分组交换: 所有在互联网上传送的数据都以IP数据报(packet) 为传送单位. packet到数据链路层就成为帧的数据部分, 加上首部和尾部就成为完整的帧 首部和尾部的一个重要作用是帧定界, 此外, 首部和尾部还包含许多必要的控制信息 每一种链路层协议都规定了所能传输的帧的数据部分长度上限 --- 最大传送单元MTU( Maximum Transfer Unit ) 透明传输 字节填充 差错检测 CRC 点对点协议PPP Point-to-Point Protocol， 是用户计算机和ISP通信时所使用的数据链路层协议 PPP协议应满足的需求 简单 封装成帧. PPP协议必须规定特殊的字符作为帧定界符 透明性 多种网络协议 PPP必须能够在同一条物理链路上同时支持多种网络层协议的运行 多种类型链路 PPP必须能在多种类型的链路上运行. 例如串行和并行, 同步和异步, 高速和低速, 电和光 差错检测( error detection ) 立即丢弃有差错的帧 检测连接状态 必须具有一种机制能够及时自动检测链路是否处于正常工作状态 最大传送单元 必须对每一种类型的点对点链路设置最大传送单元MTU的标准默认值. 这是为了促进各种书籍线之间的互操作性. 如果高层协议发送的分组过长并超过MTU的数值, PPP就要丢弃这样的帧. MTU是数据链路层的帧可以载荷的数据部分的最大长度, 而不是帧的总长度 网络层地址协商 数据压缩协商 使用广播信道的数据链路层 局域网的数据链路层 局域网最主要的特点: 网络为一个单位所拥有, 且地理范围和站点数目均有限 局域网的优点: 具有广播功能. 从一个站点可很方便地访问全网. 局域网上的主机可共享连接在局域网上的各种硬件和软件资源 便于习用的扩展和逐渐演变, 各设备的位置可灵活调整和改变 提高了系统的可靠性( reliability ) , 可用性( availability ) 和生存性( survivability ) 局域网的分类( 按拓扑 ): 星型网 总线网 环形网 共享信道要考虑的一个问题是如何使众多用户能合理且方便地共享媒体资源, 这在技术上有两种方法: 静态划分信道 . 如第二张的频分复用, 时分复用, 波分服用等. 用户只要分配到了信道就不会与其它用户发生冲突. 但这种方式代价较高, 不适合局域网 动态媒体接入控制, 又称为多点接入( multiple access ) , 分两类 随机接入: 用户可以随机地发送信息. 但如果两个或更多的用户在同一时刻发送信息, 那么在共享媒体上就要产生碰撞(即发生了冲突), 因此必须有解决碰撞的协议. 以太网属于随机接入 受控接入 用户不能随机地发送消息而必须接受一定的控制 适配器的作用 适配器和局域网之间的通信通过电缆或者双绞线以串行传输方式进行 适配器和计算机之间的通信通过计算机主板上的IO总线以并行传输方式进行 适配器的一个重要功能是进行数据串行传输和并行传输的转换 计算机的硬件地址就在适配器的ROM中, 而计算机的软件地址---IP地址, 则在计算机的存储器中 适配器在接受和发送各种帧时, 不使用计算机的CPU. 当适配器收到有差错的帧时, 就把这个帧直接丢弃而不必通知计算机. 当收到正确的帧时, 它就使用中断来通知计算机, 并交付协议栈中的网络层. 当计算机要发送IP数据报时, 就由协议栈把IP数据报向下交给适配器, 组装成帧后发送到局域网 CSMA/CD协议 Carrier Sense Multiple Access with Collision Detection，载波侦听多路访问/冲突检测协议) 早期以太网是总线型的. 属于广播通信. 为了在总线上实现一对一通信, 可以使每台计算机的适配器拥有一个和其他适配器不同的地址, 当发送数据帧时, 在帧的首部写明接收站的地址. 仅当数据帧中的目的地址与适配器ROM中存放的硬件地址一致时, 该适配器才能接收这个数据帧, 否则就丢弃. 这样, 就在总线上实现了一对一通信 以太网采取以下两种措施 无连接 不必建立链接就可以直接发送数据 适配器对发送的数据帧不进行编号, 也不要求对方发回确认 尽最大努力的交付, 即不可靠的交付. 总线特点: 在同一时间只能允许一台计算机发送数据, 解决方法: CSMA/CD协议 曼彻斯特编码 CSMA/CD的特点 多点接入 载波监听 检测信道( 发送前和发送中都要不停检测信道 ) 碰撞检测 以太网的MAC层 局域网中, 硬件地址又称为MAC地址 48位的全球地址, 就是固化在适配器的ROM中的地址 发往本站的帧有三种: 单播( unicast )帧 广播( broadcast )帧( 一对全体 ), 即发送给本局域网上所有站点的帧( 全1地址 ) 多播(multicast )帧( 一对多 ): 即发送给本局域网上部分站点的帧 所有的适配器都能识别前两种帧,有的能通过编程方式识别多播地址 扩展的以太网 在物理层扩展以太网 用集线器 缺点： 多个系的以太网通过集线器互连起来后， 多个碰撞域会合并成一个 碰撞域： 在任意时刻， 每个碰撞域中只能有一个站在发送数据 如果不同的系使用不同的以太网技术（如数据率不同）， 那么就不可能用集线器把它们互连起来。 集线器基本是个多借口的转发器， 它不能把帧进行缓存 在数据链路层扩展以太网 扩展以太网一般在数据链路层进行， 用交换机 以太网交换机的特点 交换机就是多接口的网桥 每个接口都直接与单台主机或另一个以太网交换机相连, 且工作在全双工方式 具有并行性, 能同时连通多对接口, 且多对主机能同时通信, 相互通信的主机都是独占传输媒体, 无碰撞地传输数据 是一种即插即用设备, 其内部的帧交换表(地址表)是通过自学习算法建立的 交换机能隔离冲突域,但不能隔离广播域 交换机使用硬件转发, 比使用软件转发的网桥快很多 交换机一般具有多种速率的接口, 因此不同数据率的两给局域网可以互连 部分交换机实现直通( cut through )的交换方式. 而网桥只能存储转发 以太网交换机的自学习功能 A向B发送一帧, 从接口1进入交换机. 交换机收到帧后, 先查找交换表, 没有找到应从哪个接口转发这个帧( 在MAC地址这一列中, 没有找到目的地址为B的项目 ). 接着, 交换机吧这个帧的源地址和接口1写入交换表中, 并向除接口1外的所有接口广播这个帧 C和D将丢弃这个帧, 因为目的地址不对. B会收下这个帧. 这称为过滤 假定接下来B通过接口3向A发送一帧. 交换机查找交换表, 发现交换表中的MAC地址有A, 就应当把收到的帧从接口1转发出去. 并向表中写入源地址B和接口3 交换表中每个项目都设有一定的有效时间 如果存在冗余链路, 则会导致帧在某个环路中无限兜圈子, 因此IEEE制定了生成树协议STP( Spanning Tree Protocol ), 其要点是不改变网络的实际拓扑, 而在逻辑上切断某些链路, 似的从一台主机到所有其他主机的路径是五环路的树状结构, 从而消除了兜圈子现象. 从总线到星型以太网 目前, 采用以太网交换机的星型拓扑成为首选拓扑 总线以太网使用CSMA/DA协议, 以半双工方式工作. 但以太网交换机不使用共享总线, 没有碰撞问题, 因此不使用CSMA/DA协议, 而是以全双工方式工作 因为帧结构没有改变, 所以现在的局域网还叫以太网 虚拟局域网 是由一些局域网网段构成的与物理位置无关的逻辑组, 而这些网段有某些共同的需求. ,每一个VLAN帧都有一个明确的标识符( VLAN tag, 4字节 ), 指明发送方属于哪一个VLAN 高速以太网","categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"}],"tags":[{"name":"Computer Networking","slug":"Computer-Networking","permalink":"http://lyk-love.cn/tags/Computer-Networking/"}]},{"title":"Software Development Procedure Model","slug":"Software Development Procedure Model","date":"2021-06-28T15:45:40.000Z","updated":"2022-09-26T06:39:34.939Z","comments":true,"path":"2021/06/28/Software Development Procedure Model/","link":"","permalink":"http://lyk-love.cn/2021/06/28/Software%20Development%20Procedure%20Model/","excerpt":"Outline: 软件开发的典型阶段 软件生命周期模型 软件过程模型 构建--修复模型 瀑布模型 增量迭代模型 演化模型 原型模型 螺旋模型","text":"Outline: 软件开发的典型阶段 软件生命周期模型 软件过程模型 构建--修复模型 瀑布模型 增量迭代模型 演化模型 原型模型 螺旋模型 软件开发的典型阶段 需求工程 软件设计 软件构造 软件测试 软件交互 软件维护 软件生命周期模型 为了从宏观上描述软件开发活动, 人们将软件从生产到报废的生命周期分割为不同阶段, 每个阶段有明确的典型输入/ 输出, 主要活动和执行人, 各个阶段形成明确, 连续的顺序过程, 这些阶段划分就被叫做软件生命周期模型 需求工程 软件设计 软件构造 软件测试 软件交互 软件维护 软件过程模型 构建--修复模型 瀑布模型 增量迭代模型 演化模型 将软件开发活动组织为多个迭代, 并行的瀑布式开发活动 优点: 采用迭代式开发, 具有更好的适用性 并行开发可以帮助缩短开发时间 渐进交付可以加强用户反馈, 降低开发风险 缺点: 无法在项目早期阶段确定项目范围 后续迭代的开发活动是在前导迭代基础上进行修改和扩展的, 这容易让后续迭代忽略分析与设计工作, 蜕变为构建--修复方式 原型模型 螺旋模型","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://lyk-love.cn/categories/Software-Engineering/"}],"tags":[{"name":"Software Engineering Methodology","slug":"Software-Engineering-Methodology","permalink":"http://lyk-love.cn/tags/Software-Engineering-Methodology/"}]},{"title":"Human Computer Interaction","slug":"Human Computer Interaction","date":"2021-06-28T15:25:38.000Z","updated":"2022-09-26T06:39:34.931Z","comments":true,"path":"2021/06/28/Human Computer Interaction/","link":"","permalink":"http://lyk-love.cn/2021/06/28/Human%20Computer%20Interaction/","excerpt":"Outline What is HCI Design? Objectives of HCI - Usability 目标 Three factors in HCI Human Computer Interaction HCI Design Process GUI Design","text":"Outline What is HCI Design? Objectives of HCI - Usability 目标 Three factors in HCI Human Computer Interaction HCI Design Process GUI Design What is HCI Design? Objectives of HCI - Usability 目标 易用性, 维度为: 易学性 效率 易记性 出错率 主观满意度 Three factors in HCI Human 精神模型 精神模型是用户进行人机交互时头脑中的任务模型 Try to discover your users’ mental model of the task your program helps them perform How? Be aware of the model’s inherent metaphors, which represent conceptual components of the task 差异性 按用户群体的特点: 新⼿⽤户 是对业务不熟悉的⼈，例如新员⼯或者新接触系统的⼈。为新⼿⽤户设计系统时要关注易学性，进⾏业 务导航，尽量避免出错。如果⼀个系统的⼤多数⽤户都是新⼿⽤户，整个系统的⼈机交互设计都要侧重 易学性。 专家⽤户 是能够熟练操作计算机完成业务任务的⼈，⼀般都是⻓时间使⽤软件系统并且计算机操作技能熟练的 ⼈。为专家⽤户设计系统时，要关注效率。如果⼀个系统的⼤多数⽤户都是专家⽤户，整个系统的⼈机 交互设计都要侧重效率。 熟练⽤户 是介于新⼿⽤户和专家⽤户之间的⼈。为熟练⽤户设计⼈机交互系统要在易学性和效率之间进⾏折中。 Computer 可视化设计 常见界面类型 从可视化设计语⾔Visual Basic开始，对可视化构件的布局就成为可视化设 计的主要⼯作。 常⻅的可视化构件包括：窗⼝、菜单、标签（Tab）、表单、按钮、列表、 树形控件、组合框、输⼊框等等，[Cooper2007]对此有详细的描述。 可视化设计要点 按照任务模型设计界⾯隐喻，同时不要把软件系统的内部构造机制暴露给⽤户 可视化设计还应该基于界⾯隐喻，尽可能地把功能和任务细节表现出来 Interaction Navigation 好的⼈机交互设计就像⼀个服务周到的推销员，能够主动将⾃⼰的产品和服 务简明扼要地告诉⽤户，这个就是导航。 好的导航就像⼀个好的餐厅菜单，餐厅菜单能够帮助顾客快速地找到喜欢的 ⻝物，软件系统导航也要能帮助⽤户找到任务的⼊⼝。 导航的⽬的就是为⽤户提供⼀个很好的完成任务的⼊⼝，好的导航会让这个 ⼊⼝⾮常符合⼈的精神模型。 全局结构按照任务模型将软件产品的功能组织起来，并区分不同的重要性和主题提供给不同的⽤户。 全局结构常⽤的导航控件包括窗⼝、菜单、列表、快捷⽅式、热键等等。 • 全局结构的设计主要以功能分层和任务交互过程为主要依据 局部结构通过安排界⾯布局细节，制造视觉上的线索来给⽤户提供导航。 局部结构常⽤的导航控件包括可视化控件布局与组合、按钮设置、⽂本颜⾊或字体⼤⼩等等。 局部结构的设计主要以⽤户关注的任务细节为主要依据。 Feedback [Shneiderman2003]总结的部分经验: ⽤户喜欢较短的响应时间； 较⻓的响应时间（&gt;15秒）具有破坏性； ⽤户会根据响应时间的变化调整⾃⼰的⼯作⽅式； 较短的响应时间导致了较短的⽤户思考时间； 较快的节奏可能会提⾼效率，但也会增加出错率； 根据任务选择适当的响应时间： 打字、光标移动、⿏标定位：50～150毫秒 简单频繁的任务：1秒 普通的任务：2～4秒 复杂的任务：8～12秒 响应时间适度的变化是可接受的； 意外延迟可能具有破坏性； 经验测试有助于设置适当的响应时间。 协作式设计 ⼈和计算机是⼈机交互的两⽅，其中⼈的因素是⽐较固定的，⼀定时期内不 会发⽣⼤的变化，所以要让⼆者交互顺畅，就需要让计算机更多地适应⼈的 因素，这也是⼈机交互设计以⽤户为中⼼的根本原因。 这种调整计算机因素以更好地适应并帮助⽤户的设计⽅式被称为协作式设计 设计原则 简洁设计 低出错率设计 ⼈机交互设计⾸先要帮助⼈们避免犯错，尽可能设计不让⽤户犯严重错误的系统 • 具体措施包括将不适当的菜单选项功能以灰⾊显示屏蔽 • 禁⽌在数值输⼊域中出现字⺟字符 当错误出现时，系统还要在⼈机交互中提供简洁、有建设性、具体的指导来帮助⽤户消除错误。 填写表单时如果⽤户输⼊了⽆效的编码，那么系统应该引导他们对此进⾏修改，⽽不是要求⽤户重新填写整个表单。 出错信息应当遵循以下四个简单原则[Shneiderman1982]： 应当使⽤清晰的语⾔来表达，⽽不要使⽤难懂的代 使⽤的语⾔应当精炼准确，⽽不是空泛、模糊的 应当对⽤户解决问题提供建设性的帮助 出错信息应当友好，不要威胁或责备⽤户 系统还应该提供错误恢复和故障解决帮助⼿册。 易记性设计 减少短期记忆负担。 使⽤逐层递进的⽅式展示 信息。在展现复杂信息时， 可以将其分为不同层次，递 进展现越来越丰富的信息 使⽤直观的快捷⽅式。⼈ 的记忆具有短时记忆的特 点，时间越⻓越容易忘记。 易记性设计 设置有意义的缺省值。有 意义的缺省值（例如⼤多数 ⽤户会选择的输⼊、特定的 场景条件等）可以帮助⽤户 减少输⼊负担，也可以帮助 ⽤户减少记忆负担 HCI Design Process GUI Design","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://lyk-love.cn/categories/Software-Engineering/"}],"tags":[{"name":"Software Engineering Methodology","slug":"Software-Engineering-Methodology","permalink":"http://lyk-love.cn/tags/Software-Engineering-Methodology/"}]},{"title":"Software Requirement Analysis Method","slug":"Software Reuqirement Analysis Method","date":"2021-06-28T00:09:55.000Z","updated":"2022-09-26T06:39:34.940Z","comments":true,"path":"2021/06/28/Software Reuqirement Analysis Method/","link":"","permalink":"http://lyk-love.cn/2021/06/28/Software%20Reuqirement%20Analysis%20Method/","excerpt":"Outline: 需求分析基础 ⾯向对象分析 结构化分析 使⽤需求分析⽅法细化和明确需求","text":"Outline: 需求分析基础 ⾯向对象分析 结构化分析 使⽤需求分析⽅法细化和明确需求 需求分析基础 为什么要需求分析 需求分析模型 ⾯向对象分析 • ⽤例最初由[Jacobson1992] 在 Objectory ⽅法中提出的,它将⽤例定义为“在 系统(或者⼦系统或者类)和外部对象的交互当中所执⾏的⾏为序列的描述,包 括各种不同的序列和错误的序列,它们能够联合提供⼀种有价值的服 务”[Rumbaugh2004]。 结构化分析 使⽤需求分析⽅法细化和明确需求","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://lyk-love.cn/categories/Software-Engineering/"}],"tags":[{"name":"Software Engineering Methodology","slug":"Software-Engineering-Methodology","permalink":"http://lyk-love.cn/tags/Software-Engineering-Methodology/"}]},{"title":"Software Requirement Basic","slug":"Software Requirement Basic","date":"2021-06-28T00:05:48.000Z","updated":"2022-09-26T06:39:34.940Z","comments":true,"path":"2021/06/28/Software Requirement Basic/","link":"","permalink":"http://lyk-love.cn/2021/06/28/Software%20Requirement%20Basic/","excerpt":"Outline: 需求⼯程 需求基础 需求分类","text":"Outline: 需求⼯程 需求基础 需求分类 需求⼯程 单纯的软件系统是不能解决问题的，它只有 和现实世界之间形成有效互动才能实现问题 的解决 需求工程的概念： 所有需求处理活动的总和。它收集信息、分析问题、整合观点、记录需求并验证其正确性，最终描述出软件被应⽤后与其环境互动形成的期望效应。 三个主要任务： 需求⼯程必须说明软件系统将被应⽤的应⽤环境及其⽬标，说明⽤来达成这些⽬标的软件功能，也即要同时说明软件需要“做什么”和“为什么”需要做。 需求⼯程必须将⽬标和功能反映到软件系统当中，映射为可⾏的软件⾏为，并对软件⾏为进⾏准确的规格说明。 • 现实世界是不断变化的世界，因此需求⼯程还需要妥善处理⽬标和功能随着时间演化的变动情况。 需求开发 需求获取 从⼈、⽂档或者环境当中获取需求的过程 要利⽤各种⽅法和技术来“发现”需求 ⽬标分析 根据问题确定⽬标 通过分析利害关系⼈确定⽬标 需求获取的常⻅困难 ⽤户和开发⼈员的背景不同，⽴场不同 “床边B超,肝胆胰脾” 普通⽤户缺乏概括性、综合性的表述能⼒ 不聪明的记者 ⽤户存在认知困境 平板电脑 ⽤户越俎代庖 双机热备 我们就是要求系统能够。。。，⾄于怎么实现是你开发者的事 缺乏⽤户参与 不愿参与的医⽣ ⽤户需求获取的⽅法 ⾯谈 问卷 ⽂档分析 头脑⻛暴 专题讨论 原型 需求分析 通过建模来整合各种信息，以使得⼈们更好的理解问题。 为问题定义出⼀个需求集合，这个集合能够为问题界定⼀个有效的解决⽅案。 检查需求当中存在的错误、遗漏、不⼀致等各种缺陷，并加以修正。 ⼀、边界分析 定义项⽬的范围 系统边界的定义要保证系统能够和周围环境形成有效的互动 系统⽤例图和上下文图通常被⽤来定义系统的边界 ⼆、需求建模 建模是为展现和解释信息⽽进⾏的抽象描述活动 常⽤的技术包括类图、顺序图、状态图等建模技术 需求规格说明 在系统⽤户之间交流需求信息 要简洁、精确、⼀致和易于理解 需求⼯程师在这个阶段的重要⼯作包括: ⼀、定制⽂档模版 ⼆、编写⽂档 需求验证 需求规格说明⽂档⾄少要满⾜下面⼏个标准： ⽂档内每条需求都正确、准确的反映了⽤户的意图； ⽂档记录的需求集在整体上具有完整性和⼀致性； ⽂档的组织⽅式和需求的书写⽅式具有可读性和可修改性 验证的⽅法 同级评审 原型 模拟 需求管理 保证需求作⽤的持续、稳定和有效发挥 在需求开发活动之后，设计、测试、实现等后续的软件系统开发活动都需要围绕需求开展⼯作 进⾏变更控制 纳⼊和实现合理的变更请求，拒绝不合理的变更请求，控制变更的成本和 影响范围 需求基础 需求 IEEE对需求的定义为[IEEE610.12-1990]： ⽤户为了解决问题或达到某些⽬标所需要的条件或能⼒； 系统或系统部件为了满⾜合同、标准、规范或其它正式⽂档所规定的要求⽽需要具备的条件或能⼒； 对⑴或⑵中的⼀个条件或⼀种能⼒的⼀种⽂档化表述。 需求的表述 作为⼀种期望，需求通常被表述为“系统应该…”、“在…时，系统应该…”、 “⽤户可以通过系统…”等，例如R1。 R1：系统应该允许顾客退回已经购买的产品。 需求的层次性 业务需求 系统建⽴的战略出发点，表现为⾼层次的⽬标（Objective），它描述了组织 为什么要开发系统 为了满⾜⽤户的业务需求，需求⼯程师需要描述系统⾼层次的解决⽅案，定义系统应该具备的特性（Feature） 参与各⽅必须要对⾼层次的解决⽅案达成⼀致，以建⽴⼀个共同的前景 （Vision） 特性说明了系统为⽤户提供的各项功能，它限定了系统的范围（Scope） 案例 R2：在系统使⽤3个⽉后，销售额度应该提⾼20% 可以建⽴⾼层次的解决⽅案，其系统特性如SF1～SF4所示。 SF1：管理VIP顾客信息。 SF2：提供VIP顾客服务，增加回头率。 SF3：使⽤多样化的特价⽅案，吸引顾客购买，增加销售额。 SF4：使⽤多样化的赠送⽅案，吸引顾客购买，增加销售额 ⽤户需求 执⾏实际⼯作的⽤户对系统所能完成的具体任务的期望，描述了系统能够帮助⽤户做些什么 直接⽤户 间接⽤户（通⽤软件的销售⼈员和售后⽀持⼈员） 对所有的⽤户需求，都应该有充分的问题域知识作为背景⽀持 特性 模糊、不清晰（允许适度的⽤形容词和副词） 多特性混杂 （功能和⾮功能的混杂） • 多逻辑混杂 （⼀个任务需要多次系统交互才能完成） 多逻辑混杂 （⼀个任务需要多次系统交互才能完成） 案例 SF1：管理VIP顾客信息 针对每⼀个系统特性，都可以建⽴⼀组⽤户需求。例如对SF1，可以建⽴⽤户需求组如UR1.1～UR1.7，它们中每⼀条都是⽤户完成具体任务所需要的功能： UR1.1：系统应该允许客户经理添加、修改或者删除会员个⼈信息 UR1.2：系统应该记录会员的购买信息。 UR1.3：系统应该允许客户经理查看会员的个⼈信息和购买信息。 UR1.4：系统应该允许客户经理查看所有会员的统计信息 补充问题域知识 ⽤户需求表达了⽤户对系统的期望，但是要透彻和全⾯的了解⽤户的真正意 途，仅仅拥有期望是不够的，还需要知道期望所来源的背景知识。 因此，对所有的⽤户需求，都应该有充分的问题域知识作为背景⽀持 UR1.1：系统应该允许客户经理添加、修改或者删除会员个⼈信息。 例如对UR1.1，需要补充问题域知识如下： 会员的个⼈信息有：客户编号、姓名、联系⽅式、积分 系统需求 ⽤户对系统⾏为的期望，每个系统级需求反映了⼀次外界与系统的交互⾏为，或者系统的⼀个实现细节 描述了开发⼈员需要实现什么 将⽤户需求转化为系统需求的过程是⼀个复杂的过程 ⾸先需要分析问题领域及其特性，从中发现问题域和计算机系统的共享知识，建⽴系统的知 识模型； 然后将⽤户需求部署到系统模型当中，即定义系列的系统⾏为，让它们联合起来实现⽤户需求，每⼀个系统⾏为即为⼀个系统需求。 该过程就是需求⼯程当中最为重要的需求分析活动，⼜称建模与分析活动。 案例 UR1.3：系统应该允许客户经理查看会员的个⼈信息和购买信息。 对⽤户需求UR1.3，可以依据任务中的交互细节将之转化为系统级需求SR1.3.1～ SR1.3.4。 SR1.3.1在接到客户经理的请求后，系统应该为客户经理提供所有会员的个⼈信息。 SR1.3.2在客户经理输⼊会员的客户编号时，系统要提供该会员的个⼈信息。 SR1.3.3在客户经理选定⼀个会员并申请查看购买信息时，系统要提供该会员的历史购 买记录。 SR1.3.4经理可以通过键盘输⼊客户编号，也可以通过读卡器输⼊客户编号。 需求分类 需求谱系 需求 项目需求 过程需求 系统需求 软件需求 硬件需求 其他需求 不切实际的期望 软件需求的分类 功能需求（Functional Requirement）：( 除此之外都是非功能需求 ) 和系统主要⼯作相关的需求，即在不考虑物理约束的情况下，⽤户希望系统所能够执⾏的活动，这些活动可以帮助⽤户完成任务。功能需求主要表现为系统和环境之间的⾏为交互。 性能需求（Performance Requirement）： 系统整体或系统组成部分应该拥有的性能特征，例如CPU使⽤率、内存使⽤率等。 质量属性（Quality Attribute）： 系统完成⼯作的质量，即系统需要在⼀个“好的程度”上实现功能需求，例如可靠性程度、可维护性程度等。 对外接⼝（External Interface）： 系统和环境中其他系统之间需要建⽴的接⼝，包括硬件接⼝、软件接⼝、数据库接⼝等等。 约束 • 进⾏系统构造时需要遵守的约束，例如编程语⾔、硬件设施等 功能需求 最常⻅、最主要和最重要的需求 能够为⽤户带来业务价值的系统⾏为 最需要按照三个抽象层次进⾏展开 软件产品产⽣价值的基础 需求的灵活性 PR6：98％的查询不能超过10秒。 PR7： （最低标准）在200个⽤户并发时，系统不能崩溃； （⼀般标准）在200个⽤户并发时，系统应该在80％的时间内能正常⼯作； （理想标准）在200个⽤户并发时，系统应该能保持正常的⼯作状态 非功能需求 性能需求 需要进⾏专⻔模拟和测试 速度（Speed），系统完成任务的时间，例如PR1。 PR1：所有的⽤户查询都必须在10秒内完成。 容量（Capacity），系统所能存储的数据量，例如PR2。 PR2：系统应该能够存储⾄少100万个销售信息。 吞吐量（Throughput），系统在连续的时间内完成的事务数量，例如PR3。 PR3：解释器每分钟应该⾄少解析5000条没有错误的语句。 负载（Load），系统可以承载的并发⼯作量，例如PR4。 PR4：系统应该允许50个营业服务器同时从集中服务器上进⾏数据的上传或下载。 实时性（Time-Critical），严格的实时要求，例如PR5。 PR5：监测到病⼈异常后，监控器必须在0.5秒内发出警报。 质量属性 系统为了满⾜规定的及隐含的所有要求⽽需要具备的要素称为质量 质量属性是为了度量质量要素⽽选⽤的特征 • 质量模型就是能够为质量需求的描述和评价提供⼯作基础的特征集及特征之间的联系 • 质量属性的重要性 • 对设计的影响很⼤ 对越复杂的系统越为重要 [Robert19901] ：真实的现实系统中，在决定系统的成功或失败的因素中，满⾜⾮功能属性往往比满⾜功能性需求更为重要。 常⻅质量属性 可靠性（Reliability）：在规格时间间隔内和规定条件下，系统或部件执⾏所要求能⼒的能⼒。 QA1：在进⾏数据的下载和上传中，如果⽹络故障，系统不能出现故障。 可⽤性（Availability）：软件系统在投⼊使⽤时可操作和可访问的程度或能实现其指定系统功能的概率。 QA2：系统的可⽤性要达到98%。 安全性（Security）：软件阻⽌对其程序和数据进⾏未授权访问的能⼒，未授权的访问可能是有意，也可能是⽆意的。 QA3：VIP顾客只能查看⾃⼰的个⼈信息和购买记录； 收银员只能查看，不能修改、删除VIP顾客的信息。 可维护性（Maintainability）：软件系统或部件能修改以排除故障、改进性能或其他属性或适应变更了的环 境的容易程度，包括可修改性（Modifiability）和可扩展性（Extensibility）。 QA4：如果系统要增加新的特价类型，要能够在2个⼈⽉内完成。 可移植性（Portability）：系统或部件能从⼀种硬件或软件环境转换⾄另外⼀种环境的特性。 QA5：集中服务器要能够在1⼈⽉内从Window 7操作系统更换到Solaris 10操作系统。 易⽤性（Usability）：与⽤户使⽤软件所花费的努⼒及其对使⽤的评价相关的特性。 QA6：使⽤系统1个⽉的收银员进⾏销售处理的效率要达到10件商品/分钟。 质量属性的开发 ⽤户并不能明确地提出他们对产品质量的期望 并不了解软件系统的开发过程，也就⽆从判断哪些质量属性会在怎样的程度上给设计带来多⼤ 的影响，也⽆法将他们对软件系统的质量要求细化成⼀组组的可量化的质量属性 需求⼯程师 质量属性⼤都是和功能需求联系在⼀起的，因此需要对照软件的质量属性检查每⼀项功能需 求，尽⼒去判断质量属性存在的可能性 • 形容词和副词通常意味着质量属性的存在 对于⼀些不和任何功能需求相联系的全局性质量属性，需求⼯程师要在碰到特定的实例时意识到它们的存在 对外接口 系统和环境中其他系统之间的软硬件接⼝, 包括用户界面, 硬件接口, 软件接口, 网络通信接口等. 对系统之间的软硬件接口和通信接口需要说明以下内容: 接⼝的⽤途 接⼝的输⼊输出 数据格式 命令格式 异常处理要求 ⽤户界⾯ 约束 总体上限制了开发⼈员设计和构建系统时的选择范围 系统开发及运⾏的环境 • 包括⽬标机器、操作系统、⽹络环境、编程语⾔、数据库管理系统等 C1：系统要使⽤Java语⾔进⾏开发。 问题域内的相关标准 包括法律法规、⾏业协定、企业规章等。 商业规则 ⽤户在任务执⾏中的⼀些潜在规则也会限制开发⼈员设计和构建系统的选择范围 数据需求 功能需求的补充 如果在功能需求部分明确定义了相关的数据结构，那么就不需要再⾏定义数据需求 数据需求是需要在数据库、⽂件或者其他介质中存储的数据描述，通常包括下列内容： 各个功能使⽤的数据信息； 使⽤频率； 可访问性要求； 数据实体及其关系； 完整性约束； 数据保持要求。","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://lyk-love.cn/categories/Software-Engineering/"}],"tags":[{"name":"Software Engineering Methodology","slug":"Software-Engineering-Methodology","permalink":"http://lyk-love.cn/tags/Software-Engineering-Methodology/"}]},{"title":"Project Management Basic","slug":"Project Management Basic","date":"2021-06-27T23:26:27.000Z","updated":"2022-09-26T06:39:34.937Z","comments":true,"path":"2021/06/28/Project Management Basic/","link":"","permalink":"http://lyk-love.cn/2021/06/28/Project%20Management%20Basic/","excerpt":"Outline: 项目和项目管理 团队组织与管理 软件指令保障 软件配置管理","text":"Outline: 项目和项目管理 团队组织与管理 软件指令保障 软件配置管理 项⽬和项⽬管理 项目 项⽬是具有下列特征的⼀系列活动和任务[Kerzner2009]： 具有⼀个明确的⽬标； 有限定的开始和结束⽇期； 有成本限制； 消耗⼈⼒和非⼈⼒资源； 多⼯种合作。 项⽬管理的⽬标 项目管理的目标是做到以下⽅⾯： 在限定时间内； 在⼀定的成本内； 在要求的质量⽔平上； ⾼效使⽤资源； 获得客户认可。 过程组与活动 过程组： 项⽬启动、项⽬计划、项⽬执⾏、项⽬跟踪与控制和项⽬收尾 活动 计划制定、团队管理、成本控制、质量保障、度量、过程管理、进度跟踪 与控制、⻛险管理、配置管理 团队组织与管理 团队 ⼀个协作良好的团队是任何项⽬成功的基础。 软件项⽬尤其依赖于有效的团队组织和管理：软件开发是⼀个以⼈为主的活 动，⼈⼒资源是软件项⽬最⼤的资产。 有很多实践者认为⽐⽣产⾼质量产品更⼤的成功是在⽣产过程中建⽴⼀个凝 聚的团队 团队的特征 [Katzenbach1993]将团队定义为：为了⼀致的⽬的、绩效标准、⽅法⽽共担 责任并且技能互补的少数⼈。 团队成员要具备共同的⽬标。 团队成员要共担责任。 团队成员要技能互补。 团队内部要有⼀个明确的结构。 团队结构 主程序员团队 民主团队 缺点 交流是成本 工作效率低 开放团队 优点: 激励成员主动性, 发挥其创新能力 缺点: 项目进展没有可视度 团队建设 建立团队章程 持续成功 和谐沟通 避免团队杀⼿ 避免团队杀⼿ [DeMarco1999]认为组织和管理团队时要回避下列团队杀⼿： 防范式管理。 官僚主义。 地理分散。 时间分割。 产品质量的降低。 虚假的最后期限。 ⼩圈⼦控制。 软件质量保障 软件质量 软件⼯程师也要对软件产品的质量负责。 对软件质量的要求可能是显式的，也可能是隐式的。 质量: 软件系统为满足显示及隐式的要求而需要具备的要素称为质量 ⼈们通常会选⽤系统的某些质量要素进⾏量化处理，建⽴质量特征，这些特征被称为质量属性（Quality Attribute）。 为了根据质量属性描述和评价系统的整体质量，⼈们从很多质量属性的定义当中选择了⼀些能够相互配合、相互联系的特征集，它们被称为质量模型。 质量模型 [IEEE1061-1992,1998] 和 [ISO/IEC 9126-1] 因素 功能性 可靠性 易⽤性 效率 可维护性 可移植性 质量保障 ⾥程碑 质量保障活动 需求开发 需求评审、需求度量 体系结构 体系结构评审、集成测试（持续集成） 详细设计 详细设计评审、设计度量、集成测试（持续集成） 实现 代码评审、代码度量、测试（测试驱动、持续集成） 测试 测试、测试度量 评审 在规划阶段（Planning），制定审查计划，决定审查会议的次数，安排每次审查会议的时间、地点、 参与⼈员、审查内容等等。 在总体部署阶段（Overview），向所有参与审查会议的⼈员描述待审查材料的内容、审查的⽬标以及 ⼀些假设，并分发⽂档。 在准备阶段（Preparation），审查⼈员各⾃独⽴执⾏检查任务。在检查的过程当中，他们可能会被要 求使⽤检查清单、场景等检查⽅法。检查中发现的问题会被记录下来，以准备开会讨论或者提交给收集 ⼈员。 在审查会议阶段（Inspection Meeting），通过会议讨论，识别、确认、分类发现的错误。 在返⼯阶段（Rework），修改发现的缺陷。 在跟踪阶段（Follow-up），要确认所有发现的问题都得到了解决，所有的错误都得到了修正。 质量度量 度量产⽣⾃统计控制（Statistical Control）思想。“你不能控制⾃⼰⽆法度量的东 ⻄”[DeMarco1998]。 测度（Measure）就是为了描述软件产品⽽提供的定量指标。 代码⾏数 进⾏测度的活动被称为测量（Measurement）。 度量（Metric）是软件产品在特定属性上的量化测度程度。 例如 软件配置管理 软件配置管理的动机 在软件开发活动中，除了最终产品之外，还会产⽣很多中间制品，例如需求 规格说明、需求分析模型、软件体系结构设计模型、详细设计模型等。这些 制品是不同阶段、不同⻆⾊、不同软件开发活动进⾏协同的基础。 在复杂软件系统开发中，产⽣的制品数量众多，以⾄于开发者需要维护⼀个清单才能清楚项⽬所处的状态，理解已经完成的⼯作和将要进⾏的⼯作。 某个制品发⽣变化带来的最⼤挑战是如何确保其使⽤者能够得到最新的制 品，避免开发协同出现问题。 IEEE将配置管理定义为[IEEE610.12-1990]：“⽤技术的和管理的指导和监督⽅法，来标识和说明配置项的功能和物理特征，控制对这些特征的变更，记录和报告变更处理及其实现状态，并验证与规格需求的⼀致性”。 配置项 IEEE将配置项定义为[IEEE610.12-1990]：“置于软件配置管理之下的软件配置的各种有关项⽬，包括各类管理⽂档、评审记录与⽂档、软件⽂档、源码及其可执⾏码、运⾏所需的系统软件和⽀持软件以及有关数据等”。 基线 [IEEE610.12-1990]将基线定义为：已经经过正式评审的规格说明或产品，可以作为进⼀步开发的基础， 并且只有通过正式的变更控制过程才能变更 配置管理活动 标识配置项 版本管理 变更控制 配置审计 状态报告 软件发布管理 变更控制 变更控制并不是要限制甚至拒绝变化, 它是以一种可控制的严格的方法进行变更 过程: (提请者)提请变更 (接收者)接受变更请求 (评估者)变更评估, 其内容要以正式文档的形式固定下来, 比如变更表单, 并提交给变更控制委员会 (变更控制委员会)变更决策 (修改者)执行变更 (验证者)验证变更","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://lyk-love.cn/categories/Software-Engineering/"}],"tags":[{"name":"Software Engineering Methodology","slug":"Software-Engineering-Methodology","permalink":"http://lyk-love.cn/tags/Software-Engineering-Methodology/"}]},{"title":"Software Engineering Intro","slug":"Software Engineering Intro","date":"2021-06-27T22:42:08.000Z","updated":"2022-09-26T06:39:34.939Z","comments":true,"path":"2021/06/28/Software Engineering Intro/","link":"","permalink":"http://lyk-love.cn/2021/06/28/Software%20Engineering%20Intro/","excerpt":"软件 软件以程序代码为核心， 由三个部分组成： 程序， 机器指令的集合 文档， 描述程序操作与使用的文档 数据， 程序运行时需要使用的信息","text":"软件 软件以程序代码为核心， 由三个部分组成： 程序， 机器指令的集合 文档， 描述程序操作与使用的文档 数据， 程序运行时需要使用的信息 Software is independent of hardware Software is a tool Software = programs + documents + data + knowledge • Software development is much more complicated than programming Application software originate from the reality, and reversely improve the reality 软件工程 Definition [IEEE610.12-1990] （1）The application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software; that is, the application of engineering to software. 应用系统的， 规范的， 可量化的方法来开发， 运行和维护软件， 即将工程应用到软件 （2）The study of approaches as in （1） 对（1）中各种方法的研究 Engineering 软件工程是一种工程活动, 它具备所有工程活动共同的特性. [Shaw1990]认为所有工程学科共同的特性有5点: 具有解决实际问题的动机： ⼯程学解决实际问题，⽽这些问题来源于⼯程领域之外的⼈——消费者。 应⽤科学知识指导⼯程活动： ⼯程学不依赖于个⼈的技能，⽽是强调以科学知识为指导，按照特定⽅法与技术，进⾏规律性的设计、分析等活动，实现⼯程活动 的可学习性和可重复性。 以成本效益⽐有效为基本条件： ⼯程学不单单只是解决问题，它要有效利⽤所有资源，⾄少成本要低于效益，即成本效益⽐有效。 构建机器或事物： ⼯程学强调构建实物⼯具，例如机器、事物等，并利⽤实物⼯具来解决问题。 以服务⼈类为最终⽬的： ⼯程学考虑的不是单个客户的需要，⽽是要运⽤技术和经验实现全社会的进步。 [CCSE2004]对工程师的要求: ⼯程师通过⼀系列的讨论决策，仔细评估项⽬的可选活动，并在每个决策点选择⼀种在当前环境中适合当前任务的⽅法进⾏⼯作。可以通过对成本和收益进⾏折衷分析调整相应策略。 ⼯程师需要对某些对象进⾏度量，有时需要定量的⼯作；他们要校准和确认度量⽅法，并根据经验和实验数据进⾏估算。 软件⼯程师强调项⽬设计过程的纪律性，这是团队⾼效⼯作的条件。 ⼯程师可胜任研究、开发、设计、⽣产、测试、构造、操作、管理，以及销售、咨询和培训等多种⻆⾊。 ⼯程师们需要在某些过程中使⽤⼯具，选择和使⽤合适的⼯具是⼯程的关键要素。 ⼯程师们通过专业协会发展和确认原理、标准和最佳实践⽅法，并提⾼个⼈能⼒。 ⼯程师们能够重⽤设计和设计制品。 Understanding of Software Engineering 软件⼯程是⼀种⼯程活动 软件⼯程的动机是解决实际问题 软件⼯程是科学性、实践性和⼯艺性并重的 软件⼯程追求⾜够好，不是最好 软件⼯程真正的产品是基于虚拟计算机的软件⽅案 软件⼯程的最终⽬的是要促进整个社会的进步 软件工程概览 知识域 太多了,不表 软件开发活动 软件开发是软件工程的主要任务, 包括需求开发, 软件设计, 软件构造(construction), 软件测试, 软件交付与维护等具体活动 需求开发 从空白开始, 主要目的是建立软件解决方案, 具体任务包括: 探索并明确描述现实世界信息 探索并定义问题 建立软件系统的解决方案, 使得将软件系统应用到现实世界之后能解决问题 软件解决方案也称为软件产品设计方案. 产品设计方案是从用户视角和与外界互动的方式描述产品. 需求开发产生的主要制品是软件需求规格说明( Software Requirement Specification , SRS)文档 和 需求分析模型, 软件需求规格说明文档详细描述了软件解决方案的内容, 需求分析模型重点描述了软件解决方案中的复杂技术方案 软件设计 软件设计在需求开发之后进行, 它以软件需求规格说明为基础, 主要目的是建立软件系统的构建方案, 具体任务包括: 软件体系结构设计, 确定系统的高层结构 详细设计, 将高层结构的部件设计为更详细的的模块与类, 定义模块与类的功能以及它们的接口 人机交互设计, 设计软件系统与外界的有效交互方案, 包括设计用户界面 软件构建方案又称为软件工程设计方案, 是由抽象软件实体组成的复杂概念结构. 工程方案是从生产者的角度和产品内部结构的方式描述产品. 软件设计产生的主要制品是软件设计描述(Software Design Description, SDD)文档 和 软件设计模型, SDD文档详细描述了软件构建方案的内容, 软件设计模型重点描述了软件构建方案的复杂细节 软件构造 软件构造在软件设计之后进行, 它以软件构建方案为基础, 主要目的是使用编程语言实现软件构建方案.具体任务包括: 程序设计, 以&quot;数据结构+算法&quot;的方式继续细化和深化软件构建方案基本单位(模块或者类)的设计 编程 调试 主要制品是程序源代码和编译后的可执行程序 软件测试 主要目的是验证和确认软件产品的质量, 它包含两重含义: 从技术上保证产品的质量是合格的 保证产品质量是符合需求规格的 主要制品是测试报告, 它描述了测试中发现的错误和故障 软件交付与维护 软件交付在软件产品通过所有测试之后进行, 主要目的是将软件产品交付给用户使用. 主要任务包括: 安装与部署软件系统 培训用户使用软件并提供文档支持 主要制品是用户使用手册 软件维护又称为软件演化, 在软件交付给用户之后进行, 直到软件产品消亡才结束. 角色分工 需求⼯程师，⼜被称为需求分析师： 承担需求开发任务。软件产品的需求开发⼯作通常由多个需求⼯程师来完成，他们共同组成⼀个需 求⼯程师⼩组，在⾸席需求⼯程师的领导下开展⼯作。通常⼀个团队只有⼀个需求⼯程师⼩组。 软件体系结构师： 承担软件体系结构设计任务。通常也是由多⼈组成⼀个⼩组，并在⾸席软件体系结构师的领导下开 展⼯作。通常⼀个团队只有⼀个软件体系结构师⼩组。 软件设计师： 承担详细设计任务。在软件体系结构设计完成之后，可以将其部件分配给不同的开发⼩组。开发⼩ 组中负责所分配部件详细设计⼯作的⼈员就是软件设计师。⼀个团队可能有⼀个或多个开发⼩组。 ⼀个⼩组可能有⼀个或多个软件设计师。 程序员： 承担软件构造任务。程序员与软件设计师通常是同⼀批⼈，也是根据其所分配到的任务开展⼯作。 ⼈机交互设计师： 承担⼈机交互设计任务。⼈机交互设计师与软件设计师可以是同⼀批⼈，也可以是不同⼈员。在有 多个⼩组的软件⼯程团队中，可以有⼀个单独的⼈机交互设计师⼩组，也可以将⼈机交互设计师分 配到各个⼩组。 软件测试⼈员： 承担软件测试任务。软件测试⼈员通常需要独⽴于其他的开发⼈员⻆⾊。⼀个团队可能有⼀个或多 个测试⼩组。⼀个⼩组可能有⼀个或多个软件测试⼈员。 项⽬管理⼈员： 负责计划、组织、领导、协调和控制软件开发的各项⼯作。相⽐于传统意义上的管理者，他们不完全是监控者和控制者，更多得是协调者。通常⼀个 团队只有⼀个项⽬管理⼈员。 软件配置管理⼈员： 管理软件开发中产⽣的各种制品，具体⼯作是对重要制品进⾏标识、变更控制、状态报告等。通常⼀个团队只有⼀个软件配置管理⼈员。 质量保障⼈员： 在⽣产过程中监督和控制软件产品质量的⼈员。通常⼀个团队有⼀个质量保障⼩组，由⼀个或多个⼈员组成。 培训和⽀持⼈员： 负责软件移交与维护任务。他们可以是其他开发⼈员的⼀部分，也可以是独⽴的⼈员。 ⽂档编写⼈员： 专⻔负责写作软件开发各种⽂档的⼈员。他们的存在是为了充分利⽤部分宝贵的⼈⼒资源（例如需求⼯程师和软件体系结构师），让这些⼈⼒资源从 繁杂的⽂档化⼯作中解放出来。","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://lyk-love.cn/categories/Software-Engineering/"}],"tags":[{"name":"Software Engineering Methodology","slug":"Software-Engineering-Methodology","permalink":"http://lyk-love.cn/tags/Software-Engineering-Methodology/"}]},{"title":"Sort","slug":"Sort","date":"2021-06-27T11:33:08.000Z","updated":"2022-09-26T06:39:34.940Z","comments":true,"path":"2021/06/27/Sort/","link":"","permalink":"http://lyk-love.cn/2021/06/27/Sort/","excerpt":"Outline Elementary Sort MergeSort QuickSort Priority Queues","text":"Outline Elementary Sort MergeSort QuickSort Priority Queues 快速排序 template&lt;typename T, int N&gt;int partition(T(&amp;a)[N], int lo, int hi)&#123; int i = lo, j = hi + 1; T v = a[lo]; while (true) &#123; while (a[++i] &lt; v) if (i == hi) break; while (a[--j] &gt; v) if (j == lo) break; if (i &gt;= j) break; exch(a, i, j); &#125; exch(a, lo, j); return j;&#125;template&lt;typename T,int N&gt;void exch(T (&amp;a)[N], int i , int j)&#123; T tmp = a[i]; a[i] = a[j]; a[j] = tmp;&#125;template&lt;typename T, int N&gt;void sort(T(&amp;a)[N])&#123; sort(a, 0, N - 1);&#125;template&lt;typename T, int N&gt;void sort(T ( &amp;a )[N], int lo, int hi)&#123; if (hi &lt;= lo) return; int j = partition(a, lo, hi); sort(a, lo, j - 1); sort(a, j + 1, hi);&#125;","categories":[{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"}],"tags":[]},{"title":"Modularity","slug":"Modularity","date":"2021-06-27T11:14:38.000Z","updated":"2022-09-26T06:39:34.934Z","comments":true,"path":"2021/06/27/Modularity/","link":"","permalink":"http://lyk-love.cn/2021/06/27/Modularity/","excerpt":"Outline: Modularity Information Hiding","text":"Outline: Modularity Information Hiding Modularity Computer systems are not monolithic: 计算机系统并非铁板一块, 它是可分的, 通常由大量互相交互的模块组成 they are usually composed of multiple, interacting modules. Modularity has long been seen as a key to cheap, high quality software. The goal of system design is to decide: – what the modules are; – what the modules should be; – how the modules interact with one-another. history 萌芽 Wirth1971; Parnas1972; 形成 Stevens1974; Parnas1978; Parnas1985 发展 Eder1992; Hitz1995; 反思 McConnell1996; Demarco2002 具体内容blabla一大堆,自己去看吧(っ °Д °;)っ Module What is a module? Common view: a piece of code. Too limited. Compilation unit, including related declarations and interface 编译单元, 包括相关的声明和接口, 这是狭隘的看法 David Parnas: a unit of work. 工作单元 Collection of programming units (procedures, classes, etc.) with a well-defined interface and purpose within the entire system that can be independently assigned to a developer 模块化的作用 Management: Partition the overall development effort 对整个开发过程做划分 – Divide and conquer 分治策略(计算机经典思想无处不在) Evolution: Decouple parts of a system so that changes to one part are isolated from changes to other parts 对系统进行解耦, 使得对一个部分的改变不会影响其他部分 Principle of directness (clear allocation of requirements to modules, ideally one requirement (or more) maps to one module) 一个需求对应一个模块 Principle of continuity/locality (small change in requirements triggers a change to one module only) 小型改动只需对一个模块做修改 Understanding: Permit system to be understood as composition of mind-sized chunks, e.g., the 7±2 Rule 把系统分成人脑可以理解的一个个区块 with one issue at a time, e.g., principles of locality, encapsulation, separation of concerns 使得人脑同一时间只需处理一个问题, Key issue: what criteria to use for modularization? Information Hiding Interface &amp;&amp; Implementation The interface is the visible surface of the capsule describes the essential characteristics of objects of the class which are visible to the exterior world The implementation is hidden in the capsule. The implementation hiding means that data can only be manipulated, that is updated, within the class, but it does not mean hiding interface data. Interface Interface as a contract - whatever is published by a module that Provided interface: clients of the module can depend on and Required interface: the module can depend on from other modules Syntactic interfaces How to call operations List of operation signatures Sometimes also valid orders of calling operations Semantic interfaces What the operations do, e.g., Pre- and post-conditions Use cases ⾯向对象中的接⼝ 对象之间交互的消息（⽅法名） 消息中的所有参数 消息返回结果的类型 与状态⽆关的不变量 需要处理的异常 Interface vs. Implementation Users and implementers of a module have different views of it. Interface: user’s view of a module. describes only what a user needs to know to use the module makes it easier to understand and use describes what services the module provides, but not how it’s able to provide them 只提供用户需要的服务, 但不提供实现 类的职责 什么是职责 类或对象维护⼀定的状态信息 基于状态履行行为职能的能⼒ 职责的来源 职责来源于需求, 职责的体现 封装Information Hiding Information Information -&gt;secrets what’s a “secret”? -&gt; Change Representation of data 信息的表示 Properties of a device, other than required properties 设备的属性 Implementation of world models 整个模型的实现 Mechanisms that support policies 支持策略的机制 容易更改的架构元素 hardware dependencies 硬件设备的依赖(还有外部的软件系统,经常变) External software system input and output formats 输入和输出的格式 DB, Internet, UI, … nonstandard language features and library routines; 非标准语言特性和库子程序,比如各大os Platform: os, middleware, framework… difficult design and implementation areas 包含复杂决策和实现的模块(尤其是那些写得很简陋的) especially areas that might be developed poorly and require redesign or reimplementation; Complex…, monitor, exception, log, … 异常类, log类等等 complex data structures, data structures that are used by more than one class, or data structures you haven’t designed to your satisfaction; Separate model from logic complex logic, which is almost as likely to change as complex data structures; 复杂的逻辑, 如果其依赖的数据结构变了, 那这部分基本也要变 Algorithm, schedule, time-critical, performance-critical, … global variables, which are probably never truly needed, but which always benefit from being hidden behind access routines; 全局变量, 尽量别用 Data Access Routines data-size constraints such as array declarations and loop limits; 数据大小的约束, 比如数组声明, 循环不变式之类 and business rules such as the laws, regulations, policies, and procedures that are embedded into a computer system. Information Hiding Try to localize future change Hide system details likely to change independently 把容易独立改变的系统细节隐藏起来 Separate parts that are likely to have a different rate of change 区分不同有改变速率的部分 Expose in interfaces assumptions unlikely to change 把不可能改变的假设暴露在接口中 the most common kind of secret is a design decision that you think might change. You then separate each design secret by assigning it to its own class, subroutine, or other design unit. Next you isolate (encapsulate) each secret so that if it does change, the change doesn’t affect the rest of the program.","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://lyk-love.cn/categories/Software-Engineering/"}],"tags":[{"name":"Software Engineering Methodology","slug":"Software-Engineering-Methodology","permalink":"http://lyk-love.cn/tags/Software-Engineering-Methodology/"}]},{"title":"Coupling && Cohension","slug":"Coupling && Cohension","date":"2021-06-27T11:09:16.000Z","updated":"2022-09-26T06:39:34.928Z","comments":true,"path":"2021/06/27/Coupling && Cohension/","link":"","permalink":"http://lyk-love.cn/2021/06/27/Coupling%20&&%20Cohension/","excerpt":"Outline: 耦合 内聚 耦合和内聚的度量","text":"Outline: 耦合 内聚 耦合和内聚的度量 本文所引用的设计原则参见 Design Patterns we want high cohesion and low coupling. 高内聚, 低耦合 Coupling Coupling: the measure of the strength of association established by a connection from one module to another 耦合是对模块间联系的度量 How complicated the connection is Whether the connection refers to the module itself or something inside it Connections that address or refer to a module as a whole by its name yield lower coupling than connections referring to the internal elements of another module 相比引用另一整个模块, 引用另一个模块内部的成员 的耦合性更高 在结构化编程中，耦合是对某个标签或者地址的引用： A connection is a reference to some label or address defined elsewhere 在面向对象编程中，按照模块的访问逻辑耦合可分为： Component coupling （访问耦合）：模块间的依赖关系 Inheritance coupling （继承耦合）：模块间的继承关系（以及实现等等） Types of Coupling 六种耦合, 由低到高（耦合性越低越好） ： 类型 耦合性 解释 例子 内容耦合 最高（最坏） 一个模块直接修改或者依赖于另一个模块的内容 程序跳转GOTO; 改变另一个模块的内部数据 公共耦合, 模块之间共享全局的数据 全局变量 重复耦合 模块之间有同样逻辑的重复代码 控制耦合 传递了控制信息 传递&quot;显示星期天&quot;. 传递模块和接收模块必须共享同一个内部结构和逻辑 印记耦合 共享数据结构, 但是却只用了其中的一部分 传递了整个记录给另一个模块, 另一个模块却只需要一个字段 数据耦合 最低( 最好 ) 两个模块的所有参数是同类型的数据项 传递一个整数给一个计算平方根的函数 Data Coupling 数据耦合 Connections that pass necessary data Stamp Coupling 印记耦合 Connections that pass data more than necessary Control Coupling 控制耦合 Connections that pass data and control elements Repeat Coupling 重复耦合 Common Coupling 公共耦合 Content Coupling 内容耦合 Obviously Stamp Coupling couples more Data Coupling Control Coupling also couples more than data coupling Information Hiding 访问耦合 Principles of Component Coupling 降低访问耦合的设计原则： Global Variables Consider Harmful To be Explicit Don’t repeat Programming to Interface Programming to Required Interface, not only Suffered Interface Design by Contract Contract of Module/ Class Required methods / Provided methods Contract of Methods PreCondition , PostCondition, Invariant Interface Segregation Principle(ISP) Programming to Simpler Interface Many client-specific interfaces are better than one general purpose interface 把接口变小 The Law of Demeter 继承耦合 Principles of Inherit Coupling Liskov Substitution Principle （LSP） Favor Composition Over Inheritance Cohension Cohension： 模块内部各个元素彼此结合的紧密程度的度量 Types of Cohension 从低到高： 类型 内聚性 解释 例子 偶然内聚 最低 模块执行多个不相干的操作 逻辑内聚 模块执行一系列相关操作, 每个操作的调用由其他模块来决定 把下列方法放在一个模块中: 开车去, 坐火车去, 坐飞机去 时间内聚 模块执行一系列与时间有关的操作 把下列方法放在一个模块中: 起床, 刷牙, 洗脸, 吃早餐 过程内聚 模块执行一系列与步骤顺序有关的操作 把下列方法放在一个模块中: 守门员传球给后卫, 后卫传球给中场, 中场传球给前锋, 前锋射门 通信内聚 模块执行一系列与步骤顺序有关的操作, 并且这些操作在相同的数据上进行 把下列方法放在一个模块中: 查书的名字, 查书的作者, 查书的出版商 功能内聚 模块只执行一个操作或达到一个单一的目的 下列内容都作为独立模块: 计算平方根, 决定最短路径, 压缩数据 信息内聚 最高(最好) 模块进行许多操作, 各个都有各自的入口点, 每个操作的代码相对独立, 而且所有操作都在相同的数据结构上完成 比如栈. 它包含相应的数据结构和操作. 所有的操作都是针对相同的数据结构 Principles of Cohension Single Responsibility Principle( SRP ) 耦合和内聚的度量 Coupling Metrics 模块间耦合性度量 Coupling between object classes (CBO) 对象类之间的耦合 一个类合作（即相关）的数量。当CBO增大时，不仅降低了可重用性，而且使其修改和修改后的测试变得复杂。所以，每个类的CBO值应当保持合理。这与在传统软件中减少耦合的一般原则是一致的。 A count of the number of other classes: 访问本类或者含有被本类访问的方法或变量的其他类. ( 不包括继承 ) which access a method or variable in this class, or contain a method or variable accessed by this class Not including Inheritance Want to keep this low Data abstraction coupling (DAC) 数据抽象耦合 The number of attribute having an ADT type dependent on the definitions of other classes 拥有依赖于其他类的ADT的属性的数量 Want to keep this low Ce and Ca (efferent and afferent coupling) 传出耦合和传入耦合 Ca: 传入耦合 The number of classes outside this category that depend upon classes within this category. 外部的依赖于这个category内部的类的类的数量 Ce: 传出耦合 The number of classes inside this category that depend upon classes outside this category 这个category内部的依赖于外部类的类的数量 Want to keep these low Depth of the Inheritance tree (DIT) 继承树深度 the maximum length from the node to the root of the tree 这个类的节点到继承树根节点的最大距离 as DIT grows, it becomes difficult to predict behavior of a class because of the high degree of inheritance DIT越大,这个类离根节点越远,类的行为就难以预测 Positively, large DIT values imply that many methods may be reused 好消息是, DIT越大, 意味着越多可重用的方法 Number of children (NOC) 子女数 count of the subclasses immediately subordinate to a class 就是直接隶属于该节点的节点数 as NOC grows, reuse increases NOC越大,重用越频繁 as NOC grows, abstraction can become diluted NOC越大, 抽象性就越被稀释 increase in NOC means the amount of testing will increase NOC增大意味着更多的测试 Cohesion Metrics 模块内聚性度量 Lack of cohesion in methods (LCOM) Definition: 一个类内的每种方法访问一个或多个属性（也称实例变量）。LCOM是访问一个或多个相同属性方法的数量。 Class Ck with n methods M1,…MnIj is the set of instance variables used by MjThere are n such sets I1 ,…, InP = &#123;(Ii, Ij) | (Ii ∩ Ij ) = ∅&#125; //P是访问一个类中不同属性的方法的度量, P越大, 内聚性越差Q = &#123;(Ii, Ij) | (Ii ∩ Ij ) ≠ ∅&#125; //Q是访问一个类中相同属性的方法的度量,Q越大, 说明类中这样的方法越多, 内聚性越好If all n sets Ii are ∅ then P = ∅LCOM = |P| – |Q|, if |P| &gt; |Q|LCOM = 0 otherwise Want to keep this low Many other versions of LCOM have been defined If LCOM &gt;=1, then the class should be separated","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://lyk-love.cn/categories/Software-Engineering/"}],"tags":[{"name":"Software Engineering Methodology","slug":"Software-Engineering-Methodology","permalink":"http://lyk-love.cn/tags/Software-Engineering-Methodology/"}]},{"title":"C++面向对象-Part3","slug":"C++面向对象-Part3","date":"2021-06-17T15:45:06.000Z","updated":"2022-09-26T06:39:34.925Z","comments":true,"path":"2021/06/17/C++面向对象-Part3/","link":"","permalink":"http://lyk-love.cn/2021/06/17/C++%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1-Part3/","excerpt":"Outline: Template Exception IO处理 右值引用 移动构造 外部模板 Lambda表达式 联合初始化","text":"Outline: Template Exception IO处理 右值引用 移动构造 外部模板 Lambda表达式 联合初始化 模板 多态的一种形式 源代码复用机制 参数化模块 对程序模块(如: 类, 函数)加上类型参数 对不同类型的数据实施相同的操作 多态的一种形式 C++ 类属函数( 模板函数 ) 类属类( 模板类 ) 类属函数 同一函数对不同类型的数据完成相同的操作 宏实现 #define max(a,b) ( (a)&gt;(b) ? (a) : (b) ) 缺陷: 宏没有类型检查 函数重载 int max(int, int) double max(double,double) 缺陷: 需要定义的重载函数太多 定义不全 函数指针 void sort( void*, unsigned int , unsigned int, int(*cmp)( coid*, void* ) ) 缺陷 需要定义额外参数 大量指针运算 实现起来复杂 可读性差 函数模板 template&lt; typename T &gt;void sort( T A[], unsigned int num )&#123; for( int i = 1; i &lt; num ; i++ ) for( int j = 0 ; j &lt; num - i; j++ ) &#123; if( A[j] &gt; A[j+1] ) &#123; T t = A[j]; A[j] = A[j+1]; A[j+1] = t; &#125; &#125; &#125; 不需要显式传参, 因为T具体是什么类型可以通过传递的参数推导出来 必须重载操作符 函数模板定义了一类重载的函数 编译系统自动实例化函数模板 函数模板的参数 可有多个类型参数, 用逗号分隔 template &lt; typename T1, typename T2 &gt;void f( T1 a, T2 b )&#123;&#125; 可带普通参数 必须列在类型参数之后 调用时需显式实例化 template&lt; typename T, int size &gt; void f(T a) &#123; T temp[size]; ... f&lt;int,10&gt;(1); &#125;* 模板特化 * 定义一个具有特定类型(而不是模板类型)的函数 * 必须在头部之前使用 `template&lt;&gt;`, 以显示这是前面定义的模板函数的特化 * 编译器首先匹配`普通函数`,再匹配`模板特化`, 再匹配`模板函数` * 类模板中的静态成员属于实例化后的类* 模板是一种**源代码复用**的机制 * 实例化: 生成具体的函数/类 * 函数模板的实例化 * 隐式实现 * 根据具体模板函数调用 * 类模板的实例化 * 创建对象时显式指定 * 是否实例化模板的某个实例由使用点来决定;如果未使用到一个模板的某个实例, 则编译系统不会生成相应实例的代码 * 如果在模块A中要使用模块B中定义的某模块的实例, 而在模块B中未使用这个实例, 则模块A无法使用这个实例 * **C++中模板的完整定义通常放在头文件中** * 编译方法有两种：包含式和独立编译式。独立编译理论最有，但对模板并不适用，`export`可以实现独立编译，但没有主流编译器支持，它也在新标准中被废止 包含式：声明文件包含在定义文件中，定义文件包含在应用程序文件中，只编译应用程序文件 * 模板元编程 ```c++ template&lt; int N &gt; class Fib &#123; public: enum&#123; value + Fib&lt; N - 1&gt;:: value + Fib&lt; N - 2 &gt;:: value &#125;; &#125;; template&lt;&gt; class Fib&lt;0&gt; &#123; enum &#123; value = 1 &#125;; &#125;; template&lt;&gt; class Fib&lt;1&gt; &#123; enum &#123; value = 1 &#125;; &#125;; int main() &#123; cout &lt;&lt; Fib&lt;8&gt;::value &lt;&lt; endl; &#125; 这段程序在编译器就能出结果. 元编程就是编写一个程序, 这个程序可以生成更多的程序 异常 错误 语法错误 编译系统 逻辑错误 测试 异常 运行环境造成 内存不足，文件操作失败等 异常处理 特征 可以预见 无法避免 作用 提高程序Robuntness 常见处理方法 函数参数 返回值 引用参数 逐层返回 缺陷 程序结构不清楚 C++异常处理机制 一种专门、清晰描述异常处理过程的机制 处理机制 try 监控 try &#123; &lt;语句序列&gt; &#125; throw 抛掷异常对象 throw &lt;表达式&gt; catch 捕获异常并处理 catch ( &lt;类型&gt; [ &lt; 变量&gt; ]) &#123; 语句序列 &#125; catch 类型: 异常类型精确匹配 变量: 存储异常对象, 可省 一个try语句块后面可跟多个catch语句块, 用于捕获不同类型的异常进行处理 定义异常类 注意catch 块顺序 class FileErrors&#123;&#125;;class NonExist: public FileErrors&#123;&#125;;class NonExist: public FileErrors&#123;&#125;;class WrongFormat: public FileErrors&#123;&#125;;class DiskSeekError: public FileErrors&#123;&#125;; 例题: class MyExceptionBase&#123;&#125;; //对象切片class MyExceptionDerived: public MyExceptionBase&#123;&#125;;void f( MyExceptionBase&amp; e )&#123; throw e;&#125;int main()&#123; MyExceptionDerived e; try&#123; f(e); &#125; catch ( MyExceptionDerived&amp; e ) &#123; cout &lt;&lt; &quot;MyExceptionDerived&quot; &lt;&lt; endl; &#125; catch( MyExceptionBase&amp; e ) &#123; cout &lt;&lt; &quot;MyExceptionBase&quot; &lt;&lt; endl; &#125; return 0;&#125; 这段代码会输出&quot;MyExceptionBase&quot; , 因为throw抛出对象是拷贝构造, 而父类的拷贝构造函数不是虚函数,所以会调用父类的拷贝构造函数, 进行对象切片,throw的结果是一个父类对象 特例 无参数throw 将捕获到的异常对象重新抛掷出去 catch(int) &#123;throw &#125; catch( ... ) 默认异常处理 Use destructors to prevent resource leaks template&lt; typename T &gt;class auto_ptr&#123; public: auto_ptr( T *p = nullptr ) :ptr( p ) &#123;&#125; ~auto_ptr() &#123;delete ptr;&#125; T* operator -&gt;() const &#123; return ptr; &#125; T&amp; operator *() const &#123;return *ptr;&#125; private: T* ptr;&#125;; //WINDOW_HANDOW是一个指针类型template&lt; typename T &gt;class auto_ptr&#123; public: auto_ptr( T *p = nullptr ) :ptr( p ) &#123;&#125; ~auto_ptr() &#123;delete ptr;&#125; T* operator -&gt;() const &#123; return ptr; &#125; T&amp; operator *() const &#123;return *ptr;&#125; private: T* ptr;&#125;;Class WindowHandle&#123; public: WindowHandow( WINDOW_HANDLE handler ) :w(handle) &#123;&#125; ~WindowHandle() &#123; destroyWindow(w); &#125; operator WINDOW_HANDLE() &#123; return w; &#125; //类型转换操作符， 这样就不用重载 * 和 -&gt; 了 private: WINDOW_HANDLE w; WindowHandle( const WindowHandle&amp; ); WindowHandle &amp; operator = ( const WindowHandle&amp; );&#125;; IO处理 操作符&lt;&lt;和&gt;&gt;重载 对自定义类的对象的IO 全局(友元)函数重载 将全局函数虚化 全局函数是静态绑定,无法实现多态, 但是, 可以让全局函数调用虚函数, 实现虚化 class CPoint2D&#123; double x,y; public: ... virtual void display( ostream&amp; out ) &#123; out &lt;&lt; x &lt;&lt; &#x27;,&#x27; &lt;&lt; y &lt;&lt; endl; &#125;&#125;;ostream&amp; operator &lt;&lt; ( ostream&amp; out , CPoint2D&amp; a )&#123; a.display( out ); return out;&#125;class CPoint3D: public CPoint2D&#123; double z; public: ... void display( ostream&amp; out ) &#123; CPoint2D:: display(); out &lt;&lt; &#x27;,&#x27; &lt;&lt; z &lt;&lt; endl; &#125;&#125;; Never treat arrays polymorphically! 不要在数组中放多态！ 比如说， 你想用父类的数组存放子类的对象， 当你遍历数组时， arr[i] 实际上是 i * 数组元素大小， 而数组元素大小是编译期决定的， 而实际对象类型可能比静态类型大， 所以会数组越界。 右值引用 有名称的、可以获取到存储地址的表达式即为左值；反之则是右值 其实 C++98/03 标准中就有引用，使用 &quot;&amp;&quot; 表示。但此种引用方式有一个缺陷，即正常情况下只能操作 C++ 中的左值，无法对右值添加引用。举个例子： int num = 10;int &amp;b = num; //正确int &amp;c = 10; //错误 如上所示，编译器允许我们为num左值建立一个引用，但不可以为 10 这个右值建立引用。因此，C++98/03 标准中的引用又称为左值引用。 注意，虽然 C++98/03 标准不支持为右值建立非常量左值引用，但允许使用常量左值引用引用右值(但不能修改)。也就是说，常量左值引用既可以操作左值，也可以操作右值，例如： int num = 10;const int &amp;b = num;const int &amp;c = 10; 我们知道，右值往往是没有名称的，因此要使用它只能借助引用的方式。这就产生一个问题，实际开发中我们可能需要对右值进行修改（实现移动语义时就需要），显然左值引用的方式是行不通的。 为此，C++11 标准新引入了另一种引用方式，称为右值引用，用 &quot;&amp;&amp;&quot; 表示。 需要注意的，和声明左值引用一样，右值引用也必须立即进行初始化操作，且只能使用右值进行初始化，比如： int num = 10;//int &amp;&amp; a = num; //右值引用不能初始化为左值int &amp;&amp; a = 10; 和常量左值引用不同的是，右值引用还可以对右值进行修改。例如： int &amp;&amp; a = 10;a = 100;cout &lt;&lt; a &lt;&lt; endl; 程序输出结果为 100。 另外值得一提的是，C++ 语法上是支持定义常量右值引用的，例如： const int&amp;&amp; a = 10;//编译器不会报错 但这种定义出来的右值引用并无实际用处。一方面，右值引用主要用于移动语义和完美转发，其中前者需要有修改右值的权限；其次，常量右值引用的作用就是引用一个不可修改的右值，这项工作完全可以交给常量左值引用完成。 In C++ , non-const references can bind to l-values and const references can bind to l-values or r-values, but there is nothing that can bind to a non-const r-value. class A&#123; int val; void setVal(int x) &#123; val = x ; &#125; &#125;;A getA()&#123; return A(); &#125; //返回的是个右值int main()&#123; int a = 1; int &amp;ra = a; //OK const A &amp;ca = getA(); //OK A &amp;aa = getA(); //ERROR &#125; An r-value reference can bind to an r-value 将一个右值变成有内存的变量, 也就是 可以操纵右值 int main()&#123; int a = 1; int &amp;ra = a; //OK const A &amp;cRa = getA(); //OK A &amp;&amp;aa = getA(); //ok!!! aa.setVal(2); //OK &#125; 移动构造 当类中拥有指针类型的成员变量时，拷贝构造函数中需要以深拷贝（而非浅拷贝）的方式复制该指针成员。 举个例子： #include &lt;iostream&gt;using namespace std;class demo&#123; public: demo() :num(new int(0)) &#123; cout&lt;&lt;&quot;construct!&quot;&lt;&lt;endl; &#125; //拷贝构造函数 demo(const demo &amp;d) :num(new int(*d.num)) &#123; cout&lt;&lt;&quot;copy construct!&quot;&lt;&lt;endl; &#125; ~demo() &#123; cout&lt;&lt;&quot;class destruct!&quot;&lt;&lt;endl; &#125; private: int *num;&#125;;demo get_demo()&#123; return demo();&#125;int main()&#123; demo a = get_demo(); return 0;&#125; 如上所示，我们为 demo 类自定义了一个拷贝构造函数。该函数在拷贝 d.num 指针成员时，必须采用深拷贝的方式，即拷贝该指针成员本身的同时，还要拷贝指针指向的内存资源。否则一旦多个对象中的指针成员指向同一块堆空间，这些对象析构时就会对该空间释放多次，这是不允许的。 可以看到，程序中定义了一个可返回 demo 对象的 get_demo() 函数，用于在 main() 主函数中初始化 a 对象，其整个初始化的流程包含以下几个阶段： 执行 get_demo() 函数内部的 demo() 语句，即调用 demo 类的默认构造函数生成一个匿名对象； 执行 return demo() 语句，会调用拷贝构造函数复制一份之前生成的匿名对象，并将其作为 get_demo() 函数的返回值（函数体执行完毕之前，匿名对象会被析构销毁）； 执行 a = get_demo() 语句，再调用一次拷贝构造函数，将之前拷贝得到的临时对象复制给 a（此行代码执行完毕，get_demo() 函数返回的对象会被析构）； 程序执行结束前，会自行调用 demo 类的析构函数销毁 a。 注意，目前多数编译器都会对程序中发生的拷贝操作进行优化，因此如果我们使用 VS 2017、codeblocks 等这些编译器运行此程序时，看到的往往是优化后的输出结果： construct!class destruct! 而同样的程序，如果在 Linux 上使用g++ demo.cpp -fno-elide-constructors命令运行（其中 demo.cpp 是程序文件的名称），就可以看到完整的输出结果： construct! &lt;-- 执行 demo()copy construct! &lt;-- 执行 return demo()class destruct! &lt;-- 销毁 demo() 产生的匿名对象copy construct! &lt;-- 执行 a = get_demo()class destruct! &lt;-- 销毁 get_demo() 返回的临时对象class destruct! &lt;-- 销毁 a 如上所示，利用拷贝构造函数实现对 a 对象的初始化，底层实际上进行了 2 次拷贝（而且是深拷贝）操作。当然，对于仅申请少量堆空间的临时对象来说，深拷贝的执行效率依旧可以接受，但如果临时对象中的指针成员申请了大量的堆空间，那么 2 次深拷贝操作势必会影响 a 对象初始化的执行效率。 事实上，此问题一直存留在以 C++ 98/03 标准编写的 C++ 程序中。由于临时变量的产生、销毁以及发生的拷贝操作本身就是很隐晦的（编译器对这些过程做了专门的优化），且并不会影响程序的正确性，因此很少进入程序员的视野。 那么当类中包含指针类型的成员变量，使用其它对象来初始化同类对象时，怎样才能避免深拷贝导致的效率问题呢？C++11 标准引入了解决方案，该标准中引入了右值引用的语法，借助它可以实现移动语义。 所谓移动语义，指的就是以移动而非深拷贝的方式初始化含有指针成员的类对象。简单的理解，移动语义指的就是将其他对象（通常是临时对象）拥有的内存资源“移为已用”。 以前面程序中的 demo 类为例，该类的成员都包含一个整形的指针成员，其默认指向的是容纳一个整形变量的堆空间。当使用 get_demo() 函数返回的临时对象初始化 a 时，我们只需要将临时对象的 num 指针直接浅拷贝给 a.num，然后修改该临时对象中 num 指针的指向（通常令其指向 NULL, 防止原对象被析构），这样就完成了 a.num 的初始化。 事实上，对于程序执行过程中产生的临时对象，往往只用于传递数据（没有其它的用处），并且会很快会被销毁。因此在使用临时对象初始化新对象时，我们可以将其包含的指针成员指向的内存资源直接移给新对象所有，无需再新拷贝一份，这大大提高了初始化的执行效率。 例如，下面程序对 demo 类进行了修改： #include &lt;iostream&gt;using namespace std;class demo&#123; public: demo() :num(new int(0)) &#123; cout&lt;&lt;&quot;construct!&quot;&lt;&lt;endl; &#125; demo(const demo &amp;d) :num(new int(*d.num)) &#123; cout&lt;&lt;&quot;copy construct!&quot;&lt;&lt;endl; &#125; //添加移动构造函数 demo(demo &amp;&amp;d) :num(d.num) &#123; d.num = NULL; cout&lt;&lt;&quot;move construct!&quot;&lt;&lt;endl; &#125; ~demo()&#123; cout&lt;&lt;&quot;class destruct!&quot;&lt;&lt;endl; &#125; private: int *num;&#125;;demo get_demo()&#123; return demo();&#125;int main()&#123; demo a = get_demo(); return 0;&#125; 可以看到，在之前 demo 类的基础上，我们又手动为其添加了一个构造函数。和其它构造函数不同，此构造函数使用右值引用形式的参数，又称为移动构造函数。并且在此构造函数中，num 指针变量采用的是浅拷贝的复制方式，同时在函数内部重置了 d.num，有效避免了“同一块对空间被释放多次”情况的发生。 在 Linux 系统中使用g++ demo.cpp -o demo.exe -std=c++0x -fno-elide-constructors命令执行此程序，输出结果为： construct!move construct!class destruct!move construct!class destruct!class destruct! 通过执行结果我们不难得知，当为 demo 类添加移动构造函数之后，使用临时对象初始化 a 对象过程中产生的 2 次拷贝操作，都转由移动构造函数完成。 我们知道，非 const 右值引用只能操作右值，程序执行结果中产生的临时对象（例如函数返回值、lambda 表达式等）既无名称也无法获取其存储地址，所以属于右值。当类中同时包含拷贝构造函数和移动构造函数时，如果使用临时对象初始化当前类的对象，编译器会优先调用移动构造函数来完成此操作。只有当类中没有合适的移动构造函数时，编译器才会退而求其次，调用拷贝构造函数。 在实际开发中，通常在类中自定义移动构造函数的同时，会再为其自定义一个适当的拷贝构造函数，由此当用户利用右值初始化类对象时，会调用移动构造函数；使用左值（非右值）初始化类对象时，会调用拷贝构造函数。 如果使用左值初始化同类对象，但也想调用移动构造函数完成，有没有办法可以实现呢？ 默认情况下，左值初始化同类对象只能通过拷贝构造函数完成，如果想调用移动构造函数，则必须使用右值进行初始化。C++11 标准中为了满足用户使用左值初始化同类对象时也通过移动构造函数完成的需求，新引入了 std::move() 函数，它可以将左值强制转换成对应的右值，由此便可以使用移动构造函数。 外部模板 Lambda表达式 A mechanism for specifying a function object 捕获列表 解释 [] Capture nothing [&amp;] Capture any referenced variable by reference [=] Capture any referenced variable by making a copy [=, &amp;foo] Capture any referenced variable by making a copy, but capture variable foo by reference [bar] Capture bar by making a copy; don't copy anything else 语法： [capture list] (参数) mutable noexcept/throw() -&gt; return type&#123; function body&#125;; [capture list] [ ] 方括号用于向编译器表明当前是一个 lambda 表达式，其不能被省略。在方括号内部，可以注明当前 lambda 函数的函数体中可以使用哪些“外部变量”。 所谓外部变量，指的是和当前 lambda 表达式位于同一作用域内的所有非static局部变量 lambda可以直接使用局部static变量和全局变量 (参数) 和普通函数的定义一样，lambda 匿名函数也可以接收外部传递的多个参数。和普通函数不同的是，如果不需要传递参数，可以连同 () 小括号一起省略； mutable 此关键字可以省略，如果使用则之前的 () 小括号将不能省略（参数个数可以为 0）。默认情况下，对于以值传递方式引入的外部变量，不允许在 lambda 表达式内部修改它们的值（可以理解为这部分变量都是 const 常量）。而如果想修改它们，就必须使用 mutable 关键字。 注意，对于以值传递方式引入的外部变量，lambda 表达式修改的是拷贝的那一份，并不会修改真正的外部变量； noexcept/throw() 可以省略，如果使用，在之前的 () 小括号将不能省略（参数个数可以为 0）。默认情况下，lambda 函数的函数体中可以抛出任何类型的异常。而标注 noexcept 关键字，则表示函数体内不会抛出任何异常；使用 throw() 可以指定 lambda 函数内部可以抛出的异常类型。 值得一提的是，如果 lambda 函数标有noexcept而函数体内抛出了异常，又或者使用 throw() 限定了异常类型而函数体内抛出了非指定类型的异常，这些异常无法使用 try-catch 捕获，会导致程序执行失败（本节后续会给出实例）。 -&gt; 返回值类型 指明 lambda 匿名函数的返回值类型。如果 lambda 函数体内只有一个 return 语句，或者该函数返回 void，则编译器可以自行推断出返回值类型，此情况下可以直接省略-&gt; 返回值类型。 函数体 和普通函数一样，lambda 匿名函数包含的内部代码都放置在函数体中。该函数体内除了可以使用指定传递进来的参数之外，还可以使用指定的外部变量以及全局范围内的所有全局变量。 需要注意的是，外部变量会受到以值传递还是以引用传递方式引入的影响，而全局变量则不会。换句话说，在 lambda 表达式内可以使用任意一个全局变量，必要时还可以直接修改它们的值。 其中，红色标识的参数是定义 lambda 表达式时必须写的，而绿色标识的参数可以省略。 比如，如下就定义了一个最简单的 lambda 匿名函数： []&#123;&#125; 显然，此 lambda 匿名函数未引入任何外部变量（[] 内为空），也没有传递任何参数，没有指定 mutable、noexcept 等关键字，没有返回值和函数体。所以，这是一个没有任何功能的 lambda 匿名函数。 [外部变量] 外部变量格式 功能 [] 空方括号表示当前 lambda 匿名函数中不导入任何外部变量。 [=] 只有一个 = 等号，表示以值传递的方式导入所有外部变量； [&amp;] 只有一个 &amp; 符号，表示以引用传递的方式导入所有外部变量； [val1,val2,...] 表示以值传递的方式导入 val1、val2 等指定的外部变量，同时多个变量之间没有先后次序； [&amp;val1,&amp;val2,...] 表示以引用传递的方式导入 val1、val2等指定的外部变量，多个变量之间没有前后次序； [val,&amp;val2,...] 以上 2 种方式还可以混合使用，变量之间没有前后次序。 [=,&amp;val1,...] 表示除 val1 以引用传递的方式导入外，其它外部变量都以值传递的方式导入。 [this] 表示以值传递的方式导入当前的 this 指针。 注意，单个外部变量不允许以相同的传递方式导入多次。例如 [=，val1] 中，val1 先后被以值传递的方式导入了 2 次，这是非法的。 【例 1】lambda 匿名函数的定义和使用。 #include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;int main()&#123; int num[4] = &#123;4, 2, 3, 1&#125;; //对 a 数组中的元素进行排序 sort(num, num+4, [=](int x, int y) -&gt; bool&#123; return x &lt; y; &#125; ); for(int n : num) &#123; cout &lt;&lt; n &lt;&lt; &quot; &quot;; &#125; return 0;&#125; 程序执行结果为： 2 3 41 2 3 4 程序第 9 行通过调用 sort() 函数实现了对 num 数组中元素的升序排序，其中就用到了 lambda 匿名函数。而如果使用普通函数，需以如下代码实现： #include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;//自定义的升序排序规则bool sort_up(int x,int y)&#123;return x &lt; y;&#125;int main()&#123; int num[4] = &#123;4, 2, 3, 1&#125;; //对 a 数组中的元素进行排序 sort(num, num+4, sort_up); for(int n : num) &#123; cout &lt;&lt; n &lt;&lt; &quot; &quot;; &#125; return 0;&#125; 此程序中 sort_up() 函数的功能和上一个程序中的 lambda 匿名函数完全相同。显然在类似的场景中，使用 lambda 匿名函数更有优势。 除此之外，虽然 lambda 匿名函数没有函数名称，但我们仍可以为其手动设置一个名称，比如： #include &lt;iostream&gt;using namespace std;int main()&#123; //display 即为 lambda 匿名函数的函数名 auto display = [](int a,int b) -&gt; void&#123;cout &lt;&lt; a &lt;&lt; &quot; &quot; &lt;&lt; b;&#125;; //调用 lambda 函数 display(10,20); return 0;&#125; 程序执行结果为： 10 20 可以看到，程序中使用 auto 关键字为 lambda 匿名函数设定了一个函数名，由此我们即可在作用域内调用该函数。 【例 2】值传递和引用传递的区别 #include &lt;iostream&gt;using namespace std;//全局变量int all_num = 0;int main()&#123; //局部变量 int num_1 = 1; int num_2 = 2; int num_3 = 3; cout &lt;&lt; &quot;lambda1:\\n&quot;; auto lambda1 = [=]&#123; //全局变量可以访问甚至修改 all_num = 10; //函数体内只能使用外部变量，而无法对它们进行修改 cout &lt;&lt; num_1 &lt;&lt; &quot; &quot; &lt;&lt; num_2 &lt;&lt; &quot; &quot; &lt;&lt; num_3 &lt;&lt; endl; &#125;; lambda1(); cout &lt;&lt; all_num &lt;&lt;endl; cout &lt;&lt; &quot;lambda2:\\n&quot;; auto lambda2 = [&amp;]&#123; all_num = 100; num_1 = 10; num_2 = 20; num_3 = 30; cout &lt;&lt; num_1 &lt;&lt; &quot; &quot; &lt;&lt; num_2 &lt;&lt; &quot; &quot; &lt;&lt; num_3 &lt;&lt; endl; &#125;; lambda2(); cout &lt;&lt; all_num &lt;&lt; endl; return 0;&#125; 程序执行结果为： lambda1: 1 2 310 lambda2: 10 20 30100 可以看到，在创建 lambda1 和 lambda2 匿名函数的作用域中，有 num_1、num_2 和 num_3 这 3 个局部变量，另外还有 all_num 全局变量。 其中，lambda1 匿名函数是以 [=] 值传递的方式导入的局部变量，这意味着默认情况下，此函数内部无法修改这 3 个局部变量的值，但全局变量 all_num 除外。相对地，lambda2 匿名函数以 [&amp;] 引用传递的方式导入这 3 个局部变量，因此在该函数的内部就可以访问这 3 个局部变量，还可以任意修改它们。同样，也可以访问甚至修改全局变量。 当然，如果我们想在 lambda1 匿名函数的基础上修改外部变量的值，可以借助 mutable 关键字，例如： auto lambda1 = [=]() mutable&#123; num_1 = 10; num_2 = 20; num_3 = 30; //函数体内只能使用外部变量，而无法对它们进行修改 cout &lt;&lt; num_1 &lt;&lt; &quot; &quot; &lt;&lt; num_2 &lt;&lt; &quot; &quot; &lt;&lt; num_3 &lt;&lt; endl;&#125;; 由此，就可以在 lambda1 匿名函数中修改外部变量的值。但需要注意的是，这里修改的仅是 num_1、num_2、num_3 拷贝的那一份的值，真正外部变量的值并不会发生改变。 【例 3】执行抛出异常类型 #include &lt;iostream&gt;using namespace std;int main()&#123; auto except = []()throw(int) &#123; throw 10; &#125;; try &#123; except(); &#125; catch (int) &#123; cout &lt;&lt; &quot;捕获到了整形异常&quot;; &#125; return 0;&#125; 程序执行结果为： 捕获到了整形异常 可以看到，except 匿名数组中指定函数体中可以抛出整形异常，因此当函数体中真正发生整形异常时，可以借助 try-catch 块成功捕获并处理。 在此基础上， 再看一下反例： #include &lt;iostream&gt;using namespace std;int main()&#123; auto except1 = []()noexcept&#123; throw 100; &#125;; auto except2 = []()throw(char)&#123; throw 10; &#125;; try&#123; except1(); except2(); &#125; catch(int)&#123; cout &lt;&lt; &quot;捕获到了整形异常&quot;&lt;&lt; endl; &#125; return 0;&#125; 此程序运行会直接崩溃，原因很简单，except1 匿名函数指定了函数体中不发生任何异常，但函数体中却发生了整形异常；except2 匿名函数指定函数体可能会发生字符异常，但函数体中却发生了整形异常。由于指定异常类型和真正发生的异常类型不匹配，导致try-catch无法捕获，最终程序运行崩溃。 如果不使用 noexcept 或者 throw()，则 lambda 匿名函数的函数体中允许发生任何类型的异常。 sort #include &lt;iostream&gt;#include &lt;chrono&gt;#include &lt;random&gt;#include &lt;vector&gt;#include &lt;algorithm&gt;using namespace std;class Foo&#123;public: int a; int b; Foo():a(0), b(0)&#123;&#125; ~Foo()&#123;&#125; Foo(int a, int b) &#123; this-&gt;a = a; this-&gt;b = b; &#125; // 规定对象排序的算法：先按照 a 从小到大排序；如果 a 相等，则按照 b 从小到大排序 bool operator&lt;(const Foo &amp;bar) &#123; if (this-&gt;a &lt; bar.a) &#123; return true; &#125; else if (this-&gt;a == bar.a) &#123; return this-&gt;b &lt; bar.b; &#125; return false; &#125; // 规定对象排序的算法：先按照 a 从大到小排序；如果 a 相等，则按照 b 从大到小排序 bool static decrease(const Foo &amp;foo1, const Foo &amp;foo2) &#123; if (foo1.a &gt; foo2.a) &#123; return true; &#125; else if (foo1.a == foo2.a) &#123; return foo1.b &gt; foo2.b; &#125; return false; &#125; friend inline ostream &amp; operator&lt;&lt;(ostream &amp;out, Foo &amp;foo) &#123; out &lt;&lt; foo.a &lt;&lt; &quot; &quot; &lt;&lt; foo.b &lt;&lt; endl; return out; &#125;&#125;;int main()&#123; unsigned seed = chrono::system_clock::now().time_since_epoch().count(); minstd_rand0 generator(seed); // minstd_rand0 is a standard linear_congruential_engine vector&lt;Foo&gt; myVec(10, Foo()); for (Foo &amp;foo : myVec) // 随机赋值 &#123; foo.a = generator() % 5; foo.b = generator() % 5; cout &lt;&lt; foo; &#125; sort(myVec.begin(), myVec.end()); // 排序一：默认从小到大，调用 operator &lt; cout &lt;&lt; endl &lt;&lt; &quot;after sorting using operator &lt;&quot; &lt;&lt; endl; for (Foo &amp;foo : myVec) &#123; cout &lt;&lt; foo; &#125; sort(myVec.begin(), myVec.end(), Foo::decrease); // 排序二：按照 Foo::decrease 的规则从大到小排序 cout &lt;&lt; endl &lt;&lt; &quot;after sorting using Foo::decrease()&quot; &lt;&lt; endl; for (Foo &amp;foo : myVec) &#123; cout &lt;&lt; foo; &#125; // 排序三：使用 lambda 的方式进行排序，排序的方法和 Foo::decrease 一样 sort(myVec.begin(), myVec.end(), [](const Foo &amp;foo1, const Foo &amp;foo2) &#123; if (foo1.a &gt; foo2.a) &#123; return true; &#125; else if (foo1.a == foo2.a) &#123; return foo1.b &gt; foo2.b; &#125; return false; &#125; ); cout &lt;&lt; endl &lt;&lt; &quot;after sorting using lambda&quot; &lt;&lt; endl; for (Foo &amp;foo : myVec) &#123; cout &lt;&lt; foo; &#125; system(&quot;pause&quot;); return 0;&#125; Uniform Initialization vector&lt;int&gt; vec = &#123;1,2,3&#125;;//Compiler will translate &#123;&#125; as initializer_list&lt;int&gt;template class vector&lt;T&gt;&#123; //.... vector( initializer_list&lt;T&gt; list ) &#123; for( auto it = list.begin(); it != list.end(); ++it ) push_back(*it); &#125; &#125;","categories":[{"name":"Language","slug":"Language","permalink":"http://lyk-love.cn/categories/Language/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://lyk-love.cn/tags/C/"}]},{"title":"Code Design","slug":"Code Design","date":"2021-06-17T15:41:09.000Z","updated":"2022-09-26T06:39:34.926Z","comments":true,"path":"2021/06/17/Code Design/","link":"","permalink":"http://lyk-love.cn/2021/06/17/Code%20Design/","excerpt":"Outline: 设计易读的代码 设计易维护的代码 设计可靠的代码 使用模型辅助设计复杂代码 单元测试用例 代码复杂度度量","text":"Outline: 设计易读的代码 设计易维护的代码 设计可靠的代码 使用模型辅助设计复杂代码 单元测试用例 代码复杂度度量 设计易读的代码 维护的需要 团队协作的需要 代码规范 格式 命名 临时变量命名要符合常规。 像for循环计数器、键盘输入字符等临时变量一般不要求使用有意义的名称,但是要使用符合常规的名称,例如使用i、j命名整数而不是字符,使用c、s命名字符而不是整数。 不要使用太长的名称,不利于拼写和记忆。 不要使用易混字符进行命名,常见的易混字符例如“I”(大写i)、 “1”(数字1)与“l”(小写L)、0(数字零)与o (字母)等。使用易混字符的命名例 如D0Calc与DOCalc。 不要仅仅使用不易区分的多个名称,例如Sales与Sale , SalesLineltem与SalesLineitem。 不要使用没有任何逻辑的字母缩写进行命名,例如wrttn、 wtht、 vwls、smch...... 注释 注释类型(ava) 语句注释( //) 标准注释(/* */) 文档注释(/** */) 文档注释的内容 包的总结和概述,每个包都要有概述;类和接口的描述,每个类和接口都要有概述; 类方法的描述,每个方法都要有功能概述,都要定义完整的接口描述; 字段的描述,重要字段含义、用法与约束的描 述。 Javadoc 在描述方法时, Javadoc常用的标签是: @param参数及其意义 @return返回值 @throws异常类及抛出条件 @see:引用 @since :最早使用该方法/类/接口的JDK版本) @deprecated 引起不推荐使用的警告 内部注释 注释要有意义，不要简单重复代码的含义 重视对数据类型的注释 重视对复杂控制结构的注释 设计易维护的代码 小型任务 要让程序代码可修改,就要控制代码的复杂度。这首先要求每个函数或方法的代码应该是内聚的,恰好完成一个功能与目标。 如果内聚的代码本身比较简单，复杂性可控,那么它就具有比较好的可维护性。反之,内聚的代码也可以比较复杂,典型表现是完成一-个功能需要多个步骤、代码比较长,那么就需要将其进一步分解为多个高内聚、 低耦合的小型任务。 复杂决策 使用新的布尔变量简化复杂决策 使用有意义的名称封装复杂决策 表驱动编程 数据使用 不要将变量应用于与命名不相符的目的。例如使用变量total表示销售的总价,而不是临时客串for循环的计数器。 不要将单个变量用于多个目的。在代码的前半部分使用total表示销售总价,在代码后半部分不再需要‘销售总价”信 息时再用total客串for循环的计数器也是不允许的。 限制全局变量的使用,如果不得不使用全局变量,就明确注释全局变量的声明和使用处。 不要使用突兀的数字与字符,例如15 (天)、“MALE”等,要将它们定义为常量或变量后使用。 明确依赖关系 类之间模糊的依赖关系会影响到代码的理解与修改,非常容易导致修改时产生未预期的连锁反应。 设计可靠的代码 契约式设计 异常方式 断言方式 Java中断言语句的实现 为了方便实现契约式设计, Java提供了断言语句:assert ExpressionI( : Expression2) ;: Expressionl是一-个布尔表达式,在契约式设计中可以将其 设置为前置条件或者后置条件; Expression2是一个值,各种常见类型都可以; 如果Expressionl为true ,断言不影响程序执行; 如果ExpressionI为false ,断言抛出AssertionError异常,如果存在Expression2就使用它作为参数构造AssertionError。 防御式编程 防御式编程的基本思想是:在一个方法与其他方法、操作系统、硬件等外界环境交互时,不能确保外界都是正确的,所以要在外界发生错误时,保护方法内部不受损害。 常见场景 输入参数是否合法? 用户输入是否有效? 外部文件是否存在? 对其他对象的引用是否为NULL ? 其他对象是否已初始化? 其他对象的某个方法是否已执行? 其他对象的返回值是否正确? 数据库系统连接是否正常? 网络连接是否正常? 网络接收的信息是否有效? 异常和断言都可以用来实现防御式编程,两种实现方式的差异与契约式设计的实现一样。 使用模型辅助设计复杂代码 决策表 伪代码 程序流程图 单元测试用例 为方法开发测试用例主要使用两种线索: 方法的规格 根据第一种线索，可以使用基于规格的测试技术开发测试用例,等价类划分和边界值分析是开发单元测试用例常用 的黑盒测试方法。 方法代码的逻辑结构。 根据第二种线索,可以使用基于代码的测试技术开发测试 用例,对关键、复杂的代码使用路径覆盖,对复杂代码使 用分支覆盖,简单情况使用语句覆盖。 为类开发测试用例 在复杂类中,常常有着多变的状态,每次一个方法的执行改变了类状态时,都会给其他方法带来影响,也就是说复杂类的多个方法间是互相依赖的。 所以，除了测试类的每一个方法之外,还要测试类不同方法之间的互相影响情况。 代码复杂度度量 度量的意义 基于圈复杂度,你可以衡量一 下程序代码是否需要调整。 [McConnell2004]认为: 0-5子程序可能还不错; 6-10得想办法简化子程序了; 10+把子程序的某一个部分拆分成另- -个子程序并调用它。 10个决策点的上限并不是绝对的。应该把决策点的数量当做一个警示,该警示说明某个子程序可能需要重新设计了。 [Chidamber 1994]基于所拥有方法的代码复杂度定义了类的复杂度: 类的加权方法= Sum( Ci) i=from I to n 其中, n为一个类的方法数量，Ci是第i个方法的代码复杂度。 问题代码","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://lyk-love.cn/categories/Software-Engineering/"}],"tags":[{"name":"Software Engineering Methodology","slug":"Software-Engineering-Methodology","permalink":"http://lyk-love.cn/tags/Software-Engineering-Methodology/"}]},{"title":"Software Testing","slug":"Software Testing","date":"2021-06-17T15:14:10.000Z","updated":"2022-09-26T06:39:34.940Z","comments":true,"path":"2021/06/17/Software Testing/","link":"","permalink":"http://lyk-love.cn/2021/06/17/Software%20Testing/","excerpt":"Are you building it “right”? Are you building the “right” thing? For more information, please read Automatic Testing","text":"Are you building it “right”? Are you building the “right” thing? For more information, please read Automatic Testing 软件测试的目的 Test suite: a collective term for all the tests Unit test: a “micro-test” that tests a specific feature in isolation Integration test: a “macro-test” that runs a larger part of the system to check that different feature or components work together. Regression test: a test that implements a particular pattern that previously caused a bug to ensure that the bug does not resurface. Mocking: to replace a function, module, or type with a fake implementation to avoid testing unrelated functionality. For example, you might “mock the network” or “mock the disk”. 向开发者和用户展示软件满足了需求， 表明软件产品是一个合格的产品 有效性测试 找出软件中的缺陷和不足 缺陷测试 桩程序和驱动程序 桩程序是被测试部件的交互环境,它扮演被测试部件需要调用的其他系统部件。桩程序对其他系统部件的扮演仅限于规格相同，内部代码要简单地多,通常是直接返回固定数据或者按照固定规则返回数据。 驱动程序负责创建被测试部件的执行环境,并驱动和监控被测试部件执行测试用例的过程,判定测试用例的执行结果。 单元测试 略 集成测试 自顶而下集成测试 自底向上集成测试 系统测试 单元测试、集成测试更加关注技术上的正确性,重在发现设计缺陷和代码缺陷。 系统测试则不同,它更关注不符合需求的缺陷和需求自身的内在缺陷。 系统测试关注整个系统的行为,所以不依赖于桩程序和驱动程序。但是，使用一些测试工具可以让系统测试过程更加自动化。 系统测试的功能测试计划以需求规格说明文档或用例文档为基础,主要使用随机测试和基于规格的测试技术设计功能测试用例。在测试非功能性需求时需要使用针对非功能需求的特定测试技术进行测试计划和测试用例设计。 根据测试目标的不同,有很多不同类型的系统测试:功能测试、非功能性测试、验收测试、安装测试等等。但是发生在软件测试阶段,完全由软件测试人员控制和执行的主要是功能测试和非功能性测试。 测试技术 测试用例的选择 随机测试 基于规格的技术-黑盒测试方法 基于代码的技术-白盒测试方法 特定测试技术 测试用例的选择 测试的目标是发现尽可能多的缺陷,并不绝对要求所有缺陷。 因为测试是有代价的,不仅要耗费桩程序、驱动人力等成本,更重要的是随着测试用例数量增多， 成本会直线上升 所以,软件测试人员需要仔细地选择用例在代价尽可能小的情况下发现足够多的缺陷[Zhu 1997 ]。 测试技术就是帮助软件人员设计和选择用例的。 随机测试 随机测试( 随机测试 Ad hoc Testing )是一种基于软件工程师直觉和经验的技术, 也许是实践中使用最为广泛的测试技术[SWEBOK2004]. 随机测试根据软件工程师的技能， 直觉和对类似程序的经验[MyersI979] ,从所有可能的输入值中选择输入子集,建立测试用例。 黑盒测试方法 等价类划分 有效等价类:是指对于程序的规格说明 合理、有意义的输入的数据构成的集合。利用有效等价类可检验程序是否实现了规格说明中所定的功能和性能。 例子： ID 输入 预期输出 1 payment = 100; Total=50; 50 2 payment = 100; Total=20; 输入数据无效 3 payment = 50; Total=100; 输入数据无效 如图的规格说明，可以将输入数据划分为三个等价类： 有效数据 无效数据 payment&lt;=0 无效数据 payment&lt;total 边界值分析 决策表 状态转换 为对象建立状态图,描述测试对象的状态集合，输入集合和输入导致的状态转换集合。 以状态图为基础,可建立测试对象的转换表. 状态转换表每一行都应该被设计为测试用例 白盒测试方法 语句覆盖 语句覆盖设计测试用例的标准是确保被测试对象的每一行程序代码都至少执行一次 条件覆盖 条件覆盖设计测试用例的标准是确保程序中每个判断的每个结果都至少满足一次 路径覆盖 路径覆盖测试用例的标准是确保程序中每条独立执行路径都 至少执行一次 特定测试技术 面向对象测试技术 给予状态图设计类的单元测试用例 基于协作设计类之间的集成测试用例 测试活动 测试计划 测试设计 测试执行 测试评价 测试用例日志 缺陷报告 测试度量 缺陷数据 测试覆盖率 需求覆盖率 模块覆盖率 代码覆盖率 缺陷度量 根据引入缺陷的阶段 系统需求缺陷、设计缺陷和编码缺陷 根据缺陷的影响力 严重、一般和无影响 覆盖率 需求覆盖率=被测试的需求数量/需求 总数; 模块覆盖率=被测试的模块数量/模块 总数; 代码覆盖率=被测试的代码行/代码行 数总数。","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://lyk-love.cn/categories/Software-Engineering/"}],"tags":[{"name":"Software Testing","slug":"Software-Testing","permalink":"http://lyk-love.cn/tags/Software-Testing/"}]},{"title":"Software Construction","slug":"Software Construction","date":"2021-06-17T15:14:00.000Z","updated":"2022-09-26T06:39:34.939Z","comments":true,"path":"2021/06/17/Software Construction/","link":"","permalink":"http://lyk-love.cn/2021/06/17/Software%20Construction/","excerpt":"Outline: Intro 软件构造活动 Ideas","text":"Outline: Intro 软件构造活动 Ideas Intro 定义： 通过编码、验证、单元测试、集成测试和调试等工作的结合，生产可工作的、有意义的软件的详细创建过程 软件构造除了核心的编程任务之外， 还涉及详细设计（ 数据结构与算法设计 ）、单元测试、集成与集成测试、 以及其他活动。 软件构造是设计的延续 设计与实现是有区分的。 设计是规划软件构件方案的过程，是现实依据软件构件方案建造真正产品的过程； 源程序是软件构建方案的最后一个规划， 不是产品本身， 真正的产品是运行于计算机上的由二进制代码组成的可执行程序。 源程序的生产过程----编程， 属于设计活动， 编译器完成的编译和连接才是依据规划建造软件产品的实现活动。 软件构造活动 详细设计 有些项目会将主要的详细设计工作分配在软件构造阶段完成。 不论是哪种项目，在软件构造阶段都不可避免的会涉及到详细设计的调整工作。 因为编程语言是软件设计的一个重要约束， 随着编程工作的进行和深入， 人们可能会发现与预想不一致的情况和更多的约束，这个时候就需要在软件构造阶段修改详细设计方案。 软件构造阶段详细设计使用的方法与技术与软件设计阶段是一样的，只是应用在更小的规模上。 程序代码的典型质量 易读性 易维护性 可靠性 性能 安全性 调试 Debugging is concerned with locating and repairing these errors 调试过程可以分为三个部分:重现问题、 诊断缺陷和修复缺陷。 重现问题的方法 控制输入。 找到相应的数据输入,能够重现绝大多数的问题。可以通过控制数据输入来重现问题意味着缺陷就发生在对该数据的处理代码之中。 寻找能够重现问题的数据输入可以使用问题回溯推理、内存数据监控、记录输入数据日志等方法。 控制环境。 有些问题是编译器、操作系统、数据库管理系统、网络管理系统等系统软件环境造成的,通过控制数据输入无法重现问题。这时就需要通过控制环境来重现问题。一定要记住的是,如果你进行各种手段诊断之后确信你的程序代码没有缺陷,就要警惕可能是软件环境造成了问题。 控制环境以重现问题经常使用替换法,例如替换机器、操作系统、数据库管理系统等。 寻找和定位缺陷的方法 灵活使用编译器提示。 持续缩小嫌疑代码的范围。 检查刚刚修改过的部分。 警惕已出现缺陷和常见缺陷。 利用工具。 常见错误 内存或资源泄漏; 定时错误(没有考虑特殊情况) ; 逻辑错误; 存储错误(考虑磁盘已满,文件不存在等特例 编码错误(例如条件判断不够充分)) ; 集成错误(相互之间的考虑不相容) 内存溢出(超出本身限制); 循环错误(死循环或数目不合适) ; 转换错误(字符转换等出现问题) ; 条件错误; 硬编码长度/尺寸; 指针错误(超出范围,未赋值); 版本缺陷(对以前的不兼容) ; 分配释放错误(分配两次、未分配即释放、释放两次、分配未释放) 多线程错误(同步); 不恰当重用带来的缺陷。 修复缺陷的注意点 一次只修复一 个缺陷。 修改前保留旧版本的备份,如果项目使用了配置管理系统,这个工作会由配置管理工具完成,否则就需要由程序员手动完成。 使用测试和评审验证修复的有效性。 检查和修复类似的缺陷,这可以在代码搜索、程序切片等工具的帮助下进行。 代码评审 代码评审对代码的系统检查,通常是通过同行专家评审来完成的。通过评审会议可以发现并修正之前忽略的代码错误,从而同时提高软件的质量和开发者的技巧。 代码评审一般分为正式评审、轻量级评审和结对编程。 实践经验 就算不能评审全部的代码,最少也要评审一部分( 20 -33% )代码,以促使程序员编写更好的代码。 一次评审少于200- 400行的代码。 目标为每小时低于300 -500 LOC的检查速率。 花足够的时间进行正确缓慢的评审,但是不要超过60- -90 分钟/每次。 确定代码开发者在评审开始之前就已经注释了源代码。 使用检查列表,因为它可以极大地改进代码开发者和评审者的工作。确认发现的缺陷确实得到修复了。 培养良好的代码评审文化氛围,在这样的氛围中搜索缺陷被看做是积极的活动。 采用轻量级,能用工具支持的代码评审。 集成与构建 在以分散的方式完成程序基本单位(例程、类)之后，软件构造还需要将这些分散单位集成和构建为 构件、子系统和完整系统。 集成有大爆炸式集成和增量式集成两种方式。实践中增量式集成有着更好的效果。 构建将可读的源代码转换为标准的能在计算机上运行的可执行文件。构建过程需要配置管理工具的帮 助。 构造管理 构造计划 根据整个项目的开发过程安排, 定义要开发的构建与次序,选则构造方法,明确构造任务并分配给程序员。 度量 类或者方法的复杂度、代码行、注释 配置管理 对开发配置库建立使用规则, 完成代码单位或者修改是提交 开发时要确保配置环境一致. 这点非常重要,因为企业用的配置都非常古老. 防御式编程 防御式编程用于提升代码的正确性和健壮性, 这两个性质可能会矛盾，因此要根据具体问题作出取舍 正确性(correctness): 软件按照需求正确执行任务的能力 永不返回不准确的结果，哪怕不返回结果也比返回不准确的结果好（无论如何，软件正常运行是最起码的要求，追求正确性不意味着软件可以崩溃） 人身安全攸关的软件 健壮性(robustness): 软件对于规范要求以外的输入情况的处理能力 要不断尝试采取某些措施，以保证软件可以持续地运转下去，哪怕有时做出一些不够准确的结果 消费类应用软件 主要思想: 子程序应该不因传入错误数据而被破坏，哪怕是由其他子程序产生的错误数据 要承认程序都会有问题，都会被修改， 即：进行防御式编程时，不应该存在任何假设 特点： 区别于检查错误：防御性编程并不能排除所有的程序错误 区别于调试：防御式编程是一种防卫方式，而不是补救方式 区别于测试：测试不是防御式的，测试可以验证代码现在是 正确的，但不保证在经历修改之后不会出错 错误处理技术 错误处理应该根据系统对正确性和健壮性的追求来定 这里的错误处理技术有开发阶段的，也有用于生产环境的，在开发阶段，应该尽早地引入辅助代码， 当然为了追求效率，某些辅助代码不应该被包含进在生产环境(比如assert) assert 断言用于开发阶段，不应该被编译进生产环境 断言可以提升系统的正确性，断言处理的是代码中不应发生的错误 exception 异常：把代码中的错误或异常事件传递给调用方代码的一种技术 异常用于提升系统的健壮性，异常处理的是预料中可能发生的错误， 由于程序代码或外部因素而发生， 是可以手动处理的 尤其是RuntimeException， 一般是编程问题引发的 不要滥用异常，只有真正例外的情况下才抛出异常（即其他技术无法解决的情况下） 异常会增加复杂度: 调用子程序的代码需要了解被调用代码中可能会抛出的异常，弱化了封装性 实践原则： 永远考虑异常的替换方案，不到迫不得已不要用异常 使用异常的原因是迫不得已， 而不是因为语言提供了异常机制 不能用异常来推卸责任： 可以在局部处理就在局部处理掉 避免在构造函数和析构函数中抛出异常，除非你在同一地方把它们捕获 C++中， 构造函数中的异常会造成资源泄露 在恰当的抽象层次抛出异常：确保异常的抽象层次与子程序接口的抽象层次是一致的 如果系统要使用大量异常，那么需要将异常标准化 error error不是一种错误处理技术，它和exception的区别在于， error往往是系统内部发生了错误，这一般是无法手动处理的 Java Exception and Error： ![image-20220412171602879](/Users/lyk/Library/Application Support/typora-user-images/image-20220412171602879.png) 辅助代码 实践方法 重构 为什么要重构? 因为无法预计到后续数年的修改，导致软件开发阶段的设计方案不能满足修改要求; 随着修改次数的增多,软件设计结构的质量越来越脆弱,很难继续维持可修改性。 什么是重构? 修改软件系统的严谨方法,它在不改变代码外部表现的情况下改进其内部结构。 重构的时机? 增加新的功能时。需要注意的是重构发生在新功能增加完成之后,用来消除新功能所添加代码导致的坏味道( code smell );而不是发生在新功能添加之前, 重构不改变代码外部行为,不是能够实现新功能添加的方法。 发现了缺陷进行修复时。诊断缺陷时如果发现代码存在坏味道或者修复代码会引入坏味道,就需要进行重构。 进行代码评审时。如果在评审代码时发现了坏味道,就需要进行重构。 code smell 太长的方法,往往意味着方法完成了太多的任务,不是功能内聚的,需要被分解为多个方法。[McConnell2004]认为如果方法代码长度超过了一个屏幕,就需要留心注意了。 太大的类,往往意味着类不是单一职责的 ,需要被分解为多个类。 太多的方法参数,往往意味着方法的任务太多或者参数的数据类型抽象层次太低,不符合接口最小化的低耦合原则,需要将其分解为多 个参数少的方法或者将参数包装成对象、结构体等抽象层次更高的数据类型。 多处相似的复杂控制结构,例如多处相同类型的Case结构,往往意味着多态策略不足,需要使用继承树多态机制消除复杂控制结构。 重复的代码,往往意味着隐式耦合,需要将重复代码提取为独立方法。一个类过多使用其他类的属性,往往意味着属性分配不正确或者协作设计不正 确,需要在类间转移属性或者使用方法委托代替属性访问。 过多的注释,往往意味着代码的逻辑结构不清晰或者可读性不好,需要进行逻辑结构重组或者代码重组。 测试驱动开发 测试驱动开发又被称为测试优先( Test First )的开发,随着极限编程方法的普遍应用而得到普及. 测试驱动开发要求程序员在编写一段代码之前，优先完成该段代码的测试代码。测试代码通常由测试工具自动装载执行,也可以由程序员手工执.行。完成测试代码之后,程序员再编写程序代码并在编程中重复执行测试代码,以验证程序代码的正确性。 结对编程 Two programmers working side-by-side, collaborating on the same design, algorithm, code or test One programmer, the driver, has control of the keyboard/mouse and actively implements the program The other programmer, the observer, continuously observes the work of the driver to identify tactical (syntactic, spelling, etc.) defects and also thinks strategically about the direction of the work On demand, the two programmers can brainstorm any challenging problem The two programmers periodically switch roles, they work together as equals to develop software 原理 Pair-Pressure Keep each other on task and focused Pair-Think Bring different prior experiences to the task Pair-Relaying Each, in turn, contributes to the best of their knowledge and ability Then, sit back and think while their partner fights on Pair-Reviews Continuous design and code reviews Ultimate in defect removal efficiency Removes programmers distaste for reviews 80% of all (solo) programmers don’t do them regularly or at all Defect prevention always more efficient than defect removal Pair Debugging Talking about problem in a pair can lead to a solution becoming obvious Pair-Learning Continuous reviews -&gt; learn from partners techniques, knowledge of language, domain, etc. Construction Ideas A Decade of Advances in Software Construction Design has Been Raised a Level Programming has advanced through ability to create larger code aggregations Statements Routines Classes Packages Real legacy of OO might well be larger aggregations Daily Build and Smoke Test Institutionalizes incremental integration Minimizes serious integration problems that used to be common Lots of other benefits, too Standard Libraries Good programmers have always used libraries Now provided with languages (Java, C++, .NET) Visual Basic Visual programming innovation The first development environment to make widespread use of COTS components Only language to learn Ada’s syntax lessons (case statements, control statements, etc.) Highly integrated environment Open Source Software Great aid to programmers during development Reduced barriers to making code available Opportunity to learn from available code Improved ability to read code Nice “community” of programmers The Web, for Research FAQs Discussion groups Searchability in general Widespread Use of Incremental Development( 增量开发 ) Concepts were well known in 1990s Practice is well established in 2000s Test-First Development Shortens time to defect detection Increases personal discipline Complements daily build &amp; smoke test Refactoring as a Discipline Provides a discipline for making changes Not so good as a total design strategy Good example of incrementalism Faster Computers Implications for optimization Implications for programming languages Implications for development Ten Realities of Modern Software Construction “Construction” is a Legitimate Topic Individual Variation Is Significant Personal Discipline Matters Why Personal Discipline Matters Being realistic about predicting the future Areas where discipline matters Refactoring Prototyping Optimization Minimal-complexity designs specifically Managing complexity generally Endpoints—Discipline and Courage Humphrey on PSP Beck on Extreme Programming A Focus on Simplicity Works Better than a Focus on Complexity Focus on read-time convenience, not write-time convenience Defect-Cost Increase is Alive and Well Importance of Design Technology Waves Affect Construction Practices Incremental Approaches Work Best Perspective on Incrementalism The pure waterfall model is not at all incremental or iterative—which is why it hasn’t worked very well Spiral development is highly incremental and iterative, which is part of why it does work well All projects will experience iteration at some point Think about where and when in your project you will get your incrementalism—cheaply, or expensively? The Toolbox Metaphor Continues to be Illuminating Software’s Essential Tensions Some of the Worst Construction Ideas of 1990s and 2000s","categories":[{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://lyk-love.cn/categories/Software-Engineering/"}],"tags":[{"name":"Software Engineering Methodology","slug":"Software-Engineering-Methodology","permalink":"http://lyk-love.cn/tags/Software-Engineering-Methodology/"}]},{"title":"C++面向对象 Part2","slug":"C++面向对象-Part2","date":"2021-05-26T19:04:52.000Z","updated":"2022-09-26T06:39:34.925Z","comments":true,"path":"2021/05/27/C++面向对象-Part2/","link":"","permalink":"http://lyk-love.cn/2021/05/27/C++%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1-Part2/","excerpt":"Outline: 多态 操作符重载","text":"Outline: 多态 操作符重载 多态 同一论域中一个元素可有多种解释 提高语言灵活性 程序设计语言 一名多用 ---- 函数重载 类属 ---- template OO程序设计 ---- 虚函数 操作符重载 函数重载( 见下文&quot;函数匹配&quot; ) 名同， 参数不同 静态绑定 歧义控制 顺序 更好匹配 操作符重载 动机 操作符语义 built_in类型 自定义数据类型 作用 提高可读性 提高可扩展性 函数匹配 在大多数情况下我们都能确定某次调用应该选择哪个重载函数。然而当几个重载函数的形参数量相等，以及某些形参的类型可以由其他类型转换得来时，这项工作就不那么容易了。以下面这组函数及其调用为例： void f();void f(int);void f(int, int);void f(double, double=3.14);f(5.6); 确定候选函数和可行函数 函数匹配的第一步是选定本次调用对应的重载函数集，集合中的函数成为候选函数。候选函数具备两个特征：一是与被调用的函数同名，二是其声明在调用点可见。在这个例子中，有4个名为f的候选函数。 第二步考察本次调用提供的实参，然后从候选函数中选出能被这组实参调用的函数，这些新选出的函数称为可行函数。可行函数也有两个特征：一是其形参数量与本次调用提供的实参数量相等。二是每个实参的类型与对应的形参类型相同，或者能转换成形参的类型。 我们能根据实参的数量从候选函数中排除掉两个。不使用形参的函数和使用两个int形参的函数显然都不适合本次调用，这是因为我们的调用只提供了一个实参，而它们分别有0个和2个形参。 使用一个int形参的函数和使用两个double形参的函数是可行的，它们都能用一个实参调用。其中最后那个函数本应该接受两个double值，但是因为它含有一个默认实参，所以只用一个实参也能调用它。 在使用实参数量初步判别了候选函数后，接下来考察实参的类型是否与形参匹配。和一般的函数调用类似，实参与形参的含义可能是它们具有相同的类型，也可能是实参类型和形参类型满足转换规则。在上面的例子中，剩下的两个函数都是可行的： f(int)是可行的，因为实参类型double能转换成形参类型int f(double,double)是可行的，因为它的第二个形参提供了默认值，而第一个形参的类型正好是double，与函数使用的实参类型完全一致。 寻找最佳匹配（如果有的话） 函数匹配的第三步是从可行函数中选择与本次调用最匹配的函数。在这一过程中，逐一检查函数调用提供的实参，寻找形参类型与实参类型最匹配的那个可行函数。最匹配的基本思想是，实参类型与形参类型越接近，它们匹配得越好。在我们的例子中，调用只提供了一个实参，它的类型是double。如果调用f(int)，实参将不得不从double转换成int。另一个可行函数f(double,double)则与实参精确匹配。精确匹配比需要类型转换的匹配更好。因此，编译器把f(6.5)解析成对含有两个double形参的函数的调用，并使用默认值填补我们未提供的第二个实参。 含有多个形参的函数匹配 当实参的数量有两个或更多时，函数匹配就比较复杂了。对于前面那些名为f的函数，我们来分析如下的调用会发生什么情况： ​ f(42, 2.56); 选择可行函数的方法和只有一个实参的一样，编译器选择那些形参数量满足要求且实参类型和形参类型能够匹配的函数。此例中，可行函数包括f(int, int)和f(double, double)。接下来，编译器依次检查每个实参以确定哪个函数是最佳匹配。如果有且只有一个函数满足下列条件，则匹配成功： 该函数每个实参的匹配都不劣于其他可行函数需要的匹配 只有一个实参的匹配优于其他可行函数提供的匹配 如果在检测了所以实参之后没有任何一个函数脱颖而出，则该调用是错误的。编译器将报告二义性调用的信息。 在上面的调用中，只考虑第一个实参时我们发现函数f(int, int)能精确匹配；要想匹配第二个函数，int类型必须转换成double类型。显然需要内置类型转换的匹配劣于 精确匹配，因此仅就第一个实参来说，f(int, int)比f(double, double)更好。 接下来考虑第二个实参，此时f(double, double)是精确匹配，要想调用f(int, int)必须将2.56从double类型转换成int类型，因此仅就第二个实参来说，f(double, double)更好。 编译器最终将因为这个调用具有二义性而拒绝其请求：因为每个函数函数各自在一个实参上实现了更好的匹配，从整体上无法匹配孰优孰劣。看起来我们似乎可以通过强制类型转换其中的一个实参来实参函数的匹配，但是在设计良好的系统中，不应该对实参进行强制类型转换。 实参类型转换 为了确定最佳匹配，编译器将实参类型到形参类型的转换划分成几个等级，具体排序如下所示： 1.精确匹配，包括以下情况： 实参类型和形参类型相同 实参从数组类型或函数类型转换成对应的指针类型 向实参添加顶层const或者从实参中删除顶层const 2.通过const转换实现的匹配 3.通过类型提升实现的匹配 4.通过算术类型转换或指针转换实现的匹配 5.通过类类型转换实现的匹配 需要类型提升和算术类型转换的匹配 内置类型的提升和转换可能在函数匹配时产生意想不到的结果，幸运的是，在设计良好的系统中函数很少会含有下面例子类似的形参。 分析函数前，我们应该知道小整型一般都会提升到int类型或更大的整数类型。假设有两个函数，一个接受int，另一个接受short，则只有当调用的是short类型的值时才会选择short版本的函数。有时候，即使实参是一个很小的整数值，也会直接将它提升成int类型；此时使用short版本反而会导致类型转换： void ff(int);void ff(short);ff(&#x27;a&#x27;); //char提升成int，调用ff(int) 所有的算术类型转换的级别都一样。例如，从int向unsigned int的转换并不比int向double的转换级别高： void mainip(long);void mainip(float);mainip(3.14); //错误，二义性调用 字面值3.14的类型是double，它既能转换成long也能转换成float。因为存在两种可能的算术类型转换，所以该调用具有二义性。 函数匹配和const实参 如果重载函数的区别在于它们的引用类型的形参是否引用了const，或者指针类型的形参是否指向const，则当调用发生时编译器通过实参是否是常量来决定选择哪个函数： Record lookup(Account &amp;); //函数的参数是Account的引用Record lookup(const Account &amp;); //函数的参数是一个常量引用const Account a;Account b;lookup(a); //调用Record lookup(const Account &amp;);lookup(b); //调用Record lookup(Account &amp;); 在第一个调用中，我们传入的是const对象a。因为不能把普通引用绑定到const对象上，所以此例中唯一可行的函数是以常量引用作为形参的那个函数，并且调用该函数与实参a精确匹配。 在第二个调用中，我们传入的是非常量对象b。对于这个调用来说，两个函数都是可行的，因为我们既可以使用b初始化常量引用也可以用它初始化非常量引用。然而，用非常量对象初始化常量引用需要类型转换，接受非常量形参的版本则与b精确匹配。因此，应该选用非常量版本的函数。 指针类型的形参也类似，如果两个函数的唯一区别是它的指针形参指向常量或非常量，则编译器能通过实参是否是常量决定选用哪个函数：如果实参是指向常量的指针，调用形参是const*的函数；如果实参是指向非常量的值，调用形参是普通指针函数。 操作符重载 可以作为成员函数重载 也可以作为全局函数重载 重载时,必须至少包含一个用户自定义类型的操作数( 否则你就会更改编译器定义的操作符语义,这是不允许的 ) 不可重载的操作符 . .* :: ?: 基本原则 方式 类成员函数 带有类参数的全局函数 遵循原有语法 单目/双目 优先级 结合性 笨蛋写法: class Complex&#123; double real, imag; public: Complex() &#123; real = 0 ; imag = 0; &#125; Complex( double r , double i ) &#123; real = r; imag = i; &#125; Complex add( Complex &amp;x );&#125;;int main()&#123; Complex a(1,2),b(2,4),c; c = a.add(b);&#125; 作为成员函数重载 将运算符重载函数声明为类的成员函数时，二元运算符的参数只有一个，一元运算符不需要参数。之所以少一个参数，是因为这个参数是隐含的。 一般是内联函数 class Complex&#123; double real, imag; public: Complex() &#123; real = 0 ; imag = 0; &#125; Complex( double r , double i ) &#123; real = r; imag = i; &#125; Complex operator+ ( Complex &amp;x) &#123; Complex temp; temp.real = real + x.real; temp.real = imag + x.imag; return temp; &#125; //重载一元负号，不需要参数 complex operator -() &#123; return complex(-real, -image); &#125;&#125;;int main()&#123; Complex a(1,2),b(2,4),c; c = a + b; //作为成员函数重载时, 左式会被转换为 c = a.operator+(b);&#125; 作为全局函数重载 将运算符重载函数声明为全局函数时，二元操作符就需要两个参数，一元操作符需要一个参数( 因为没有this ) . 而且其中必须有一个参数是自定义类型，好让编译器区分这是程序员自定义的运算符，防止程序员修改用于内置类型的运算符的性质。 class Complex&#123; double real, imag; public: Complex() &#123; real = 0 ; imag = 0; &#125; Complex( double r , double i ) &#123; real = r; imag = i; &#125; friend Complex operator+ ( Complex &amp;x , Complex &amp;y ); //友元函数&#125;;Complex operator+ ( Complex &amp;x , Complex &amp;y )&#123; Complex temp; temp.real = x.real + y.real; temp.imag = x.imag + y.imag; return temp;&#125;int main()&#123; Complex a(1,2),b(2,4),c; c = a + b; //作为全局函数重载时,会被转换为 +(a,b)&#125; &lt;&lt; #include &lt;iostream&gt;using namespace std;class Point&#123;public: Point()&#123;&#125;; Point (int x, int y): x(x),y(y) &#123;&#125;; friend ostream &amp;operator&lt;&lt;(ostream &amp;out , const Point &amp;a); private: int x,y;&#125;;ostream &amp;operator&lt;&lt;(ostream &amp;out , const Point &amp;a)&#123; out &lt;&lt; &quot;&lt;Point&gt;( &quot; &lt;&lt; a.x &lt;&lt; &quot;, &quot; &lt;&lt; a.y &lt;&lt; &quot;)&quot;; return out;&#125;int main() &#123; Point a(2,4),b(5,3); Point c = a + b; cout &lt;&lt; c &lt;&lt; endl; //转换为 &#125; &lt;&lt;只能作为全局函数重载, 因为如果作为成员函数重载, 那么它的第一个参数必定是this,这显然是不合理的, &lt;&lt;的第一个参数应该是cout. 比如说, 对于cout &lt;&lt; c ,如果&lt;&lt;作为成员函数重载 , 那么应该会写成c &lt;&lt; cout 这种形式,这是很离谱的. &gt;&gt;同理, &lt;&lt;和&gt;&gt;都只能作为全局函数重载 注： 模板的&lt;&lt;重载， 只要加上泛型参数就行了 双目操作符重载 类成员函数 格式 &lt;ret type&gt; operator#( &lt;arg&gt; ) this隐含 使用 &lt;class name&gt; a, b;a # b;a.operator#(b); //等价 全局函数 友元friend &lt;ret type&gt; operator#( &lt;arg1&gt;, &lt;arg2&gt;) 格式&lt;ret type&gt; operator#( &lt;arg1&gt;, &lt;arg2&gt;) 推荐所有的双目操作符都以全局函数重载, 因为可以实现交换律: 通常只将那些不需要实现交换律的二元运算符重载为成员函数. 比如 赋值和复合赋值运算符=,+= , -= , *=, /=, &amp;= . 因为其左操作数( 成为宿主对象 ) 和右操作数( 成为参数对象 ) 的作用不相同, 在这些操作符中, 左操作数扮演的角色与右操作数不同. 左操作数代表左值, 右操作数代表右值, 操作的返回值一般为产生副作用后的宿主对象的值. class CL&#123; int count; public friend CL operator+ ( int i , CL &amp; a ); public friend CL operator+ ( CL &amp; a , int i );&#125;;//可以实现交换律obj + 10;10 + obj; 限制 = () [] 不能作为全局函数重载,只能作为成员函数重载 原因: 对于() [], 它们都需要先计算出左边对象的类型,再计算函数. 为了确保这件事,编译器规定这两个操作符只能作为成员函数重载( this总是优先被计算出 ) 对于= 由于以拷贝的方式初始化一个对象时，会调用拷贝构造函数；当给一个对象赋值时，会调用重载过的赋值运算符。因此=不能作为成员函数重载 永远不要重载&amp;&amp;和|| , 重载后无法实现短路 过度设计 = 错误设计 class Rational&#123; public: Rational(int,int); const Ration&amp; operator*(const Rational &amp;r) const; private: int n , d;&#125;;//operator* 的函数体// 尝试1,这是一个栈区的构造函数,错在不能返回一个局部变量的引用或指针return Rational( n* (r.n), d* ( r.d ));//尝试2,这样是错的,因为对于 w = a*b*c ; a*b的返回的对象是个匿名对象,再也找不到了,会造成内存泄漏. 也就是说,不能支持链式调用Rational *result = new Rational( n*(r.n), d*(r.d) );return *result; //尝试3,用一个类静态变量,实现支持链式调用,这样也是错的,因为//if( (a*b) == (c*d) ) 对于这种写法来说是永真式,返回的永远是同一个static Rational result; //设置一个类静态变量result.n = n*(r.n);result.d = d*(r.d);return result; 可以看到,对于const Ration&amp; operator*(const Rational &amp;r) const;这种写法,实际上是错误的, 所以操作符重载的标准写法一直都是Ration operator*(const Rational &amp;r) , 结尾的const没什么意义,所以也省略了. a++ 和 ++a 后缀运算符返回的是右值, 前缀运算符返回的是左值( 返回的就是a, prefix ++等价于a = a+1 ) 为区别前置和后置运算符，C++编译器要求，需要在后缀运算符重载函数中加参数int，这个类型在此除了以示区别之外并不代表任何实际含义, 被称为哑整型参数dummy interger parameter；如果不加，编译器无法区分是前置++，还是后置++，导致报错。 class Counter&#123; int value; public: Counter() &#123;value = 0 &#125; //前置++运算符，需要引用返回，不需要参数。返回自增后的值，且返回的是一个左值 Counter&amp; operator++() //++a &#123; value++; return *this; &#125; //后置++，不需要引用返回，需要参数区分。返回自增前的值，且返回的是一个右值 Counter operator ++(int) //a++ //dummy argument &#123; Counter tmp = *this; value++; return tmp; &#125; &#125;; 前缀运算符 返回更改后的宿主对象( 宿主对象不能是常量 ), 这意味着我们不应该创建新对象, 而是应该更改宿主对象, 并返回宿主对象, 这意味着我们可以通过引用返回, C++允许级联这个运算符( ++++x 和 ----x ), 这意味着返回对象不能是常量, 必须是左值 后缀运算符 需要哑元参数, 哑元参数的作用是创建一个唯一的前面来区分前缀和后缀运算符, 这在程序中被忽略 返回对象是在更改前创建的临时对象,这意味着我们不能通过引用返回该对象. 返回的对象是常量( 右值 ) , 因为C++ 不允许级联操作, 例如 a++-- 或 a---- = 当我们没有自己设计等号运算符的重载函数，编译器会自动生成一个浅拷贝的赋值运算符的重载函数。 浅拷贝：只是简单地将一个对象的内存数据赋值给另一个对象，如果这个对象成员变量引用了外部资源时（new），那么这两个对象的成员变量都指向这个空间，当这两个对象生存周期结束时，进行析构，那么就会崩溃，对同一块内存我们delete了两次 逐个成员赋值( member-wise assignment ) 对含有对象成员的类, 该定义是递归的 赋值操作符重载不能被继承 对于如下代码,存在许多问题 class A&#123; int x,y; char *p; public: A( int i , int j , char *s ) :x(i),y(j) &#123; p = new char[ strlen(s) + 1 ]; strcpy(p,s); &#125; virtual ~A() &#123; delete[] p; &#125; A&amp; operator =( A &amp;a ) &#123; x = a.x; y = a.y; delete[] p; p = new char[ strlen(a.p) + 1 ]; strcpy( p, a.p ); return *this; &#125;&#125;; 问题一,对于: A a,b;b = a; 由于new会抛异常,当new发生错误的时候, 按理来说b是不变的, 但现在b的成员指针成为了空指针. 问题二, 无法支持自我赋值s = s, 这样会报错( 对于 strcpy( p, a.p ), p 和a.p都被删了 ) 安全的写法是: char *p2 = new char[ strlen(a.p) + 1 ];strcpy(p2,a.p);delete p;p = p2;return *this 赋值是一种不对称操作, 左操作数是一个接受操作副作用的佐治对象, 右操作数是一个不应该再处理过程中更改的右值对象. 必须指出, 要使用此操作符, 左操作数和右操作数必须已经存在. 换言之, 此运算符不同于拷贝构造函数, 后者从现有对象创建新对象. 对于赋值运算符, 两个对象都必须存在. 我们只更改左对象, 使右对象是右对象的精确副本. 重载辅助运算符需要验证宿主对象和参数对象不是同一个对象(地址不同). 如果对象是在堆中创建的, 这一点尤其重要. 由于在复制参数对象之前必须删除宿主对象, 如果两个对象相同, 则参数对象( 与宿主对象的物理地址相同 ) 也被删除了. 从而没有了要复制的内容. 赋值运算符是右向左结合的. 换言之,有 z = y = x, 这被解释为 z = ( y = x ) . 但是,C++要求z被看作对y的引用, 这就是返回的对象必须通过引用返回的原因. A &amp;A:: operator =( const A &amp;right )&#123; if( *this != right ) //检查二者是不是同一个对象 &#123; number = right.number; // number是A的成员 &#125; return *this;&#125; [] class string &#123; char *p; public: string( char *p1 ) &#123; p = new char[ strlen(p1) + 1 ]; strcpy( p, p1 ); &#125; //更改器函数 char&amp; operator [](int i) &#123; return p[i]; &#125; //访问器函数 const char operator [](int i) const // const string * const this &#123; return p[i]; &#125; virtual ~string()&#123; delete[] p; &#125; &#125;;//匹配 char&amp; operator [](int i)string s( &quot;aacd&quot; );//匹配 const char operator [](int i) constconst string cs(&quot;const&quot;);s[2] = &#x27;b&#x27;;cout &lt;&lt; cs[0]; 在下标操作符的重载中,由于不知道用户会调用哪个版本,所以应当始终重载两个版本 由于函数末尾const的作用实际上是改变参数列表( 将string * const this 更改为const string * const this),所以编译器函数匹配时会精准匹配 多维数组 class Array2D&#123;public: class Arr1D &#123; public: Array1D( int *p ) &#123; this -&gt; p = p; &#125; int&amp; operator[](int index) const &#123; return p[index]; &#125; private: int *p; &#125;; Array2D( int n1, int n2 ) &#123; p = new int[n1 * n2] ; num1 = n1; num2 = n2; &#125; virtual ~Array2D() &#123; delete[] p; &#125; Array1D operator[] ( int index ) &#123; return p+ index * num2; &#125; //隐式转换, 将 int* 转换为 Array1D const Array1D operator[] ( int index ) const &#123;return p + index * num2; &#125; private: int *p; int num1, num2;&#125;; 如果构造函数只有一个参数, 那这个参数就可以用来隐式转换. 前提是构造函数前面没有explicit关键字 ,如果加了, 就代表这个构造函数只能显式调用, 不能隐式转换. () ()有三个用途: 优先级声明, 函数调用, 类型转换. 后两者都可以重载 &amp; A* operator&amp;() &#123; return this; &#125;const A* operator&amp;() const&#123; return this; &#125; 函数调用操作符 实现了函数对象, 相当于更高级的函数指针. 函数对象可以保存状态 class Function&#123; double para; int lowerBound, upperBound; public: double operator() ( double , int, int );&#125;;Func f;f( 2.4, 0 ,8 ) //函数对象 类型转换运算符 减少混合计算中需要定义的操作符重载函数的数量 重载数值型，如int class Rational&#123; public： Rational( int n1, int n2 ) &#123; n = n1; d = n2; &#125;//什么都不返回，没有返回值//只能转换对象本身，所以不需要参数。因为只能对调用它的对象做类型转换，//只能作为成员函数 operator double() &#123; return (double) (n/d); &#125; private: int n,d;&#125;; -&gt; -&gt;智能指针 -&gt;为二元运算符 , 重载的时候按一元操作符重载描述, 编译器会在描述后对它重写. A a;A -&gt; f();a.operator-&gt;( f ); // ?????//编译器会将它改写为a.operator -&gt;() -&gt;f(); 首先, 按照一元操作符描述, a.operator-&gt;( f ); 会被描述为``a.operator -&gt;(). 实际上a.operator -&gt;() 返回的是a的指针, 然后编译器将其改写, 再加上不重载的 -&gt; f()` 实际上a.operator -&gt;()可以返回一个也重载了-&gt;的对象, 然后-&gt; f()再进行重载..... 你可以这么写, 但是非常蠢 new 、delete new, delete 频繁调用系统的存储管理, 影响效率( 实际上分配内存降低了系统的很大效率 ) 程序自身管理内存, 提高效率 方法 调用系统存储分配, 申请一块较大的内存 针对该内存, 自己管理存储分配, 去配 通过重载new与delete来实现 重载的new和delete是静态成员. 由于它们肯定是静态的, 所以可加可不加static关键字 重载的new和delete遵循类的访问控制,可继承 重载new void *operator new( size_t size, ...) 名: operator new 返回类型: void* 第一个参数: size_t ( unsigned int ) 系统自动计算对象的大小, 并传值给size 其他参数: 可有可无 A *p = new(...) A, 表示传给new的其他实参 new的重载可以有多个 如果重载了new, 那么通过new动态创建该类的对象时将不再调用内置的(预定义的)new 调用全局new: :: operator new 定位new: 在栈上new出一块空间, 这样可以复用栈区地址 重载 delete void operator delete( void*p, size_t size ) 名: operator delete 返回类型: void 第一个参数: void * 被撤销对象的地址 第二个参数: 可有可无; 如果有,则必须是 size _t类型 被撤销对象的实际大小 delete的重载只能有一个 如果重载了delete,那么通过delete撤销对象时将不再调用内置的(预定义的delete)","categories":[{"name":"Language","slug":"Language","permalink":"http://lyk-love.cn/categories/Language/"}],"tags":[{"name":"C++","slug":"C","permalink":"http://lyk-love.cn/tags/C/"}]},{"title":"尼金斯基手记","slug":"尼金斯基手记","date":"2021-05-18T09:50:29.000Z","updated":"2022-09-26T06:39:34.943Z","comments":true,"path":"2021/05/18/尼金斯基手记/","link":"","permalink":"http://lyk-love.cn/2021/05/18/%E5%B0%BC%E9%87%91%E6%96%AF%E5%9F%BA%E6%89%8B%E8%AE%B0/","excerpt":"《尼金斯基手记》是现代芭蕾舞的开创者尼金斯基在精神上脱离社会的束缚，但心中还残存有一丝理性时的 作品。文中记述了他悲惨的童年、辉煌的艺术生涯、家人的隔离疏远和对生活的焦虑。书中充满了作者的幻想、记忆和喃喃自语，如同一个装疯的&quot;哈姆雷特&quot;，透露了这个世界的真相。由于社会和时代道德的束缚，该书几经波折，直到20世纪末才全部得见天日。","text":"《尼金斯基手记》是现代芭蕾舞的开创者尼金斯基在精神上脱离社会的束缚，但心中还残存有一丝理性时的 作品。文中记述了他悲惨的童年、辉煌的艺术生涯、家人的隔离疏远和对生活的焦虑。书中充满了作者的幻想、记忆和喃喃自语，如同一个装疯的&quot;哈姆雷特&quot;，透露了这个世界的真相。由于社会和时代道德的束缚，该书几经波折，直到20世纪末才全部得见天日。 据一些回忆文章说，尼金斯基很早就有情绪上的严重问题。大约在1918至1919年的冬天，他听从医生的嘱咐举家到瑞士休养。期间，他写下秘不示人的四册笔记，并将它们分别定名为情感、生命、死亡和信件。也正是在这期间他的精神失常现象日趋严重，甚至使家人受到惊吓。但这并未阻碍他的创作，他不断构想出许多新的舞蹈，并画了大量素描作品。 正如他的素描大多以圆为主题，对他有深入研究的精神病医生认为，这是尼金斯基&quot;在面临威胁他存在的分裂力量时努力保持自我平衡的一种企图&quot;。的确，《手记》中也可以看出他为了与人群趋同所付出的努力。 尼金斯基的一生可以贯穿在情感的无处安放。他的手记事实上是一部忏悔录。这部忏悔录应当和奥古斯丁的《忏悔录》齐名，应该提倡所有专业人士给予关注。奥古斯丁年轻时所有的失败和激情的降落，在尼金斯基身上有类似的痕迹。人不完美，人无法完善，人需要救赎。尼金斯基除了在人的世界的跌宕，还有对神的世界的追逐。他寻求爱，只是在寻求的中途屈服于现&quot;《尼金斯基手记》三部曲&quot;情感&quot;&quot;生命&quot;&quot;死亡&quot;，是一次艰难的精神忏悔录。他一次次将自己剖析，一次次依靠上帝赋予的残存理性进行自我防卫。这也是一次优美而动人的亲吻旅程。他的爱情是上帝。从&quot;情感&quot;部分对人的畸形的爱，到&quot;死亡&quot;部分对人公义和仁慈的爱，上帝是完善尼金斯基自我塑造的终极链接。在完善的过程当中，尼金斯基进行了一次艰难的蜕变，对世事本相也有了本质的洞穿。 所谓的&quot;疯狂&quot;，构成了尼金斯基的一种暴力倾向，这仅在法译本前言里有所提及，在他的文字里却不着痕迹。从他的文字来看，&quot;疯狂&quot;带给他的，只是一种呓语般的行文风格:短促，像即将窒息;重复，像喃喃低语;有的时候条理清楚，有的时候语无伦次--都是他思维状态的投射;只是一种对&quot;神&quot;的遵从:排除病态的幻觉因素，这可以被看作是对情感和直觉的诉求，他鄙视&quot;聪明&quot;(理智)，认为&quot;聪明&quot;是一种缺陷，会阻碍人的真正的理解力;此外，还有一种&quot;隔&quot;:他能清楚地意识到人们看他时的眼光，可是他仍然爱，他呻吟般说着他爱妻子，爱女儿，爱这个世界;可这种爱终究只是一种玻璃屋中的爱，他能看到屋外人的形色，洞若观火，并且他充沛地爱着，然而这爱只是硬邦邦地撞在玻璃围墙上。 精彩句子摘录: &quot;我不喜欢娱乐，我了解什么是娱乐，我并不愉快，因为我知道欢愉就是死亡，心智的死亡，我害怕死亡，所以我热爱生命。 &quot; &quot;他的聪明头脑破坏了他的情感。&quot; &quot;我不喜欢过去了的时代，因为我活在今天。&quot; &quot;我不想轻举妄动，我不想变得愚蠢，因为对我来说那是死亡。&quot; &quot;我是个有情感的丑陋的人，所以我跳驼子和畸形的热的舞。我是个懂得形式和美的艺术家。美不是相对的。美是神。神在情感和美之中。美在情感之中。我喜欢美，因为我深刻地感觉过美，我了解它。那些又思想的人关于美所写的一切都很愚蠢。人不必讨论美，也不必批评。美并不是批评，我也不是批评。批评是聪明的，我不需要聪明，因为我去创造美，我感受到美并感受到爱。&quot; &quot;他们没有对人的情感，他们只要钱。钱会让灵魂痛苦。&quot; &quot;小人物在寻找他们的幸福，有钱人却四处搜刮金钱，最后导致小人物只好往自己头上射一颗子弹。&quot; &quot;科学家说上帝不存在。但我认为上帝存在着。我感觉得到他，而不是去思考他。&quot; &quot;我喜欢犹太人，因为他们懂得畏惧。&quot; &quot;我寻求爱，但我发现爱并不存在。它是污秽的，大家要的爱是吹捧和恭维。&quot; &quot;我很怕人，他们感受不到我，但却了解我。我很怕人群--因为他们既没有感受到我也不能了解我，而且要我过和他们一样的生活。他们要我跳令人欢愉的舞蹈。我不喜欢欢愉，我只喜欢生命。 &quot; &quot;今天她(尼金斯基的妻子)在精神上用感觉爱我，我和想对她说，有一天我们要在精神上结婚，因为没有精神我无法爱人。&quot; &quot;我要问他们一个关于生命的问题，如果他们能够深深感受到我--我就得救了;如果不能，如果他们没有感觉到我，不了解我。我就是一个习惯不幸福的人，我会感到很痛苦。&quot;","categories":[{"name":"Art","slug":"Art","permalink":"http://lyk-love.cn/categories/Art/"}],"tags":[]},{"title":"尼金斯基给佳吉列夫的最后一封信","slug":"尼金斯基给佳吉列夫的最后一封信","date":"2021-05-18T09:49:11.000Z","updated":"2022-09-26T06:39:34.943Z","comments":true,"path":"2021/05/18/尼金斯基给佳吉列夫的最后一封信/","link":"","permalink":"http://lyk-love.cn/2021/05/18/%E5%B0%BC%E9%87%91%E6%96%AF%E5%9F%BA%E7%BB%99%E4%BD%B3%E5%90%89%E5%88%97%E5%A4%AB%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E5%B0%81%E4%BF%A1/","excerpt":"由传奇舞者尼金斯基寄给他的老板佳吉列夫, 这封信也许并未寄出","text":"由传奇舞者尼金斯基寄给他的老板佳吉列夫, 这封信也许并未寄出 给某人: 我不想叫出你的名字，因为人们都不想叫出你的名字。我不会在仓促之中给你写信，因为我不会让你以为我很紧张，我不是个容易紧张的人，我喜欢平静地写信，我喜欢写信，但我不喜欢写信时过于注重那些华美的文字修饰，我没学过如何去修饰文字，我只需要写出我心中的想法。我并不怕你，我知道你很讨厌我，但我爱你如同我爱其他每一个人。 我不想同你一起工作，我要告诉你一件事，我很努力地在工作，我并没死掉，我活得好好的，神在我身上活着，我也活在神的身上。神在我身上活着，我努力在舞蹈方面取得进步，我的舞蹈已经进步了很多。我努力地写作，但我不懂如何去使用华丽的词藻，我知道你一向喜欢华丽的词藻，而我可不喜欢;你喜欢结党营利，我厌恶这一套。我不是一个死的物，我是个活人;你才是一个死的物，因为你的意愿已经死了。我不会把你看成朋友，因为我知道你是我的敌人。但我不是你的敌人，敌人不属于神，神没有敌人。敌人在寻求死亡，而我则在寻求生命;我怀有很大的爱，而你则是恶毒的化身。我不是凶残的野兽，你才是。凶残的野兽不会懂得去爱人，而我爱人们，陀思妥耶夫斯基也爱人们。我不是一个白痴，我是一个真实的人，陀思妥耶夫斯基的&quot;白痴&quot;也是一个真实的人。我是白痴，陀思妥耶夫斯基也是白痴。你认为我很愚蠢，而我却看到你处在真正的愚蠢中，我们都认为对方很愚蠢。 我不要成群结党，我不喜欢成群结党，你喜欢别人在你面前俯首听命，我也曾经这样。你辱骂那些在你面前服从的人，我喜欢那些服从的人:我吸引他们，而你则是使他们畏惧。他们对你的服从是虚假的，对我的才是真的。我不要你冲我做出那种微笑，因为你的微笑是死的，我不是死亡，所以我从不微笑。我写东西并不是为了自我嘲弄，我为哭泣而写作。我是一个有情感和理性的人，你有聪明，却缺乏情感，你的情感很低劣，而我的情感很强壮。**你要我堕落，而我要拯救你:我爱你，你却不爱我;我希望你好，而你却要毁掉我。**我了解你的诡计，我假装很神经质，我装做我很愚蠢。我不是孩子，我是神，我是你身上的神。你是野兽，而我是爱，你已经不爱那些人，而我还爱他们，我爱每个人。我不要思想，不要听。我不属于你，你也并不属于我，我会永远爱你，我属于你，也属于我自己。我要和自己结合在一起。我属于你，我也属于我自己。 …… 我想写信给你，但不想同你一起工作，**因为你的意图不在工作中。**我知道你很懂得伪装，我不喜欢会伪装的人，我只在对人有益时才会接受伪装。你是个邪恶的人，你并不是沙皇，而我才是沙皇。你并不是我的沙皇，我才是你的沙皇。你要伤害我，而我不会伤害你。你很邪恶，而我是一支摇篮曲。睡吧，睡，睡，睡，睡，安心地睡，睡，睡。睡，睡。","categories":[{"name":"Art","slug":"Art","permalink":"http://lyk-love.cn/categories/Art/"}],"tags":[]},{"title":"Database Paradigms and BNCF","slug":"Database Paradigms and BNCF","date":"2021-05-14T18:37:20.000Z","updated":"2022-09-26T06:39:34.928Z","comments":true,"path":"2021/05/15/Database Paradigms and BNCF/","link":"","permalink":"http://lyk-love.cn/2021/05/15/Database%20Paradigms%20and%20BNCF/","excerpt":"Outline: 第一范式 第二范式 第三范式 部分依赖、完全依赖、传递依赖","text":"Outline: 第一范式 第二范式 第三范式 部分依赖、完全依赖、传递依赖 一、第一范式 数据库每一列都是不可分的基本数据项（原子数据项） 在任何一个关系数据库中，第一范式（1NF）是对关系模式的基本要求，不满足第一范式（1NF）的数据库就不是关系数据库。就比如说：下面一个数据表 编号 姓名 地址 001 李一 山东，青岛 002 李二 山东，济南 003 李三 山东，济宁 上表所示的地址，就不符合第一范式，因为地址那一列可以分为省份和市区，故可以修改为 编号 姓名 省份 市区 001 李一 山东 青岛 002 李二 山东 济南 003 李三 山东 济宁 ​ 二、第二范式 第二范式要求在满足第一范式的基础上，非主属性必须完全依赖于候选键，也就是要消除非主属性对任一候选键的部分依赖。（完全依赖，部分依赖在后文） 2NF的违例只会出现在候选键由超过一个字段构成的表中，因为对单关键字字段不存在部分依赖问题。 就比如说： 订单号 产品号 产品数量 产品价格 订单时间 订单金额 2001 001 3 8.5 20200224 25.5 2002 002 2 7.5 20200424 17 2002 003 2 6.5 20200424 17 2003 001 2 8.5 20200324 16 如上图加粗字体所示，可能对于同一个订单，含有不同的产品，因此主键必须是产品号和订单号联合组成。但可以发现产品数量、产品价格与订单号、产品号都有关，不过订单时间与订单金额仅与订单号有关，这就违反了第二范式。故可以修改为： 订单 产品号 产品数量 产品价格 2001 001 3 8.5 2002 002 2 7.5 2002 003 2 6.5 2003 001 2 8.5 订单号 订单时间 订单金额 2001 20200224 25.5 2002 20200424 17 2002 20200424 17 2003 20200324 16 三、第三范式 第三范式要求在满足第二范式的基础上，任何非主属性不依赖于其他非主属性，即在第二范式的基础上消除传递依赖。第三范式要求数据表的每一列都与主键直接相关，而不是间接相关。（不存在非关键字段对任一候选关键字段的传递依赖） 就比如说： 学号 姓名 性别 班主任姓名 班主任性别 班主任年龄 2001 李一 男 陈毅 男 35 2003 李二 男 陈毅 男 35 2004 李三 男 王玉 男 26 2005 李四 男 王玉 男 26 从上表可以看出，所有属性都完全依赖于学号，故符合第二范式，但是班主任性别和班主任年龄直接依赖于班主任姓名，而不是主键学号，故不符合第三范式。可修改为： 学号 姓名 性别 班主任姓名 2001 李一 男 2003 李二 男 陈毅 2004 李三 男 王玉 2005 李四 男 王玉 班主任姓名 班主任性别 班主任年龄 陈毅 男 35 陈毅 男 35 王玉 女 26 王玉 女 26 这样就满足第三范式了。 例子2： 表：(学号, 姓名, 年龄, 所在学院, 学院地点, 学院电话) 该表中候选字段只有“学号”，于是“学号”做主键。由于主键是单一属性，所以不存在非主属性对主键的部分函数依赖的问题，所以必然满足第二范式。但是存在如下传递依赖 (学号) → (所在学院) → (学院地点, 学院电话) 学院地点和学院电话传递依赖于学号，而学院地点和学院电话都是非关键字段，即表中出现了“某一非关键字段可以确定出其它非关键字段”的情况，于是违反了第三范式。 解决办法： 把原表分成两个表： 学生：(学号, 姓名, 年龄, 所在学院)； 学院：(学院, 地点, 电话)。 四、部分依赖、完全依赖、传递依赖 部分函数依赖：设X,Y是关系R的两个属性集合，存在X→Y，若X’是X的真子集，存在X’→Y，则称Y部分函数依赖于X。 比如说：C可以通过AB得到，并且C也可以仅通过A得到，仅通过B得到， 那么就说C部分依赖AB。 完全函数依赖：设X,Y是关系R的两个属性集合，X’是X的真子集，存在X→Y，但对每一个X’都有X’ !→Y，则称Y完全函数依赖于X。 比如说：C可以通过AB得到，并且C不可以仅通过A得到，也不可以仅通过B得到， 那么就说C完全依赖AB. 传递函数依赖：设X,Y,Z是关系R中互不相同的属性集合，存在X→Y(Y !→X),Y→Z，则称Z传递函数依赖于X。 比如说：B可以通过A得到，C可以通过B得到，那么就称C传递依","categories":[{"name":"Technology","slug":"Technology","permalink":"http://lyk-love.cn/categories/Technology/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://lyk-love.cn/tags/Database/"}]},{"title":"Database Candidate Key","slug":"Database Candidate Key","date":"2021-05-14T14:53:56.000Z","updated":"2022-09-26T06:39:34.928Z","comments":true,"path":"2021/05/14/Database Candidate Key/","link":"","permalink":"http://lyk-love.cn/2021/05/14/Database%20Candidate%20Key/","excerpt":"Outline: 候选键定义 求候选键步骤 示例","text":"Outline: 候选键定义 求候选键步骤 示例 候选键定义 首先来看候选键的定义：若关系中的某一属性组的值能唯一地标识一个元组，则称该属性组为候选键。 若W是候选键，则必须满足两个条件：W的闭包是U；W没有冗余。 设关系模式R中U=ABC.......等N个属性，U中的属性在FD中有四种范围： (1)左右出现; (2)只在左部出现; (3)只在右部出现; (4)不在左右出现; 求候选键步骤 按以下步骤求候选键： 1.只在FD右部出现的属性，不属于候选码; 2.只在FD左部出现的属性，一定存在于某候选码当中; 3.外部属性一定存在于任何候选码当中; 4.其他属性逐个与2,3的属性组合，求属性闭包，直至X的闭包等于U,若等于U,则X为候选码。 示例 例1：R&lt;U,F&gt;,U=(A,B,C,D,E,G),F={AB--&gt;C,CD--&gt;E,E--&gt;A.A--&gt;G},求候选码。 因G只在右边出现,所以G一定不属于候选码;而B,D只在左边出现,所以B,D一定属于候选码;BD的闭包还是BD,则对BD进行组合,除了G以外,BD可以跟A,C,E进行组合 先看ABD ABD本身自包ABD,而AB--&gt;C,CD--&gt;E,A--&gt;G,所以ABD的闭包为ABDCEG=U 再看BDC CD--&gt;E,E--&gt;A,A--&gt;G,BDC本身自包,所以BDC的闭包为BDCEAG=U 最后看BDE E--&gt;A,A--&gt;G,AB--&gt;C,BDE本身自包,所以BDE的闭包为BDEAGC=U 因为(ABD)、(BCD)、(BDE)的闭包都是ABCDEG所以本问题的候选码有3个分别是ABC、BCD和BDE 例2：R&lt;U,F&gt;,U=(A,B,C),F={AB--&gt;C,C--&gt;B},求候选码。 因为A只出现在左边，所以A一定是候选键。A的闭包还是A，则对A进行组合，可以和B,C进行组合。 首先看AB，AB本身自包AB，而AB--&gt;C，所以AB的闭包是ABC=U。 再看AC，AC本身自包AC，而C--&gt;B，所以AC的闭包是ABC=U。 因为AB,AC的闭包都是ABC，也就是U，所以候选键是AB，AC。","categories":[{"name":"Technology","slug":"Technology","permalink":"http://lyk-love.cn/categories/Technology/"}],"tags":[{"name":"Database","slug":"Database","permalink":"http://lyk-love.cn/tags/Database/"}]}],"categories":[{"name":"Computer Science","slug":"Computer-Science","permalink":"http://lyk-love.cn/categories/Computer-Science/"},{"name":"Toolkit","slug":"Toolkit","permalink":"http://lyk-love.cn/categories/Toolkit/"},{"name":"Technology","slug":"Technology","permalink":"http://lyk-love.cn/categories/Technology/"},{"name":"Art","slug":"Art","permalink":"http://lyk-love.cn/categories/Art/"},{"name":"Artificial Intelligence","slug":"Artificial-Intelligence","permalink":"http://lyk-love.cn/categories/Artificial-Intelligence/"},{"name":"Algorithm","slug":"Algorithm","permalink":"http://lyk-love.cn/categories/Algorithm/"},{"name":"Language","slug":"Language","permalink":"http://lyk-love.cn/categories/Language/"},{"name":"Software Engineering","slug":"Software-Engineering","permalink":"http://lyk-love.cn/categories/Software-Engineering/"},{"name":"Life","slug":"Life","permalink":"http://lyk-love.cn/categories/Life/"},{"name":"Frontend","slug":"Frontend","permalink":"http://lyk-love.cn/categories/Frontend/"},{"name":"Potpourri","slug":"Potpourri","permalink":"http://lyk-love.cn/categories/Potpourri/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://lyk-love.cn/categories/Machine-Learning/"},{"name":"Natural Science","slug":"Natural-Science","permalink":"http://lyk-love.cn/categories/Natural-Science/"},{"name":"Literature","slug":"Literature","permalink":"http://lyk-love.cn/categories/Literature/"},{"name":"Business","slug":"Business","permalink":"http://lyk-love.cn/categories/Business/"},{"name":"Physiology","slug":"Physiology","permalink":"http://lyk-love.cn/categories/Physiology/"},{"name":"History","slug":"History","permalink":"http://lyk-love.cn/categories/History/"},{"name":"Mathematics","slug":"Mathematics","permalink":"http://lyk-love.cn/categories/Mathematics/"},{"name":"Psychology","slug":"Psychology","permalink":"http://lyk-love.cn/categories/Psychology/"}],"tags":[{"name":"Computer Architecture","slug":"Computer-Architecture","permalink":"http://lyk-love.cn/tags/Computer-Architecture/"},{"name":"Block Chain","slug":"Block-Chain","permalink":"http://lyk-love.cn/tags/Block-Chain/"},{"name":"Movie","slug":"Movie","permalink":"http://lyk-love.cn/tags/Movie/"},{"name":"AI","slug":"AI","permalink":"http://lyk-love.cn/tags/AI/"},{"name":"Compilers","slug":"Compilers","permalink":"http://lyk-love.cn/tags/Compilers/"},{"name":"Art Theory","slug":"Art-Theory","permalink":"http://lyk-love.cn/tags/Art-Theory/"},{"name":"Linux","slug":"Linux","permalink":"http://lyk-love.cn/tags/Linux/"},{"name":"Algorithm design and analysis","slug":"Algorithm-design-and-analysis","permalink":"http://lyk-love.cn/tags/Algorithm-design-and-analysis/"},{"name":"shell","slug":"shell","permalink":"http://lyk-love.cn/tags/shell/"},{"name":"Python","slug":"Python","permalink":"http://lyk-love.cn/tags/Python/"},{"name":"Database","slug":"Database","permalink":"http://lyk-love.cn/tags/Database/"},{"name":"Music","slug":"Music","permalink":"http://lyk-love.cn/tags/Music/"},{"name":"Git","slug":"Git","permalink":"http://lyk-love.cn/tags/Git/"},{"name":"Computer Networking","slug":"Computer-Networking","permalink":"http://lyk-love.cn/tags/Computer-Networking/"},{"name":"C++","slug":"C","permalink":"http://lyk-love.cn/tags/C/"},{"name":"Software Testing","slug":"Software-Testing","permalink":"http://lyk-love.cn/tags/Software-Testing/"},{"name":"Cloud Computation","slug":"Cloud-Computation","permalink":"http://lyk-love.cn/tags/Cloud-Computation/"},{"name":"Editor","slug":"Editor","permalink":"http://lyk-love.cn/tags/Editor/"},{"name":"Compiler","slug":"Compiler","permalink":"http://lyk-love.cn/tags/Compiler/"},{"name":"Java","slug":"Java","permalink":"http://lyk-love.cn/tags/Java/"},{"name":"Docker","slug":"Docker","permalink":"http://lyk-love.cn/tags/Docker/"},{"name":"K8s","slug":"K8s","permalink":"http://lyk-love.cn/tags/K8s/"},{"name":"Shell","slug":"Shell","permalink":"http://lyk-love.cn/tags/Shell/"},{"name":"Terminal","slug":"Terminal","permalink":"http://lyk-love.cn/tags/Terminal/"},{"name":"OS Basic","slug":"OS-Basic","permalink":"http://lyk-love.cn/tags/OS-Basic/"},{"name":"Hardware","slug":"Hardware","permalink":"http://lyk-love.cn/tags/Hardware/"},{"name":"Network Security","slug":"Network-Security","permalink":"http://lyk-love.cn/tags/Network-Security/"},{"name":"Distributed System","slug":"Distributed-System","permalink":"http://lyk-love.cn/tags/Distributed-System/"},{"name":"Cryptology","slug":"Cryptology","permalink":"http://lyk-love.cn/tags/Cryptology/"},{"name":"HTML","slug":"HTML","permalink":"http://lyk-love.cn/tags/HTML/"},{"name":"JS","slug":"JS","permalink":"http://lyk-love.cn/tags/JS/"},{"name":"React","slug":"React","permalink":"http://lyk-love.cn/tags/React/"},{"name":"Empirical SE","slug":"Empirical-SE","permalink":"http://lyk-love.cn/tags/Empirical-SE/"},{"name":"CICD","slug":"CICD","permalink":"http://lyk-love.cn/tags/CICD/"},{"name":"Spring","slug":"Spring","permalink":"http://lyk-love.cn/tags/Spring/"},{"name":"Neuroscience","slug":"Neuroscience","permalink":"http://lyk-love.cn/tags/Neuroscience/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"http://lyk-love.cn/tags/Machine-Learning/"},{"name":"awk","slug":"awk","permalink":"http://lyk-love.cn/tags/awk/"},{"name":"Latin American literatures","slug":"Latin-American-literatures","permalink":"http://lyk-love.cn/tags/Latin-American-literatures/"},{"name":"需求与商业模式创新","slug":"需求与商业模式创新","permalink":"http://lyk-love.cn/tags/%E9%9C%80%E6%B1%82%E4%B8%8E%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F%E5%88%9B%E6%96%B0/"},{"name":"人体结构与疾病","slug":"人体结构与疾病","permalink":"http://lyk-love.cn/tags/%E4%BA%BA%E4%BD%93%E7%BB%93%E6%9E%84%E4%B8%8E%E7%96%BE%E7%97%85/"},{"name":"Chinese History","slug":"Chinese-History","permalink":"http://lyk-love.cn/tags/Chinese-History/"},{"name":"Optimization Methods","slug":"Optimization-Methods","permalink":"http://lyk-love.cn/tags/Optimization-Methods/"},{"name":"实验心理学","slug":"实验心理学","permalink":"http://lyk-love.cn/tags/%E5%AE%9E%E9%AA%8C%E5%BF%83%E7%90%86%E5%AD%A6/"},{"name":"Software Engineering Methodology","slug":"Software-Engineering-Methodology","permalink":"http://lyk-love.cn/tags/Software-Engineering-Methodology/"}]}